Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ For each explicit discourse connective (such as “but” or “so”), annotators identified the two text spans between which the relation holds and the sense of the relation.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ Only the Naive Bayes classification results are given, as space is limited and MaxEnt and AdaBoost gave slightly lower accuracies overall.
Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al, 2009a) and (Lin et al, 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses can reach 91.8% when we simply mapped the ground truth implicit connective of each test instance to its most frequent sense. $$$$$ We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses.

In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ Implicit discourse relations abound in text and readers easily recover the sense of such relations during semantic interpretation.
In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress.
In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, first-last words of arguments, cross-argument word pairs, ever used in (Pitler et al, 2009a), production rules of parse trees of arguments used in (Lin et al, 2009), and intra-argument word pairs inspired by the work of (Saito et al, 2006). $$$$$ Because of this, the recall of the classification should be the primary metric of success, while the Marcu and Echihabi (2001) experiments report only accuracy.

Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). $$$$$ Blair-Goldensohn et al. (2007) proposed several refinements of the word pair model.
Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). $$$$$ They delete the connective and use [Arg1, Arg2] as an example of an implicit relation.
Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). $$$$$ We have presented the first study that predicts implicit discourse relations in a realistic setting (distinguishing a relation of interest from all others, where the relations occur in their natural distributions).
Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al, 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). $$$$$ Consider the example shown below: The government says it has reached most isolated townships by now, but because roads are blocked, getting anything but basic food supplies to people remains difficult.

Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ In our experiments on implicits, the first and last words are not connectives.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ A sequence model also leads to over 1% absolute improvement for the task.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ Many of the useful features, syntax in particular, exploit the fact that both arguments of the connective are found in the same sentence.
Following the work of (Pitler et al, 2009a), we used sections 2-20 as training set, sections 21-22 as test set, and sections 0-1 as development set for parameter optimization. $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).

Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ Wordpairs-selected Here, only word pairs from Wordpairs-PDTBImpl with non-zero information gain on the TextRels corpus were retained.
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ Surprisingly, polarity was actually one of the worst classes of features for Comparison, achieving an f-score of 16.33 (in contrast to using the first, last and first three words of the sentences as features, which leads to an f-score of 21.01).
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations.
Here the numbers of training and test instances for Expansion relation are different from those in (Pitler et al, 2009a). $$$$$ We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”.

Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ Explicit relations are easy to identify.
Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ Each relation in the PDTB takes two arguments.
Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ This work was partially supported by NSF grants IIS-0803159, IIS-0705671 and IGERT 0504487.
Table 2 summarizes the best performance achieved by the baseline system in comparison with previous state-of-the-art performance achieved in (Pitler et al, 2009a). $$$$$ In addition, we revisit past approaches using lexical pairs from unannotated text as features, explain some of their shortcomings and propose modifications.

Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set. $$$$$ Experiments on artificial implicits Marcu and Echihabi (2001) proposed a method for cheap acquisition of training data for discourse relation sense prediction.
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set. $$$$$ The words “popular” and “oblivion” are almost antonyms, and one might hypothesize that their occurrence in the two text spans is what triggers the contrast relation between the sentences.
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set. $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.
Table 2: Performance comparison of the baseline system with the system of (Pitler et al, 2009a) on test set. $$$$$ However, since Expansion forms the largest class of relations, its f-score is still the highest overall.

 $$$$$ Wordpairs-PDTBExpl In this case, the model was formed by using the word pairs from the explicit relations in the sections of the PDTB used for training.
 $$$$$ To identify features useful for classifying comparison vs other relations, we chose a random sample of 5000 examples for Contrast and 5000 Other relations (2500 each of Cause and No-relation).
 $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).
 $$$$$ In Example (1) above, the annotators identified “because” as the most appropriate connective between the sentences, and then labeled the implicit discourse relation Contingency.

Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ Our best combination of features outperforms the baseline from data intensive approaches by 4% for comparison and 16% for contingency.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ So reporting results on explicit and implicit relations separately will allow for clearer tracking of progress.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ We examine the most informative word pair features and find that they are not the semantically-related pairs that researchers had hoped.
Although on Comparison relation there is only as light improvement (+1.07%), our two best systems both got around 10% improvements of f score over a state-of-the-art system in (Pitler et al, 2009a). $$$$$ The most general senses (comparison, contingency, temporal and expansion) can be disambiguated in explicit relations with 93% accuracy based solely on the discourse connective used to signal the relation (Pitler et al., 2008).

This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ We analyze word pair features used in prior work that were intended to capture such semantic oppositions.
This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.
This also encourages our future work on finding the most suitable connectives for implicit relation recognition. From this table, we found that, using only predicted implicit connectives achieved an comparable performance to (Pitler et al, 2009a), although it was still a bit lower than our best baseline. $$$$$ The data, from now on referred to as TextRels, has explicit contrast and causal relations which were extracted from the English Gigaword Corpus (Graff, 2003) which contains over four million newswire articles.

Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ Later work (Blair-Goldensohn et al., 2007; Sporleder and Lascarides, 2008) has discovered that the models learned do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data.
Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ Wellner et al. (2006) used the GraphBank (Wolf and Gibson, 2005), which contains 105 Associated Press and 30 Wall Street Journal articles annotated with discourse relations.
Since (Pitler et al, 2009a) used different selection of instances for Expansion sense, we cannot make a direct comparison. $$$$$ However, some implicits have been found to be related by connective-like expressions which often appear in the beginning of the second argument.

Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ We include a feature for the presence or absence of modals in Arg1 and Arg2, features for specific modal words, and their cross-products.
Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ The senses are arranged in a hierarchy, allowing for annotations as specific as Contingency.Cause.reason.
Specifically, the model for the Comparison relation achieves an f-score of 26.02% (5% over the previous work in (Pitler et al, 2009a)). $$$$$ However, they are still a useful source of information for discourse relation prediction.

Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ In a similar vein, Lapata and Lascarides (2004) used pairings of only verbs, nouns and adjectives for predicting which temporal connective is most suitable to express the relation between two given text spans.
Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ We present a series of experiments on auidentifying the sense of imrelations, i.e. relations that are not marked with a discourse connective such as “but” or “because”.
Furthermore, the models for Contingency and Temporal relation achieve 35.72% and 13.76% f-score respectively, which are comparable to the previous work in (Pitler et al, 2009a). $$$$$ Also unlike prior work, we separate the task from the easier task of explicit discourse prediction.

(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ In the PDTB, explicit and implicit relations are clearly distinguished, allowing us to concentrate solely on the implicit relations.
(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ Since both jump and rise are members of the Rise class, new jump examples can be classified using past rise examples.
(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ Our work is also informed by the long tradition of data intensive methods that rely on huge amounts of unannotated text rather than on manually tagged corpora (Marcu and Echihabi, 2001; Blair-Goldensohn et al., 2007).
(Pitler et al, 2009a) performed implicit relation classification on the second version of the PDTB. $$$$$ For the complete set of 10,000 examples, word pair features were computed.

Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ However, they are still a useful source of information for discourse relation prediction.
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ Given two text spans, previous work has used the cross-product of the words in the spans as features.
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ This makes sense, as Pitler et al. (2008) found that implicit contingencies are often found immediately following explicit comparisons.
Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al (2009). $$$$$ But automatic sense prediction for implicit relations is an outstanding challenge in discourse processing.

However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. $$$$$ Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. $$$$$ Context, in terms of paragraph boundaries and nearby explicit relations, also proves to be useful for the prediction of implicit discourse relations.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. $$$$$ The words in the two text spans of a relation are taken from the gold-standard annotations in the PDTB.
However, the approach taken by Pitler et al (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. $$$$$ The approach is tested using binary classification between relations on balanced data, a setting very different from that of any realistic application.

An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ In this resource, each sentiment word is annotated as positive, negative, both, or neutral.
An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ Their idea is to use unambiguous patterns such as [Arg1, but Arg2.] to create synthetic examples of implicit relations.
An analysis in (Pitler et al, 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. $$$$$ For example, a question-answering application that needs to identify causal relations (i.e. as in Girju (2003)), must not only differentiate causal relations from comparison relations, but also from expansions, temporal relations, and possibly no relation at all.

Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ We hypothesized that verb chunks that contain more words, such as “They [are allowed to proceed]” often contain rationales afterwards (signifying Contingency relations), while short verb phrases like “They proceed” might occur more often in Expansion or Temporal relations.
Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations, but then trained on manually annotated implicit relations.
Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ They delete the connective and use [Arg1, Arg2] as an example of an implicit relation.
Our word pair features outperform the previous formulation (represented by the results reported by (Pitler et al, 2009), but used by virtually all previous work on this task). $$$$$ But automatic sense prediction for implicit relations is an outstanding challenge in discourse processing.

 $$$$$ We use several linguistically informed features, including polarity tags, Levin verb classes, length of verb phrases, modality, context, and lexical features.
 $$$$$ We would like to thank Sasha Blair-Goldensohn for providing us with the TextRels data and for the insightful discussion in the early stages of our work.
 $$$$$ For all experiments, we used sections 2-20 of the PDTB for training and sections 21-22 for testing.

 $$$$$ The best use of word pair features is Wordpairsselected.
 $$$$$ The most beneficial application of such features is when they are selected from a large unannotated corpus of explicit relations, but then trained on manually annotated implicit relations.
 $$$$$ Each word’s polarity was assigned according to its entry in the Multi-perspective Question Answering Opinion Corpus (Wilson et al., 2005).
