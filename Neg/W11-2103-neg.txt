Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al, 2011). $$$$$ As in previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa.
Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al, 2011). $$$$$ We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.
Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al, 2011). $$$$$ As in previous editions of this workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from European languages into English, and vice versa.
Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al, 2011). $$$$$ The views and findings are the authors’ alone.

Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. $$$$$ One of the test sets contains the “raw” SMS messages as they were sent, and the other contains messages that were cleaned up by human post-editors.
Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. $$$$$ We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.
Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. $$$$$ HR0011-06-C-0022, the US National Science Foundation under grant IIS-0713448, and the CoSyne project FP7-ICT-4248531 funded by the European Commission.
Spearman's rank correlation coefficients on the document (system) level between all the metrics and the human ranking are computed on the English, French, Spanish, German and Czech texts generated by various translation systems in the frame work of the third (Callison-Burch et al, 2008), fourth (Callison-Burch et al, 2009), fifth (Callison Burch et al, 2010) and sixth (Callison-Burch et al, 2011) shared translation tasks. $$$$$ We measured the metrics’ segment-level scores with the human rankings using Kendall’s tau rank correlation coefficient.

And researchers have grown increasingly concerned that automatic metrics have a strong bias towards preferring statistical translation outputs; the NIST (2008, 2010), MATR (Gao et al, 2010) and WMT (Callison-Burch et al, 2011) evaluations held during the last five years have provided ample evidence that automatic metrics yield results that are inconsistent with human evaluations when comparing statistical, rule-based, and human outputs. $$$$$ This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.
And researchers have grown increasingly concerned that automatic metrics have a strong bias towards preferring statistical translation outputs; the NIST (2008, 2010), MATR (Gao et al, 2010) and WMT (Callison-Burch et al, 2011) evaluations held during the last five years have provided ample evidence that automatic metrics yield results that are inconsistent with human evaluations when comparing statistical, rule-based, and human outputs. $$$$$ We are dying of hunger Mwen se [FIRSTNAME][LASTNAME] mwen nan aken mwen se yon j`en ki ansent mwen te genyen yon paran ki tap ede li mouri p`otoprens, mwen pral akouye nan k`omansman feviye alongside the SMS data.
And researchers have grown increasingly concerned that automatic metrics have a strong bias towards preferring statistical translation outputs; the NIST (2008, 2010), MATR (Gao et al, 2010) and WMT (Callison-Burch et al, 2011) evaluations held during the last five years have provided ample evidence that automatic metrics yield results that are inconsistent with human evaluations when comparing statistical, rule-based, and human outputs. $$$$$ As with previous workshops, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.
And researchers have grown increasingly concerned that automatic metrics have a strong bias towards preferring statistical translation outputs; the NIST (2008, 2010), MATR (Gao et al, 2010) and WMT (Callison-Burch et al, 2011) evaluations held during the last five years have provided ample evidence that automatic metrics yield results that are inconsistent with human evaluations when comparing statistical, rule-based, and human outputs. $$$$$ Participants used their systems to translate two test sets consisting of 1,274 unseen Haitian Creole SMS messages.

Provided for the Sixth EMNLP Workshop on Statistical Machine Translation (Callison-Burch et al 2011) extracted from the annotated NPs in the Penn Tree bank 3.0 corpus. $$$$$ Mayi-a 125gd, avan sete 100gd.
Provided for the Sixth EMNLP Workshop on Statistical Machine Translation (Callison-Burch et al 2011) extracted from the annotated NPs in the Penn Tree bank 3.0 corpus. $$$$$ The overall K is 0.320, but it ranges from 0.264 for GermanEnglish, to 0.477 for Spanish-English.
Provided for the Sixth EMNLP Workshop on Statistical Machine Translation (Callison-Burch et al 2011) extracted from the annotated NPs in the Penn Tree bank 3.0 corpus. $$$$$ We also conducted a pilot ‘tunable metrics’ task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality.

These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). $$$$$ As for the system combination track (Table 9), the CMU-HEAFIELD-COMBO entry performed quite well, being a winner in seven out of eight language pairs.
These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). $$$$$ Thank you.
These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). $$$$$ The numbers in each of the tables’ cells indicate the percentage of times that the system in that column was judged to be better than the system in that row.
These systems have been dominating the area in the recent years (Callison-Burch et al, 2011). $$$$$ Continuing our practice from last year’s workshop, we separated the test set into a tuning set and a final held-out test set for system combinations.

2 Domain Source Target# Test Language Language sets Europarl (Koehn, 2005) Fr, De, Es En 4 En Fr, De, Es KFTT (Neubig, 2011) Jp, En En, Jp 2 EMEA (Tiedemann, 2009) Da, De En 4 News (Callison-Burch et al, 2011) Cz, En, Fr, De, Es Cz, En, Fr, De, Es 3Table 2 $$$$$ Munro (2010) describes how a text-message-based emergency reporting system was set up by a consortium of volunteer organizations named “Mission 4636” after a free SMS short code telephone number that they established.
2 Domain Source Target# Test Language Language sets Europarl (Koehn, 2005) Fr, De, Es En 4 En Fr, De, Es KFTT (Neubig, 2011) Jp, En En, Jp 2 EMEA (Tiedemann, 2009) Da, De En 4 News (Callison-Burch et al, 2011) Cz, En, Fr, De, Es Cz, En, Fr, De, Es 3Table 2 $$$$$ Let us focus on inter-annotator agreement rates in the individual track (excluding reference comparisons), in the top right portion of Table 49.
2 Domain Source Target# Test Language Language sets Europarl (Koehn, 2005) Fr, De, Es En 4 En Fr, De, Es KFTT (Neubig, 2011) Jp, En En, Jp 2 EMEA (Tiedemann, 2009) Da, De En 4 News (Callison-Burch et al, 2011) Cz, En, Fr, De, Es Cz, En, Fr, De, Es 3Table 2 $$$$$ Because there were so many systems and data conditions the significance of each pairwise comparison needs to be quantified.
2 Domain Source Target# Test Language Language sets Europarl (Koehn, 2005) Fr, De, Es En 4 En Fr, De, Es KFTT (Neubig, 2011) Jp, En En, Jp 2 EMEA (Tiedemann, 2009) Da, De En 4 News (Callison-Burch et al, 2011) Cz, En, Fr, De, Es Cz, En, Fr, De, Es 3Table 2 $$$$$ participants in the system combination shared task.

(Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks. $$$$$ For English-German the rule-based MT systems performed well.
(Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks. $$$$$ HR0011-06-C-0022, the US National Science Foundation under grant IIS-0713448, and the CoSyne project FP7-ICT-4248531 funded by the European Commission.
(Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks. $$$$$ This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.
(Callison-Burch et al, 2011) The workshop's human evaluation component has been gradually refined over several years, and as a consequence it has produced a fantastic collection of publicly available data consisting primarily of pairwise judgements of translation systems made by human assessors across a wide variety of languages and tasks. $$$$$ We applied the Sign Test to measure which comparisons indicate genuine differences (rather than differences that are attributable to chance).

The tables in the Appendix of Callison-Burch et al (2011) report p-values of up to 1%, computed for every pairwise comparison in the dataset. $$$$$ This work was supported in parts by the EuroMatrixPlus project funded by the European Commission (7th Framework Programme), the GALE program of the US Defense Advanced Research Projects Agency, Contract No.
The tables in the Appendix of Callison-Burch et al (2011) report p-values of up to 1%, computed for every pairwise comparison in the dataset. $$$$$ This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.
The tables in the Appendix of Callison-Burch et al (2011) report p-values of up to 1%, computed for every pairwise comparison in the dataset. $$$$$ HR0011-06-C-0022, the US National Science Foundation under grant IIS-0713448, and the CoSyne project FP7-ICT-4248531 funded by the European Commission.

For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7). $$$$$ A big thank you to Ondˇrej Bojar, Simon Carter, Christian Federmann, Will Lewis, Rob Munro and Herv´e Saint-Amand, and to the shared task participants.
For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7). $$$$$ The numbers in each of the tables’ cells indicate the percentage of times that the system in that column was judged to be better than the system in that row.
For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7). $$$$$ This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.
For comparison, the WMT11 rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7). $$$$$ (Not all systems supported all language pairs.)

shared pilot task, this did not hold (Callison-Burch et al, 2011). $$$$$ I ask help for us, I await your response.
shared pilot task, this did not hold (Callison-Burch et al, 2011). $$$$$ All of the in-domain training data is in the raw format.
shared pilot task, this did not hold (Callison-Burch et al, 2011). $$$$$ There were a two additions to this year’s workshop that were not part of previous workshops: • Haitian Creole featured task – In addition to translation between European language pairs, we featured a new translation task: translating Haitian Creole SMS messages that were sent to an emergency response hotline in the immediate aftermath of the 2010 Haitian earthquake.
shared pilot task, this did not hold (Callison-Burch et al, 2011). $$$$$ • Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system.

To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al 2007) for the WMT 2011 German-English translation task (Callison-Burch et al 2011). $$$$$ We applied the Sign Test to measure which comparisons indicate genuine differences (rather than differences that are attributable to chance).
To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al 2007) for the WMT 2011 German-English translation task (Callison-Burch et al 2011). $$$$$ This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.
To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al 2007) for the WMT 2011 German-English translation task (Callison-Burch et al 2011). $$$$$ We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.
To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al 2007) for the WMT 2011 German-English translation task (Callison-Burch et al 2011). $$$$$ This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.

Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al, 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. $$$$$ The numbers in each of the tables’ cells indicate the percentage of times that the system in that column was judged to be better than the system in that row.
Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al, 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. $$$$$ What distinguishes those two language pairs from each other?
Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al, 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. $$$$$ We did not calculate correlations with the human judgments for the system combinations for the out of English direction, because none of them had more than 4 items.
Although the purely data-driven approaches achieve significant results as shown in the evaluation campaigns (Callison-Burch et al, 2011), according to the human evaluation, the final outputs of the SMT systems are still far from satisfactory. $$$$$ In addition to this data, participants in the featured task were allowed to use any of the data provided in the standard translation task, as well as linguistic tools such as taggers, parsers, or morphological analyzers.

The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al 2011). $$$$$ These text messages were sent by people in Haiti in the aftermath of the January 2010 earthquake.
The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al 2011). $$$$$ For the test and development sets the informants also edited the English translations.
The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al 2011). $$$$$ One of the test sets contains the “raw” SMS messages as they were sent, and the other contains messages that were cleaned up by human post-editors.
The goal of our experiments is to demonstrate the behaviour of the decoder and characterise its response to changes in the fundamental search parameters. The SMT models for our experiments were created with a subset of the training data for the English-French shared task at the WMT 2011 workshop (Callison-Burch et al 2011). $$$$$ This performance is carried over to the Haitian Creole task, where it again comes out on top (Table 10).

The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper. $$$$$ This year, we opted to reduce the number of pairwise comparisons with the hope that we would be more likely to find statistically significant differences between the systems in the same groups.
The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper. $$$$$ • Tunable metric shared task – We conducted a pilot of a new shared task to use evaluation metrics to tune the parameters of a machine translation system.
The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper. $$$$$ We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.
The example is taken from the parallel corpus of English and Haitian Kre`yol text messages used in the 2010 Shared Task for the Workshop on Machine Translation (Callison-Burch et al, 2011), which is the corpus used for evaluation in this paper. $$$$$ Rather, their entries were created by translating the test data via their web interfaces.4 The data used to construct these systems is not subject to the same constraints as the shared task participants.

The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al, 2011). $$$$$ Details of the data are given in Table 5.
The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al, 2011). $$$$$ NUS-TESLA-F goes from being a bottom entry to being a top entry, with CU-SEMPOS-BLEU also benefiting, changing from the middle to the top rank.
The Tunable Metrics task at WMT2011 concluded that BLEU is still the easiest to tune (Callison-Burch et al, 2011). $$$$$ As with previous workshops, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.

Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al, 2011). $$$$$ This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.
Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al, 2011). $$$$$ Here we aim to investigate the question of whether better metrics will result in better quality output when a system is optimized to them.
Inter-annotator agreement was computed using an adaptation of the kappa index with pairwise rank comparisons (Callison-Burch et al, 2011). $$$$$ We did not calculate correlations with the human judgments for the system combinations for the out of English direction, because none of them had more than 4 items.

(Callison-Burch et al, 2011). $$$$$ This is also evident from the fact that 38% of pairwise comparisons indicated a tie between the two systems, with the tie rate increasing to a full 47% when excluding comparisons involving the reference.
(Callison-Burch et al, 2011). $$$$$ As with previous workshops, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets form a valuable resource for research into statistical machine translation, system combination, and automatic evaluation of translation quality.
(Callison-Burch et al, 2011). $$$$$ HR0011-06-C-0022, the US National Science Foundation under grant IIS-0713448, and the CoSyne project FP7-ICT-4248531 funded by the European Commission.
(Callison-Burch et al, 2011). $$$$$ The tuning portion was distributed to system combination participants along with reference translations, to aid them set any system parameters.

For the Haitian Creole English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al, 2011). $$$$$ We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics.
For the Haitian Creole English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al, 2011). $$$$$ The views and findings are the authors’ alone.
For the Haitian Creole English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al, 2011). $$$$$ The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.

It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al, 2011). $$$$$ On the other hand, the close results might be an artifact of the language pair choice.
It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al, 2011). $$$$$ This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at EMNLP 2011.
It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al, 2011). $$$$$ However, one of the crawled systems, ONLINE-B, performs consistently well, being one of the winners in all eight language pairs.
It has been shown to give better correlations than BLEU for many European languages including English (Callison-Burch et al, 2011). $$$$$ In order to evaluate whether the quality was better, we conducted a manual evaluation, in the same fashion that we evaluate the different MT systems submitted to the shared translation task.

Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011). $$$$$ Some statistics about the training materials are given in Figure 1.
Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011). $$$$$ They provided the data used in the Haitian Creole featured translation task.
Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011). $$$$$ Through a series of rapid prototyping efforts, the team improved their system to deal with non-standard orthography, reduced pronouns, and SMS shorthand.
Extensions of the last two are included in this study together with alignments based on hidden Markov model (HMM) (Vogel et al, 1996) and inversion transduction grammars (ITG) (Wu, 1997). System combinations produced via confusion net work decoding using different hypothesis alignment algorithms have been entered into open evaluations, most recently in 2011 Workshop on Statistical Machine Translation (Callison-Burch et al, 2011). $$$$$ As for the system combination track (Table 9), the CMU-HEAFIELD-COMBO entry performed quite well, being a winner in seven out of eight language pairs.
