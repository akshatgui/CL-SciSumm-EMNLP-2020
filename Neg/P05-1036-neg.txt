Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ This work was supported by NSF grant IIS0112435.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ This work was supported by NSF grant IIS0112435.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ We create a development corpus of 25 sentences from the training data in order to adjust this parameter.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ That we require a parameter to encourage compression is odd as K&M required a parameter to discourage compression, but we address this point in the penultimate section.

We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ One example of a rule we would like to automatically discover would allow us to compress all of our design goals or In the limit such rules blur the distinction between compression and paraphrase.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ However, the way we determine Pexpand(l  |s) changes because we no longer have a parallel text.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ Thus the last fraction in Equation 7 is equal to one and can be ignored.

Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ The K&M model uses parse trees for the sentences.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ Example 2 shows how unsupervised and semisupervised techniques can be used to improve compression.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ Because the supervised version tends to do quite well, and its main problem is that the model tends to pick longer compressions than a human would, it seems reasonable to incorporate the unsupervised version into our supervised model, in the hope of getting more rules to use.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ This work was supported by NSF grant IIS0112435.

Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ Finally, we point out problems with modeling the task in this way.
Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.
Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ In these cases sentence pairs, or sections of them, are ignored.
Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ None of the special rules are applied.

 $$$$$ Since many nodes that should not be deleted are not labeled with their syntactic role, we add another constraint that disallows deletion of NPs.
 $$$$$ The only time we use an unsupervised compression probability is when there is no supervised version of the unsupervised rule.
 $$$$$ To determine this, you divide the count of NP → DT JJ NN = 3 by all the possible long versions of NP → DT JJNN = 3.
 $$$$$ In the following sections, we discuss the K&M noisy-channel model.

 $$$$$ Even with the unsupervised constraint from section 3, the fact that we have artificially created our joint rules gives us some fairly ungrammatical compressions.
 $$$$$ To determine this, you divide the count of NP → DT JJ NN = 3 by all the possible long versions of NP → DT JJNN = 3.
 $$$$$ In generating new short sentences, if we have compression probabilities in the supervised version, we use those, including the special rules.

 $$$$$ In generating new short sentences, if we have compression probabilities in the supervised version, we use those, including the special rules.
 $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.
 $$$$$ Because the supervised version tends to do quite well, and its main problem is that the model tends to pick longer compressions than a human would, it seems reasonable to incorporate the unsupervised version into our supervised model, in the hope of getting more rules to use.
 $$$$$ They suggest areas for future research.

 $$$$$ Note that importance is a somewhat arbitrary distinction, since according to our judges, all of the computer-generated versions do as well in importance as the human-generated versions.
 $$$$$ The source model stays the same, and we still pay a probability cost in the channel model for every subtree deleted.
 $$$$$ We conclude with the problems inherent in both models.
 $$$$$ The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.

Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ Since many nodes that should not be deleted are not labeled with their syntactic role, we add another constraint that disallows deletion of NPs.
Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ This work was supported by NSF grant IIS0112435.
Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ Others have performed the sentence compression task using syntactic approaches to this problem (Mani et al., 1999) (Zajic et al., 2004), but we focus exclusively on the K&M formulation.
Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ Even with the unsupervised constraint from section 3, the fact that we have artificially created our joint rules gives us some fairly ungrammatical compressions.

Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ In generating new short sentences, if we have compression probabilities in the supervised version, we use those, including the special rules.
Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ One particular type of rule, such as NP(1) → NP(2) CC NP(3), where the parent has at least one child with the same label as itself, and the resulting compression is one of the matching children, such as, here, NP(2).
Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.

While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ We use a program to label syntactic arguments with the roles they are playing (Blaheta and Charniak, 2000), and the rules for complement/adjunct distinction given by (Collins, 1997) to never allow deletion of the complement.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ For a compression to occur, it needs to be less desirable to add an adjective in the channel model than in the source model.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ This work was supported by NSF grant IIS0112435.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ Typically, abstracts do not seem to contain short sentences matching long ones elsewhere in a paper, and we would prefer a much larger corpus.

Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ Summarization in general, and sentence compression in particular, are popular topics.
Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ We then present our cleaned up, and slightly improved noisy-channel model.
Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ We use a program to label syntactic arguments with the roles they are playing (Blaheta and Charniak, 2000), and the rules for complement/adjunct distinction given by (Collins, 1997) to never allow deletion of the complement.
Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ The equation they get for the channel probabilities in their example is similar to the channel probabilities we give in Figures 3 and 4.

See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ Finally, we point out problems with modeling the task in this way.
See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ Later, we will describe additional constraints that help even more.
See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ We conclude with the problems inherent in both models.
See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ However, more training data is always the easiest cure to statistical problems.

 $$$$$ K&M: Many debugging features, including user-defined points and variable-watching and message-watching windows, have been added. supervised: Many features, including user-defined break points and variable-watching and windows, have been added. super (+ extra rules, constraints): Many debugging features have been added. unsuper (+ constraints): Many debugging features, including user-defined break points and variable-watching and message-watching windows, have been added. semi-supervised (+ constraints): Many debugging features have been added. super (+ extra rules, constraints): Trackstar supports only the critical path method (CPM) of scheduling. unsuper (+ constraints): Trackstar supports only the critical path method of project scheduling. semi-supervised (+ constraints): Trackstar supports only the critical path method of project scheduling. original: The faster transfer rate is made possible by an MTI-proprietary data buffering algorithm that off-loads lock-manager functions from the Q-bus host, Raimondi said. human: The algorithm off-loads lock-manager functions from the Q-bus host.
 $$$$$ This supervised version compresses better than either version of the supervised noisy-channel model that lacks these rules.
 $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.

Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ One difficulty in the use of training data is that so many compressions cannot be modeled by our simple method.
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ First, we can get good compressions without paired training data.

Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ The count of NP → DT JJNN = 3, and the possible long versions of NP → DT NN are itself (with count of 3) and NP → DT JJ NN (with count of 4), yielding a sum of 7.
Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ They suggest areas for future research.
Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ Summarization - Step Sentence Knight and Marcu (Knight and Marcu, 2000) (K&M) present a noisy-channel model for sentence compression.

Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ In particular, we learned that adding an additional rule type improved compression, and that enforcing some deletion constraints improves grammaticality.
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ We use a program to label syntactic arguments with the roles they are playing (Blaheta and Charniak, 2000), and the rules for complement/adjunct distinction given by (Collins, 1997) to never allow deletion of the complement.
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ One would hope that real compressed sentences are more probable as a member of the set of compressed sentences than they are as simply a member of all English sentences.
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ This work was supported by NSF grant IIS0112435.

In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ This work was supported by NSF grant IIS0112435.
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ Analogously, in the sentence compression model, the short string is the original sentence and someone adds noise, resulting in the longer sentence.
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ The only time we use an unsupervised compression probability is when there is no supervised version of the unsupervised rule.
