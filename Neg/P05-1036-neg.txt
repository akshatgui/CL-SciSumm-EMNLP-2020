Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ This work was supported by NSF grant IIS0112435.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ Because the supervised version tends to do quite well, and its main problem is that the model tends to pick longer compressions than a human would, it seems reasonable to incorporate the unsupervised version into our supervised model, in the hope of getting more rules to use.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ The source model stays the same, and we still pay a probability cost in the channel model for every subtree deleted.
Recently Turner and Charniak (2005) presented supervised and semi-supervised versions of the Knight and Marcu noisy-channel model. $$$$$ They suggest areas for future research.

We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ First, let us define svo (shorter version of) to be: r1 svo r2 iff the righthand side of r1 is a subsequence of the righthand side of r2.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ The unsupervised version, on the other hand, sees both PRN → lrb NP rrb and PRN → NP in its training data, and the semi-supervised version capitalizes on this particular unsupervised rule.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ In generating new short sentences, if we have compression probabilities in the supervised version, we use those, including the special rules.
We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). $$$$$ However, the way we determine Pexpand(l  |s) changes because we no longer have a parallel text.

Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ Though the problem is simpler, it is still pertinent to current needs; generation of captions for television and audio scanning services for the blind (Grefenstette, 1998), as well as compressing chosen sentences for headline generation (Angheluta et al., 2004) are examples of uses for sentence compression.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.
Alternatively, the rules of compression are approximated from a non-parallel corpus (e.g., the Penn Treebank) by considering context-free grammar derivations with matching expansions (Turner and Charniak 2005). $$$$$ They suggest areas for future research.

Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ The main difficulty in using this method is the lack of data; Knight and Marcu use a corpus of 1035 training sentences.
Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ This work was supported by NSF grant IIS0112435.
Although both models yield comparable performance, Turner and Charniak (2005) show that the latter is not an appropriate compression model since it favours uncompressed sentences over compressed ones. $$$$$ We also show that it is possible to perform an unsupervised version of the compression task, which performs remarkably well.

 $$$$$ Since many nodes that should not be deleted are not labeled with their syntactic role, we add another constraint that disallows deletion of NPs.
 $$$$$ We also show that it is possible to perform an unsupervised version of the compression task, which performs remarkably well.
 $$$$$ It simplifies general compression problems, such as text-to-abstract conversion, by eliminating the need for coherency between sentences.
 $$$$$ In addition to simplifying the task, K&M’s noisy-channel formulation is also appealing.

 $$$$$ We conclude with the problems inherent in both models.
 $$$$$ We achieved much success using syntactic labels to constrain compressions, and there are surely other constraints that can be added.
 $$$$$ Thus they divided the language model into two parts.
 $$$$$ One example of a rule we would like to automatically discover would allow us to compress all of our design goals or In the limit such rules blur the distinction between compression and paraphrase.

 $$$$$ We start by making our noisy channel notation a original: Many debugging features, including user-defined break points and variable-watching and message-watching windows, have been added. human: Many debugging features have been added.
 $$$$$ This seems surprising, considering that the model we are using has had some success, but it makes intuitive sense.
 $$$$$ We then present our cleaned up, and slightly improved noisy-channel model.

 $$$$$ In the following sections, we discuss the K&M noisy-channel model.
 $$$$$ The only time we use an unsupervised compression probability is when there is no supervised version of the unsupervised rule.
 $$$$$ We presented four judges with nine compressed versions of each of the 32 long sentences: A humangenerated short version, the K&M version, our first supervised version, our supervised version with our special rules, our supervised version with special rules and additional constraints, our unsupervised version, our supervised version with additional constraints, our semi-supervised version, and our semisupervised version with additional constraints.

Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ Example 3 shows an instance of our initial supervised versions performing far worse than the K&M model.
Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ In fact, the most likely short sentence will, in general, be the same length as the long sentence.
Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. $$$$$ The model is further simplified by being constrained to word deletion: no rearranging of words takes place.

Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ The K&M model uses parse trees for the sentences.
Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ Many compressions do not align exactly.
Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.
Turner and Charniak (2005) have shown that applying handcrafted rules for trimming sentences can improve both content and linguistic quality. $$$$$ Though the problem is simpler, it is still pertinent to current needs; generation of captions for television and audio scanning services for the blind (Grefenstette, 1998), as well as compressing chosen sentences for headline generation (Angheluta et al., 2004) are examples of uses for sentence compression.

While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ We conclude with the problems inherent in both models.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ One example of a rule we would like to automatically discover would allow us to compress all of our design goals or In the limit such rules blur the distinction between compression and paraphrase.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ This work was supported by NSF grant IIS0112435.
While data sparsity is a common problem of many NLP tasks, it is much more severe for sentence compression, leading Turner and Charniak (2005) to question the applicability of the channel model for this task altogether. $$$$$ We start by making our noisy channel notation a original: Many debugging features, including user-defined break points and variable-watching and message-watching windows, have been added. human: Many debugging features have been added.

Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ We use a program to label syntactic arguments with the roles they are playing (Blaheta and Charniak, 2000), and the rules for complement/adjunct distinction given by (Collins, 1997) to never allow deletion of the complement.
Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ This work was supported by NSF grant IIS0112435.
Turner and Charniak (2005) question the viability of a noisy channel model for the sentence compression task. $$$$$ Adding extra constraints improves our unsupervised compressions, and gives us better performance on the supervised version as well.

See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ This framework independently models the grammaticality of s (with P(s)) and whether s is a good compression of l (P(l  |s)).
See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ Our semisupervised version, which we hoped would have good compression rates and grammaticality, had good grammaticality but lower compression than desired.
See (Turner and Charniak, 2005) for a discussion of problems that can occur for text compression when using a language model trained on data from the uncompressed side. $$$$$ The rules it does model, immediate constituent deletion, as in taking out the ADVP , of S → ADVP , NP VP ., are certainly common, but many good deletions are more structurally complicated.

 $$$$$ More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task.
 $$$$$ In generating new short sentences, if we have compression probabilities in the supervised version, we use those, including the special rules.
 $$$$$ We then present our cleaned up, and slightly improved noisy-channel model.
 $$$$$ K&M did not have a true syntax-based language model to use as we have.

Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ Since many nodes that should not be deleted are not labeled with their syntactic role, we add another constraint that disallows deletion of NPs.
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ However, more training data is always the easiest cure to statistical problems.
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ Since some of these “training” pairs are likely to be fairly poor compressions, due to the artificiality of the construction, we restrict generation of short sentences to not allow deletion of the head of any subtree.
Turner and Charniak (Turner and Charniak, 2005) added some special rules and applied this method to unsupervised learning to overcome the lack of training data. $$$$$ The unsupervised version probably can also be further improved.

Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ In this model, the probabilities do not sum to one, because they pay the probabilistic price for guessing the word “toys” twice, based upon two different conditioning events.
Turner and Charniak (Turner and Charniak, 2005) revised and improved Knight and Marcu's algorithm; however, their algorithm also uses only mother and daughter relations and has the same problem. $$$$$ Example 4 gives an example of the K&M model being outperformed by all of our other models.

Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ Despite this lack of training data, very good results were obtained both by the K&M model and by our variant.
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ Even with the unsupervised constraint from section 3, the fact that we have artificially created our joint rules gives us some fairly ungrammatical compressions.
Turner and Charniak (Turner and Charniak, 2005) solve this problem by appending special rules that are applied when a mother node and its daughter node have the same label. $$$$$ K&M did not have a true syntax-based language model to use as we have.

In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ There are two main ideas to take away from these results.
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ The rules it does model, immediate constituent deletion, as in taking out the ADVP , of S → ADVP , NP VP ., are certainly common, but many good deletions are more structurally complicated.
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ Our starting implementation is intended to follow the K&M model fairly closely.
In addition, for some monolingual translation domains, it has been argued that it is not appropriate to train a language model using data from the input domain (Turner and Charniak, 2005). $$$$$ This supervised version compresses better than either version of the supervised noisy-channel model that lacks these rules.
