This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings. $$$$$ All of these preferences are expressed by probabilities conditioned on lexical heads.
This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings. $$$$$ To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.
This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings. $$$$$ Model 1 achieves 87.7% constituent precision and 87.5% consituent recall on sentences of up to 100 words in length in section 23 of the treebank, and Models 2 and 3 give further improvements to 88.3% constituent precision and 88.0% constituent recall.
This figure shows the amount of time (excluding any startup overhead) spent parsing or bracketing using this system (the two lowest lines) versus the parsers of Collins (2003) and Charniak (2000) run with default settings. $$$$$ (Figure 3 gives an example that illustrates this.)

Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002). $$$$$ The models are generative models in which parse trees are decomposed into a number of steps in a top-down derivation of the tree and the decisions in the derivation are modeled as conditional probabilities.
Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002). $$$$$ This is a problem for the subcategorization probabilities in models 2 and 3: The probability of having zero subjects, Pl,({}  |S, VP, verb), will be fairly high because of this.
Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002). $$$$$ In addition to introducing the three parsing models and evaluating their performance on the Penn Wall Street Journal Treebank, we have aimed in our discussion (in sections 7 and 8) to give more insight into the models: their strengths and weaknesses, the effect of various features on parsing accuracy, and the relationship of the models to other work on statistical parsing.
Reranking of n-best lists has recently become popular in several natural language problems, including parsing (Collins, 2003), machine translation (Och and Ney, 2002) and web search (Joachims, 2002). $$$$$ My Ph.D. thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent Ph.D. thesis adviser, and for contributing in many ways to this research.

Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach. $$$$$ .
Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach. $$$$$ My Ph.D. thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent Ph.D. thesis adviser, and for contributing in many ways to this research.
Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach. $$$$$ The second is a technical condition that guarantees that the stochastic process generating trees terminates in a finite number of steps with probability one.
Collins (2003) uses both Markov Random Fields and boosting, Och and Ney (2002) use a maximum entropy ranking scheme, and Joachims (2002) uses a support vector approach. $$$$$ The parsing approaches we have described concentrate on breaking down context-free rules in the treebank into smaller components.

All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser. $$$$$ The rule has the benefit of improving efficiency by reducing the number of constituents in the chart.
All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser. $$$$$ Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.
All the COL03 systems are results obtained using the restriction of the output of Collins (2003) parser. $$$$$ The precision and recall of the traces found by Model 3 were 93.8% and 90.1%, respectively (out of 437 cases in section 23 of the treebank), where three criteria must be met for a trace to be “correct”: (1) It must be an argument to the correct headword; (2) It must be in the correct position in relation to that headword (preceding or following); 15 Magerman (1995) collapses ADVP and PRT into the same label; for comparison, we also removed this distinction when calculating scores.

My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres. $$$$$ The models are generative models in which parse trees are decomposed into a number of steps in a top-down derivation of the tree and the decisions in the derivation are modeled as conditional probabilities.
My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres. $$$$$ Then we set Analogous definitions for f2 and u2 lead to A2 = f2 f2+5u2 .
My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres. $$$$$ The models are generative models in which parse trees are decomposed into a number of steps in a top-down derivation of the tree and the decisions in the derivation are modeled as conditional probabilities.
My guess is that the features used in e.g., the Collins (2003) or Charniak (2000) parsers are probably close to optimal for English Penn Treebank parsing (Marcus et al, 1993), but that other features might improve parsing of other languages or even other English genres. $$$$$ To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.

The corpora are first parsed using Collins's parser (Collins, 2003) with the boundaries of all the entity mentions kept. $$$$$ Formally, we will always take a lexicalized nonterminal P(h) to expand deterministically (with probability one) in this way if P is a part-of-speech symbol.
The corpora are first parsed using Collins's parser (Collins, 2003) with the boundaries of all the entity mentions kept. $$$$$ I would like to thank the members of my thesis committee—Aravind Joshi, Mark Liberman, Fernando Pereira, and Mark Steedman—for the remarkable breadth and depth of their feedback.
The corpora are first parsed using Collins's parser (Collins, 2003) with the boundaries of all the entity mentions kept. $$$$$ LR/LP = labeled recall/precision.
The corpora are first parsed using Collins's parser (Collins, 2003) with the boundaries of all the entity mentions kept. $$$$$ We use the PARSEVAL measures (Black et al. 1991) to compare performance: number of correct constituents in proposed parse number of constituents in proposed parse number of correct constituents in proposed parse number of constituents in treebank parse Crossing brackets = number of constituents that violate constituent boundaries with a constituent in the treebank parse For a constituent to be “correct,” it must span the same set of words (ignoring punctuation, i.e., all tokens tagged as commas, colons, or quotation marks) and have the same label15 as a constituent in the treebank parse.

To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ Of particular relevance is other work on parsing the Penn WSJ Treebank (Jelinek et al. 1994; Magerman 1995; Eisner 1996a, 1996b; Collins 1996; Charniak 1997; Goodman 1997; Ratnaparkhi 1997; Chelba and Jelinek 1998; Roark 2001).
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ The output of the parser is now enhanced to show trace coindexations in wh-movement cases.
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ In conclusion, we would like to highlight the following points: subcategorization parameters performs very poorly (76.5% precision, 75% recall), suggesting that the adjacency feature is capturing some subcategorization information in the model 1 parser.

As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information. $$$$$ The output of the parser is now enhanced to show trace coindexations in wh-movement cases.
As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information. $$$$$ This would allow the model to handle cases like the first example in Figure 15 correctly.
As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information. $$$$$ The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.
As pointed out by an anonymous reviewer of Collins (2003), removing outermost punctuation might discard useful information. $$$$$ Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.

We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) - both sentence-final and sentence-initial - could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. $$$$$ Given that the LHS of the rule has a gap, there are three ways that the gap can be passed down to the RHS: Head: The gap is passed to the head of the phrase, as in rule (3) in Figure 7.
We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) - both sentence-final and sentence-initial - could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. $$$$$ This article describes three statistical models for natural language parsing.
We suspect that identities of punctuation marks (Collins, 2003, Footnote 13) - both sentence-final and sentence-initial - could be of extra assistance in grammar induction, specifically for grouping imperatives, questions, and so forth. $$$$$ The generation of punc=1 along with ADJP(old) in the example implicitly requires generation of a punctuation tag/word pair through the Pp parameter.

The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). $$$$$ Section 3 introduces the three probabilistic models.
The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). $$$$$ Finally, thanks to the anonymous reviewers for their comments.
The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). $$$$$ My Ph.D. thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent Ph.D. thesis adviser, and for contributing in many ways to this research.
The first, proposed by (Johnson, 1998), is the annotation of parental history, and the second encodes a head-outward generation process (Collins, 2003). $$$$$ To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.

Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. $$$$$ Statistical parsing approaches tackle the ambiguity problem by assigning a probability to each parse tree, thereby ranking competing trees in order of plausibility.
Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. $$$$$ Figure 2 shows a tree that will be used as an example throughout this article.
Collins (2003) proposes to generate the head of a phrase first and then generate its sisters using Markovian processes, thereby exploiting head/sister-dependencies. $$$$$ All of these preferences are expressed by probabilities conditioned on lexical heads.

A formal overview of the transformation and its correspondence to (Collins, 2003)'s models is available at (Hageloh, 2007). $$$$$ Second, the lexical items are filled in.
A formal overview of the transformation and its correspondence to (Collins, 2003)'s models is available at (Hageloh, 2007). $$$$$ We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples.
A formal overview of the transformation and its correspondence to (Collins, 2003)'s models is available at (Hageloh, 2007). $$$$$ We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples.
A formal overview of the transformation and its correspondence to (Collins, 2003)'s models is available at (Hageloh, 2007). $$$$$ We also give a breakdown of precision and recall results in recovering various types of dependencies.

In a second set of experiments we use an unlexicalized head driven baseline a la (Collins, 2003) located on the (0, 0, 0) coordinate. $$$$$ LR/LP = labeled recall/precision.
In a second set of experiments we use an unlexicalized head driven baseline a la (Collins, 2003) located on the (0, 0, 0) coordinate. $$$$$ Statistical parsing approaches tackle the ambiguity problem by assigning a probability to each parse tree, thereby ranking competing trees in order of plausibility.
In a second set of experiments we use an unlexicalized head driven baseline a la (Collins, 2003) located on the (0, 0, 0) coordinate. $$$$$ Note that the first item of the conjunct is taken as the head of the phrase. process, namely, the generation of the coordinator tag/word pair, parameterized by the P,, parameter.
In a second set of experiments we use an unlexicalized head driven baseline a la (Collins, 2003) located on the (0, 0, 0) coordinate. $$$$$ Finally, section 8 gives more discussion, by comparing the models to others that have been applied to parsing the treebank.

In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a la (Collins,2003). $$$$$ My Ph.D. thesis is the basis of the work in this article; I would like to thank Mitch Marcus for being an excellent Ph.D. thesis adviser, and for contributing in many ways to this research.
In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a la (Collins,2003). $$$$$ All words occurring less than six times14 in training data, and words in test data that have never been seen in training, are replaced with the UNKNOWN token.
In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a la (Collins,2003). $$$$$ First, Table 3 has a breakdown of precision and recall by constituent type.
In our next set of experiments we evaluate the contribution of the depth dimension to extensions of the head-driven unlexicalized variety a la (Collins,2003). $$$$$ Many NLP applications will need this information to extract predicate-argument structure from parse trees.

This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it can not be collapsed into any one of the previous two. $$$$$ I would like to thank the members of my thesis committee—Aravind Joshi, Mark Liberman, Fernando Pereira, and Mark Steedman—for the remarkable breadth and depth of their feedback.
This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it can not be collapsed into any one of the previous two. $$$$$ 3.2.2 Probabilities over Subcategorization Frames.
This dimension is orthogonal to the vertical (Collins, 2003) and horizontal (Johnson, 1998) dimensions previously outlined by Klein and Manning (2003), and it can not be collapsed into any one of the previous two. $$$$$ We will extend the left and right sequences to include a terminating STOP symbol, allowing a Markov process to model the left and right sequences.

Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. $$$$$ To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies.
Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. $$$$$ We use the smoothing method described in Bikel et al. (1997), which is derived from a method described in Witten and Bell (1991).
Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. $$$$$ We analyze various characteristics of the models through experiments on parsing accuracy, by collectingfrequencies ofvarious structures in the treebank, and through linguistically motivated examples.
Even when the breakdown for particular node types is presented (e.g. Collins, 2003), the interaction between node errors is not taken into account. $$$$$ Jelinek et al. (1994), and Magerman (1995).

Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. $$$$$ Collins (2000) uses a technique based on boosting algorithms for machine learning that reranks n-best output from model 2 in this article.
Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. $$$$$ Next, we make the assumption that the modifiers are generated independently of each other: In summary, the generation of the RHS of a rule such as (2), given the LHS, has been decomposed into three steps:5 For example, the probability of the rule S(bought) → NP(week) NP(IBM) VP(bought) would be estimated as In this example, and in the examples in the rest of the article, for brevity we omit the part-of-speech tags associated with words, writing, for example S(bought) rather than S(bought,VBD).
Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. $$$$$ Statistical parsing approaches tackle the ambiguity problem by assigning a probability to each parse tree, thereby ranking competing trees in order of plausibility.
Inspired by the work of Collins (2003), the generative model builds trees by recursively creating nodes at each level according to a Markov process. $$$$$ The work benefited greatly from discussions with Jason Eisner, Dan Melamed, Adwait Ratnaparkhi, and Paola Merlo.

Motivated by Collins syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. $$$$$ A strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al. 1994 and Magerman 1995).
Motivated by Collins syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. $$$$$ The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature.
Motivated by Collins syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. $$$$$ Each derivation corresponds to a tree-sentence pair that is well formed under the grammar.
Motivated by Collins syntactic parsing models (Collins, 2003), we consider the generation process for a hybrid sequence from an MR production as a Markov process. $$$$$ Figure 21 shows a case of PP attachment.

For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. $$$$$ Gildea (2001) reports on experiments investigating the utility of different features in bigram lexical-dependency models for parsing.
For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. $$$$$ In the three models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree.
For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. $$$$$ A lexicalized rule is predicted in two steps.
For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. $$$$$ In particular, the start points of base-NPs are often marked with a determiner or another distinctive item, such as an adjective.

First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al (2008). $$$$$ • Section 7.3 described how the three models are well-suited to the Penn Treebank style of annotation, and how certain phenomena (particularly the distance features) may fail to be modeled correctly given treebanks with different annotation styles.
First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al (2008). $$$$$ Of the 437 cases, 341 were string-vacuous extraction from subject position, recovered with 96.3% precision and 98.8% recall; and 96 were longer distance cases, recovered with 81.4% precision and 59.4% recall.16
First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al (2008). $$$$$ The models in Collins (1996) showed that the distance between words standing in head-modifier relationships was important, in particular, that it is important to capture a preference for right-branching structures (which almost translates into a preference for dependencies between adjacent words) and a preference for dependencies not to cross a verb.
First, we consider the integration of the generative model for phrase-structure parsing of Collins (2003), with the second-order discriminative dependency parser of Koo et al (2008). $$$$$ Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.
