For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ This adds detail to the Penn Treebank that necessary for many
For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ Some translation is required to compare our brackets to DepBank dependencies.
For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ Air Force is a specific entity and should form a separate constituent underneath the NP, as in our new annotation scheme: We use NML to specify that Air Force together is a nominal modifier of contract.
For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ We then examine the consistency and reliability of our annotations.

We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ This data has been used to evaluate most research since.
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ The results on the new corpus are even more surprising, with the adjacency model outperforming the dependency model by a wide margin.
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately.
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat.

Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ We also ran an experiment where the new NML and JJP labels were relabelled as NP and AnJP.
Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ This data has been used to evaluate most research since.
Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ For instance, CCGbank (Hockenmaier, 2003) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (CCG) (Steedman, 2000) derivations.

 $$$$$ The bracketing guidelines (Bies et al., 1995) also mention the considerable difficulty of identifying the correct scope for nominal modifiers.
 $$$$$ Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus.
 $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.

The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below $$$$$ The bracketing tool often suggests a bracketing using rules based mostly on named entity tags, which are drawn from the BBN corpus (Weischedel and Brunstein, 2005).
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below $$$$$ Finally, we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing (Lauer, 1995; Nakov and Hearst, 2005) and full parsing (Collins, 1999).
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below $$$$$ However, this only resolves NPs dominating MWEs or NEs.
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below $$$$$ An extra pass was made through the corpus, ensuring that every instance of these phrases was bracketed consistently.

PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ We draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web (Brants and Franz, 2006).
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ This adds detail to the Penn Treebank that necessary for many
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ For the original bracketing of the Treebank, annotators performed at 375–475 words per hour after a few weeks, and increased to about 1000 words per hour after gaining more experience (Marcus et al., 1993).
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ We then examine the consistency and reliability of our annotations.

This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ These results demonstrate that high agreement rates are achievable for these annotations.
This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing.

This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ We then examine the consistency and reliability of our annotations.
This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ We ran the tool over the original corpus, following NE-based suggestions where possible.
This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ The dependency model compares the association between words 1-2 to words 1-3, while the adjacency model compares words 1-2 to words 2-3.
This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat.

Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ We have created a new NP bracketing data set from our extended Treebank by extracting all rightmost three noun sequences from base-NPs.
Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing.
Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP).

Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ The following NP is an example of the flat structure of base-NPs within the Penn Treebank:
Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ We then examine the consistency and reliability of our annotations.
Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ Base-NP structure is also important for annotated data derived from the Penn Treebank.

Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP).
Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ For our annotation process, counting each word in every NP shown, our speed was around 800 words per hour.

 $$$$$ Due to the mis-alignment of NEs and right-branching NPs, the increase in performance was negligible.
 $$$$$ We find that when evaluated against our annotations, the Fscore is 50.71%.
 $$$$$ Understanding base-NP structure is important, since otherwise parsers will propose nonsensical noun phrases like Force contract by default and pass them onto downstream components.

Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ Lapata and Keller (2004) derive estimates from web counts, and only compare at a lexical level, achieving 78.7% accuracy.
Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ This significantly simplified and sped up the manual annotation process.
Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat.
Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ We find that the old and new versions are identical in 88.65% of NPs, with labelled precision, recall and F-score being 97.17%, 76.69% and 85.72% respectively.

Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ We could use multi-word expressions (MWEs) to identify some structure.
Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ Secondly, certain phrases that occurred numerous times and were non-trivial to bracket, e.g.
Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ For instance, CCGbank (Hockenmaier, 2003) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (CCG) (Steedman, 2000) derivations.
Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003).

 $$$$$ We then examine the consistency and reliability of our annotations.
 $$$$$ We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003).
 $$$$$ We also give an analysis of our extended Treebank, quantifying how much structure we have added, and how it is distributed across NPs.
 $$$$$ Our initial experiments are presented in Section 6.1.

 $$$$$ Table 8 shows that the new brackets make parsing marginally more difficult overall (by about 0.5% in F-score).
 $$$$$ Air Force is a specific entity and should form a separate constituent underneath the NP, as in our new annotation scheme: We use NML to specify that Air Force together is a nominal modifier of contract.
 $$$$$ Other inconsistencies are rare, but will be examined and corrected in a future release.
 $$$$$ We then examine the consistency and reliability of our annotations.

 $$$$$ Each section contains around 2500 ambiguous NPs, i.e. annotating took approximately 5 seconds per NP.
 $$$$$ Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank.
 $$$$$ There are genuine flat cases, primarily names like John A. Smith, that we would like to distinguish from implicitly right-branching NPs in the next version of the corpus.
 $$$$$ The following NP is an example of the flat structure of base-NPs within the Penn Treebank:

We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ When they shuffled their data with Lauer’s to create a new test and training split, their accuracy increased to 83.1% which may be a result of the 10% duplication in Lauer’s test set.
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank.
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ We check the correctness of the corpus by measuring interannotator agreement, by reannotating the first section, and by comparing against the sub-NP structure in DepBank (King et al., 2003).

Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ This adds detail to the Penn Treebank that necessary for many
Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality.
Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat.

Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP).
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ He uses Roget’s thesaurus to smooth words into semantic classes, and then calculates association between classes based on their counts in a “training set” also drawn from Grolier’s.
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus.
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus.
