For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ We describe the annotation guidelines and process, including the use of named entity data to improve annotation quality.
For WSJ parsing, we use the standard train (02-21) / dev (22) / test (23) split and apply the NP bracketing patch by Vadas and Curran (2007). $$$$$ Certain common patterns, such as three words beginning with a determiner, are unambiguous, and were filtered out.

We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ This new resource will allow any system or annotated corpus developed from the Penn Treebank, to represent noun phrase structure more accurately.
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ This adds detail to the Penn Treebank that necessary for many
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ This shows the difficulty of correctly bracketing NPs.
We apply an automatic conversion process using the gold-standard NP data annotated by Vadas and Curran (2007a). $$$$$ From this we generated a list of phrases, and then made another pass through the corpus, synchronising all instances that contained one of these phrases.

Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank.
Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ This means that performance for other phrases is hardly changed by the new NP brackets.
Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ Many approaches to identifying base noun phrases have been explored as part of chunking (Ramshaw and Marcus, 1995), but determining sub-NP structure is rarely addressed.
Recently, Vadas and Curran (2007a) annotated internal NP structure for the entire Penn Treebank, providing a large gold-standard corpus for NP bracketing. $$$$$ We wanted to keep our extended Treebank as similar to the original as possible, so that they remain comparable.

 $$$$$ The results of this analysis are quite positive.
 $$$$$ Adding this annotation better represents the true syntactic and semantic structure, which will improve the performance of downstream NLP systems.
 $$$$$ An example is given showing typical words that match the POS tags.

The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). $$$$$ For our annotation process, counting each word in every NP shown, our speed was around 800 words per hour.
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). $$$$$ We found however, that while there are certainly difficult cases, the vast majority are quite simple and can be annotated reliably.
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). $$$$$ An additional grammar rule is needed just to get a parse, but it is still not correct (Hockenmaier, 2003, p. 64).
The Vadas and Curran (2007a) annotation scheme inserts NML and JJP brackets to describe the correct NP structure, as shown below: (NP (NML (NN lung) (NN cancer)) (NNS deaths)). $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.

PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ This adds detail to the Penn Treebank that necessary for many
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ 22851 of these (37.49%) had brackets inserted by the annotator.
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ Secondly, certain phrases that occurred numerous times and were non-trivial to bracket, e.g.
PropBank (Palmer et al, 2005) is used as a gold-standard to inform these decisions, similar to the way that we use the Vadas and Curran (2007a) data. $$$$$ The annotation was performed by the first author.

This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.
This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank.
This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ This tells us that there were many brackets originally missed that were added in the second pass.
This section describes the process of converting the Vadas and Curran (2007a) data to CCG derivations. $$$$$ We developed a bracketing tool, which identifies ambiguous NPs and presents them to the user for disambiguation.

This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ The annotation guidelines for this biomedical corpus (an addendum to the Penn Treebank guidelines) introduce the use of NML nodes to mark internal NP structure.
This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing.
This simple heuristic captures NP structure not explicitly annotated by Vadas and Curran (2007a). $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.

Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ Our initial experiments are presented in Section 6.1.
Vadas and Curran (2007a) describe using NE tags during the annotation process, suggesting that NER based features will be helpful in a statistical model. $$$$$ We could use multi-word expressions (MWEs) to identify some structure.

Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus.
Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ For example, Question Answering (QA) systems need to supply an NP as the answer to a factoid question, often using a parser to identify candidate NPs to return to the user.
Vadas and Curran (2007a) experienced a similar drop in performance on Penn Tree bank data, and noted that the F-score for NML and JJP brackets was about 20% lower than the overall figure. $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.

Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ Unfortunately, the Penn Treebank does not annotate the internal structure of base noun phrases, instead leaving them flat.
Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ Our main contribution is a gold-standard labelled bracketing for every ambiguous noun phrase in the Penn Treebank.
Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ For instance, CCGbank (Hockenmaier, 2003) was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (CCG) (Steedman, 2000) derivations.
Recent annotations by Vadas and Curran (2007a) added NP structure to the PTB. $$$$$ We then examine the consistency and reliability of our annotations.

 $$$$$ This does not work perfectly, and mismatches occur because of which dependencies DepBank marks explicitly, and how it chooses heads.
 $$$$$ This meant the model would not have to discriminate between two different types of noun and adjective structure.
 $$$$$ The Penn Treebank (Marcus et al., 1993) is perhaps the most influential resource in Natural Language Processing (NLP).
 $$$$$ This significantly simplified and sped up the manual annotation process.

Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ Adding this annotation better represents the true syntactic and semantic structure, which will improve the performance of downstream NLP systems.
Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ Adding this annotation better represents the true syntactic and semantic structure, which will improve the performance of downstream NLP systems.
Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). $$$$$ Air Force is a specific entity and should form a separate constituent underneath the NP, as in our new annotation scheme: We use NML to specify that Air Force together is a nominal modifier of contract.

Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ However, this only resolves NPs dominating MWEs or NEs.
Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ On the other hand, the precision on NML and JJP constituents was quite high, so the parser is able to identify at least some of the structure very well.
Vadas and Curran (2007a) annotated NP-internal structure by adding annotations whenever there is a left-bracketing. $$$$$ With a large annotated corpus, we can now run supervised NP bracketing experiments.

 $$$$$ This significantly simplified and sped up the manual annotation process.
 $$$$$ We draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web (Brants and Franz, 2006).
 $$$$$ He achieves 80.7% accuracy using POS tags to indentify bigrams in the training set.
 $$$$$ Finally, we use resource to determine using several statistical approaches, thus demonstrating the utility of the corpus.

 $$$$$ This adds detail to the Penn Treebank that necessary for many
 $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.
 $$$$$ The following NP is an example of the flat structure of base-NPs within the Penn Treebank:

 $$$$$ Our annotation guidelines) are based on those developed for annotating full sub-NP structure in the biomedical domain (Kulick et al., 2004).
 $$$$$ Nakov and Hearst (2005) also use web counts, but incorporate additional counts from several variations on simple bigram queries, including queries for the pairs of words concatenated or joined by a hyphen.
 $$$$$ This suggests that a NE feature may help to identify the correct bracketing in one third of cases.
 $$$$$ Counting matched brackets is a harsher evaluation, as there are many NPs that both annotators agree should have no additional bracketing, which are not taken into account by the metric.

We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ We found however, that while there are certainly difficult cases, the vast majority are quite simple and can be annotated reliably.
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ We then examine the consistency and reliability of our annotations.
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ An extra pass was made through the corpus, ensuring that every instance of these phrases was bracketed consistently.
We use Vadas and Curran (2007a)'s annotations (Section 3) to create training, development and testing data for base NPs, using standard splits of the Penn Treebank (Table 1). $$$$$ He achieves 80.7% accuracy using POS tags to indentify bigrams in the training set.

Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ In particular, we want to determine how useful named entities are in determining the correct bracketing.
Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ Since CCG derivations are binary branching, they cannot directly represent the flat structure of the Penn Treebank base-NPs.
Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. $$$$$ It is used as a standard training and evaluation corpus in many syntactic analysis tasks, ranging from part of speech (POS) tagging and chunking, to full parsing.

Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ The dependency model compares the association between words 1-2 to words 1-3, while the adjacency model compares words 1-2 to words 2-3.
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ We also chose not to alter the existing Penn Treebank annotation, even though the annotators found many errors during the annotation process.
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ We then examine the consistency and reliability of our annotations.
Vadas' internal noun phrase structure has been used in previous work on constituent parsing using Collins parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. $$$$$ The Penn Treebank does not annotate base noun phrases committing only to flat structures that ignore the of English This means that tools trained on Treebank data cannot the correct internal structure of This paper details the process of adding gold-standard bracketing within each noun phrase in the Penn Treebank.
