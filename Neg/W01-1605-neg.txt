The additional two systems were $$$$$ In this stage of the training, the focus was on segmenting hard copy texts into EDUs, and learning the mechanics of the tool.
The additional two systems were $$$$$ This was followed by a one-month reassessment phase, during which we measured consistency across the group on a select set of documents, and refined the annotation rules.
The additional two systems were $$$$$ The reliability of a dialogue structure coding Linguistics 13-32.

We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RSTDTB) (Carlsonetal., 2001) distributed by the Linguistic Data Consortium (LDC). $$$$$ In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research.
We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RSTDTB) (Carlsonetal., 2001) distributed by the Linguistic Data Consortium (LDC). $$$$$ Multinuclear relations hold among two or more spans of equal weight in the discourse structure.
We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RSTDTB) (Carlsonetal., 2001) distributed by the Linguistic Data Consortium (LDC). $$$$$ The RST relation set is rich and the concept of nuclearity, somewhat interpretive.
We experimentally evaluated the test collection for single document summarization contained in the RST Discourse Treebank (RSTDTB) (Carlsonetal., 2001) distributed by the Linguistic Data Consortium (LDC). $$$$$ Annotators developed different strategies for analyzing a document and building up the corresponding discourse tree.

Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus. $$$$$ Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs.
Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus. $$$$$ In combination with the tree structure, the concept of nuclearity also guided an annotator to capture one of a number of possible stylistic interpretations.
Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus. $$$$$ Ideally, we would like to capture that kind of rhetorical information regardless of the syntactic form in which it is conveyed.
Two of the main corpora with discourse annotations are the RST Discourse Treebank (RSTDT) (Carlson et al., 2001) and the Penn Discourse Treebank (PDTB) (Prasad et al, 2008a), which are both based on the Wall Street Journal (WSJ) corpus. $$$$$ In addition, limitations of the theoretical approach became more apparent over time.

Fortunately, RST Discourse Treebank (RSTDT) (Carlson et al, 2001) is an available resource to help with. $$$$$ For details on obtaining the corpus, annotation software, tagging guidelines, and related documentation and resources, see: http://www.isi.edu/~marcu/discourse.
Fortunately, RST Discourse Treebank (RSTDT) (Carlson et al, 2001) is an available resource to help with. $$$$$ The reliability of a dialogue structure coding Linguistics 13-32.
Fortunately, RST Discourse Treebank (RSTDT) (Carlson et al, 2001) is an available resource to help with. $$$$$ During this process, we regularly tracked interannotator agreement (see Section 4.2).
Fortunately, RST Discourse Treebank (RSTDT) (Carlson et al, 2001) is an available resource to help with. $$$$$ 1997.

In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is $$$$$ However, rich theoretical approaches to discourse/text analysis (Van Dijk and Kintsch, 1983; Meyer, 1985; Grosz and Sidner, 1986; Mann and Thompson, 1988) have yet to be applied on a large scale.
In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is $$$$$ During this process, we regularly tracked interannotator agreement (see Section 4.2).
In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is $$$$$ Syntactic checking involved ensuring that the tree had a single root node and comparing the tree to the document to check for missing sentences or fragments from the end of the text.
In the Cause versus Contrast case, their reported performance exceeds ours significantly; however, in a subset of their experiments which test Cause versus Contrast on instances from the human annotated RSTBank corpus (Carlson et al., 2001) where no cue phrase is present, they report only 63% accuracy over a 56% baseline (the baseline is $$$$$ The initial focus was on resolving segmentation differences, but over time this shifted to addressing issues of relations and nuclearity.

 $$$$$ The final inventory of rhetorical relations is data driven, and is based on extensive analysis of the corpus.
 $$$$$ Table 1 shows average kappa statistics reflecting the agreement of three annotators at various stages of the tasks on selected documents.

For the first, the labelled/unlabelled relations f scores are 50.3% /73.0% and for the latter, they are 75.3% /84.0% $$$$$ The procedural knowledge available at the EDU level is likely to need further refinement for higher-level text spans along the lines of other work which posits a few macro-level relations for text segments, such as Ferrari (1998) or Meyer (1985).
For the first, the labelled/unlabelled relations f scores are 50.3% /73.0% and for the latter, they are 75.3% /84.0% $$$$$ Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs.
For the first, the labelled/unlabelled relations f scores are 50.3% /73.0% and for the latter, they are 75.3% /84.0% $$$$$ Developing corpora with these kinds of rich annotation is a labor-intensive effort.

The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora $$$$$ Thus, the RST Corpus provides an additional level of linguistic annotation to supplement existing annotated resources.
The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora $$$$$ Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic.
The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora $$$$$ These involved two types of tasks: checking the validity of the trees and tracking inter-annotator consistency.
The generator is informed by a corpus study of embedded discourse units on two discourse annotated corpora $$$$$ and Anne Anderson.

We evaluate DPLP on the RST Discourse Tree bank (Carlson et al, 2001), comparing against state-of-the-art results. $$$$$ The reliability of a dialogue structure coding Linguistics 13-32.
We evaluate DPLP on the RST Discourse Tree bank (Carlson et al, 2001), comparing against state-of-the-art results. $$$$$ In the end, we chose the clause as the elementary unit of discourse, using lexical and syntactic clues to help determine boundaries: Relative clauses, nominal postmodifiers, or clauses that break up other legitimate EDUs, are treated as embedded discourse units: a relaxation of controls on exports to the Soviet bloc,] [is questioning...]wsj_2326 Finally, a small number of phrasal EDUs are allowed, provided that the phrase begins with a strong discourse marker, such as because, in spite of, as a result of, according to.
We evaluate DPLP on the RST Discourse Tree bank (Carlson et al, 2001), comparing against state-of-the-art results. $$$$$ Thus, the RST Corpus provides an additional level of linguistic annotation to supplement existing annotated resources.

To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001). $$$$$ 1997.
To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001). $$$$$ Annotators reviewed each tree for syntactic and semantic validity.
To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001). $$$$$ and Anne Anderson.
To compare with previous works on RSTDT, we use the 18 coarse-grained relations defined in (Carlson et al, 2001). $$$$$ Because our annotators had to select among multiple choices at each stage of the discourse annotation process, and because decisions made at one stage influenced the decisions made during subsequent stages, we could not apply Wiebe et al.’s method.

(Carlson et al 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. $$$$$ The final tagged corpus contains 21,789 EDUs with an average of 56.59 EDUs per document.
(Carlson et al 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. $$$$$ 1997.
(Carlson et al 2001) reported relatively high levels of inter-annotator agreement, this was based on an annotation procedure where the annotators were allowed to iteratively revise the instructions based on joint discussion. $$$$$ Developing corpora with these kinds of rich annotation is a labor-intensive effort.

To demonstrate the functionality of our system without relying on still imperfect discourse parsing, we use the RST parsed Wall Street Journal corpus as input (Carlson et al, 2001). $$$$$ The reliability of a dialogue structure coding Linguistics 13-32.
To demonstrate the functionality of our system without relying on still imperfect discourse parsing, we use the RST parsed Wall Street Journal corpus as input (Carlson et al, 2001). $$$$$ The average number of words per EDU is 8.1.
To demonstrate the functionality of our system without relying on still imperfect discourse parsing, we use the RST parsed Wall Street Journal corpus as input (Carlson et al, 2001). $$$$$ Preliminary steps toward the creation of a discourse and text In of the First International Conference on Language

In Discourse Tree Bank (Carlson et al, 2001) only 26% of Contrast relations were indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases. $$$$$ Moreover, using the RST approach, the resultant tree structure, like a traditional outline, imposed constraints that other discourse representations (e.g., graph) would not.
In Discourse Tree Bank (Carlson et al, 2001) only 26% of Contrast relations were indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases. $$$$$ The tool enables an annotator to segment a text into units, and then build up a hierarchical structure of the discourse.
In Discourse Tree Bank (Carlson et al, 2001) only 26% of Contrast relations were indicated by cue phrases while in NTC-7 about 70% of Contrast were indicated by cue phrases. $$$$$ Building the RST Corpus involved more than a dozen people on a full or part-time basis over a oneyear time frame (Jan. – Dec. 2000).

They use the RST corpus (Carlson et al,2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ It is relatively straightforward for the annotator to make a decision on assignment of nuclearity and relation at the inter-clausal level, but this becomes more complex at the intersentential level, and extremely difficult when linking large segments.
They use the RST corpus (Carlson et al,2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic.
They use the RST corpus (Carlson et al,2001), which contains 385 Wall Street Journal articles annotated following the Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises.

(Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al, 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ Thus, while rules for segmenting were fairly precise, annotators relied on heuristics requiring more human judgment to assign relations and nuclearity.
(Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al, 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ 2.
(Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al, 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ Table 2 reports final results for all pairs of taggers who double-annotated four or more documents, representing 30 out of the 53 documents that were double-tagged.
(Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al, 2001) which is annotated based on Rhetorical Structure Theory (Mann and Thompson, 1988). $$$$$ The final tagging manual, over 80 pages in length, contains extensive examples from the corpus to illustrate text segmentation, nuclearity, selection of relations, and discourse cues.

The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), is a corpus annotated in the framework of RST. $$$$$ However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy.
The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), is a corpus annotated in the framework of RST. $$$$$ Our main goal in undertaking this effort was to create a reference corpus for community-wide use.
The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), is a corpus annotated in the framework of RST. $$$$$ However, as examples 2-4 illustrate, separating rhetorical from syntactic analysis is not always easy.
The RST Discourse Treebank (RST-DT) (Carlson et al, 2001), is a corpus annotated in the framework of RST. $$$$$ When building the tree in this fashion, the annotator must anticipate the upcoming discourse structure, possibly for a large span.

In the corpus of Rhetorical Structure trees built by Carlson et al (2001), for example, we have observed that only 61 of 238 CONTRAST relation sand 79 out of 307 EXPLANATION-EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase. $$$$$ The reliability of a dialogue structure coding Linguistics 13-32.
In the corpus of Rhetorical Structure trees built by Carlson et al (2001), for example, we have observed that only 61 of 238 CONTRAST relation sand 79 out of 307 EXPLANATION-EVIDENCE relations that hold between two adjacent clauses were marked by a cue phrase. $$$$$ There were two basic orientations for document analysis – hard copy or graphical visualization with the tool.

However, empirical work of Marcu (2000) and Carlson et al (2001) suggests that the majority of occurrences of but, for example, do signal CONTRAST relations. $$$$$ The resulting corpus contains 385 documents of American English selected from the Penn Treebank (Marcus et al., 1993), annotated in the framework of Rhetorical Structure Theory.
However, empirical work of Marcu (2000) and Carlson et al (2001) suggests that the majority of occurrences of but, for example, do signal CONTRAST relations. $$$$$ Two essential considerations from the outset were that the corpus needed to be consistently annotated, and that it would be made publicly available through the Linguistic Data Consortium for a nominal fee to cover distribution costs.
However, empirical work of Marcu (2000) and Carlson et al (2001) suggests that the majority of occurrences of but, for example, do signal CONTRAST relations. $$$$$ The 78 relations used in annotating the corpus can be partitioned into 16 classes that share some type of rhetorical meaning: Attribution, Background, Cause, Comparison, Condition, Contrast, Elaboration, Enablement, Evaluation, Explanation, Joint, Manner-Means, Topic-Comment, Summary, Temporal, TopicChange.
However, empirical work of Marcu (2000) and Carlson et al (2001) suggests that the majority of occurrences of but, for example, do signal CONTRAST relations. $$$$$ We anticipate that the RST Corpus will be multifunctional and support a wide range of language engineering applications.

To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al (2001). $$$$$ Annotation of a single document could take anywhere from 30 minutes to several hours, depending on the length and topic.
To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al (2001). $$$$$ It is inevitable that any decision on how to bracket elementary discourse units necessarily involves some compromises.
To test this, we used the corpus of discourse trees built in the style of RST by Carlson et al (2001). $$$$$ Two principle goals underpin the creation of this discourse-tagged corpus: 1) The corpus should be grounded in a particular theoretical approach, and 2) it should be sufficiently large enough to offer potential for wide-scale use – including linguistic analysis, training of statistical models of discourse, and other computational linguistic applications.

If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al, 2001). $$$$$ Giacomo Ferrari.
If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al, 2001). $$$$$ That is why annotators typically used this approach on short documents, but resorted to other strategies for longer documents.
If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al, 2001). $$$$$ Moreover, using the RST approach, the resultant tree structure, like a traditional outline, imposed constraints that other discourse representations (e.g., graph) would not.
If no cue phrases are used to signal the relation between two elementary discourse units, an automatic discourse labeler can at best guess that an ELABORATION relation holds between the units, because ELABORATION relations are the most frequently used relations (Carlson et al, 2001). $$$$$ In this paper, we recount our experience in developing a large resource with discourse-level annotation for NLP research.
