SEE allowed the judges to step through predefined units of the model summary (elementary discourse units/EDUs) (Soricut and Marcu, 2003) and for each unit of that summary, mark the sentences in the peer summary that expressed [all (4), most (3), some (2), hardly any (1) or none (0)] of the content in the current model summary unit. $$$$$ Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory (Mann and Thompson, 1988).
SEE allowed the judges to step through predefined units of the model summary (elementary discourse units/EDUs) (Soricut and Marcu, 2003) and for each unit of that summary, mark the sentences in the peer summary that expressed [all (4), most (3), some (2), hardly any (1) or none (0)] of the content in the current model summary unit. $$$$$ The arrows link the satellite to the nucleus of a rhetorical relation.
SEE allowed the judges to step through predefined units of the model summary (elementary discourse units/EDUs) (Soricut and Marcu, 2003) and for each unit of that summary, mark the sentences in the peer summary that expressed [all (4), most (3), some (2), hardly any (1) or none (0)] of the content in the current model summary unit. $$$$$ the structure probabilities and the relation probabilities associated with a tuple in a discourse tree.
SEE allowed the judges to step through predefined units of the model summary (elementary discourse units/EDUs) (Soricut and Marcu, 2003) and for each unit of that summary, mark the sentences in the peer summary that expressed [all (4), most (3), some (2), hardly any (1) or none (0)] of the content in the current model summary unit. $$$$$ We replace the syntactic parse trees produced by Charniakâ€™s parser at 90% accuracy ( ) with the corresponding Penn Treebank syntactic parse trees produced by human annotators ( ).

Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. $$$$$ The performance is assessed using labeled recall and labeled precision as defined by the standard Parseval metric (Black et al., 1991).
Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. $$$$$ We believe that semantic/discourse segmentation is a notoriously under-researched problem.
Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. $$$$$ A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser.
Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. $$$$$ For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles.

Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). $$$$$ We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). $$$$$ We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
Our model was trained and tested on RST-DT (2002) and achieves a performance of up to 86.12% F-Score, which is comparable to Soricut and Marcu (2003). $$$$$ We found it useful to also compact these relations into classes, as described by Carlson et al. (2003), and operate with the resulting 18 labels as well (seen as coarser granularity rhetorical relations).

Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ Our statistical model assigns a segmenting probability for each word , where boundary, no-boundary .
Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ The most likely parse is then given by formula (2).
Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ However, experiments show that there is much to be gained if better discourse segmentation algorithms are found; 83% accuracy on this task is not sufficient for building highly accurate discourse trees.
Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ For each word , the upper-most node with lexical head which has a right sibling node determines the features on the basis of which we decide whether to insert a discourse boundary.

Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). $$$$$ We used this doubly-annotated subset to compute human agreement on the task of discourse structure derivation.
Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). $$$$$ Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory (Mann and Thompson, 1988).
Since segmentation is the first stage of discourse parsing, quality discourse segments are critical to building quality discourse representations (Soricut and Marcu, 2003). $$$$$ Within a rhetorical relation a discourse span is also labeled as either NUCLEUS or SATELLITE.

Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. $$$$$ (See (Carlson et al., 2003) for details concerning the corpus and the annotation process.)
Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. $$$$$ We used this doubly-annotated subset to compute human agreement on the task of discourse structure derivation.
Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. $$$$$ Yet, they built discourse structures at sentence level that are not only consistent with the syntactic structures of sentences, but also derivable from them.
Soricut and Marcu (2003) construct a statistical discourse segmenter as part of their sentence-level discourse parser (SPADE), the only implementation available for our comparison. $$$$$ The baseline algorithms are too simplistic to yield good results (recall figures of 28.2% and 25.4%).

Soricut and Marcu (2003) use syntactic features to identify sentence-internal RST structure. $$$$$ According to our hypothesis, the discourse boundary inserted between the words says and it is best explained not by the words alone, but by the lexicalized syntactic structure [VP(says) [VBZ(says) SBAR(will)]], signaled by the boxed nodes in Figure 2.
Soricut and Marcu (2003) use syntactic features to identify sentence-internal RST structure. $$$$$ Given a syntactic tree , the algorithm inserts a boundary after each word for which boundary .
Soricut and Marcu (2003) use syntactic features to identify sentence-internal RST structure. $$$$$ The models use syntactic and lexical features.
Soricut and Marcu (2003) use syntactic features to identify sentence-internal RST structure. $$$$$ The discourse parsing model we propose uses the dominance set to compute the probability of a discourse parse tree according to formula (4).

The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long sentences. $$$$$ We found it useful to also compact these relations into classes, as described by Carlson et al. (2003), and operate with the resulting 18 labels as well (seen as coarser granularity rhetorical relations).
The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long sentences. $$$$$ The dominance set of a DS-LST is given by all the edu pairs linked through a head node and an attachment node in the DS-LST.
The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long sentences. $$$$$ The span of is ,and set has two elements involving edus from it, namely the dominance relationships and .
The test set includes only sentences for which our English parser (Soricut and Marcu, 2003) could produce a parse tree, which effectively excluded a few very long sentences. $$$$$ Our results suggest that more effort needs to be put on semantic/discourse-based segmentation.

One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). $$$$$ A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). $$$$$ For example, Gildea and Jurafsky (2002) developed statistical models for automatically inducing semantic roles.
One exception is Marcu's work (Marcu, 1997, 1999) (see also Soricut and Marcu (2003) for constructing discourse structures for individual sentences). $$$$$ A simple interpolation method is used for smoothing to accommodate data sparseness.

Within Rhetorical Structure Theory (RST), Soricut and Marcu (2003) have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. $$$$$ The baseline algorithm has a performance of 23.4% and 20.7% F-score, when using 18 labels and 110 labels, respectively.
Within Rhetorical Structure Theory (RST), Soricut and Marcu (2003) have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. $$$$$ Because our model is concerned with discourse segmentation at sentence level, we define boundary , i.e., the sentence boundary is always a discourse boundary as well.
Within Rhetorical Structure Theory (RST), Soricut and Marcu (2003) have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. $$$$$ The arrows link the satellite to the nucleus of a rhetorical relation.
Within Rhetorical Structure Theory (RST), Soricut and Marcu (2003) have developed two probabilistic models for identifying clausal elementary discourse units and generating discourse trees at the sentence level. $$$$$ Horizontal lines correspond to text spans, and vertical lines identify text spans which are nuclei.

Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ The counts for the dependency sets are also smoothed using symbolic names for the edu identifiers and accounting only for the distance between them.
Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ An example of a discourse structure is the tree given in Figure 1.
Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003). $$$$$ Fortunately, recent annotation projects have taken significant steps towards developing semantic (Fillmore et al., 2002; Kingsbury and Palmer, 2002) and discourse (Carlson et al., 2003) annotated corpora.

their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). $$$$$ In our running example, for ENABLEMENT-NS , .
their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). $$$$$ Our discourse parser implements a classical bottom-up algorithm.
their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). $$$$$ The corpus comes conveniently partitioned into a Training set of 347 articles (6132 sentences) and a Test set of 38 articles (991 sentences).

Soricut and Marcu (2003) also build up RST sentential trees to use in discourse parsing. $$$$$ The most interesting finding is that these dominance relations encode sufficient information to enable the derivation of discourse structures that are almost indistinguishable from those built by human annotators.
Soricut and Marcu (2003) also build up RST sentential trees to use in discourse parsing. $$$$$ This is even more remarkable given that the discourse corpus (RST-DT, 2002) was built with no syntactic theory in mind.
Soricut and Marcu (2003) also build up RST sentential trees to use in discourse parsing. $$$$$ Arrows are labeled with the name of the rhetorical relation that holds between the linked units.

Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks. $$$$$ The models use syntactic and lexical features.
Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks. $$$$$ Our evaluation shows that our discourse model is sophisticated enough to match near-human levels of performance.
Though statistical methods have been used to induce such trees (Soricut and Marcu, 2003), they are not used for ordering and other text-structuring tasks. $$$$$ A simple interpolation method is used for smoothing to accommodate data sparseness.

(Soricut and Marcu, 2003) and (Polanyi et al., 2004) implement models to perform discourse parsing. $$$$$ The absence of semantic and discourse annotated corpora prevented similar developments in semantic/discourse parsing.

A discourse tree (Soricut and Marcu, 2003). $$$$$ Therefore, our Training section consists of a set of 5809 triples of the form which are used to train the parameters of the statistical models.
A discourse tree (Soricut and Marcu, 2003). $$$$$ In the case of (the probability of the relation ), we keep both the lexical heads and the syntactic labels, but filter out the edu identifiers (clearly, the relation between two spans does not depend on the positions of the spans involved); also, we filter out all the elements of whose dominance relationship does not hold across the two sub-spans of .
A discourse tree (Soricut and Marcu, 2003). $$$$$ A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.
A discourse tree (Soricut and Marcu, 2003). $$$$$ Our statistical approach to sentence segmentation uses two components: a statistical model which assigns a probability to the insertion of a discourse boundary after each word in a sentence, and a segmenter, which uses the probabilities computed by the model for inserting discourse boundaries.

Soricut and Marcu (2003) introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels. $$$$$ This is even more remarkable given that the discourse corpus (RST-DT, 2002) was built with no syntactic theory in mind.
Soricut and Marcu (2003) introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels. $$$$$ Discourse segmentation is the process in which a given text is broken into non-overlapping segments called elementary discourse units (edus).
Soricut and Marcu (2003) introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels. $$$$$ We introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees.
Soricut and Marcu (2003) introduce a statistical discourse segmenter, which is trained on RST DT to label words with boundary or no-boundary labels. $$$$$ The exception edu is edu 1.

Like Soricut and Marcu (2003), they formulate the discourse segmentation task as a binary classification problem of deciding whether a word is the boundary or no-boundary of EDUs. $$$$$ The second baseline ( ) uses syntactic information; because long sentences often have embedded sentences, inserts discourse boundaries after each text span whose corresponding syntactic subtree is labeled S, SBAR, or STNV.
Like Soricut and Marcu (2003), they formulate the discourse segmentation task as a binary classification problem of deciding whether a word is the boundary or no-boundary of EDUs. $$$$$ The first baseline ( ) uses punctuation to determine when to insert a boundary; because commas are often used to indicate breaks inside long sentences, inserts discourse boundaries after each comma.
Like Soricut and Marcu (2003), they formulate the discourse segmentation task as a binary classification problem of deciding whether a word is the boundary or no-boundary of EDUs. $$$$$ Given a set of adequate parameters , our discourse model estimates the goodness of a discourse parse tree using formula (3). ce set extracted from a DS-LST. goodness of the structure of We expect these probabilities to prefer the hierarchical structure (1, (2, 3)) over ((1,2), 3) for the discourse tree in Figure 1.
Like Soricut and Marcu (2003), they formulate the discourse segmentation task as a binary classification problem of deciding whether a word is the boundary or no-boundary of EDUs. $$$$$ All probabilities used were estimated from our training corpus.

Soricut and Marcu (2003) and Subba and Di Eugenio (2007) use boundary labels, which are assigned to words at the end of EDUs. $$$$$ Horizontal lines correspond to text spans, and vertical lines identify text spans which are nuclei.
Soricut and Marcu (2003) and Subba and Di Eugenio (2007) use boundary labels, which are assigned to words at the end of EDUs. $$$$$ A discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8% over a state-ofthe-art decision-based discourse parser.
Soricut and Marcu (2003) and Subba and Di Eugenio (2007) use boundary labels, which are assigned to words at the end of EDUs. $$$$$ Horizontal lines correspond to text spans, and vertical lines identify text spans which are nuclei.
Soricut and Marcu (2003) and Subba and Di Eugenio (2007) use boundary labels, which are assigned to words at the end of EDUs. $$$$$ A set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance.

SPADE is the work of Soricut and Marcu (2003). $$$$$ We compare the performance of our probabilistic discourse segmenter with the performance of the decisionbased segmenter proposed by (Marcu, 2000) and the performance of two baseline algorithms.
SPADE is the work of Soricut and Marcu (2003). $$$$$ A good model of discourse segmentation needs to account both for local interactions at the word level and for global interactions at more abstract levels.
SPADE is the work of Soricut and Marcu (2003). $$$$$ Our results suggest that more effort needs to be put on semantic/discourse-based segmentation.
SPADE is the work of Soricut and Marcu (2003). $$$$$ Each document in the corpus is paired with a discourse structure (tree) that was manually built in the style of Rhetorical Structure Theory (Mann and Thompson, 1988).
