Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically.
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ Surprisingly, the simple NN method performs very similarly to the more sophisticated IB.
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ Our investigation revealed that polysemy has a considerable impact on the clusters formed: polysemic verbs with a clear predominant sense and those with similar regular polysemy are frequently classified together.
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ Nevertheless, the IB method has the clear advantage that it allows for more clusters to be produced.

The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the potential and limitations of clustering in inducing semantic information from polysemic SCF data.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ For this reason, we decided to use only K(V ) as output and defer a further exploration of the soft output to future work.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ The test verbs and their classes are shown in table 1.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ The IB quantifies the relevance information of a SCF distribution with respect to output clusters, through their mutual information I(Clusters; SCFs).

Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.
Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ In contrast to earlier work, we give special emphasis to polysemy.
Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ We also plan to work on improving the accuracy of subcategorization acquisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different approaches in clustering, developing our gold standard classification further, and extending our experiments to a larger number of verbs and verb classes.

Table 1 $$$$$ It outputs only one clustering configuration, and therefore does not allow examination of different cluster granularities.
Table 1 $$$$$ Data clustering is a process which aims to partition a given set into subsets (clusters) of elements that are similar to one another, while ensuring that elements that are not similar are assigned to different clusters.
Table 1 $$$$$ A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying
Table 1 $$$$$ The monosemous one lists only a single sense for each test verb, that corresponding to its predominant (most frequent) sense in WordNet.

We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ Section 3 describes the method for subcategorization acquisition and section 4 presents the approach to clustering.
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ Let denote the most probable cluster of a verb V .
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ The scope was restricted to these two frames to prevent sparse data problems in clustering.
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ Data clustering is a process which aims to partition a given set into subsets (clusters) of elements that are similar to one another, while ensuring that elements that are not similar are assigned to different clusters.

We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ 3) Problems in SCF acquisition: These were not numerous but occurred e.g. when the system could not distinguish between different control (e.g. subject/object equi/raising) constructions (B3).
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ The classes are indicated by number codes from the classifications of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The predominant sense is indicated by bold font.
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ The relevance information is maximized, while the compression information I(Clusters; Verbs) is minimized.

 $$$$$ This incorporates Levin’s classes, 26 additional classes by Dorr (1997)1, and 57 new classes for verb types not covered comprehensively by Levin or Dorr.
 $$$$$ We employed as a gold standard a substantially extended version of Levin’s classification constructed by Korhonen (2003).
 $$$$$ We first evaluated the clusters against the predominant sense, i.e. using the monosemous gold standard.
 $$$$$ A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying

By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ Homonymic verbs or verbs with strong irregular polysemy tend to resist any classification.

We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.
We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ Being based on pairwise similarities, it shows better performance than IB on the pairwise measure.
We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying
We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ This incorporates Levin’s classes, 26 additional classes by Dorr (1997)1, and 57 new classes for verb types not covered comprehensively by Levin or Dorr.

We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997).
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ 78 of these ‘coarse-grained’ SCFs appeared in our data.
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ Hence, we use SCFs as relevance features to guide the clustering process.6 comparing the probability of a randomly chosen pair of verbs verbi and verbj to share the same predominant sense (4.5%) with the probability obtained when verbj is the JS-divergence We chose two clustering methods which do not involve task-oriented tuning (such as pre-fixed thresholds or restricted cluster sizes) and which approach data straightforwardly, in its distributional form: (i) a simple hard method that collects the nearest neighbours (NN) of each verb (figure 1), and (ii) the Information Bottleneck (IB), an iterative soft method (Tishby et al., 1999) based on information-theoretic grounds.

For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997).
For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ To ensure that our gold standard covers all (or most) senses of these verbs, we looked into WordNet (Miller, 1990) and assigned all the WordNet senses of the verbs to gold standard classes.2 Two versions of the gold standard were created: monosemous and polysemic.
For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ We discuss our gold standards and the choice of test verbs in section 2.

Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour.
Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ This factor compensates for a bias towards small clusters.
Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ Our investigation revealed that polysemy has a considerable impact on the clusters formed: polysemic verbs with a clear predominant sense and those with similar regular polysemy are frequently classified together.
Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ In contrast to previous work, we particularly focus on clustering polysemic verbs.

 $$$$$ Hence, we use SCFs as relevance features to guide the clustering process.6 comparing the probability of a randomly chosen pair of verbs verbi and verbj to share the same predominant sense (4.5%) with the probability obtained when verbj is the JS-divergence We chose two clustering methods which do not involve task-oriented tuning (such as pre-fixed thresholds or restricted cluster sizes) and which approach data straightforwardly, in its distributional form: (i) a simple hard method that collects the nearest neighbours (NN) of each verb (figure 1), and (ii) the Information Bottleneck (IB), an iterative soft method (Tishby et al., 1999) based on information-theoretic grounds.
 $$$$$ Let denote the most probable cluster of a verb V .
 $$$$$ When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.
 $$$$$ It outputs only one clustering configuration, and therefore does not allow examination of different cluster granularities.

There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ We obtain our SCF data using the subcategorization acquisition system of Briscoe and Carroll (1997).
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) > 90% which makes the output clustering approximately hard.
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the potential and limitations of clustering in inducing semantic information from polysemic SCF data.

In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ However, it is likely that the IB performance suffered due to our choice of test data.
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold standard.

We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ The scope was restricted to these two frames to prevent sparse data problems in clustering.
We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ This ensures optimal compression of data through clusters.
We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.

The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ The monosemous one lists only a single sense for each test verb, that corresponding to its predominant (most frequent) sense in WordNet.
The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ The SCFs abstract over specific lexicallygoverned particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement.
The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ For this reason, we decided to use only K(V ) as output and defer a further exploration of the soft output to future work.
The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ We employed as a gold standard a substantially extended version of Levin’s classification constructed by Korhonen (2003).

Korhonen et al (2003) observed the opposite with general language data. $$$$$ Levin’s taxonomy of verbs and their classes (Levin, 1993) is the largest syntactic-semantic verb classification in English, employed widely in evaluation of automatic classifications.
Korhonen et al (2003) observed the opposite with general language data. $$$$$ In contrast to previous work, we particularly focus on clustering polysemic verbs.
Korhonen et al (2003) observed the opposite with general language data. $$$$$ The tradeoff between the two constraints is realized nearest neighbour of verbi (36%). through minimizing the cost term: where Q is a parameter that balances the constraints.
Korhonen et al (2003) observed the opposite with general language data. $$$$$ Hence, the relevance information grows with K. Accordingly, we consider as the most informative output configurations those for which the relevance information increases more sharply between K − 1 and K clusters than between

This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ Let denote the most probable cluster of a verb V .
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ A principled evaluation scheme was introduced which enabled us to investigate the effect of polysemy on the resulting classification.
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ The lexicon was evaluated against manually analysed corpus data after an empirically defined threshold of 0.025 was set on relative frequencies of SCFs to remove noisy SCFs.
