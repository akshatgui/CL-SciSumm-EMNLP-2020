Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ We use clustering for partitioning a set of verbs.
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ The former are noticeably better than the latter (e.g.
Schulteim Walde and Brew (2002) used the k-Means (Forgy, 1965) algorithm to cluster SCF distributions for monose mous verbs while Korhonen et al (2003) applied other clustering methods to cluster polysemic SCF data. $$$$$ The scope was restricted to these two frames to prevent sparse data problems in clustering.

The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated SCF data semantically.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ 5.4) the measures we use are also applicable for evaluation against a polysemous gold standard.
The most closely related work to our polysemy aware task of unsupervised verb class induction is the work of Korhonen et al (2003), who used distributions of sub categorization frames to cluster verbs. $$$$$ A principled evaluation scheme was introduced which enabled us to investigate the effect of polysemy on the resulting classification.

Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ For this reason, we decided to use only K(V ) as output and defer a further exploration of the soft output to future work.
Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ A SCF lexicon was acquired using this system from the British National Corpus (Leech, 1992, BNC) so that the maximum of 7000 citations were used per test verb.
Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ The input data to clustering was obtained from the automatically acquired SCF lexicon for our 110 test verbs (section 2).
Korhonen et al (2003) evaluated hard clusterings based on a gold standard with multiple classes per verb. $$$$$ This ensures optimal compression of data through clusters.

Table 1: An excerpt of the gold-standard verb classes for several verbs from Korhonen et al (2003). $$$$$ It is also highly sensitive to noise.
Table 1: An excerpt of the gold-standard verb classes for several verbs from Korhonen et al (2003). $$$$$ It outputs only one clustering configuration, and therefore does not allow examination of different cluster granularities.
Table 1: An excerpt of the gold-standard verb classes for several verbs from Korhonen et al (2003). $$$$$ In this way, the IB yields structural information beyond clustering.

We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ In this paper, we focus on the particular task of classifying subcategorization frame (SCF) distributions in a semantically motivated manner.
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold standard.
We first evaluate our induced verb classes on the test set created by Korhonen et al (2003) (Table 1 of their paper) which was created by considering verb polysemy on the basis of Levin's classes and the LCS database (Dorr, 1997). $$$$$ For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) > 90% which makes the output clustering approximately hard.

We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ A principled evaluation scheme was introduced which enabled us to investigate the effect of polysemy on the resulting classification.
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.
We first implemented a soft clustering method for verb class induction proposed by Korhonen et al (2003). $$$$$ A1), indicating that clustering monosemous verbs is fairly ‘easy’.

 $$$$$ While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the potential and limitations of clustering in inducing semantic information from polysemic SCF data.
 $$$$$ In contrast to previous work, we particularly focus on clustering polysemic verbs.
 $$$$$ This involved applying the NN and IB methods to cluster polysemic SCF distributions extracted from corpus data using Briscoe and Carroll’s (1997) system.

By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ We propose a novel approach, which involves: (i) obtaining SCF frequency information from a lexicon extracted automatically using the comprehensive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this information.
By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ We chose for evaluation the IB results for K = 25, 35 and 42.
By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ We describe a new approach which involves clustering subcategorizaframe distributions using the Information Bottleneck and nearest neighbour methods.
By following the method of Korhonen et al (2003), prepositional phrases (pp) are parameterized for two frequent sub categorization frames (NP and NP PP), and the unfiltered raw frequencies of subcategorization frames are used as features to represent a verb. $$$$$ A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying

We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ When we removed the filtering threshold, and evaluated the noisy distribution, F-measure4 dropped from 44.9 to 38.51.5
We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the potential and limitations of clustering in inducing semantic information from polysemic SCF data.
We evaluate the single-class output for each verb based on the predominant gold-standard classes, which are defined for each verb in the test set of Korhonen et al (2003). $$$$$ In contrast to previous work, we particularly focus on clustering polysemic verbs.

We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) > 90% which makes the output clustering approximately hard.
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ Although it is quite extensive, it is not exhaustive.
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ Therefore we employed the more sophisticated IB method as well.
We evaluate these single-class outputs in the same manner as Korhonen et al (2003), using the gold standard with multiple classes, which we also use for our multi-class evaluations. $$$$$ These were obtained by parameterizing two high frequency SCFs for prepositions: the simple PP and NP + PP frames.

For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ Hence, we use SCFs as relevance features to guide the clustering process.6 comparing the probability of a randomly chosen pair of verbs verbi and verbj to share the same predominant sense (4.5%) with the probability obtained when verbj is the JS-divergence We chose two clustering methods which do not involve task-oriented tuning (such as pre-fixed thresholds or restricted cluster sizes) and which approach data straightforwardly, in its distributional form: (i) a simple hard method that collects the nearest neighbours (NN) of each verb (figure 1), and (ii) the Information Bottleneck (IB), an iterative soft method (Tishby et al., 1999) based on information-theoretic grounds.
For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ 78 of these ‘coarse-grained’ SCFs appeared in our data.
For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ When we removed the filtering threshold, and evaluated the noisy distribution, F-measure4 dropped from 44.9 to 38.51.5
For baselines, we once more adopt the Nearest Neighbor (NN) and Information Bottleneck (IB) methods proposed by Korhonen et al (2003), and LDA-frames proposed by Materna (2012). $$$$$ We also plan to work on improving the accuracy of subcategorization acquisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different approaches in clustering, developing our gold standard classification further, and extending our experiments to a larger number of verbs and verb classes.

Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ Classifications which aim to capture the close relation between the syntax and semantics of verbs have attracted a considerable research interest in both linguistics and computational linguistics (e.g.
Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ (Jackendoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)).
Korhonen et al (2003) reported that the highest modified purity was 49% against predominant classes and 60% against multiple classes. $$$$$ In addition, a set of 160 fine grained frames were employed.

 $$$$$ Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not).
 $$$$$ When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.
 $$$$$ Levin’s taxonomy of verbs and their classes (Levin, 1993) is the largest syntactic-semantic verb classification in English, employed widely in evaluation of automatic classifications.

There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ We chose for evaluation the IB results for K = 25, 35 and 42.
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ The IB quantifies the relevance information of a SCF distribution with respect to output clusters, through their mutual information I(Clusters; SCFs).
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) > 90% which makes the output clustering approximately hard.
There are a few exceptions to this tradition, such as Pereira et al (1993), Rooth et al (1999), Korhonen et al (2003), who used soft clustering methods for multiple assignment to verb semantic classes. $$$$$ Although it is quite extensive, it is not exhaustive.

In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ Table 3 shows the results against polysemic and monosemous gold standards.
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ This ensures optimal compression of data through clusters.
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ When we removed the filtering threshold, and evaluated the noisy distribution, F-measure4 dropped from 44.9 to 38.51.5
In this paper, we extend an existing approach to lexical classification (Korhonen et al, 2003) and apply it (without any domain specific tuning) to the domain of biomedicine. $$$$$ Homonymic verbs or verbs with strong irregular polysemy tend to resist any classification.

We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ Hence, the relevance information grows with K. Accordingly, we consider as the most informative output configurations those for which the relevance information increases more sharply between K − 1 and K clusters than between
We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ We use clustering for partitioning a set of verbs.
We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ The IB method gives an indication of the most informative values of K.7 Intensifying the weight Q attached to the relevance information I(Clusters; SCFs) allows us to increase the number K of distinct clusters being produced (while too small Q would cause some of the output clusters to be identical to one another).
We extended the system of Korhonen et al (2003) with additional clustering techniques (introduce din sections 3.2.2 and 3.2.4) and used it to obtain the classification for the biomedical domain. $$$$$ As the method is global, it performs better when the target classes are represented by a high number of verbs.

The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ Homonymic verbs or verbs with strong irregular polysemy tend to resist any classification.
The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.
The closest possible comparison point is (Korhonen et al, 2003) which reported 50-59% mPUR and 15-19% APP on using IB to assign 110 polysemous (general language) verbs into 34 classes. $$$$$ Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997), document classification (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002).

Korhonen et al (2003) observed the opposite with general language data. $$$$$ In the future, we plan to investigate the use of soft clustering (without hardening the output) and develop methods for evaluating the soft output against polysemous gold standards.
Korhonen et al (2003) observed the opposite with general language data. $$$$$ Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data.
Korhonen et al (2003) observed the opposite with general language data. $$$$$ The method yielded 71.8% precision and 34.5% recall.
Korhonen et al (2003) observed the opposite with general language data. $$$$$ In contrast to previous work, we particularly focus on clustering polysemic verbs.

This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ We associate with each cluster its most prevalent semantic class, and denote the number of verbs in a cluster K that take its prevalent class by nprevalent(K).
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ These observations confirm that evaluation against a polysemic gold standard is necessary in order to fully explain the results from clustering.
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ A2).
This use of frame is different than that used for subcate gorization frames, which are also used to induce word classes (e.g., Korhonen et al, 2003). $$$$$ When the weight of relevance grows, the assignment to clusters is more constrained and p(K|V ) becomes more similar to hard clustering.
