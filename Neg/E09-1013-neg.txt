More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ The latter is used to derive a mapping of the induced senses to the gold standard labels.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003).
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Graph-based methods have also been applied to the sense induction task.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed.

(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.
(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ Sense induction seeks to automatically identify word senses directly from a corpus.
(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ Placing these values in Equation 4 we obtain the following: #m+S·α Putting it all together, we arrive at the final update equation for the Gibbs sampling: Note that when dealing with a single layer, Equation 8 collapses to: where #m(si) indicates the number of elements (e.g., words) in the context window assigned to sense si.

Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ In the future, we hope to explore more rigorous parameter estimation techniques.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.

Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Because we are dealing with multiple layers, there is an element of overlap involved.
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.

We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text.
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task.
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ This is the purpose of the Gibbs sampling procedure.

This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.
This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ This is identical to the update equation in the original, word-based LDA model.
This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.

Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.
Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ This distributes α according to the relative size of each layer in the instance.
Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s. Each word wi in the context window is generated by first sampling a sense from the sense distribution, then choosing a word from the sense-context distribution.
Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.

LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation).
LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.
LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ Our underlying assumption is that the context window around the target word can have multiple representations, all of which share the same sense distribution.

Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ Rl is the Dirichlet prior for the featuresense distribution � in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).

It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.
It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ This distributes α according to the relative size of each layer in the instance.
It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ The mapping is then used to calculate the system’s F-Score on the test corpus.

In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).
In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ Sense induction seeks to automatically identify word senses directly from a corpus.
In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ Sense induction is typically treated as an unsupervised clustering problem.

Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ The first idea that comes to mind, is to use the same model while treating various features as word-like elements.
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ Our system labels each instance with the single, most probable sense.
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ In these cases, the layer contributes a close-to-uniform estimation of the sense distribution, which confuses the combined model.

We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). $$$$$ The sparser layers, notably word n-grams and dependencies, fare comparatively worse.
We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). $$$$$ Treating each layer individually, we integrate over the possible values of θ, obtaining a similar count-based term: where #l(si) indicates the number of elements in layer l assigned the sense si, #l indicates the number of elements in layer l, i.e., the size of the layer and S the number of senses.
We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). $$$$$ Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.
We consider three latent models: the Singular Value Decomposition (SVD) (Schu?tze, 1998), Non-negative Matrix Factorization (NMF) (Vande Cruysand Apidianaki, 2011), and Latent Dirichlet Allocation (Brody and Lapata, 2009). $$$$$ Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.

Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ Sense induction seeks to automatically identify word senses directly from a corpus.
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.

Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. $$$$$ The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.
Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. $$$$$ Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.
Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA: Blei et al (2003)) and derivative approaches - and use the topic model to determine the appropriate sense granularity. $$$$$ Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).

In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ It also finds a separate sense for “drug dealing” (Sense 5) and “enforcement” (Sense 8).
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.

In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ This is the purpose of the Gibbs sampling procedure.
In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.
In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ Acknowledgments The authors acknowledge the support of EPSRC (grant EP/C538447/1).

 $$$$$ Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.
 $$$$$ Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.
 $$$$$ The remainder of this paper is structured as follows.
 $$$$$ We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.

Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.

Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ However, in view of our task, we are more interested in estimating θ, the sense-context distribution which can be obtained as in Equation 7, but taking into account all sense assignments, without removing assignment i.
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ Table 1 illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).
