More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.
More related to our work are (Brody and Lapata, 2009) or (Toutanova and Johnson, 2008) who use LDA-based models which induce latent variables from task-specific data rather than from simple documents. $$$$$ Our system labels each instance with the single, most probable sense.

(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.
(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).
(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.
(Brody and Lapata, 2009) apply such a model for word sense induction on a set of 35 target nouns. $$$$$ We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.

Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ Then, the word sense is selected based on the word, neighbor, and topic.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ The first idea that comes to mind, is to use the same model while treating various features as word-like elements.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.
Theoretically, their latent variable formulation has served as a foundation for more robust models of other linguistic phenomena (Brody and Lapata, 2009). $$$$$ Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).

Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts.
Tiered clustering is a discrete clustering method, as opposed to methods such as (Brody and Lapata, 2009) that assign a distribution of word senses to each word instance. $$$$$ Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies).

We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ It also finds a separate sense for “drug dealing” (Sense 5) and “enforcement” (Sense 8).
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ In this case, however, all layers must be considered: Here λl is the weight for the contribution of layer l, and αl is the portion of the Dirichlet prior for the sense distribution θ in the current layer.
We extracted pseudo documents from a 10-word window centered on the corresponding word token for each word type following Brody and Lapata (2009). $$$$$ We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.

This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ Considerable latitude is allowed in selecting and representing the cooccurring contexts.
This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.
This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ Our system labels each instance with the single, most probable sense.
This introduces the complication of choosing the right number of possible senses, hence a Bayesian approach to WSI was proposed which deals with this problem within a principled generative framework (Brody and Lapata, 2009). $$$$$ Sense induction seeks to automatically identify word senses directly from a corpus.

Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.
Topic models have also been applied to other classes of semantic task, for example word sense disambiguation (Li et al., 2010), word sense induction (Brody and Lapata, 2009) and modelling human judgements of semantic association (Griffiths et al, 2007). $$$$$ This paper presents a novel Bayesian approach to sense induction.

LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.
LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.
LDA model has also been applied to WSI (Brody and Lapata, 2009). $$$$$ In this case, however, all layers must be considered: Here λl is the weight for the contribution of layer l, and αl is the portion of the Dirichlet prior for the sense distribution θ in the current layer.

Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ Considerable latitude is allowed in selecting and representing the cooccurring contexts.
Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009). $$$$$ This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.

It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.
It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ Under the second scheme, the gold standard is partitioned into a test and training corpus.
It has been demonstrated to be highly effective in a wide range of tasks, including multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008), information retrieval (Wei and Croft, 2006) and image labelling (Feng and Lapata, 2010). $$$$$ From these assignments, we must determine the sense distribution of the instance as a whole.

In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.
In the short time since its inception, topic modelling (Blei et al, 2003) has become a mainstream technique for tasks as diverse as multi document summarisation (Haghighi and Vanderwende, 2009), word sense discrimination (Brody and Lapata, 2009), sentiment analysis (Titov and McDonald, 2008) and information retrieval (Wei and Croft, 2006). $$$$$ The second, in-domain, corpus was built from selected portions of the Wall Street Journal.

Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.
Later, Brody and Lapata (2009) combined different feature sets using a probabilistic Word Sense Induction model and found that only some combinations produced an improved system. $$$$$ The second, in-domain, corpus was built from selected portions of the Wall Street Journal.

We consider three latent models $$$$$ Sense 1 corresponds to the “enforcement” sense of drug, Sense 2 refers to “medication”, Sense 3 to the “drug industry” and Sense 4 to “drugs research”.
We consider three latent models $$$$$ Sense induction seeks to automatically identify word senses directly from a corpus.
We consider three latent models $$$$$ Layers containing more elements (e.g.

Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ Sense induction seeks to automatically identify word senses directly from a corpus.
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ Specifically, in order to derive the update function used in the Gibbs sampler, we must provide the conditional probability of the i-th variable being assigned sense si in layer l, given the feature value fi of the context variable and the current sense assignments of all the other variables in the data (s−i): p(si|s−i, f) — p(fi|s, f −i,R) · p(si|s−i,a) (2) The probability of a single sense assignment, si, is proportional to the product of the likelihood (of feature fi, given the rest of the data) and the prior probability of the assignment. smaller ones (e.g.

Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA $$$$$ Under this premise, we should expect different senses to be signaled by different lexical distributions.
Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA $$$$$ The latter is used to derive a mapping of the induced senses to the gold standard labels.
Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA $$$$$ Considerable latitude is allowed in selecting and representing the cooccurring contexts.
Building on the work of Brody and Lapata (2009) and others, we approach WSI via topic modelling - using Latent Dirichlet Allocation (LDA $$$$$ The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.

In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ In the future, we hope to explore more rigorous parameter estimation techniques.
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ The general trends are similar.
In the remainder of this section, we refer to Brody and Lapata (2009) as BL, and Yao and Durme (2011) as YVD. $$$$$ We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.

In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ We experimented with values ranging from three to nine senses.
In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.
In their work, they model the target word instances as samples from a multinomial distribution over senses which are successively characterized as distributions over words (Brody and Lapata, 2009). $$$$$ Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984).

 $$$$$ This paper presents a novel Bayesian approach to sense induction.
 $$$$$ Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.
 $$$$$ Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account.
 $$$$$ This is the purpose of the Gibbs sampling procedure.

Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ Sense induction is the task of discovering automatically all possible senses of an ambiguous word.
Results are shown through comparison against Latent Dirichlet Allocation (LDA), a parametric Bayesian model employed by Brody and Lapata (2009) for this task. $$$$$ This paper presents a novel Bayesian approach to sense induction.

Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ Then, the word sense is selected based on the word, neighbor, and topic.
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 2007 (Agirre and Soroa, 2007). $$$$$ Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.
