 $$$$$ Lev Ratinov was supported by the Air Force Research Laboratory (AFRL) under prime contract no.
 $$$$$ That is, if we have induced an embedding for 12/3/2008 , we will use the embedding of 12/3/2008 , and *DD*/*D*/*DDDD* in the baseline features listed above.
 $$$$$ In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking.
 $$$$$ After choosing hyperparameters to maximize the dev F1, we would retrain the model using these hyperparameters on the full 8936 sentence training set, and evaluate on test.

Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). $$$$$ Lev Ratinov was supported by the Air Force Research Laboratory (AFRL) under prime contract no.
Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). $$$$$ One can map F to matrix f of size W x d, where d << C, using some function g, where f = g(F). fw represents word w as a vector with d dimensions.
Unless stated otherwise we use word-vectors of size 50, initialized using the embeddings provided by Turian et al (2010) based on the model of Collobert and Weston (2008). $$$$$ Joseph Turian and Yoshua Bengio acknowledge the following agencies for research funding and computing support: NSERC, RQCHP, CIFAR.

In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). $$$$$ In Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.
In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). $$$$$ Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier.
In the first experiment, we use the semi-supervised training strategy described previously and initialize our models with the embeddings provided by Turian et al (2010). $$$$$ Ratinov and Roth (2009) describe different sequence encoding like BILOU and BIO, and show that the BILOU encoding outperforms BIO, and the greedy inference performs competitively to Viterbi while being significantly faster.

Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. $$$$$ A word representation is a mathematical object associated with each word, often a vector.
Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. $$$$$ We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.
Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. $$$$$ We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008).
Follow (Turian et al., 2010), we randomly initialize all parameters to [-0.1, 0.1], and use stochastic gradient descent to minimize the ranking loss with a fixed learning rate 0.01. $$$$$ In Turian et al. (2009), we hypothesized on the basis of solely the HLBL NER curve that higher-dimensional word embeddings would give higher accuracy.

Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. $$$$$ Brown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004; Liang, 2005; Ratinov & Roth, 2009), PCFG parsing (Candito & Crabb´e, 2009), dependency parsing (Koo et al., 2008; Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009).
Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. $$$$$ This supports our hypothesis that, for rare words, Brown clustering produces better representations than word embeddings that haven’t received sufficient training updates.
Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. $$$$$ We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences.
Just as monolingual word clusters are broadly applicable as feature sin monolingual models for linguistic structure prediction (Turian et al, 2010), the resulting cross-lingual word clusters can be used as features in various cross lingual direct transfer models. $$$$$ If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.

256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. $$$$$ Goldberg et al. (2009) use an HMM to assign POS tags to words, which in turns improves the accuracy of the PCFG-based Hebrew parser.
256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. $$$$$ This is similar in magnitude to our experiments.
256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. $$$$$ We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and—for NER—Lin and Wu (2009).
256 cross-lingual word clusters and the same feature templates as Ta?ckstro?m et al (2012), with the exception that the transition factors are not conditioned on the input. The features used are similar to those used by Turian et al (2010), but include cross-lingual rather than monolingual word clusters. $$$$$ One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical.

Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. $$$$$ But, if only one word representation is to be used, Brown clusters have the highest accuracy.
Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. $$$$$ Suzuki and Isozaki (2008) present a semisupervised extension of CRFs.
Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. $$$$$ You can download word features, for use in existing NLP systems, as well as our here:
Each term's feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. $$$$$ This delexicalization is performed separately from using the word representation.

Further details and evaluations of these embeddings are discussed in Turian et al (2010). $$$$$ Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?).
Further details and evaluations of these embeddings are discussed in Turian et al (2010). $$$$$ The word embeddings, however, are real numbers that are not necessarily in a bounded range.
Further details and evaluations of these embeddings are discussed in Turian et al (2010). $$$$$ The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing.
Further details and evaluations of these embeddings are discussed in Turian et al (2010). $$$$$ Ours is the first work to systematically compare different word representations in a controlled way.

As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). $$$$$ Deschacht and Moens (2009) use a latent-variable language model to improve semantic role labeling.
As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). $$$$$ Unlabeled data is used for inducing the word representations.
As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). $$$$$ Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing.
As the dataset is rather small, we use lower-dimensional word vectors with d=32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al, 2010). $$$$$ A word representation is a mathematical object associated with each word, often a vector.

Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). $$$$$ We use the following baseline set of features When using the lexical features, we normalize dates and numbers.
Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). $$$$$ The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando & Zhang, 2005; Suzuki & Isozaki, 2008; Suzuki et al., 2009).
Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). $$$$$ We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.
Each dimension of the word embeddings expresses a latent feature of the words, hopefully reflecting useful semantic and syntactic regularities (Turian et al, 2010). $$$$$ Fw has dimensionality W, which can be too large to use Fw as features for word w in a supervised model.

We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. $$$$$ Joseph Turian and Yoshua Bengio acknowledge the following agencies for research funding and computing support: NSERC, RQCHP, CIFAR.
We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
We evaluate C&W word embeddings with 25, 50 and 100 dimensions as well as HLBL word embeddings with 50 and 100 dimensions that are introduced in Turian et al (2010) and can be downloaded here. $$$$$ Accordingly, we use greedy inference and BILOU text chunk representation.

This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. $$$$$ You can download word features, for use in existing NLP systems, as well as our here:
This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. $$$$$ Kaski (1998) uses this technique to produce 100-dimensional representations of documents.
This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. $$$$$ We minimize this loss stochastically over the n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table.
This is because the RCV1 corpus used to induce the word embeddings (Turian et al, 2010) does not cover spoken language words in cts very well. $$$$$ In the last section, we show how many unlabeled words were used.

The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). $$$$$ Thank you to Magnus Sahlgren, Bob Carpenter, Percy Liang, Alexander Yates, and the anonymous reviewers for useful discussion.
The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). $$$$$ You can download word features, for use in existing NLP systems, as well as our here:
The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). $$$$$ Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S´en´ecal (2003).
The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al (2010) and Collobert and Weston (2008)). $$$$$ Another approach to word representation is to learn a distributed representation.

Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. $$$$$ Given an n-gram, the model concatenates the embeddings of the n − 1 first words, and learns a linear model to predict the embedding of the last word.
Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. $$$$$ Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for development.
Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. $$$$$ Thank you to Magnus Sahlgren, Bob Carpenter, Percy Liang, Alexander Yates, and the anonymous reviewers for useful discussion.
Experiments with the Brown clusters (Brown et al, 1992) provided by Turian et al (2010) in lieu of suffixes were not promising. $$$$$ We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features.

Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. $$$$$ One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context.
Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. $$$$$ After cleaning, there are 37 million words (58% of the original) in 1.3 million sentences (41% of the original).
Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. $$$$$ This leads to a one-hot representation over a smaller vocabulary size.
Following Turian et al (2010), we use Percy Liang's implementation of this algorithm for our comparison, and we test runs with 100, 320, and 1000 clusters. $$$$$ We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.

We downloaded these embeddings from Turian et al (2010). $$$$$ However, the one-hot representation of a word suffers from data sparsity: Namely, for words that are rare in the labeled training data, their corresponding model parameters will be poorly estimated.
We downloaded these embeddings from Turian et al (2010). $$$$$ One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a significant increase in accuracy.
We downloaded these embeddings from Turian et al (2010). $$$$$ Replicating our experiments You can visit http://metaoptimize.com/ projects/wordreprs/ to find: The word representations we induced, which you can download and use in your experiments; The code for inducing the word representations, which you can use to induce word representations on your own data; The NER and chunking system, with code for replicating our experiments.
We downloaded these embeddings from Turian et al (2010). $$$$$ For this reason, NER results that use RCV1 word representations are a form of transductive learning.

Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). $$$$$ They perform hard clustering using the Viterbi algorithm.
Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). $$$$$ The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992).
Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). $$$$$ The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al., 2008; Ratinov & Roth, 2009).
Besides language modeling, word embeddings induced by neural language models have been useful in chunking, NER (Turian et al, 2010), parsing (Socher et al, 2011b), sentiment analysis (Socher et al., 2011c) and paraphrase detection (Socher et al, 2011a). $$$$$ This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences.

For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. $$$$$ So it is a class-based bigram language model.
For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. $$$$$ We find further by combining word representations.
For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. $$$$$ It runs in time O(V·K2), where V is the size of the vocabulary and K is the number of clusters.
For the Brown algorithm, we are contrasting cluster count choices of 320 and 1000, based on reports of other successful applications [Turian et al, 2010], with clustering models trained on monolingual data from the Europarl corpus and the News Commentary corpus. $$$$$ One approach that is becoming popular is to use unsupervised methods to induce word features—or to download word features that have already been induced—plug these word features into an existing system, and observe a significant increase in accuracy.

Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. $$$$$ The cleaned RCV1 corpus has 269K word types.
Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. $$$$$ By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers.
Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. $$$$$ Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation.
Turian et al (2010) show that adapting from CoNLL to MUC-7 (Chinchor, 1998) data (thus between different newswire sources), the best unsupervised feature (Brown clusters) improves F1 from .68 to .79. $$$$$ ˇReh˚uˇrek and Sojka (2010) describe an incremental approach to inducing LSA and LDA topic models over 270 millions word tokens with a vocabulary of 315K word types.

The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature. $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature. $$$$$ (Not to be confused with distributional representations.)
The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature. $$$$$ Thank you to Andriy Mnih for inducing his embeddings on RCV1 for us.
The robustness of this simple approach is well documented; e.g., Turian et al (2010) show that the baseline model (gazetteer features without unsupervised features) produces an F1 of .778 against .788 of the best unsupervised word representation feature. $$$$$ Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging.
