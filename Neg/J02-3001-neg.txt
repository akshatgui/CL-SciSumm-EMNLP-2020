Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. $$$$$ They show promise for a more sophisticated approach to generalizing beyond the relatively small number of frames considered in the tasks.

In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC.
In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ Semantic roles do not seem to be simple functions of a sentence’s syntactic tree structure, and lexical statistics were found to be extremely valuable, as has been the case in other natural language processing applications.
In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ Much else remains to be done to apply the system described here to the interpretation of general text.

Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). $$$$$ Such common frames might allow a question-answering system to take a question like (5) and discover that (6) is relevant in constructing an answer to the question: This shallow semantic level of interpretation has additional uses outside of generalizing information extraction, question answering, and semantic dialogue systems.
Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). $$$$$ Integrating semantic interpretation and syntactic parsing yielded only the slightest gain, showing that although probabilistic models allow easy integration of modules, the gain over an unintegrated system may not be large because of the robustness of even simple probabilistic systems.
Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). $$$$$ Traditional parsing and understanding systems, including implementations of unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each way in which semantic roles may be realized syntactically.

The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). $$$$$ The table also illustrates cases of sparse data for various feature combinations.
The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). $$$$$ Sentences from each subcorpus were then annotated by hand, marking boundaries of each frame element expressed in the sentence and assigning tags for the annotated constituent’s frame semantic role, syntactic category (e.g., noun phrase or prepositional phrase), and grammatical function in relation to the target word (e.g., object or complement of a verb).
The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). $$$$$ Our system for choosing the most likely overall assignment of roles for all the frame elements of a sentence uses an approximation that we derive beginning with the true probability of the optimal role assignment r*: where P(r1...n I t,f1...n) represents the probability of an overall assignment of roles ri to each of the n constituents of a sentence, given the target word t and the various features fi of each of the constituents.
The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). $$$$$ Traditional parsing and understanding systems, including implementations of unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each way in which semantic roles may be realized syntactically.

Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). $$$$$ Tests of interannotator agreement were performed for data from a small number of predicates before the final consistency check.
Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). $$$$$ Given an input sentence and a target word frame, the system labels constituents with either abstract semantic roles, such as or more domain-specific semantic roles, such as and The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.
Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). $$$$$ These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses.
Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). $$$$$ Although lexical statistics are quite accurate on the data covered by observations in the training set, the sparsity of their coverage led us to introduce semantically motivated knowledge sources, which in turn allowed us to compare automatically derived and hand-built semantic resources.

Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.
Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ We would not expect head word to be as valuable for other phrase types.
Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ This goal is not as far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms.
Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ The curve labeled “interpolation” in Figure 10 reflects a linear interpolation of the form Note that this method can identify only those frame elements that have a corresponding constituent in the automatically generated parse tree.

Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafsky's pioneer work (Gildea and Jurafsky, 2002). $$$$$ Although this .38 is not directly comparable to the .66–.82 interannotator agreements, it’s clear that the performance of our system still falls significantly short of human performance on the task.
Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafsky's pioneer work (Gildea and Jurafsky, 2002). $$$$$ Traditional parsing and understanding systems, including implementations of unification-based grammars such as Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag 1994), rely on hand-developed grammars that must anticipate each way in which semantic roles may be realized syntactically.
Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafsky's pioneer work (Gildea and Jurafsky, 2002). $$$$$ At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.
Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafsky's pioneer work (Gildea and Jurafsky, 2002). $$$$$ Given an input sentence and a target word frame, the system labels constituents with either abstract semantic roles, such as or more domain-specific semantic roles, such as and The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project.

The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002). $$$$$ This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.
The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002). $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.
The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002). $$$$$ When combined with the full system described above, clustered statistics increase performance on NP constituents from 83.4% to 85.0% (statistically significant at p < .05).

Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). $$$$$ A more sophisticated integrated system for identifying and labeling frame elements is described in Section 7.1.
Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). $$$$$ Test sentences were parsed, were annotated with these features, and were then passed through the classifiers.
Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). $$$$$ For example, the JUDGEMENT frame contains roles like JUDGE, EVALUEE, and REASON, and the STATEMENT frame contains roles like SPEAKER, ADDRESSEE, and MESSAGE, as the following examples show: These shallow semantic roles could play an important role in information extraction.
Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). $$$$$ Although lexical statistics are quite accurate on the data covered by observations in the training set, the sparsity of their coverage led us to introduce semantically motivated knowledge sources, which in turn allowed us to compare automatically derived and hand-built semantic resources.

Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.
Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles, such as AGENT or PATIENT, or more domain-specific semantic roles, such as SPEAKER, MESSAGE, and TOPIC.
Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.
Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ 4.1.5 Voice.

While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. $$$$$ Results were roughly comparable for the two types of semantic roles: overall performance was 82.1% for thematic roles, compared to 80.4% for framespecific roles.
While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. $$$$$ The examples above also illustrate a few of the phenomena that make it hard to identify frame elements automatically.

For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. $$$$$ A complete list of the semantic domains represented in our data is shown in Table 1, along with representative frames and predicates.
For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. $$$$$ Our system for choosing the most likely overall assignment of roles for all the frame elements of a sentence uses an approximation that we derive beginning with the true probability of the optimal role assignment r*: where P(r1...n I t,f1...n) represents the probability of an overall assignment of roles ri to each of the n constituents of a sentence, given the target word t and the various features fi of each of the constituents.
For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. $$$$$ The “Labeled Recall” column shows how often the frame element is correctly identified, whereas the “Unlabeled Recall” column shows how often a constituent with the given role is correctly identified as being a frame element, even if it is labeled with the wrong role.

Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.
Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ For example, person has as hypernyms both life form and causal agent.
Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ The probability distributions calculated from the training data were P(fe  |path), P(fe  |path, t), and P(fe  |h, t), where fe indicates an event where the parse constituent in question is a frame element, path the path through the parse tree from the target word to the parse constituent, t the identity of the target word, and h the head word of the parse constituent.
Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.

Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. $$$$$ There are a total of 49,013 annotated sentences and 99,232 annotated frame elements (which do not include the target words themselves).
Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. $$$$$ P(r I h, pt, t) predicts the correct role most often (87.4% of the time) when training data for a particular head word have been seen.

Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ present a system for identifying the semantic relationships, or filled by constituents of a sentence within a semantic frame.
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ It is interesting to note that the path feature performs better when not conditioned on the domain.
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ We compare hand-built and automatically derived resources for providing this information.
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.

Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ In this article, we aim to develop a statistical system for automatically learning to identify all semantic roles for a wide variety of predicates in unrestricted text.
Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ Much else remains to be done to apply the system described here to the interpretation of general text.
Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ Unlike probabilistic clustering, the bootstrapping technique can make use of only those sentences containing the target words in question.
Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ Assignment of semantic roles is an important part of language understanding, and the problem of how to assign such roles has been attacked by many computational systems.

The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al, 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word; head word, combination features of frame evoking word; phrase type, and combination features of voice; phrase type. $$$$$ Results for systems based on linear interpolation are shown in the first row of Table 5.
The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al, 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word; head word, combination features of frame evoking word; phrase type, and combination features of voice; phrase type. $$$$$ A more complete semantic analysis system would thus require a module for frame disambiguation.
The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al, 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word; head word, combination features of frame evoking word; phrase type, and combination features of voice; phrase type. $$$$$ This article describes an algorithm for identifying the semantic roles filled by constituents in a sentence.
The characteristics of x are: frame, frame evoking word, head word, content word (Surdeanu et al, 2003), first/last word, head word of left/right sister, phrase type, position, voice, syntactic path (directed/undirected/partial), governing category (Gildea and Jurafsky, 2002), WordNet supersense in the phrase, combination features of frame evoking word; head word, combination features of frame evoking word; phrase type, and combination features of voice; phrase type. $$$$$ Table 12 shows results on noun phrases for the bootstrapping method.

 $$$$$ We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.
 $$$$$ Various methods of extending the coverage of lexical statistics indicated that the broader coverage of automatic clustering outweighed its imprecision.
 $$$$$ Since there is no canonical set of abstract semantic roles, we decided upon the list shown in Table 17.
 $$$$$ The automatic clustering described above can be seen as an imperfect method of deriving semantic classes from the vocabulary, and we might expect a hand-developed set of classes to do better.

Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.
Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.
Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ Semantic roles are one of the oldest classes of constructs in linguistic theory, dating back thousands of years to Panini’s k¯araka theory (Misra 1966; Rocher 1964; Dahiya 1995).
Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ The second most common category is prepositional phrases.

This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). $$$$$ For this reason, it is interesting to calculate how many true frame elements overlap with the results of the system, relaxing the criterion that the boundaries must match exactly.
This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). $$$$$ Writing such grammars is time consuming, and typically such systems have limited coverage.
