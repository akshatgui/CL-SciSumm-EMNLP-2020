Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. $$$$$ The table also illustrates cases of sparse data for various feature combinations.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. $$$$$ In this case, the system identifies Plot of precision/recall curve for various methods of identifying frame elements.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. $$$$$ Automatically learning generalizations about the semantics and syntactic behavior of predicates is an exciting problem for the years to come.
Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. $$$$$ We are interested only in finding the role with the highest probability, and a role given a small, nonzero probability by smoothing techniques will still not be chosen as the classifier’s output.

In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ The curve labeled “interpolation” in Figure 10 reflects a linear interpolation of the form Note that this method can identify only those frame elements that have a corresponding constituent in the automatically generated parse tree.
In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ Section 7 examines techniques for adding knowledge about systematic alternations in verb argument structure with sentence-level features.
In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ A more sophisticated integrated system for identifying and labeling frame elements is described in Section 7.1.
In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997). $$$$$ For example, in the context of the Air Traveler Information System (ATIS) for spoken dialogue, Miller et al. (1996) computed the probability that a constituent such as Atlanta filled a semantic slot such as DESTINATION in a semantic frame for air travel.

Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). $$$$$ A more sophisticated integrated system for identifying and labeling frame elements is described in Section 7.1.
Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). $$$$$ We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence.
Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). $$$$$ Furthermore, the cross-frame experiments here are dependent on the mapping of frame-level roles to abstract thematic roles.

The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). $$$$$ Sentences from each subcorpus were then annotated by hand, marking boundaries of each frame element expressed in the sentence and assigning tags for the annotated constituent’s frame semantic role, syntactic category (e.g., noun phrase or prepositional phrase), and grammatical function in relation to the target word (e.g., object or complement of a verb).
The first SRL model on FrameNet was proposed by Gildea and Jurafsky (2002). $$$$$ Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task.

Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). $$$$$ These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses.
Many of the features used are inspired by those used in semantic role labeling systems (Gildea and Jurafsky, 2002). $$$$$ Carefully choosing sentence-level features for representing alternations in verb argument structure allowed us to introduce dependencies between frame element decisions within a sentence without adding too much complexity to the system.

Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ Carefully choosing sentence-level features for representing alternations in verb argument structure allowed us to introduce dependencies between frame element decisions within a sentence without adding too much complexity to the system.
Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ Distribution role, given the features described above and the predicate, or target word, t: P(r I h, pt, gov, position, voice, t) where r indicates semantic role, h head word, and pt phrase type.
Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ P(fe  |path, t) performs relatively poorly because of fragmentation of the training data (recall that only about 30 sentences are available for each target word).
Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. $$$$$ Table 12 shows results on noun phrases for the bootstrapping method.

Generally speaking, these SRL approaches use a two-stage architecture $$$$$ As can be seen in the table, the vocabulary used for clustering includes almost all (97.9%) of the test data, and the decrease in accuracy from direct lexical statistics to clustered statistics is relatively small (from 87.0% to 79.7%).
Generally speaking, these SRL approaches use a two-stage architecture $$$$$ Thus the mechanism is similar to the beam search used to prune nonequivalent edges, but a lower threshold was used for equivalent edges (1e vs. 1 100).
Generally speaking, these SRL approaches use a two-stage architecture $$$$$ The table also illustrates cases of sparse data for various feature combinations.
Generally speaking, these SRL approaches use a two-stage architecture $$$$$ This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.

The features are listed as follows $$$$$ Carefully choosing sentence-level features for representing alternations in verb argument structure allowed us to introduce dependencies between frame element decisions within a sentence without adding too much complexity to the system.
The features are listed as follows $$$$$ Assignment of semantic roles is an important part of language understanding, and the problem of how to assign such roles has been attacked by many computational systems.
The features are listed as follows $$$$$ We used various lexical clustering algorithms to generalize across possible fillers of roles.

Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). $$$$$ This work was primarily funded by National Science Foundation grant ITR/HCI #0086132 to the FrameNet project.
Automatic Semantic Role Labeling (SRL) is a natural language processing (NLP) technique that maps sentences to semantic representations and identifies the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). $$$$$ Some sample values from these distributions are shown in Table 8.

Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles.
Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame.
Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. $$$$$ This goal is not as far away as it once was, thanks to the development of large semantic databases such as WordNet (Fellbaum 1998) and progress in domain-independent machine learning algorithms.

While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. $$$$$ We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame.
While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. $$$$$ A path feature looking for frame elements for a target word in another part of the sentence may examine the internal structure of a constituent, violating the independence assumptions of the chart parser.
While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. $$$$$ STATE Rex spied out Sam Maggott hollering at all and sundry and making good use of his over-sized red gingham handkerchief.

For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. $$$$$ The relevant production in the parse tree is highlighted.
For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.
For example, simply adding whether each word is part of a noun or verb phrase using the hand annotated parse tree (the so-called GOV feature from (Gildea and Jurafsky, 2002)) improves the performance of our system from 83.95% to 85.8%. $$$$$ To do this, counts of training data were percolated up the semantic hierarchy in a technique similar to that of, for example, McCarthy (2000).

Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ The path feature, however, describes the syntactic relation between the target word (that is, the predicate invoking the semantic frame) and the constituent in question, whereas the gov feature is independent of where the target word appears in the sentence; that is, it identifies all subjects whether they are the subject of the target word or not.
Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ Results are shown in Table 7.
Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). $$$$$ Pronouns that refer to inanimate, or both animate and inanimate, objects were not included.

Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. $$$$$ The head of a prepositional phrase (PP) is considered to be the preposition, according to the rules we use, and because the set of prepositions is small, coverage is not as great a problem.
Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. $$$$$ As shown in Table 11, accuracy for the WordNet technique is roughly the same as that in the automatic clustering results in Table 10: 84.3% on NPs, as opposed to 85.0% with automatic clustering.
Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. $$$$$ Thus, the focus of the project was on completeness of examples for lexicographic needs, rather than on statistically representative data.
Their features are usually extended from Gildea and Jurafsky (2002)'s work, which uses flat information derived from a parse tree. $$$$$ We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.

Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ Phrase types other than NP and PP make up only a small proportion of the data.
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ Section 4 describes the basic syntactic and lexical features used by our system, which are derived from a Penn Treebank–style parse of individual sentences to be analyzed.
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ We apply statistical techniques that have been successful for the related problems of syntactic parsing, part-of-speech tagging, and word sense disambiguation, including probabilistic parsing and statistical classification.
Gildea and Jurafsky (2002) showed that classification accuracy was improved by manually replacing FrameNet roles into 18 thematic roles. $$$$$ We then retrained our classifier and achieved roughly comparable results; overall performance was 82.1% for abstract thematic roles, compared to 80.4% for frame-specific roles.

Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ The features used were the path feature of Section 4.1.3, the identity of the target word, and the identity of the constituent’s head word.
Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ These last systems make use of a limited amount of hand labor to accept or reject automatically generated hypotheses.
Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ Because the automatically labeled data are not as accurate as the annotated data, we can do slightly better by using the automatic data only in cases where no training data are available, backing off to the distribution Panto from Ptrai,,,.
Some previous studies have employed this idea to remedy the data sparseness problem in the training data (Gildea and Jurafsky, 2002). $$$$$ Adding semantic role subcategorization information to this syntactic information could extend this idea to use richer semantic knowledge.

The characteristics of x are $$$$$ Integrating semantic interpretation and syntactic parsing yielded only the slightest gain, showing that although probabilistic models allow easy integration of modules, the gain over an unintegrated system may not be large because of the robustness of even simple probabilistic systems.
The characteristics of x are $$$$$ These roles are defined at the level of semantic frames of the type introduced by Fillmore (1976), which describe abstract actions or relationships, along with their participants.
The characteristics of x are $$$$$ Much else remains to be done to apply the system described here to the interpretation of general text.

 $$$$$ The examples above also illustrate a few of the phenomena that make it hard to identify frame elements automatically.
 $$$$$ Many of these sets of roles have been proposed by linguists as part of theories of linking, the part of grammatical theory that describes the relationship between semantic roles and their syntactic realization.
 $$$$$ We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and its position in the sentence.
 $$$$$ We are grateful to Chuck Fillmore, Andreas Stolcke, Jerry Feldman, and three anonymous reviewers for their comments and suggestions, to Collin Baker for his assistance with the FrameNet data, and to Mats Rooth and Sabine Schulte im Walde for making available their parsed corpus.

Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ Combining the annotated and automatically labeled data increases coverage from 41.6% to 54.7% and performance to 44.5%.
Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ The table also illustrates cases of sparse data for various feature combinations.
Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall.
Gildea and Jurafsky (2002) is the only one applying selectional preferences in a real SRL task. $$$$$ Our study also allowed us to compare the usefulness of different features and feature combination methods in the semantic role labeling task.

This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). $$$$$ Like the governing-category feature described above, the parse tree path feature (path) is designed to capture the syntactic relation of a constituent to the rest of the sentence.
This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). $$$$$ We compare hand-built and automatically derived resources for providing this information.
This includes the majority of work on shallow semantic analysis (Gildea and Jurafsky, 2002, inter alia). $$$$$ We also explore the integration of role labeling with statistical syntactic parsing and attempt to generalize to predicates unseen in the training data.
