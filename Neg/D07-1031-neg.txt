[vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). $$$$$ , xn) (here, the wordsof the corpus) by first using a Markov model to gen erate a sequence of hidden states y = (y0, . . .
[vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). $$$$$ Also, the Bayesian framework permits a wide variety of different priors besides Dirichlet priors explored here.
[vbhmm]: Bayesian HMM with variational Bayes (Johnson, 2007). $$$$$ This also explains why the many-to-1 accuracy is so much better than the one-to-one accuracy; presumably several hidden 299 Estimator 1-to-1 Many-to-1 VI H(T |Y ) H(Y |T ) EM (50) 0.40 (0.02) 0.62 (0.01) 4.46 (0.08) 1.75 (0.04) 2.71 (0.06) VB(0.1, 0.1) (50) 0.47 (0.02) 0.50 (0.02) 4.28 (0.09) 2.39 (0.07) 1.89 (0.06) VB(0.1, 10?4) (50) 0.46 (0.03) 0.50 (0.02) 4.28 (0.11) 2.39 (0.08) 1.90 (0.07) VB(10?4, 0.1) (50) 0.42 (0.02) 0.60 (0.01) 4.63 (0.07) 1.86 (0.03) 2.77 (0.05) VB(10?4, 10?4) (50) 0.42 (0.02) 0.60 (0.01) 4.62 (0.07) 1.85 (0.03) 2.76 (0.06) GS(0.1, 0.1) (50) 0.37 (0.02) 0.51 (0.01) 5.45 (0.07) 2.35 (0.09) 3.20 (0.03) GS(0.1, 10?4) (50) 0.38 (0.01) 0.51 (0.01) 5.47 (0.04) 2.26 (0.03) 3.22 (0.01) GS(10?4, 0.1) (50) 0.36 (0.02) 0.49 (0.01) 5.73 (0.05) 2.41 (0.04) 3.31 (0.03) GS(10?4, 10?4) (50) 0.37 (0.02) 0.49 (0.01) 5.74 (0.03) 2.42 (0.02) 3.32 (0.02) EM (40) 0.42 (0.03) 0.60 (0.02) 4.37 (0.14) 1.84 (0.07) 2.55 (0.08) EM (25) 0.46 (0.03) 0.56 (0.02) 4.23 (0.17) 2.05 (0.09) 2.19 (0.08) EM (10) 0.41 (0.01) 0.43 (0.01) 4.32 (0.04) 2.74 (0.03) 1.58 (0.05)Table 1: Evaluation of models produced by the various estimators.

We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). $$$$$ We find that the HMMs es timated by EM generally assign a roughlyequal number of word tokens to each hid den state, while the empirical distribution of tokens to POS tags is highly skewed.
We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). $$$$$ Finally, several authors have proposed using information-theoretic measures of the divergence between the hidden state and POS tag sequences.
We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). $$$$$ (this will be true if, for example, there is sufficient data that the posterior distribution has a peaked mode): P(x,y, ?, ?) ?

We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.
We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ We find that the HMMs es timated by EM generally assign a roughlyequal number of word tokens to each hid den state, while the empirical distribution of tokens to POS tags is highly skewed.
We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ However, bitag models are rich enough to capture at least some distributional information (i.e., the tag 296for a word depends on the tags assigned to its neighbours).
We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted (note that if the number ofhidden states and POS tags differ, some will be unas signed).

These figures are the MCMC settings that provided the best results in Johnson (2007). $$$$$ It is well known that Expectation-Maximization (EM) performs poorly in unsupervised induction of linguistic structure (Carroll and Charniak, 1992;Merialdo, 1994; Klein, 2005; Smith, 2006).
These figures are the MCMC settings that provided the best results in Johnson (2007). $$$$$ Specifically, if the parameter estimate attime ` is (?(`), ?(`)), then the re-estimated parame ters at time `+ 1 are: ?(`+1)y?|y = E[ny?,y]/E[ny] (2) ?(`+1)x|y = E[nx,y]/E[ny] 6.95E+06 7.00E+06 7.05E+06 7.10E+06 7.15E+06 0 250 500 750 1000 ? lo g lik el ih oo d Iteration Figure 1: Variation in negative log likelihood with increasing iterations for 10 EM runs from different random starting points.where nx,y is the number of times observation x oc curs with state y, ny?,y is the number of times state y?

There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporat ing some of these in an unsupervised tagging model.
There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ follows y and ny is the number of occurences of state y; all expectations are taken with respect to the model (?(`), ?(`)).We took care to implement this and the other al gorithms used in this paper efficiently, since optimal performance was often only achieved after several hundred iterations.
There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.

The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. $$$$$ t p?(t) log p?(t) I(Y, T ) = ? y,t p?(y, t) log p?(y, t) p?(y)p?(t) H(Y |T ) = H(Y )?
The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. $$$$$ This is also consistent with the fact that the cross-entropy H(T |Y ) of tags given hidden states is relatively low(i.e., given a hidden state, the tag is relatively predictable), while the cross-entropy H(Y |T ) is rela tively high.
The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. $$$$$ follows y, and ny is the number of times state y occurs; these counts are from (x?i,y?i), i.e., excluding xi and yi.

Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ We call the accuracy of the POS sequence obtained using this map its 1-to-1 accuracy.
Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ We explain this by observing that the distribution of hidden states to words proposed by the EM estimated HMMs is relatively uniform, while the empirical distribution of POS tags is heavily skewed towards a few high-frequency tags.
Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted (note that if the number ofhidden states and POS tags differ, some will be unas signed).
Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ for each POS(Haghighi and Klein, 2006).

Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.
Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. $$$$$ In ret rospect one can certainly find reasons to explain this failure: after all, likelihood does not appear in thewide variety of linguistic tests proposed for identi fying linguistic structure (Fromkin, 2001).This paper focuses on unsupervised part-ofspeech (POS) tagging, because it is perhaps the sim plest linguistic induction task.
Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. $$$$$ We focus on first-order Hid den Markov Models (HMMs) in which the hidden state is interpreted as a POS tag, also known as bitag models.

As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ If a system is permitted to posit an unbounded number of hidden states (which is notthe case here) then it can achieve a perfect many-to 1 accuracy by placing every word token into its own unique state.
As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ It is well known that Expectation-Maximization (EM) performs poorly in unsupervised induction of linguistic structure (Carroll and Charniak, 1992;Merialdo, 1994; Klein, 2005; Smith, 2006).
As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ states are being mapped onto a single POS tag.

The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the same as the standard deviation.
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ By contrast, the EM distribution is much flatter.
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ It is well known that Expectation-Maximization (EM) performs poorly in unsupervised induction of linguistic structure (Carroll and Charniak, 1992;Merialdo, 1994; Klein, 2005; Smith, 2006).

In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ We also point out the high variance in all of these estimators, and that they requiremany more iterations to approach conver gence than usually thought.
In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ Specifically, given a corpus labeled with hidden states and POS tags, if p?(y), p?(t) and p?(y, t) are the empirical probabilities of a hidden state y, a POS tag t, and the cooccurance of y and t respectively, then the mutual information I , entropies H and variation of information VI are defined as follows: H(Y ) = ? ?
In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ Wecan understand these results by comparing the dis tribution of words to hidden states to the distribution of words to POS tags in the gold-standard evaluation corpus.
In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ (5) 301 P(yi|x,y?i, ?) ?

Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. $$$$$ The VI is the sum of these conditional entropies.
Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. $$$$$ We regard the assignments of hidden states and POS tags to the words of the cor pus as two different ways of clustering those words,and evaluate the conditional entropy of each clus tering conditioned on the other.
Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. $$$$$ As Clark (2003) points out, many-to-1 accuracy has several defects.
Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. $$$$$ As Figure 3 shows, VB can produce distributions of hidden states that are peaked in the same way that POS tags are.

We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). $$$$$ This shows that rather than fixing the number of hidden states in advance, the Bayesian prior can determine the number of states; this idea is more fully developed in the infinite HMM of Beal et al.
We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). $$$$$ Just as with EM, we experimented with a variety of annealing regimes, but were unable to find any which significantly improved accuracy or posterior likelihood.We also experimented with evaluating state se quences found using maximum posterior decoding(i.e., model parameters are estimated from the posterior sample, and used to perform maximum posterior decoding) rather than the samples from the pos terior produced by the Gibbs sampler.

This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ There is certainly much useful information that bitag HMMs models cannot capture.
This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ It is difficult to compare these with previous work, but Haghighi and Klein (2006) report that in a completely unsupervised setting, their MRF model, which uses a large set of additional features and amore complex estimation procedure, achieves an average 1-to-1 accuracy of 41.3%.
This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ Computationally it is very similar to EM, and each iteration takes essentially the same time as an EM iteration.

In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007). $$$$$ We call this themany-to-1 accuracy of the hidden state sequence be cause several hidden states may map to the same POS tag (and some POS tags may not be mapped to by any hidden states at all).
In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007). $$$$$ In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?
In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007). $$$$$ (This ap proach cannot be used in an unsupervised setting since the empirical tag distribution is not available).
In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.

 $$$$$ We insert endmarkers at the beginning and ending of the corpus and between sentence boundaries, and constrain the estimator to associate endmarkers with a state that never appears with any other observationtype (this means each sentence can be processed in dependently by first-order HMMs; these endmarkers are ignored during evaluation).In more detail, the HMM is specified by multi nomials ?y and ?y for each hidden state y, where ?y specifies the distribution over states following y and ?y specifies the distribution over observations x given state y. yi | yi?1 = y ? Multi(?y) xi | yi = y ? Multi(?y) (1)We used the Forward-Backward algorithm to perform Expectation-Maximization, which is a procedure that iteratively re-estimates the model param eters (?, ?), converging on a local maximum of the likelihood.
 $$$$$ Note that 1-to-1 accuracy at termination ranges from 0.38 to 0.45; a spread of 0.07.We obtained a dramatic speedup by working directly with probabilities and rescaling after each ob servation to avoid underflow, rather than workingwith log probabilities (thanks to Yoshimasa Tsu 298 0.35 0.37 0.39 0.41 0.43 0.45 0.47 0 250 500 750 10001 to -1 a ccura cy IterationFigure 2: Variation in 1-to-1 accuracy with increas ing iterations for 10 EM runs from different random starting points.
 $$$$$ We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced.
 $$$$$ Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it erations.

 $$$$$ The Gibbs sampler produces state sequences y sampled from the posterior distribution: P(y|x, ?) ?
 $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
 $$$$$ As Clark (2003) points out, many-to-1 accuracy has several defects.
 $$$$$ Reported values are means over all runs, followed by standard deviations.

 $$$$$ Wecan understand these results by comparing the dis tribution of words to hidden states to the distribution of words to POS tags in the gold-standard evaluation corpus.
 $$$$$ For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the same as the standard deviation.
 $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.

For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ This is also consistent with the fact that the cross-entropy H(T |Y ) of tags given hidden states is relatively low(i.e., given a hidden state, the tag is relatively predictable), while the cross-entropy H(Y |T ) is rela tively high.
For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ It is difficult to compare these with previous work, but Haghighi and Klein (2006) report that in a completely unsupervised setting, their MRF model, which uses a large set of additional features and amore complex estimation procedure, achieves an average 1-to-1 accuracy of 41.3%.
For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ If a system is permitted to posit an unbounded number of hidden states (which is notthe case here) then it can achieve a perfect many-to 1 accuracy by placing every word token into its own unique state.

(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state. $$$$$ evaluation, where sev eral hidden states may correspond to the same POStag.
(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state. $$$$$ We insert endmarkers at the beginning and ending of the corpus and between sentence boundaries, and constrain the estimator to associate endmarkers with a state that never appears with any other observationtype (this means each sentence can be processed in dependently by first-order HMMs; these endmarkers are ignored during evaluation).In more detail, the HMM is specified by multi nomials ?y and ?y for each hidden state y, where ?y specifies the distribution over states following y and ?y specifies the distribution over observations x given state y. yi | yi?1 = y ? Multi(?y) xi | yi = y ? Multi(?y) (1)We used the Forward-Backward algorithm to perform Expectation-Maximization, which is a procedure that iteratively re-estimates the model param eters (?, ?), converging on a local maximum of the likelihood.
(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state. $$$$$ Finally, we show that a similar increase in accuracy can be achieved by reducing the number of hidden states in the models estimated by EM.
(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state. $$$$$ evaluation, where sev eral hidden states may correspond to the same POStag.
