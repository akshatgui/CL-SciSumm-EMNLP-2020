[vbhmm] $$$$$ t p?(t) log p?(t) I(Y, T ) = ? y,t p?(y, t) log p?(y, t) p?(y)p?(t) H(Y |T ) = H(Y )?
[vbhmm] $$$$$ We also show that EM does nearly as well as VB when the number of hidden HMM states is dramatically reduced.
[vbhmm] $$$$$ However, as Banko and Moore (2004) point out, the accuracy achieved by these unsupervised methodsdepends strongly on the precise nature of the su pervised training data (in their case, the ambiguity of the tag lexicon available to the system), which makes it more difficult to understand the behaviour of such systems.

We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). $$$$$ The VI is the sum of these conditional entropies.
We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). $$$$$ We investigate Gibbs Sampling (GS) and Variational Bayes(VB) estimators and show that VB con verges faster than GS for this task and thatVB significantly improves 1-to-1 tagging ac curacy over EM.
We fixed the and parameters to 0.1, values that appeared to be reasonable based on Johnson (2007), and which were also used by Graca et al (2009). $$$$$ We make a ?mean-field?

We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ Moreover, in separate experiments we found that the maximum marginal state sequence almost always scored higherthan the Viterbi state sequence in all of our evalua tions, and at modest numbers of iterations (up to 50) often scored more than 5% better.We also noticed a wide variance in the perfor mance of models due to random initialization (both ? and ? are initially jittered to break symmetry); thiswide variance was observed with all of the estima tors investigated in this paper.
We evaluate on a 1-to-1 mapping between unsupervised tags and gold labels, as well as many-to-1 (M-to-1), corresponding to the evaluation mappings used in Johnson (2007). $$$$$ Toutanova etal.

These figures are the MCMC settings that provided the best results in Johnson (2007). $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.
These figures are the MCMC settings that provided the best results in Johnson (2007). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
These figures are the MCMC settings that provided the best results in Johnson (2007). $$$$$ Haghighi and Klein (2006) propose constrain ing the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag.
These figures are the MCMC settings that provided the best results in Johnson (2007). $$$$$ , xn) (here, the wordsof the corpus) by first using a Markov model to gen erate a sequence of hidden states y = (y0, . . .

There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ We also point out the high variance in all of these estimators, and that they requiremany more iterations to approach conver gence than usually thought.
There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ It?s well-known thataccuracy often decreases after the first few EM it erations (which we also observed); however in our experiments we found that performance improves again after 100 iterations and continues improving roughly monotonically.
There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ Since we evaluatedthe accuracy of the estimated tags after each iteration, it was important that decoding be done effi ciently as well.
There is much potential for further work in this direction, experimenting with more training data or more estimation iterations, or even looking at different estimators as suggested in Johnson (2007) and Ravi et al (2010b). $$$$$ This captures the intu ition that most word types only belong to one POS,since the minimum number of non-zero state-toobservation transitions occurs when each observa tion type is emitted from only one state.

The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. $$$$$ Second, these posterior counts(which are in fact parameters of the Dirichlet pos terior Q2) are passed through the function f(v) = exp?(v), which is plotted in Figure 5.
The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.
The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed. $$$$$ For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the same as the standard deviation.

Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ If a system is permitted to posit an unbounded number of hidden states (which is notthe case here) then it can achieve a perfect many-to 1 accuracy by placing every word token into its own unique state.
Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it erations.
Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ For the estimators with 10 runs, a 3-standard error 95% confidence interval is approximately the same as the standard deviation.
Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. $$$$$ d?Because Dirichlet priors are conjugate to multinomials, it is possible to integrate out the model parameters ? and ? to yield the conditional distribu tion for yi shown in Figure 4.

Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. $$$$$ If a system is permitted to posit an unbounded number of hidden states (which is notthe case here) then it can achieve a perfect many-to 1 accuracy by placing every word token into its own unique state.
Johnson (2007) reports results for different numbers of hidden states but it is unclear how to make this choice a priori, while Goldwater & Griffiths (2007) leave this question as future work. $$$$$ In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?

As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ (This ap proach cannot be used in an unsupervised setting since the empirical tag distribution is not available).
As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ We experimented with dramatic reductions in the number of hidden states in the HMMs estimated by EM.
As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporat ing some of these in an unsupervised tagging model.
As Johnson (2007) clearly explained, training the HMM with EM leads to poor results in PoS tagging. $$$$$ MCMC en compasses a broad range of sampling techniques, including component-wise Gibbs sampling, which is the MCMC technique we used here (Robert and Casella, 2004; Bishop, 2006).

The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ Finally, we also experimented with annealing, in which the parameters ? and ? are raised to the power 1/T , where T is a ?temperature?
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ (2003) describe a wide variety of morphological and distributional features useful for POS tagging, and Clark (2003) proposes ways of incorporat ing some of these in an unsupervised tagging model.
The fact that different authors use different versions of the same gold standard to evaluate similar experiments (e.g. Goldwate & Griffiths (2007) versus Johnson (2007)) supports this claim. $$$$$ Toutanova etal.

In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ We investigate Gibbs Sampling (GS) and Variational Bayes(VB) estimators and show that VB con verges faster than GS for this task and thatVB significantly improves 1-to-1 tagging ac curacy over EM.
In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ This mapping is found by greedily assigning hidden states to POS tags until either the hidden states or POS tags are exhausted (note that if the number ofhidden states and POS tags differ, some will be unas signed).
In particular, Toutanova and Johnson (2007) demonstrate good performance on unsupervised part-of-speech tagging (using a dictionary) with a Bayesian model similar to our own. $$$$$ Expectation-Maximization There are several excellent textbook presentations of Hidden Markov Models and the Forward-Backward algorithm for Expectation-Maximization (Jelinek, 1997; Manning and Schu?tze, 1999; Bishop, 2006),so we do not cover them in detail here.

Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. $$$$$ The Dirichlet prior ?y that controls 302sparsity of the state-to-state transitions had little ef fect on the results.
Johnson (2007) observed that EM tends to create word clusters of uniform size, which does not reflect the way words cluster into parts of speech in natural languages. $$$$$ The Dirichlet prior ?y that controls 302sparsity of the state-to-state transitions had little ef fect on the results.

We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). $$$$$ We also point out the high variance in all of these estimators, and that they requiremany more iterations to approach conver gence than usually thought.
We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). $$$$$ Each EM and VB run consisted of 1,000 iterations, while each GS run consisted of 50,000 it erations.
We evaluated POS tagging accuracy using the lenient many-to-1 evaluation approach (Johnson, 2007). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.

This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ In ret rospect one can certainly find reasons to explain this failure: after all, likelihood does not appear in thewide variety of linguistic tests proposed for identi fying linguistic structure (Fromkin, 2001).This paper focuses on unsupervised part-ofspeech (POS) tagging, because it is perhaps the sim plest linguistic induction task.
This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ (2003).
This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ We investigate Gibbs Sampling (GS) and Variational Bayes(VB) estimators and show that VB con verges faster than GS for this task and thatVB significantly improves 1-to-1 tagging ac curacy over EM.
This mapping technique is based on the many-to-one scheme used for evaluating unsupervised part-of-speech induction (Johnson, 2007). $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.

In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007). $$$$$ While most researchers use Viterbidecoding to find the most likely state sequence, maximum marginal decoding (which labels the observa tion xi with the state yi that maximizes the marginal probability P(yi|x, ?, ?)) is faster because it re-uses the forward and backward tables already constructed by the Forward-Backward algorithm.
In related fields of NLP lately Dirichlet priors have been investigated, e.g. (Johnson, 2007). $$$$$ In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?

 $$$$$ for each POS(Haghighi and Klein, 2006).
 $$$$$ All of the experiments described below have the same basic structure: an estimator is used to infera bitag HMM from the unsupervised training cor pus (the words of Penn Treebank (PTB) Wall Street Journal corpus (Marcus et al, 1993)), and then the resulting model is used to label each word of that corpus with one of the HMM?s hidden states.
 $$$$$ We also point out the high variance in all of these estimators, and that they requiremany more iterations to approach conver gence than usually thought.
 $$$$$ This should force the hidden states to bemore densely populated and improve 1-to-1 accu racy, even though this means that there will be nohidden states that can possibly map onto the less fre quent POS tags (i.e., we will get these words wrong).

 $$$$$ Most previous work exploiting unsupervised training data for inferring POS tagging models has focused on semi-supervised methods in the in which the learner is provided with a lexicon specifying the possible tags for each word (Merialdo, 1994; Smith and Eisner, 2005; Goldwater and Griffiths, 2007) or a small number of ?prototypes?
 $$$$$ Similarly, as ?y approaches zero the state-to-state transitions become sparser.There are two main techniques for Bayesian esti mation of such models: Markov Chain Monte Carlo(MCMC) and Variational Bayes (VB).
 $$$$$ As Clark (2003) points out, many-to-1 accuracy has several defects.
 $$$$$ Since we evaluatedthe accuracy of the estimated tags after each iteration, it was important that decoding be done effi ciently as well.

 $$$$$ states are being mapped onto a single POS tag.
 $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
 $$$$$ This motivates a Bayesian approach using a sparse prior to bias the estimator toward such a skewed distribution.
 $$$$$ Wecan understand these results by comparing the dis tribution of words to hidden states to the distribution of words to POS tags in the gold-standard evaluation corpus.

For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ Haghighi and Klein (2006) propose constrain ing the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag.
For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ Haghighi and Klein (2006) propose constrain ing the mapping from hidden states to POS tags so that at most one hidden state maps to any POS tag.
For instance, on unsupervised part-of speech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). $$$$$ This paper investigates why the HMMs es timated by Expectation-Maximization (EM) produce such poor results as Part-of-Speech(POS) taggers.

(Johnson, 2007) criticizes the standard EM based HMM approaches because of their poor performance on the unsupervised POS tagging and their tendency to assign equal number of words to each hidden state. $$$$$ for each POS(Haghighi and Klein, 2006).
