Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ Features in the lexical category are applicable to non-pronominal mentions only.
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ The conditions include all the partially-formed entities before, the focus entity index, and the active mention.
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ To see this effect, we decode the Devtest set by varying the start penalty and the result is depicted in Figure 2.

Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ The start penalty can be used to balance entity miss and false alarm.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ Neither (Soon et al., 2001) nor (Ng and Cardie, 2002) searches for the global optimal entity in that they make locally independent decisions during search.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ In-focus entities are marked on the solid arrows, and active mentions are marked by *.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported.

As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ Suppose a document has three mentions “Mr.
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ Coreference resolution in this context is defined as partitioning mentions into entities.
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ Variable stores the cumulative score for a coreference result .
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ The ACE coreference system is trained with documents (about words) of ACE 2002 training data.

To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ This makes it easy to rank them using the “local” linking and starting probabilities as the number of factors is the same.
To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ Since the ACE-value is an entity-level metric and is weighted heavily toward NAME entities, we also measure our system’s performance by an entity-constrained mention F-measure (henceforth “ECM-F”).
To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ The Bell tree representation is also incremental in that mentions are added sequentially.

Details of this process can be found in Luo et al (2004). $$$$$ The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate.
Details of this process can be found in Luo et al (2004). $$$$$ This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.
Details of this process can be found in Luo et al (2004). $$$$$ This paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes.
Details of this process can be found in Luo et al (2004). $$$$$ Features in the lexical category are applicable to non-pronominal mentions only.

 $$$$$ A Maximum Entropy model is used to rank these paths.
 $$$$$ The initial node consists of the first partial entity [1] (i.e., node (a) in Figure 1).
 $$$$$ Distance features are computed by taking minimum between mentions in the entity and the active mention.
 $$$$$ In Section 2, we present how the Bell tree can be used to represent the process of creating entities from mentions and the search space.

Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ The mention-pair model is appealing for its simplicity: features are easy to compute over a pair of menif otherwise Category Features Remark Lexical exact_strm 1 if two mentions have the same spelling; 0 otherwise left_subsm 1 if one mention is a left substring of the other; 0 otherwise right_subsm 1 if one mention is a right substring of the other; 0 otherwise acronym 1 if one mention is an acronym of the other; 0 otherwise edit_dist quantized editing distance between two mention strings spell pair of actual mention strings ncd number of different capitalized words in two mentions Distance token_dist how many tokens two mentions are apart (quantized) sent_dist how many sentences two mentions are apart (quantized) gap_dist how many mentions in between the two mentions in question (quantized) Syntax POS_pair POS-pair of two mention heads apposition 1 if two mentions are appositive; 0 otherwise Count count pair of (quantized) numbers, each counting how many times a mention string is seen Pronoun gender pair of attributes of {female, male, neutral, unknown } number pair of attributes of {singular, plural, unknown} possessive 1 if a pronoun is possessive; 0 otherwise reflexive 1 if a pronoun is reflexive; 0 otherwise tions; its drawback is that information outside the mention pair is ignored.
Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ We propose to use the Bell tree to represent the process of forming entities from mentions.
Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ The process of forming entities from mentions can be represented by a tree structure.

 $$$$$ N66001-99-2-8916.
 $$$$$ Apart from basic features in Table 1, composite features can be generated by taking conjunction of basic features.
 $$$$$ The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate.
 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.

 $$$$$ This is because the ACE-value is a weighted metric.
 $$$$$ For each ACE entity, a canonical mention is defined as the longest NAME mention if available; or if the entity does not have a NAME mention, the most recent NOMINAL mention; if there is no NAME and NOMINAL mention, the most recent pronoun mention.
 $$$$$ The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported.
 $$$$$ (4) is not the only way can be approximated.

 $$$$$ This makes it easy to rank them using the “local” linking and starting probabilities as the number of factors is the same.
 $$$$$ The disparity is intentional as the starting action is influenced by all established entities on the left.
 $$$$$ To compensate the model inaccuracy, we introduce a “starting penalty” to balance the linking and starting scores.
 $$$$$ Fortunately, the starting probability can be computed using link probabilities (1), as shown now.

Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ Neither (Soon et al., 2001) nor (Ng and Cardie, 2002) searches for the global optimal entity in that they make locally independent decisions during search.
Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ A binary maximum entropy model is trained to compute the linking probability between a partial entity and a mention.
Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ One may consider training directly and use it to score paths in the Bell tree.
Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ This paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes.

As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ The full entity-mention model has about features, due to less number of conjunction features and training examples.
As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ The partial entity which the active mention considers linking with is said to be in-focus.
As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ As shown in Section 2, the search space of the coreference problem can be represented by the Bell tree.

For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ Statistics of the three test sets is summarized in Table 2.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ However, models in (McCallum and Wellner, 2003) compute directly the probability of an entity configuration conditioned on mentions, and it is not clear how the models can be factored to do the incremental search, as it is impractical to enumerate all possible entities even for documents with a moderate number of mentions.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ Leaves of the decision tree are labeled with “link” or “not-link” in training.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.

Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ In this paper, we propose to use the Bell tree to represent the process of forming entities from mentions.
Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ However, models in (McCallum and Wellner, 2003) compute directly the probability of an entity configuration conditioned on mentions, and it is not clear how the models can be factored to do the incremental search, as it is impractical to enumerate all possible entities even for documents with a moderate number of mentions.
Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ The process of forming entities from mentions can be represented by a tree structure.

 $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
 $$$$$ The partial entity which the active mention considers linking with is said to be in-focus.
 $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
 $$$$$ We propose to use the Bell tree to represent the process of forming entities from mentions.

Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ The mention-pair model is appealing for its simplicity: features are easy to compute over a pair of menif otherwise Category Features Remark Lexical exact_strm 1 if two mentions have the same spelling; 0 otherwise left_subsm 1 if one mention is a left substring of the other; 0 otherwise right_subsm 1 if one mention is a right substring of the other; 0 otherwise acronym 1 if one mention is an acronym of the other; 0 otherwise edit_dist quantized editing distance between two mention strings spell pair of actual mention strings ncd number of different capitalized words in two mentions Distance token_dist how many tokens two mentions are apart (quantized) sent_dist how many sentences two mentions are apart (quantized) gap_dist how many mentions in between the two mentions in question (quantized) Syntax POS_pair POS-pair of two mention heads apposition 1 if two mentions are appositive; 0 otherwise Count count pair of (quantized) numbers, each counting how many times a mention string is seen Pronoun gender pair of attributes of {female, male, neutral, unknown } number pair of attributes of {singular, plural, unknown} possessive 1 if a pronoun is possessive; 0 otherwise reflexive 1 if a pronoun is reflexive; 0 otherwise tions; its drawback is that information outside the mention pair is ignored.
Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ Similarly, mention 3 will be active in the next stage and can take five possible actions, which create five possible coreference results shown in node (c1) through (c5).
Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ Coreference resolution in this context is defined as partitioning mentions into entities.

Distances have been used in e.g. Luo et al (2004). $$$$$ In this paper, we will adopt the terminologies used in the Automatic Content Extraction (ACE) task (NIST, 2003).
Distances have been used in e.g. Luo et al (2004). $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.
Distances have been used in e.g. Luo et al (2004). $$$$$ Subsequent mentions are added to the tree in the same manner.
Distances have been used in e.g. Luo et al (2004). $$$$$ The second mention is Figure 1: Bell tree representation for three mentions: numbers in [] denote a partial entity.

 $$$$$ Since MUC does not require entity label and level, the conversion from ACE to MUC is “lossless.” Table 5 tabulates the test results on the true mentions of the MUC6 formal test set.
 $$$$$ A Maximum Entropy model is used to rank these paths.
 $$$$$ Apart from basic features in Table 1, composite features can be generated by taking conjunction of basic features.

Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ N66001-99-2-8916.
Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ The model is appealing in that it can potentially overcome the limitation of mention-pair model in which dependency among mentions other than the two in question is ignored.

They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ For an active mention index , define for some the set of indices of the partially-established entities to the left of (note that ), and the set of the partially-established entities.
They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ Details of the mention detection and coreference system can be found in (Florian et al., 2004).
