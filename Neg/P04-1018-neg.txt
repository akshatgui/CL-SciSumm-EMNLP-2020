Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ To use the official MUC scorer, we convert the output of the ACE-style coreference system back into the MUC format.
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
Luo et al (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning. $$$$$ We use a maximum entropy model to rank paths in the Bell tree, which is discussed in Section 3.

Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ Since the search space is large (even for a document with a moderate number of mentions), it is difficult to estimate a distribution over leaves directly.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ A separate documents ( words) is used as the development-test (Devtest) set.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ The model (9) is very similar to the model in (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002) while (8) has more conditions.
Luo et al (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree. $$$$$ For the entity-mention model, events are generated by walking through the Bell tree.

As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ A Maximum Entropy model is used to rank these paths.
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ Features involving the dummy mention are essentially computed with the single (normal) mention, and therefore the starting model is weak.
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ State-of-the-art performance has been achieved on the ACE coreference data across three languages.
As observed by Luo et al (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score - significantly higher than any published system. $$$$$ However, models in (McCallum and Wellner, 2003) compute directly the probability of an entity configuration conditioned on mentions, and it is not clear how the models can be factored to do the incremental search, as it is impractical to enumerate all possible entities even for documents with a moderate number of mentions.

To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ We also train a coreference system using the MUC6 data and competitive results are obtained.
To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ N66001-99-2-8916.
To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ For the entity-mention model, events are generated by walking through the Bell tree.
To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al (2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree. $$$$$ N66001-99-2-8916.

Details of this process can be found in Luo et al (2004). $$$$$ A second layer of nodes are created to represent the two possible outcomes.
Details of this process can be found in Luo et al (2004). $$$$$ Since starting a new entity means that does not link with any entities in , the probability of starting ,provided that the marginal is known.
Details of this process can be found in Luo et al (2004). $$$$$ The rest of the paper is organized as follows.
Details of this process can be found in Luo et al (2004). $$$$$ We also train a coreference system using the MUC6 data and competitive results are obtained.

 $$$$$ We also observed that, because the score between the infocus entity and the active mention is computed by (9) in the mention-pair model, the mention-pair sometimes mistakenly places a male pronoun and female pronoun into the same entity, while the same mistake is avoided in the entity-mention model.
 $$$$$ In this paper, we propose to use the Bell tree to represent the process of forming entities from mentions.
 $$$$$ This is done at line 6 and 11 in Algorithm 1.
 $$$$$ Under the derivation illustrated in Figure 1, each leaf node in the Bell tree corresponds to a possible coreference outcome, and there is no other way to form entities.

Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ A Maximum Entropy model is used to rank these paths.
Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ The advantage is that we do not need to train a starting model.
Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ Statistics of the three test sets is summarized in Table 2.
Luo et al (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together. $$$$$ The same set of basic features in Table 1 is used in the entity-mention model, but feature definitions are slightly different.

 $$$$$ We also would like to thank the anonymous reviewers for suggestions of improving the paper.
 $$$$$ Thus, the search problem reduces to creating the Bell tree while keeping track of path scores and picking the top-N best paths.
 $$$$$ (9) further assumes that the entity-mention score can be obtained by the maximum mention pair score.

 $$$$$ Figure 1 illustrates how the Bell tree is created for a document with three mentions.
 $$$$$ The recovered named-entity label is propagated to all mentions belonging to the same entity.
 $$$$$ We studied two maximum entropy models, namely the mention-pair model and the entitymention model, both of which can be used to score entity hypotheses.
 $$$$$ A second layer of nodes are created to represent the two possible outcomes.

 $$$$$ A second layer of nodes are created to represent the two possible outcomes.
 $$$$$ The root node is the initial state of the process, which consists of a partial entity containing the first mention of a document.
 $$$$$ The disparity is intentional as the starting action is influenced by all established entities on the left.
 $$$$$ State-of-the-art performance has been achieved on the ACE coreference data across three languages.

Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ To our knowledge, the paper is the first time the Bell tree is used to represent the search space of the coreference resolution problem.
Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ One common strategy shared by (Soon et al., 2001; Ng and Cardie, 2002; Ittycheriah et al., 2003) is that a statistical model is trained to measure how likely a pair of mentions corefer; then a greedy procedure is followed to group mentions into entities.
Luo et al (2004) propose a system that performs coreference resolution by doing search in a large space of entities. $$$$$ This makes it easy to rank them using the “local” linking and starting probabilities as the number of factors is the same.

As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported.
As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ There exists a large body of literature on the topic of coreference resolution.
As a base line, we follow the solution proposed in (Luo et al, 2004) to design a set of first-order features. $$$$$ The link model is then the probability linking the active mention with the in-focus entity .

For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ After retrained with Chinese and Arabic data (much less training data than English), the system got and ACE-value on the system mentions of ACE 2003 evaluation data for Chinese and Arabic, respectively.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ We propose to use the Bell tree to represent the process of forming entities from mentions.
For example, Luo et al (2004) apply the ANY predicate to generate cluster-level features for their entity-mention model, which does not perform as well as the mention-pair model. $$$$$ N66001-99-2-8916.

Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ The Bell tree represents the search space of the coreference resolution problem.
Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ At test time, the system checks a mention against all its preceding mentions, and the first one labeled with “link” is picked as the antecedent.
Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ After presenting the search strategy in Section 4, we show the experimental results on the ACE 2002 and 2003 data, and the Message Understanding Conference (MUC) (MUC, 1995) data in Section 5.
Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al, 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b). $$$$$ None of the ECM-F differences between MP and EM is statistically significant at . generic type “UNKNOWN” is assigned to these mentions.

 $$$$$ When considering the mention pair “Clinton” and “she”, the model may tend to link them because of their proximity; But this mistake can be easily avoided if “Mr.
 $$$$$ The number of common mentions of the best alignment is (i.e., The system is first developed and tested using the ACE data.
 $$$$$ One may consider training directly and use it to score paths in the Bell tree.

Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ The second row is the full mention-pair model, the third through seventh row correspond to models by removing the syntactic features (i.e., POS tags and apposition features), count features, distance features, mention type and level information, and pair of mention-spelling features.
Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ For example, in the following sentence, mentions are underlined: “The American Medical Association voted yesterday to install the heir apparent as its president-elect, rejecting a strong, upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership.” “American Medical Association”, “its” and “group” belong to the same entity as they refer to the same object.
Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ This paper proposes a new approach for resolution which uses the tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes.
Our existing co-reference module is a state-of the-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). $$$$$ N66001-99-2-8916.

Distances have been used in e.g. Luo et al (2004). $$$$$ N66001-99-2-8916.
Distances have been used in e.g. Luo et al (2004). $$$$$ The link model is then the probability linking the active mention with the in-focus entity .
Distances have been used in e.g. Luo et al (2004). $$$$$ N66001-99-2-8916.

 $$$$$ We studied two maximum entropy models, namely the mention-pair model and the entitymention model, both of which can be used to score entity hypotheses.
 $$$$$ We prune the search space in the following places: Local pruning: any children with a score below a fixed factor of the maximum score are pruned.
 $$$$$ Under the derivation illustrated in Figure 1, each leaf node in the Bell tree corresponds to a possible coreference outcome, and there is no other way to form entities.
 $$$$$ Leaves of the decision tree are labeled with “link” or “not-link” in training.

Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ Part of entity types can be recovered from the corresponding named-entity annotations.
Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ The coreference performance on the 2002 and 2003 Automatic Content Extraction (ACE) data will be reported.
Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
Luo et al (2004) perform the clustering step within a Bell tree representation. $$$$$ Neither (Soon et al., 2001) nor (Ng and Cardie, 2002) searches for the global optimal entity in that they make locally independent decisions during search.

They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ Sometimes the ACE-value increases after removing a set of features, but the ECM-F measure tracks nicely the trend that the more features there are, the better the performance is.
They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ While this approach has yielded encouraging results, the way mentions are linked is arguably suboptimal in that an instant decision is made when considering whether two mentions are linked or not.
They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ In the entity-mention model, “ncd”,“spell” and “count” features are computed over the canonical mention of the in-focus entity and the active mention.
They report considerable improvements over state-of the-art systems including Luo et al (2004). $$$$$ The model depends on all partial entities , which can be very expensive.
