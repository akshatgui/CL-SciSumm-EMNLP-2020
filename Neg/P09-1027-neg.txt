To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. $$$$$ Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.
To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. $$$$$ In the problem of crossdomain text classification, the labeled and unlabeled data come from different domains, and their underlying distributions are often different from each other, which violates the basic assumption of traditional classification learning.

To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. $$$$$ Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method.
To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. $$$$$ During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations.
To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. $$$$$ During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations.

But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ We also thank the anonymous reviewers for their useful comments.
But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.
But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ However, the annotated corpora for Chinese sentiment classification are scarce and it is not a trivial task to manually label reliable Chinese sentiment corpora.

For comparsion, we use the same data set in (Wan, 2009) $$$$$ Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification.
For comparsion, we use the same data set in (Wan, 2009) $$$$$ Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.
For comparsion, we use the same data set in (Wan, 2009) $$$$$ The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way.

The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ Only Chinese-to-English translation is needed.
The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g.
The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ SVM(EN): This method applies the inductive SVM with only English features for sentiment classification in the English view.

We compare our procedure with the co-training scheme reported in (Wan, 2009) $$$$$ We also thank the anonymous reviewers for their useful comments.
We compare our procedure with the co-training scheme reported in (Wan, 2009) $$$$$ Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers.
We compare our procedure with the co-training scheme reported in (Wan, 2009) $$$$$ Note that the training set and the unlabeled set are used in the training phase, while the test set is blind to the training phase.

More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. $$$$$ We also thank the anonymous reviewers for their useful comments.
More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. $$$$$ Given: In the training phase, the co-training algorithm learns two separate classifiers: Cen and C,n.

Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ The proposed co-training approach is described in detail in Section 3.
Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ However, the annotated corpora for Chinese sentiment classification are scarce and it is not a trivial task to manually label reliable Chinese sentiment corpora.
Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ The framework of the proposed approach is illustrated in Figure 1.
Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g.

A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).
A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ The metrics are defined the same as in general text categorization.
A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration.
A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ The experimental results show the effectiveness of the proposed approach.

Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. $$$$$ Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.
Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. $$$$$ We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text.
Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. $$$$$ The SVM classifier is adopted as the basic classifier in the proposed approach.

In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.
In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ The approach has the effect of “smoothing” the learning curves.
In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ We propose a cotraining approach to making use of unlabeled Chinese data.

In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ We used the standard precision, recall and Fmeasure to measure the performance of positive and negative class, respectively, and used the accuracy metric to measure the overall performance of the system.
In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ 2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service.
In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification.

The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).
The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. $$$$$ The feature size is measured as the proportion of the selected features against the total features (i.e.
The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. $$$$$ And the unlabeled set is not used.
The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. $$$$$ We also thank the anonymous reviewers for their useful comments.

Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). $$$$$ The approach has the effect of “smoothing” the learning curves.
Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). $$$$$ Lastly, we use the classifier to classify the unlabeled Chinese reviews.
Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). $$$$$ However, there are many freely available English sentiment corpora on the Web.

The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents.
The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification.
The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ TSVM(ENCN2): This method combines the results of TSVM(EN) and TSVM(CN) by averaging the prediction values.

Wan (2009) constructs a multilingual classifier using co-training. $$$$$ Sentiment classification is the task of identifying the sentiment polarity of a given text.
Wan (2009) constructs a multilingual classifier using co-training. $$$$$ Then, we can view the classification problem in two independent views: Chinese view with only Chinese features and English view with only English features.
Wan (2009) constructs a multilingual classifier using co-training. $$$$$ Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chinese reviews into English reviews.
Wan (2009) constructs a multilingual classifier using co-training. $$$$$ Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting.

Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. $$$$$ We can also see that the two component classifier has similar trends with the cotraining approach.
Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. $$$$$ In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification.
Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. $$$$$ This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data.

SVM $$$$$ During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations.
SVM $$$$$ Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers.
SVM $$$$$ DauméIII and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model.

Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ The reviews focused on such products as mp3 players, mobile phones, digital camera and laptop computers.
Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ However, there are many freely available English sentiment corpora on the Web.
Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews.

Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ In this study, we adopt the widely-used SVM classifier (Joachims, 2002).
Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration.
Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ The experimental results show the effectiveness of the proposed approach.
