To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. $$$$$ Figures 6 show the comparison results of different feature sizes for the co-training approach and two strong baselines.
To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. $$$$$ We also thank the anonymous reviewers for their useful comments.
To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. $$$$$ Google Translate2, Yahoo Babel Fish3 and Windows Live Translate4.

To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. $$$$$ Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.
To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. $$$$$ Standard Na√Øve Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.
To reduce this kind of error introduced by the translator, Wan in (Wan, 2009) applied a co-training scheme. $$$$$ The sentiment polarity is usually positive or negative and the text genre is usually product review.

But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews.
But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001).
But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ The framework of the proposed approach is illustrated in Figure 1.
But as the conditional distribution can be quite different for the original language and the pseudo language produced by the machine translators, these two strategies give poor performance as reported in (Wan, 2009). $$$$$ We also thank the anonymous reviewers for their useful comments.

For comparsion, we use the same data set in (Wan, 2009): Test Set (Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones). $$$$$ Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.
For comparsion, we use the same data set in (Wan, 2009): Test Set (Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones). $$$$$ The proposed co-training approach is described in detail in Section 3.
For comparsion, we use the same data set in (Wan, 2009): Test Set (Labeled Chinese Reviews): The data set contains a total of 886 labeled product reviews in Chinese (451 positive reviews and 435 negative ones). $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).

The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ In this study, we adopt the widely-used SVM classifier (Joachims, 2002).
The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).
The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text.
The features we used are bigrams and unigrams in the two languages as in (Wan, 2009). $$$$$ Table 1 shows the comparison results.

We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009). $$$$$ In the training phase, the input is the labeled English reviews and some amounts of unlabeled Chinese reviews1.
We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009). $$$$$ In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification.
We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009). $$$$$ First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese test reviews and additional unlabeled reviews into English reviews.
We compare our procedure with the co-training scheme reported in (Wan, 2009): CoTrain: The method with the best performance in (Wan, 2009). $$$$$ And the performance of the co-training approach with large p and n will more quickly become unchanged, because the approach runs out of the limited examples in the unlabeled set more quickly.

More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).
More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. $$$$$ Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification.
More recently, Wan (2009) proposed a co training approach to tackle the problem of cross lingual sentiment classification by leveraging an available English corpus for Chinese sentiment classification. $$$$$ The framework of the proposed approach is illustrated in Figure 1.

Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ In the experiments, the proposed co-training approach (CoTrain) is compared with the following baseline methods: SVM(CN): This method applies the inductive SVM with only Chinese features for sentiment classification in the Chinese view.
Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.
Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). $$$$$ Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method.

A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods.
A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification.
A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ However, there are many freely available English sentiment corpora on the Web.
A related, yet more sophisticated technique is proposed in (Wan,2009), where a co-training approach is used to leverage resources from both a source and a target language. $$$$$ Table 1 shows the comparison results.

Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. $$$$$ We can see that the performance of the cotraining approach with the balanced growth can be improved after a few iterations.
Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. $$$$$ Then, we can view the classification problem in two independent views: Chinese view with only Chinese features and English view with only English features.
Wan (2009) combined the annotated English reviews, unannotated Chinese reviews and their translations to co-train two separate classifiers for each language, respectively. $$$$$ Typical text classifiers include Support Vector Machine (SVM), Na√Øve Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN) , etc.

In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages.
In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ We also thank the anonymous reviewers for their useful comments.
In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).
In the document-level review polarity classification experiment, we used the dataset adopted in (Wan, 2009). $$$$$ Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.

In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g.
In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001).
In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem.
In the review polarity classification experiment, we use unigram, bigram of Chinese words as features which is suggested by (Wan, 2009). $$$$$ During the bootstrapping process of smoothed co-training, the classifier at each iteration is replaced with a majority voting scheme applied to all classifiers constructed at previous iterations.

The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. $$$$$ In this paper, we propose to use the co-training approach to address the problem of cross-lingual sentiment classification.
The original annotations 1104 can be produced either manually or automatically. Wan (2009) constructs a multilingual classifier using co-training. $$$$$ Standard Na√Øve Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language.

Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). $$$$$ We also thank the anonymous reviewers for their useful comments.
Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). $$$$$ A review is represented by both its English view and its Chinese view.
Methods which follow this two step approach include the EM-based approach by Rigutini et al (2005), the CCA approach by Fortuna and Shawe-Taylor (2005), the information bottleneck approach by Ling et al (2008), and the co-training approach by Wan (2009). $$$$$ We also thank the anonymous reviewers for their useful comments.

The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ 2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service.
The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification.
The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).
The proposed co-regression algorithm can make full use of both the features in the source language and the features in the target language in a unified framework similar to (Wan 2009). $$$$$ However, there are many freely available English sentiment corpora on the Web.

Wan (2009) constructs a multilingual classifier using co-training. $$$$$ Lastly we conclude this paper in Section 5.
Wan (2009) constructs a multilingual classifier using co-training. $$$$$ We also thank the anonymous reviewers for their useful comments.
Wan (2009) constructs a multilingual classifier using co-training. $$$$$ And the unlabeled set is used.
Wan (2009) constructs a multilingual classifier using co-training. $$$$$ In order to address the above problem, we propose to use the co-training approach to make use of some amounts of unlabeled Chinese reviews to improve the classification accuracy.

Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. $$$$$ After a large number of iterations, the performance of the cotraining approach decreases because noisy training examples may be selected from the remaining unlabeled set.
Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. $$$$$ This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data.
Wan (2009) proposes to use ensemble method to train better Chinese sentiment classification model on English labeled data and their Chinese translation. $$$$$ The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification.

SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009). $$$$$ We will employ the structural correspondence learning (SCL) domain adaption algorithm used in (Blitzer et al., 2007) for linking the translated text and the natural text.
SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009). $$$$$ To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages.
SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009). $$$$$ In future work, we will improve the sentiment classification accuracy in the following two ways: 1) The smoothed co-training approach used in (Mihalcea, 2004) will be adopted for sentiment classification.
SVM: We train a SVM classifier on the Chinese labeled data.MT-Cotrain: This is the co-training based approach described in (Wan, 2009). $$$$$ This work was supported by NSFC (60873155), RFDP (20070001059), Beijing Nova Program (2008B03), National High-tech R&D Program (2008AA01Z421) and NCET (NCET-08-0006).

Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ A few commercial machine translation services can be publicly accessed, e.g.
Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ Typical text classifiers include Support Vector Machine (SVM), Na√Øve Bayes (NB), Maximum Entropy (ME), K-Nearest Neighbor (KNN) , etc.
Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ 2) The feature distributions of the translated text and the natural text in the same language are still different due to the inaccuracy of the machine translation service.
Wan (2009) also leveraged an available English corpus for Chinese sentiment classification by using the co-training approach to make full use of both English and Chinese features in a unified framework. $$$$$ And the unlabeled set is used.

Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ Section 4 shows the experimental results.
Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ Because most previous work focuses on English sentiment classification, many annotated corpora for English sentiment classification are freely available on the Web.
Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ The SVM classifier is adopted as the basic classifier in the proposed approach.
Sentiment classification can be performed on words, sentences or documents, and is generally categorized into lexicon-based and corpus-based classification method (Wan, 2009). $$$$$ This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data.
