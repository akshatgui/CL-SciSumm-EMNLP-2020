Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. $$$$$ Table 2 shows the accuracies of the models on validation data.
Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. $$$$$ In the multi lingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. $$$$$ The value h = 0 is used for dependencies where the head is a special root-symbol of the sentence.

Carreras (2007) employs his own extension of Eisner's algorithm for the case of projective trees and second-order models that include head grandparent relations. $$$$$ As in the Eisner approach, our algo rithm visits sentence spans in a bottom up fashion, and constructs a chart with two types of dynamic programming structures, namely open and closedstructures?see Figure 2 for a diagram.
Carreras (2007) employs his own extension of Eisner's algorithm for the case of projective trees and second-order models that include head grandparent relations. $$$$$ In the multi lingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.

 $$$$$ In this paper we extend the parsing model with other typesof second-order relations.
 $$$$$ Our experiments show that considering higher-order information yields signifi cant improvements in parsing accuracy, but comes at a high cost in terms of both timeand memory consumption.

 $$$$$ In this section we sketch an extension of the projective dynamic programming algorithm of Eisner (1996; 2000) for the higher-order model defined above.
 $$$$$ The definition of ?2(x, h, m, c) is: ? dir ? cpos(xh) ? cpos(xm) ? cpos(xc) ? dir ? cpos(xh) ? cpos(xc) ? dir ? cpos(xm) ? cpos(xc) ? dir ? form(xh) ? form(xc) ? dir ? form(xm) ? form(xc) ? dir ? cpos(xh) ? form(xc) ? dir ? cpos(xm) ? form(xc) ? dir ? form(xh) ? cpos(xc) ? dir ? form(xm) ? cpos(xc)
 $$$$$ Our experiments show that considering higher-order information yields signifi cant improvements in parsing accuracy, but comes at a high cost in terms of both timeand memory consumption.

 $$$$$ 3.1 Impact of Higher-Order Factorization.
 $$$$$ We compared four models at increasing orders of factorizations.
 $$$$$ Ourmodel represents dependency trees with factors that include three types of relations be tween the tokens of a dependency and theirchildren.

To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. $$$$$ The second-orderrelations that we incorporate in the model yield significant improvements in accuracy.
To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. $$$$$ In the multi lingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
To reap the benefits of these advances, we use a higher-order projective dependency parsing algorithm (Carreras, 2007) which is an extension of the span-based parsing algorithm (Eisner, 1996), for syntactic dependency parsing. $$$$$ The higher-order model defines additional m h ch cmi cmoThey 1 2 - - sold 2 0 - 1 51,200 3 4 - - cars 4 2 - 3 in 5 2 4 - 7the 6 7 - - U.S. 7 5 - 6 Table 1: Higher-order factors for an example sentence.

For more details of the second-order parsing algorithm, see (Carreras, 2007). $$$$$ Ourmodel represents dependency trees with factors that include three types of relations be tween the tokens of a dependency and theirchildren.
For more details of the second-order parsing algorithm, see (Carreras, 2007). $$$$$ In the experiments presented above we observed that the algorithmdoes not over-fit, and that after two or three train ing epochs only small variations in accuracy occur.
For more details of the second-order parsing algorithm, see (Carreras, 2007). $$$$$ Such factorizations allow the definition of second-order features associated with sibling and grand-parental relations.

We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). $$$$$ Table 2 shows the accuracies of the models on validation data.
We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). $$$$$ For some languages, our models obtain state-of-the-art results.One drawback of our approach is that the inference algorithms for higher-order models are very expensive.
We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). $$$$$ The score of a factor is: score2(w,x, ?h, m, l, ch, cmi, cmo?)

The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. $$$$$ days of computation, or a maximum of 15 epochs.
The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. $$$$$ in the exam ple), which has a grand-parental relation with the two candidate tokens that the phrase may attach?see e.g.
The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. $$$$$ Such factorizations allow the definition of second-order features associated with sibling and grand-parental relations.
The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. $$$$$ UAS LAS Arabic 1.21 1.8GB 81.48 70.20 Basque 33.15 1.2GB 81.08 75.73 Catalan 5.50 1.7GB 92.46 87.60 Chinese 1461.66 60MB 86.20 80.86 Czech 18.19 1.8GB 85.16 78.60 English 15.57 1.0GB 90.63 89.61 Greek 8.10 250MB 81.37 73.56 Hungarian 5.65 1.6GB 79.92 75.42 Italian 12.44 900MB 87.19 83.46 Turkish 116.55 600MB 82.41 75.85 Average - - 84.79 79.09 Table 3: Performance of the higher-order projective models on the multilingual track of the CoNLL-2007 task.

Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). $$$$$ To train models, weused ?projectivized?
Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). $$$$$ In the multi lingual exercise of the CoNLL-2007 shared task (Nivre et al, 2007), our system obtains the best accuracy for English, and the second best accuracies for Basque and Czech.
Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). $$$$$ n] is the index of the head token, 957 lh m ccch mi mo Figure 1: A factor in the higher-order parsing model.

It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. $$$$$ The information included in the factors determines the type of features that the model can exploit.
It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. $$$$$ For the singleroot case, it is necessary to treat the root token dif ferently than other tokens.
It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. $$$$$ For languages with many dependency la bels or long sentences, training and parsing becomes impractical for current machines.
It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. $$$$$ The first twocolumns report the speed (in sentences per minute) and mem ory requirements of the training algorithm?these evaluationswere made on the first 1,000 training sentences with a Dual Core AMD OpteronTM Processor 256 at 1.8GHz with 4GB of memory.

The second order algorithm of Carreras (2007) uses in addition to McDonaldand Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. $$$$$ (which governs ?the U.S.?)
The second order algorithm of Carreras (2007) uses in addition to McDonaldand Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. $$$$$ where, as in the first-order model, h, m and l are respectively the head, modifier and label of the main dependency of the factor; ch is the child of h in [h . . .m] that is closest to m; cmi is child of m inside [h . . .m]that is furthest from m; cmo is the child of m outside [h . . .

The dependency parser is based on Carreras's algorithm (Carreras, 2007) and second order spanning trees. $$$$$ In particular, we incorpo rate relations between the head and modifier tokens and the children of the modifier.
The dependency parser is based on Carreras's algorithm (Carreras, 2007) and second order spanning trees. $$$$$ versions of the training depen dency trees.21We are grateful to the providers of the treebanks that con stituted the data for the shared task (Hajic?
The dependency parser is based on Carreras's algorithm (Carreras, 2007) and second order spanning trees. $$$$$ The second model is similar to that of McDonald and Pereira (2006): a factor consists of a main labeleddependency and the head child closest to the modifier (ch).

 $$$$$ Ourmodel represents dependency trees with factors that include three types of relations be tween the tokens of a dependency and theirchildren.
 $$$$$ For languages with many dependency la bels or long sentences, training and parsing becomes impractical for current machines.
 $$$$$ We present experiments with a dependency parsing model defined on rich factors.

Carreras (2007) introduced the left-most and right-most grandchild as factors. $$$$$ (which governs ?the U.S.?)
Carreras (2007) introduced the left-most and right-most grandchild as factors. $$$$$ The second allows many dependencies involving the root token.
Carreras (2007) introduced the left-most and right-most grandchild as factors. $$$$$ We present experiments with a dependency parsing model defined on rich factors.

We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. $$$$$ We present experiments with a dependency parsing model defined on rich factors.
We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. $$$$$ However, the inference algorithms for our factorization are very ex pensive in terms of time and memory consumption,and become impractical when dealing with many la bels or long sentences.
We use the factor model of Carreras (2007) as starting point for our experiments, cf. Section 4. $$$$$ In particular, we incorpo rate relations between the head and modifier tokens and the children of the modifier.

We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ A significant part of the system and the code was based on my previous system in the CoNLL-X task, developed with Mihai Surdeanu and Llu??s Ma`rquez at the UPC.
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ Specifically, these approaches considered sibling relations of the modifier token (Eisner, 1996; McDonald and Pereira, 2006).
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ Our experiments show that considering higher-order information yields signifi cant improvements in parsing accuracy, but comes at a high cost in terms of both timeand memory consumption.
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ The last two columns report unlabelled (UAS) and labelled (LAS) attachment scores on test data.

We start with the model introduced by Carreras (2007). $$$$$ We extend the projective pars ing algorithm of Eisner (1996) for our case,and train models using the averaged perceptron.
We start with the model introduced by Carreras (2007). $$$$$ We have presented dependency parsing models that exploit higher-order factorizations of trees.
We start with the model introduced by Carreras (2007). $$$$$ In the experiments presented above we observed that the algorithmdoes not over-fit, and that after two or three train ing epochs only small variations in accuracy occur.

Second order factors of Carreras (2007). $$$$$ The author was supported by the Catalan Ministry of Innovation, Universities and Enterprise.
Second order factors of Carreras (2007). $$$$$ second-order features through a function ?2(x, h, m, c) which maps a head, a modifierand a child in a feature vector in Rd2 . The param eters of the model are a collection of four vectors for each dependency label: wl1 ? Rd1 as in the first-order model; and wlh,wlmi and wlmo, all three in Rd2 and each associated to one of the adjacent dependencies in the factor.
Second order factors of Carreras (2007). $$$$$ Thus, a promising line of research is the investigation of methods toefficiently incorporate higher-order relations in dis criminative parsing.
Second order factors of Carreras (2007). $$$$$ in the exam ple), which has a grand-parental relation with the two candidate tokens that the phrase may attach?see e.g.

 $$$$$ = ?1(x, h, m) ? wl1.The higher-order model defined in this paper de composes a dependency structure into factors that include children of the head and the modifier.
 $$$$$ A significant part of the system and the code was based on my previous system in the CoNLL-X task, developed with Mihai Surdeanu and Llu??s Ma`rquez at the UPC.
 $$$$$ Let dir be ?right?
