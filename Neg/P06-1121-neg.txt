If we analyze these three models in terms of expressive power, the Galley et al (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang's model. $$$$$ We contrast differentapproaches on real examples, show that our esti mates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.
If we analyze these three models in terms of expressive power, the Galley et al (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang's model. $$$$$ We compare our syntax-based system against an implementation of the alignment template (AlTemp) approach to MT (Och and Ney, 2004), which is widely considered to represent the state of the art in the field.
If we analyze these three models in terms of expressive power, the Galley et al (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang's model. $$$$$ Similarly to (Poutsma, 2000; Wu, 1997; Yamadaand Knight, 2001; Chiang, 2005), the rules dis cussed in this paper are equivalent to productions of synchronous tree substitution grammars.
If we analyze these three models in terms of expressive power, the Galley et al (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang's model. $$$$$ fail to account for human translation behavior.Galley et al (2004) alleviate this modeling prob lem and present a method for acquiring millions of syntactic transfer rules from bilingual corpora,which we review below.

The xRS formalism utilized by Galley et al (2006) allows for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases. $$$$$ Syn tactic approaches seek to remedy these problems.In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Gal ley et al, 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we constructa large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words.
The xRS formalism utilized by Galley et al (2006) allows for the use of translation rules that have multi-level target tree annotations and discontinuous source language phrases. $$$$$ Context-free grammars (such as PennTreebank and Chiang?s grammars) make independence assumptions that are arguably often unrea sonable, but as our work suggests, relaxations of these assumptions by using contextually richer rules results in translations of increasing quality.

The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et als (2006) and can easily exploit and expand on previous research in phrase-based machine translation. $$$$$ While tree-to-tree grammars are richer formalisms thatprovide the potential benefit of rules that are lin guistically better motivated, modeling the syntax of both languages comes as an extra cost, and itis admittedly more helpful to focus our syntac tic modeling effort on the target language (e.g., English) in cases where it has syntactic resources (parsers and treebanks) that are considerably moreavailable than for the source language.
The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et als (2006) and can easily exploit and expand on previous research in phrase-based machine translation. $$$$$ It is appealing to model the transformation of pi into f using tree-to-string (xRs) transducers, sincetheir theory has been worked out in an exten sive literature and is well understood (see, e.g.,(Graehl and Knight, 2004)).

 $$$$$ While tree-to-tree grammars are richer formalisms thatprovide the potential benefit of rules that are lin guistically better motivated, modeling the syntax of both languages comes as an extra cost, and itis admittedly more helpful to focus our syntac tic modeling effort on the target language (e.g., English) in cases where it has syntactic resources (parsers and treebanks) that are considerably moreavailable than for the source language.
 $$$$$ While tree-to-tree grammars are richer formalisms thatprovide the potential benefit of rules that are lin guistically better motivated, modeling the syntax of both languages comes as an extra cost, and itis admittedly more helpful to focus our syntac tic modeling effort on the target language (e.g., English) in cases where it has syntactic resources (parsers and treebanks) that are considerably moreavailable than for the source language.
 $$$$$ (d) VP(x0:VBP, x1:NP) ? x0, x1 (e) VBP(include) ?-?

Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. $$$$$ We contrast differentapproaches on real examples, show that our esti mates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rules provide a 3.63 BLEU point increase over minimal rules.
Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. $$$$$ We compare our syntax-based system against an implementation of the alignment template (AlTemp) approach to MT (Och and Ney, 2004), which is widely considered to represent the state of the art in the field.
Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. $$$$$ 2.1 Tree-to-string alignments.
Syntax-driven (Galley et al, 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process. $$$$$ r2: X(a, Y(b, c)) ? b?, a?, c?

(Galley et al, 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. $$$$$ Context-free grammars (such as PennTreebank and Chiang?s grammars) make independence assumptions that are arguably often unrea sonable, but as our work suggests, relaxations of these assumptions by using contextually richer rules results in translations of increasing quality.
(Galley et al, 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. $$$$$ We believe it will be beneficial to account for this finding in future work in syntax-based SMT and in efforts to improve upon (Chiang, 2005).
(Galley et al, 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal. $$$$$ for probabilities assigned by our models.

Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al, 2006). $$$$$ We believe it will be beneficial to account for this finding in future work in syntax-based SMT and in efforts to improve upon (Chiang, 2005).
Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al, 2006). $$$$$ This property does not hold for nodes outside of F . For instance, PP[4-5] and VBG[4] are two nodes of the same graph frontier, but they cannot be ordered because of their overlapping spans.
Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al, 2006). $$$$$ For example, some xRs 961 rules may describe the transformation of does not into ne ... pas in French.
Similarly, the tree-to-string syntax-based transduction approach offers a complete translation framework (Galley et al, 2006). $$$$$ The performance with our largest rule set represents a 3.63 BLEU point increase (14.8% relative) compared to using only minimal rules, which indicates positive prospects for using even larger rules.

 $$$$$ Its rule binarization is described in (Zhang et al, 2006).

Six MT systems were combined $$$$$ Now we give a brief overview of how such transformational rules are acquired automaticallyin GHKM.1 In Figure 1, the (pi, f ,a) triple is represented as a directed graph G (edges going down ward), with no distinction between edges of pi and alignments.
Six MT systems were combined $$$$$ We believe it will be beneficial to account for this finding in future work in syntax-based SMT and in efforts to improve upon (Chiang, 2005).
Six MT systems were combined $$$$$ We believe that our tree-to-string model has several advantages over tree-to-tree transformations such as the ones acquired by Poutsma (2000).

In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al, 2006). $$$$$ Finally, we show that our contextually richer rules provide a 3.63 BLEU point increase over those of (Galley et al, 2004).
In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al, 2006). $$$$$ We believe that our tree-to-string model has several advantages over tree-to-tree transformations such as the ones acquired by Poutsma (2000).
In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al, 2006). $$$$$ Second, we pro pose probability estimates and a training procedure for weighting these rules.
In this paper, we focus on syntactic translation with tree-transducer rules (Galley et al, 2006). $$$$$ Statistical MT has made great progress in the last few years, but current translation models are weakon re-ordering and target language fluency.

GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al, 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). $$$$$ For example, the span of VP[4-5] either precedes or follows, but never overlaps the span of any node n?
GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al, 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). $$$$$ Furthermore, our decoder doesn?t incorporate any syntax-based language model, and admittedly our ability to penal ize ill-formed parse trees is still limited.Finally, we evaluated our system on the NIST 02 test set with the three different rule sets (see Table 6).
GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al, 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). $$$$$ AcknowledgmentsWe would like to thank anonymous review ers for their helpful comments and suggestions.
GIZA++ union alignments have been used in the state-of-the-art syntax-based statistical MT system described in (Galley et al, 2006) and in the hierarchical phrase-based system Hiero (Chiang, 2007). $$$$$ We compare our syntax-based system against an implementation of the alignment template (AlTemp) approach to MT (Och and Ney, 2004), which is widely considered to represent the state of the art in the field.

Using these alignments, which we refer to as GIZA++ union + link deletion, we train a syntax-based translation system similar to that described in (Galley et al., 2006). $$$$$ Context-free grammars (such as PennTreebank and Chiang?s grammars) make independence assumptions that are arguably often unrea sonable, but as our work suggests, relaxations of these assumptions by using contextually richer rules results in translations of increasing quality.
Using these alignments, which we refer to as GIZA++ union + link deletion, we train a syntax-based translation system similar to that described in (Galley et al., 2006). $$$$$ We presented some theoretical argumentsfor not limiting extraction to minimal rules, val idated them on concrete examples, and presented experiments showing that contextually richer rulesprovide a 3.63 BLEU point increase over the min imal rules of (Galley et al, 2004).
Using these alignments, which we refer to as GIZA++ union + link deletion, we train a syntax-based translation system similar to that described in (Galley et al., 2006). $$$$$ Similarly to (Poutsma, 2000; Wu, 1997; Yamadaand Knight, 2001; Chiang, 2005), the rules dis cussed in this paper are equivalent to productions of synchronous tree substitution grammars.

For example, Galley et al (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al, 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. $$$$$ For any frontier of graph G containinga given node n ? F , spans on that frontier de fine an ordering between n and each other frontier node n?.
For example, Galley et al (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al, 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. $$$$$ AcknowledgmentsWe would like to thank anonymous review ers for their helpful comments and suggestions.

Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. $$$$$ With a difference of 6.4 BLEU points for both language pairs, we consider the resultsof our syntax-based system particularly promis ing, since these are the highest scores to date thatwe know of using linguistic syntactic transformations.
Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. $$$$$ The concern in GHKM was to extract minimalrules, whereas ours is to extract rules of any arbi trary size.
Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. $$$$$ With the notable exception of Poutsma, most related works rely on models that are restricted to synchronous context-free grammars (SCFG).While the state-of-the-art hierarchical SMT system (Chiang, 2005) performs well despite stringent constraints imposed on its context-free gram mar, we believe its main advantage lies in its ability to extract hierarchical rules across phrasal boundaries.
Galley et al (2006 )argued that breaking a single tree pair into multiple decompositions is important for correct probability modeling. $$$$$ We presented some theoretical argumentsfor not limiting extraction to minimal rules, val idated them on concrete examples, and presented experiments showing that contextually richer rulesprovide a 3.63 BLEU point increase over the min imal rules of (Galley et al, 2004).

For example, Galley et al (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. $$$$$ A particular instance may look like this: VP(AUX(does), RB(not), x0:VB) ? ne, x0, pas lhs(ri) can be any arbitrary syntax tree fragment.Its leaves are either lexicalized (e.g. does) or variables (x0, x1, etc).
For example, Galley et al (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. $$$$$ We will see in Sections 3 and 5 why extracting only minimal rules can be highly problematic.
For example, Galley et al (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. $$$$$ On the other hand, if we had good rule probabilities, we could compute the most likely (Viterbi) derivations for each trainingexample.
For example, Galley et al (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. $$$$$ An empirical evaluation against a state-of-the-art SMT system similar to (Och and Ney, 2004) indicates positive prospects.

Following Galley et al (2006)'s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ HR0011 06-C-0022.
Following Galley et al (2006)'s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ We compare our syntax-based system against an implementation of the alignment template (AlTemp) approach to MT (Och and Ney, 2004), which is widely considered to represent the state of the art in the field.
Following Galley et al (2006)'s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ AcknowledgmentsWe would like to thank anonymous review ers for their helpful comments and suggestions.

As shown in the following parts of this paper, it works very well with the existing techniques, such as rule composing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ A particular instance may look like this: VP(AUX(does), RB(not), x0:VB) ? ne, x0, pas lhs(ri) can be any arbitrary syntax tree fragment.Its leaves are either lexicalized (e.g. does) or variables (x0, x1, etc).
As shown in the following parts of this paper, it works very well with the existing techniques, such as rule composing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ We also usereal examples to show that our probability mod els estimated from a large number of derivations favor phrasal re-orderings that are linguistically well motivated.

In this work, the issue of translation rule extraction is studied in the string-to-tree model proposed by Galley et al (2006). $$$$$ Second, we pro pose probability estimates and a training procedure for weighting these rules.
In this work, the issue of translation rule extraction is studied in the string-to-tree model proposed by Galley et al (2006). $$$$$ AcknowledgmentsWe would like to thank anonymous review ers for their helpful comments and suggestions.
In this work, the issue of translation rule extraction is studied in the string-to-tree model proposed by Galley et al (2006). $$$$$ In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests.

Finally, the rule composing method (Galley et al, 2006) is used to compose two or more minimal GHKM or SPMT rules having shared states to form larger rules. $$$$$ Also, on the one hand, our AlTemp system represents quite mature technology, and in corporates highly tuned model parameters.
Finally, the rule composing method (Galley et al, 2006) is used to compose two or more minimal GHKM or SPMT rules having shared states to form larger rules. $$$$$ We presented some theoretical argumentsfor not limiting extraction to minimal rules, val idated them on concrete examples, and presented experiments showing that contextually richer rulesprovide a 3.63 BLEU point increase over the min imal rules of (Galley et al, 2004).

Our baseline MT system is built based on the string-to-tree model proposed in (Galley et al, 2006). $$$$$ Its rule binarization is described in (Zhang et al, 2006).
Our baseline MT system is built based on the string-to-tree model proposed in (Galley et al, 2006). $$$$$ HR0011 06-C-0022.
Our baseline MT system is built based on the string-to-tree model proposed in (Galley et al, 2006). $$$$$ In this paper, we developed probability models for the multi-level transfer rules presented in (Galley et al, 2004), showed how to acquire larger rules that crucially condition on more syntactic context, and how to pack multiple derivations, including interpretations of unaligned words, into derivation forests.
