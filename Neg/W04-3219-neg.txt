In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ The lattice was realized as a set of |S |+ 1 vertices v0..v|S |and a set of edges between those vertices; each edge was labeled with a sequence of words and a real number.
In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ Barzilay & Lee (2003) exploit the metainformation implicit in dual collections of newswire articles, but focus on learning sentence-level patterns that provide a basis for generation.
In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ Multisequence alignment (MSA) is used to identify sentences that share formal (and presumably semantic) properties.
In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ One further simplification was made.

However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ However, as we show below, the approach may be of limited generality, even within the training domain.
However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ While the alternations our system produces are currently limited in character, the field of SMT offers a host of possible enhancements—including reordering models—affording a natural path for future improvements.
However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ We showed that this approach can be used to generate paraphrases that are preferred by humans to sentence-level paraphrases produced by other techniques.
However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ We greatly appreciate the careful comments of three anonymous reviewers.

Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.
Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned from much simpler techniques.
Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.
Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ Collocations and other non-compositional phrases receive a higher probability as a whole than they would as independent single word replacements.

Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ First and foremost, phrasal scoring by Model 1 avoids the sparsity problems associated with estimating each phrasal replacement probability with MLE (Vogel et al. 2003).
Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ We are grateful to Mo Corston-Oliver, Jeff Stevenson, Amy Muia, and Orin Hargraves of the Butler Hill Group for their work in annotating the data used in the experiments.
Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ Following Och & Ney’s methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required).

Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ In addition, fast algorithms exist for computing the n-best lists over a lattice (Soong & Huang 1991).
Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ By adopting at the outset a paradigm geared toward generating sentences, this approach overcomes many problems encountered by task-specific approaches.
Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993).
Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ We will be experimenting with more sophisticated decoder models designed to handle reordering and mappings to discontinuous elements.

A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ This paper has also benefited from discussions with Ken Church, Mark Johnson, and Steve Richardson.
A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ Mean Levenshtein distance was 5.17; mean sentence length was 18.6 words.
A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ This paper has also benefited from discussions with Ken Church, Mark Johnson, and Steve Richardson.

As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below).
As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ We presented a novel approach to the problem of generating sentence-level paraphrases in a broad semantic domain.
As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ In addition, fast algorithms exist for computing the n-best lists over a lattice (Soong & Huang 1991).
As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ We presented a novel approach to the problem of generating sentence-level paraphrases in a broad semantic domain.

Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ We then produced paraphrases with each of the following systems and compared them with MSA and WN: For the sake of consistency, we did not use the judgments provided by Barzilay and Lee; instead we had two raters judge whether the output from each system was a paraphrase of the input sentence.
Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ A high u value permits more conservative paraphrases.
Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ Their methodology provides striking results within a limited domain characterized by a high frequency of stereotypical sentence types.
Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ In each case, however, the information extracted is limited to a small set of patterns.

Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below).
Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ While the alternations our system produces are currently limited in character, the field of SMT offers a host of possible enhancements—including reordering models—affording a natural path for future improvements.
Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ A monotone phrasal decoder generates contextual replacements.
Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ Particularly noteworthy is the lossiness of MSA seen in row 4.

Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003).
Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ We presented a novel approach to the problem of generating sentence-level paraphrases in a broad semantic domain.
Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ We are grateful to Mo Corston-Oliver, Jeff Stevenson, Amy Muia, and Orin Hargraves of the Butler Hill Group for their work in annotating the data used in the experiments.

Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g.
Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ A monotone phrasal decoder generates contextual replacements.
Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ We remain, however, solely responsible for this content.
Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ Finally, since source and target languages are identical, we added an identity mapping for each source word si: an edge from vi-1 to vi with label si and a uniform probability u.

Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ In addition to obtaining greater absolute quantities of data in the form of clustered articles, we also seek to extract aligned sentence pairs that instantiate a richer set of phenomena.
Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993).
Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ Figure 2 presents an example.
Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ A second important contribution of this work is a method for building and tracking the quality of large, alignable monolingual corpora from structured news data on the Web.

Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ This allows for handling unseen words.
Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.
Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.
Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ A monotone phrasal decoder generates contextual replacements.

Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ Changing a single content word is a legitimate form of paraphrase, and the ability to paraphrase across an arbitrarily large sentence set and arbitrary domains is a desideratum of paraphrase research.
Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ Shinyama et al. (2002) identify dependency paths in two collections of newspaper articles.
Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ The results of this final evaluation are summarized in Table 2.
Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ We also plan to pursue better (automated) metrics for paraphrase evaluation.

Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. $$$$$ We greatly appreciate the careful comments of three anonymous reviewers.
Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. $$$$$ Consider the third and fourth rows of Table 3, which indicate the extent of embellishment and lossiness found in MSA paraphrases and the topranked PR paraphrases.
Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. $$$$$ A monotone phrasal decoder generates contextual replacements.

Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ In addition to obtaining greater absolute quantities of data in the form of clustered articles, we also seek to extract aligned sentence pairs that instantiate a richer set of phenomena.
Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ We showed that this approach can be used to generate paraphrases that are preferred by humans to sentence-level paraphrases produced by other techniques.
Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ Lin & Pantel (2002) derive inference rules by parsing text fragments and extracting semantically similar paths.
Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ One major agenda item therefore will be acquisition of larger (and more diverse) data sets.

For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. $$$$$ One data source has been multiple translations of classic literary works (Barzilay & McKeown 2001; Ibrahim 2002; Ibrahim et al. 2003).
For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. $$$$$ Changing a single content word is a legitimate form of paraphrase, and the ability to paraphrase across an arbitrarily large sentence set and arbitrary domains is a desideratum of paraphrase research.
For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. $$$$$ Impressionistically, discrete events like sudden disasters, business announcements, and deaths tend to yield tightly focused clusters, while ongoing stories like the SARS crisis tend to produce very large and unfocused clusters.
For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. $$$$$ pairs (limited to those containing no more than five cepts, for reasons of computational efficiency) occurring in at least one aligned sentence somewhere in our training corpus into a single replacement database.

Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ In particular, we show that SMT techniques can be extended to paraphrase given sufficient monolingual parallel data.1 We show that a huge corpus of comparable and alignable sentence pairs can be culled from ready-made topical/temporal clusters of news articles gathered on a daily basis from thousands of sources on the World Wide Web, thereby permitting the system to operate outside the narrow domains typical of existing systems.
Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003).
Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ As in (Vogel et al. 2003), we assigned probabilities to these phrasal replacements via IBM Model 1.
Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ We will be experimenting with more sophisticated decoder models designed to handle reordering and mappings to discontinuous elements.

Quirk et al (2004) first recast paraphrase generation as monolingual SMT. $$$$$ The MSA WordNet baseline was created by selecting a subset of the words in each test sentence—proportional to the number of words replaced by MSA in the same sentence—and replacing each with an arbitrary word from its most frequent WordNet synset.
Quirk et al (2004) first recast paraphrase generation as monolingual SMT. $$$$$ Although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data.
Quirk et al (2004) first recast paraphrase generation as monolingual SMT. $$$$$ Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.

The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ Pang et al. (2003) obtain parallel monolingual texts from a set of 100 multiply-translated news articles.
The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ We presented a novel approach to the problem of generating sentence-level paraphrases in a broad semantic domain.
The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below).
The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ Consider the third and fourth rows of Table 3, which indicate the extent of embellishment and lossiness found in MSA paraphrases and the topranked PR paraphrases.
