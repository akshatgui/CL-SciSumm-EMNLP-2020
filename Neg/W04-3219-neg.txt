In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ A second important contribution of this work is a method for building and tracking the quality of large, alignable monolingual corpora from structured news data on the Web.
In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ The raters were presented with an input sentence and an output paraphrase from each system in random order to prevent bias toward any particular judgment.
In particular, for this study we employ the MSR Paraphrase Corpus (Quirk et al, 2004). $$$$$ To exploit richer data sets, we will also seek to address the monotone limitation of our decoder that further limits the complexity of our paraphrase output.

However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ To exploit richer data sets, we will also seek to address the monotone limitation of our decoder that further limits the complexity of our paraphrase output.
However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ To evaluate the quality of generation, we followed the lead of Barzilay & Lee (2003).
However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ Although simple, this approach has proven effective in SMT for several reasons.
However, it has been shown that the coverage of the paraphrase patterns is not high enough, especially when the used paraphrase pat terns are long or complicated (Quirk et al, 2004). $$$$$ One further simplification was made.

Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al. (1993).
Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ By adopting at the outset a paradigm geared toward generating sentences, this approach overcomes many problems encountered by task-specific approaches.
Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned from much simpler techniques.
Researchers employ the existing SMT models for PG (Quirk et al, 2004). $$$$$ We believe that the opposite is true.

Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ Thousands of news sources worldwide are competing to cover the same stories, in real time.
Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ Their methodology provides striking results within a limited domain characterized by a high frequency of stereotypical sentence types.
Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ The ability to categorize distinct word sequences as “meaning the same thing” is vital to applications as diverse as search, summarization, dialog, and question answering.
Baseline-1 follows the method pro posed in (Quirk et al, 2004), which generates paraphrases using typical SMT tools. $$$$$ Their methodology provides striking results within a limited domain characterized by a high frequency of stereotypical sentence types.

Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ We held out a set of news clusters from our training data and extracted a set of 250 sentence pairs for blind evaluation.
Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003).
Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ In the past, the lack of such a data source has hampered paraphrase research; our approach removes this obstacle.
Quirk et al (2004) also generate sentential paraphrases using a monolingual corpus. $$$$$ The performance of CL is likewise abysmal—again a language model does nothing to help.

A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ Lattices are in turn mapped to templates that can be used to produce novel transforms of input sentences.
A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ In more detail, we first gathered lexical translation probabilities of the form P(s  |t) by running five iterations of Model 1 on the training corpus.
A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ For example: We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al. 2004).
A similar approach to our sentence reconstruction method has been developed by Quirk et al (2004) for paraphrase generation. $$$$$ One data source has been multiple translations of classic literary works (Barzilay & McKeown 2001; Ibrahim 2002; Ibrahim et al. 2003).

As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ We also plan to pursue better (automated) metrics for paraphrase evaluation.
As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ Figure 1 shows an example of a monolingual alignment produced by Giza++.
As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ This was intended to isolate the contribution of the language model from the replacement model.
As sentential paraphrasing is more likely to alter meaning, Quirk et al (Quirk et al, 2004) approached paraphrasing as a monotonous decoding by a phrase-based SMT system. $$$$$ This paper has also benefited from discussions with Ken Church, Mark Johnson, and Steve Richardson.

Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ Multisequence alignment (MSA) is used to identify sentences that share formal (and presumably semantic) properties.
Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ In the past, the lack of such a data source has hampered paraphrase research; our approach removes this obstacle.
Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like Moses (Koehn et al, 2007). $$$$$ Although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data.

Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ The correlation between acceptability and PR sentence rank validates both the ranking algorithm and the evaluation methodology.
Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.
Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ One major agenda item therefore will be acquisition of larger (and more diverse) data sets.
Quirk et al (2004) built a paraphrase generation model from a monolingual comparable corpus based on a statistical machine translation framework, where the language model assesses the grammaticality of the translations ,i.e., generated expressions. $$$$$ Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.

Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ Unless one is prepared to assume that similar templates can be found for most sentence types, scalability and domain extensibility appear beyond the reach of MSA.
Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ A second important contribution of this work is a method for building and tracking the quality of large, alignable monolingual corpora from structured news data on the Web.
Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ One potential concern is that PR paraphrases usually involve simple substitutions of words and short phrases (a mean edit distance of 2.9 on the top ranked sentences), whereas MSA outputs more complex paraphrases (reflected in a mean edit distance of 25.8).
Most paraphrase generators use some standard SMT decoding algorithms (Quirk et al, 2004) or some off-the-shelf decoding tools like MOSES. $$$$$ Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.

Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.
Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: T and S being sentences in the same language.
Another piece of related work, (Quirk et al, 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. $$$$$ Mean Levenshtein distance was 5.17; mean sentence length was 18.6 words.

Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ We showed that this approach can be used to generate paraphrases that are preferred by humans to sentence-level paraphrases produced by other techniques.
Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ In the past, the lack of such a data source has hampered paraphrase research; our approach removes this obstacle.
Typical examples are paraphrasing using bilingual (Callison-Burch et al, 2006) or monolingual (Quirket al, 2004) data. $$$$$ The raters were presented with an input sentence and an output paraphrase from each system in random order to prevent bias toward any particular judgment.

Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ We are grateful to Mo Corston-Oliver, Jeff Stevenson, Amy Muia, and Orin Hargraves of the Butler Hill Group for their work in annotating the data used in the experiments.
Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.
Quirk et al (2004) build a monolingual translation system using a corpus of sentence pairs extracted from news articles describing same events. $$$$$ Barzilay & Lee (2003) exploit the metainformation implicit in dual collections of newswire articles, but focus on learning sentence-level patterns that provide a basis for generation.

Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ We are grateful to Mo Corston-Oliver, Jeff Stevenson, Amy Muia, and Orin Hargraves of the Butler Hill Group for their work in annotating the data used in the experiments.
Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ We greatly appreciate the careful comments of three anonymous reviewers.
Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ In each case, however, the information extracted is limited to a small set of patterns.
Quirk et al (2004) present an end-to-end paraphrasing system inspired by phrase-based machine translation that can both ac quire paraphrases and use them to generate new strings. $$$$$ Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003).

Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. $$$$$ We also plan to pursue better (automated) metrics for paraphrase evaluation.
Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. $$$$$ Given that our domain is restricted to the generation of monolingual paraphrase, interesting output can be produced without tackling the difficult problem of inter-phrase reordering.7 Therefore, along the lines of Tillmann et al. (1997), we rely on only monotone phrasal alignments, although we do allow intra-phrasal reordering.
Although there is a greater supply of paraphrasing corpora, such as the Multiple-Translation Chinese (MTC) corpus 1 and the Microsoft Research (MSR) Paraphrase Corpus (Quirk et al, 2004),. $$$$$ Much work obviously remains to be done.

Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web.
Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al., 2003).
Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ Relying on edit distance to identify likely paraphrases has the unfortunate result of excluding interesting sentence pairs that are similar in meaning though different in form.
Outside of NLI, prior research has also explored the task of monolingual word alignment using extensions of statistical MT (Quirk et al, 2004) and multi-sequence alignment (Barzilay and Lee, 2002). $$$$$ We accomplished this by using methods from the field of SMT, which is oriented toward learning and generating exactly the sorts of alternations encountered in monolingual paraphrase.

For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. $$$$$ We greatly appreciate the careful comments of three anonymous reviewers.
For other SMT methods, see Quirk et al (2004) and Bannard and Callison-Burch (2005) among others. $$$$$ The ability to categorize distinct word sequences as “meaning the same thing” is vital to applications as diverse as search, summarization, dialog, and question answering.

Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ Each sentence was normalized to lower case, and the pairs were filtered to reject: A total of 139K non-identical sentence pairs were obtained.
Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ Since the size of this data set made it difficult to obtain statistically significant results, we also included 141 randomly selected sentences from held-out clusters.
Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ Since, on our first pass, we found inter-rater agreement to be somewhat low (84%), we asked the raters to make a second pass of judgments on those where they disagreed; this significantly improved agreement (96.9%).
Paraphrase generation can be viewed as monolingual machine translation (Quirk et al, 2004), which typically includes a translation model and a language model. $$$$$ When an input sentence closely matches a template, results can be stunning.

Quirk et al (2004) first recast paraphrase generation as monolingual SMT. $$$$$ This paper has also benefited from discussions with Ken Church, Mark Johnson, and Steve Richardson.
Quirk et al (2004) first recast paraphrase generation as monolingual SMT. $$$$$ This is reflected in Table 3, which provides a breakdown of four dimensions of interest, as provided by one of our independent evaluators.
Quirk et al (2004) first recast paraphrase generation as monolingual SMT. $$$$$ We are grateful to Mo Corston-Oliver, Jeff Stevenson, Amy Muia, and Orin Hargraves of the Butler Hill Group for their work in annotating the data used in the experiments.

The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ In particular, we show that SMT techniques can be extended to paraphrase given sufficient monolingual parallel data.1 We show that a huge corpus of comparable and alignable sentence pairs can be culled from ready-made topical/temporal clusters of news articles gathered on a daily basis from thousands of sources on the World Wide Web, thereby permitting the system to operate outside the narrow domains typical of existing systems.
The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ Since the size of this data set made it difficult to obtain statistically significant results, we also included 141 randomly selected sentences from held-out clusters.
The SMT-based paraphrasing model used by Quirk et al (2004) was the noisy channel model of Brown et al (1993), which identified the optimal paraphrase T of a sentence S by finding. $$$$$ Inspection reveals that 15 of the 59 MSA paraphrases, or 25.4%, are based on a single high-frequency, domain-specific template (essentially a running tally of deaths in the Israeli-Palestinian conflict).
