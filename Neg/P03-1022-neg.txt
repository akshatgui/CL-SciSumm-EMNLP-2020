Consider the following example (see Figure 1 for an illustration) $$$$$ Because we included these cases in our evaluation we consider our approach at least comparable to Byron’s system when she uses only domain-independent semantics.
Consider the following example (see Figure 1 for an illustration) $$$$$ We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system.
Consider the following example (see Figure 1 for an illustration) $$$$$ We refined distance features and used metrics from information retrieval for determining non-NP-antecedents.

 $$$$$ However, the most important problem is the large amount of pronouns without antecedents.
 $$$$$ That way we are able to determine relatively powerful yet computationally cheap features.
 $$$$$ The most obvious difference is that in spoken dialogue there is an abundance of (personal and demonstrative) pronouns with non-NP-antecedents or no antecedents at all.

 $$$$$ During evaluation, the list of all correct links is used as the key list against which the response list produced by the classifier (cf. above) is compared.
 $$$$$ When evaluating her algorithm with only domain-independent semantics, Byron achieved 51% success rate.
 $$$$$ They measure the distance in markables between antecedent and anaphor, but in doing so they take the agreement value of the anaphor into account.
 $$$$$ Singleton they pronouns, in particular, are typical for spoken language (as opposed to written text).

Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e.g. rewriting rule sets. $$$$$ Byron extends a pronoun resolution algorithm (Tetrault, 2001) with semantic filtering, thus enabling it to resolve anaphors with non-NPantecedents as well.
Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e.g. rewriting rule sets. $$$$$ These facts have to be kept in mind when comparing our results to results of coreference resolution in written text.
Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e.g. rewriting rule sets. $$$$$ There are important differences between written text and spoken dialogue which have to be accounted for.
Xiaofeng et al. (2004) or Strube and Müller (2003) have shown the feasibility of decision trees for the domain of anaphora resolution; we have chosen this approach as it makes it possible to easily switch the information set for training and evaluation as opposed to e.g. rewriting rule sets. $$$$$ Then they combined these features and achieved results which were close to the best overall results they report.

 $$$$$ We apply a decision tree based approach to pronoun resolution in spoken dialogue.
 $$$$$ Finally, the feature wdist ic (25) measures the word-based distance between two expressions.
 $$$$$ We would like to thank Susanne Wilhelm and Lutz Wind for doing the annotations, Kerstin Sch¨urmann, Torben Pastuch and Klaus Rothenh¨ausler for helping with the data preparation.
 $$$$$ We believe that our system is more robust than hers and that it can more easily be ported to new domains.

 $$$$$ Table 4 gives the results for 3mf pronouns.
 $$$$$ We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features.
 $$$$$ In the following example, the “that ” in utterance (A3) refers back to utterance (A1).
 $$$$$ For every word, we calculated IDF as log(553/N ), with N =number of documents containing the word.

 $$$$$ Our system deals with pronouns with NPand non-NP-antecedents.
 $$$$$ We start with a model based on a set of predefined baseline features.
 $$$$$ For every word, we calculated IDF as log(553/N ), with N =number of documents containing the word.
 $$$$$ The underlying assumption is that the higher the importance of a non-NP expression, the higher the probability of its being referred back to.

Strube and Mu?ller (2003) propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data. $$$$$ The features are based on a verb list compiled from 553 Switchboard dialogues.3 For every verb occurring in the corpus, this list contains up to three entries giving the absolute count of cases where the verb has a direct argument of type NP, VP or S. When the verb list was produced, pronominal arguments were ignored.
Strube and Mu?ller (2003) propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data. $$$$$ The pronoun in (A5) is different in that it is indeed referential: it refers back to“that ” from (A3).
Strube and Mu?ller (2003) propose a similar idea, but aim instead at finding a subset of the available features with which the resulting coreference classifier yields the best clustering-level accuracy on held-out data. $$$$$ Table 5 shows the results for 3n pronouns.

 $$$$$ However, pronouns with NP-antecedents (like 3rd pers. masculine/feminine pronouns, cf. he in the example below) still constitute the largest fraction of all coreferential pronouns in the Switchboard corpus.
 $$$$$ The baseline model corresponds to a pronoun resolution algorithm commonly applied to written text, i.e., it uses only the features in the first two parts of Table 3.
 $$$$$ We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron’s (2002) manually tuned system.
 $$$$$ For anaphors with non-NP-antecedents, additional training and test data instances had to be generated.

 $$$$$ We would like to thank Susanne Wilhelm and Lutz Wind for doing the annotations, Kerstin Sch¨urmann, Torben Pastuch and Klaus Rothenh¨ausler for helping with the data preparation.
 $$$$$ Spoken dialogue contains more pronouns with nonNP-antecedents than written text does.

 $$$$$ These studies suggest that the performance of a pronoun resolution algorithm can be improved considerably by enabling it to resolve also pronouns with non-NP-antecedents.
 $$$$$ Instead, rather simple distance metrics were preferred.
 $$$$$ This is in contrast to the majority of other work on feature selection for anaphora resolution, which was hardly ever done systematically.
 $$$$$ However, the most important problem is the large amount of pronouns without antecedents.

 $$$$$ It has been demonstrated that systems based on these approaches achieve a performance that is comparable to hand-crafted systems.
 $$$$$ Instead, rather simple distance metrics were preferred.
 $$$$$ If an expression is used to refer to an entity that is not referred to by any other expression, it is considered a singleton.
 $$$$$ Our system deals with pronouns with NPand non-NP-antecedents.

 $$$$$ When evaluating her algorithm with only domain-independent semantics, Byron achieved 51% success rate.
 $$$$$ Our system deals with pronouns with NPand non-NP-antecedents.
 $$$$$ Finally, the resulting pairs were labelled P or N and added to the instances generated with NP-antecedents.

This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)). $$$$$ Our system deals with pronouns with NPand non-NP-antecedents.
This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)). $$$$$ Then they combined these features and achieved results which were close to the best overall results they report.
This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)). $$$$$ A major problem for pronoun resolution in spoken dialogue is the large number of personal and demonstrative pronouns which are either not referential at all (e.g. expletive pronouns) or for which a particular antecedent cannot easily be determined by humans (called vague anaphors by Eckert & Strube (2000)).
This suggests that a robust model of discourse structure could complement current robust interpretation systems, which tend to focus on only one aspect of the semantically ambiguous material, such as pronouns (e.g., Strube and Muller (2003)), definite descriptions (e.g., Vieira and Poesio (2000)), or temporal expressions (e.g., Wiebe et al (1998)). $$$$$ Because we included these cases in our evaluation we consider our approach at least comparable to Byron’s system when she uses only domain-independent semantics.

 $$$$$ Because of the difficulties a pronoun resolution algorithm encounters in spoken dialogue, previous approaches were applied only to tiny domains, they needed deep semantic analysis and discourse processing and relied on hand-crafted knowledge bases.
 $$$$$ We apply a decision tree based approach to pronoun resolution in spoken dialogue.
 $$$$$ Spoken dialogue contains more pronouns with nonNP-antecedents than written text does.
 $$$$$ The calculation of the IC was done as described for the ante ic feature.

 $$$$$ Table 2 shows the distribution of the agreement attribute (i.e. person, gender, and number) for the pronominal expressions in our corpus.
 $$$$$ For every non-NP-markable, an average TF*IDF value was calculated as the TF*IDF sum of all words comprising the markable, divided by the number of words in the markable.

 $$$$$ A3: [His] mother comes in and says, why did you let [him] [play in the dirt] , A:4 I guess [[he] ’s enjoying himself] .
 $$$$$ As for the first two pronouns in (B4), following Eckert & Strube (2000) and Byron (2002) we assume that referring expressions in disfluencies, abandoned utterances etc. are excluded from the resolution.
 $$$$$ While we were (almost) satisfied with the performance of these features, the major problem for a spoken dialogue pronoun resolution algorithm is the abundance of pronouns without antecedents.
 $$$$$ Note the relatively high number of singletons among the personal and demonstrative pronouns (223 for it, 60 for they and 82 for that).

 $$$$$ Because we included these cases in our evaluation we consider our approach at least comparable to Byron’s system when she uses only domain-independent semantics.
 $$$$$ The feature ante tfifd (23) is supposed to capture the relative importance of an expression for a dialogue.
 $$$$$ B& [It] ’s healthy, .. .

 $$$$$ We refined distance features and used metrics from information retrieval for determining non-NP-antecedents.
 $$$$$ Byron extends a pronoun resolution algorithm (Tetrault, 2001) with semantic filtering, thus enabling it to resolve anaphors with non-NPantecedents as well.
 $$$$$ The most obvious difference is that in spoken dialogue there is an abundance of (personal and demonstrative) pronouns with non-NP-antecedents or no antecedents at all.
 $$$$$ A3: [His] mother comes in and says, why did you let [him] [play in the dirt] , A:4 I guess [[he] ’s enjoying himself] .

 $$$$$ Taken together, the dialogues contain 30810 tokens (words and punctuation) in 3275 sentences / 1771 turns.
 $$$$$ We would like to thank Susanne Wilhelm and Lutz Wind for doing the annotations, Kerstin Sch¨urmann, Torben Pastuch and Klaus Rothenh¨ausler for helping with the data preparation.
 $$$$$ Pronoun resolution in spoken dialogue also has to deal with the whole range of difficulties that come with processing spoken language: disfluencies, hesitations, abandoned utterances, interruptions, backchannels, etc.
