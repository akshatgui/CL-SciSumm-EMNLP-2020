More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms.
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003).
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx.
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms.

A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ It is difficult to say whether such improvement will carry over to other comparable corpora with less document structure and meta-data.
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ The links are weighted by their inverse frequency in the document, so a link that appears often does not contribute much to this feature’s value.
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ Also we discard the contribution from any context position and word pair that relates to more than 1,000 distinct source or target words, since it explodes the computational overhead and has little impact on the final similarity score. model, and another using this new HMM model.
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ The links are weighted by their inverse frequency in the document, so a link that appears often does not contribute much to this feature’s value.

This methodology is similar to that of Smith et al (2010). $$$$$ We propose an alternative approach: we learn a ranking model, which, for each sentence in the source document, selects either a sentence in the target document that it is parallel to, or “null”.
This methodology is similar to that of Smith et al (2010). $$$$$ This helps address modeling word pairs that are out-of-vocabulary with respect to the seed parallel lexicon, while avoiding some of the issues in bootstrapping.
This methodology is similar to that of Smith et al (2010). $$$$$ (See Section 2.1 for a more detailed description of the types of nonparallel corpora.)

Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ In Section 4, we compare our approach with previous methods on datasets derived from Wikipedia for three language pairs (Spanish-English, German-English, and Bulgarian-English), and show improvements in downstream SMT performance by adding the parallel data we extracted.
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ A global sentence alignment model is able to capture this phenomenon.
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.

For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ Word-level induced lexicon features A common problem with approaches for parallel sentence classification, which rely heavily on alignment models trained from unrelated corpora, is low recall due to unknown words in the candidate sentence-pairs.
For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ Using an IBM Model 1 word translation table P(vt|vs) estimated on the seed parallel corpus, we estimate a cross-lingual context distribution as fine the similarity of a words ws and wt as one minus the Jensen-Shannon divergence of the distributions over positions and target words.2 Given this small set of feature functions, we train the weights of a log-linear ranking model for P(wt|ws, T, S), based on the word-level annotated Wikipedia article pairs.
For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ In the sequence model, we use additional distortion features, which only look at the difference between the position of the previous and current aligned sentences.
For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ Also by modeling the sentence alignment of the articles globally, we were able to show a substantial improvement in task accuracy.

Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.
Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ Translation quality suffers when a system is not trained on any data from the domain it is tested on.
Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ Likewise we gather a distribution over target words and contexts for each target headword P(o, vt|wt).

For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ We also report recall at precision of 90 and 80 percent.
For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ Hopefully this will encourage research into Wikipedia as a resource for machine translation.
For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ Also by modeling the sentence alignment of the articles globally, we were able to show a substantial improvement in task accuracy.

As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages.
As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ After a model is trained, we generate a new translation table Pl,(t|s) which is defined as Pl,,(t|s) ∝ EtET,sCS P(t|s,T,S).
As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ The following features are used in the lexicon model: Translation probability.

While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents.
While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ Therefore, we experimented with two broad domain test sets.
While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.

Smith et al (2010) extended these previous lines of work in several directions. $$$$$ In Wikipedia article pairs, it is common for parallel sentences to occur in clusters.
Smith et al (2010) extended these previous lines of work in several directions. $$$$$ We briefly describe the lexicon model and its use in sentence-extraction.
Smith et al (2010) extended these previous lines of work in several directions. $$$$$ The sheer volume of extracted parallel sentences within Wikipedia is a somewhat surprising result in the light of Wikipedia’s construction.

To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ The corpora Fung and Cheung (2004) examine are quasi-comparable: they contain bilingual documents which are not necessarily on the same topic.
To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.
To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ This helps address modeling word pairs that are out-of-vocabulary with respect to the seed parallel lexicon, while avoiding some of the issues in bootstrapping.

Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ Hopefully this will encourage research into Wikipedia as a resource for machine translation.
Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ The domain of the parallel corpus also strongly influences the quality of translations produced.
Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ We are also releasing several valuable resources to the community to facilitate further research: manually aligned document pairs, and an edited test set.
