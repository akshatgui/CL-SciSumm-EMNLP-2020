More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ While parallel corpora may be scarce, comparable, or semi-parallel corpora are readily available in several domains and language pairs.
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ For each source headword ws, we collect a distribution over context positions o ∈ {−2, −1, +1, +2} and context words vs in those positions based on a count of times a context word occurred at that offset from a headword: P(o, vs|ws) ∝ weight(o) · Qws, o, vs).
More recently, (Smith et al, 2010) reported significant improvements mining parallel Wikipedia articles using more sophisticated indicators of sentence parallelism, incorporating a richer set of features and cross-sentence dependencies within a Conditional Random Fields (CRFs) model. $$$$$ (See Section 2.1 for a more detailed description of the types of nonparallel corpora.)

A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue.
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ The above features are all defined on sentence pairs, and are included in the binary classifier and ranking model.
A recent study by Smith et al (2010) extracted parallel sentences from comparable corpora to extend the existing resources. $$$$$ Hopefully this will encourage research into Wikipedia as a resource for machine translation.

This methodology is similar to that of Smith et al (2010). $$$$$ The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.
This methodology is similar to that of Smith et al (2010). $$$$$ The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.
This methodology is similar to that of Smith et al (2010). $$$$$ In most previous work on extraction of parallel sentences from comparable corpora, some coarse document-level similarity is used to determine which document pairs contain parallel sentences.
This methodology is similar to that of Smith et al (2010). $$$$$ The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training.

Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ A global sentence alignment model is able to capture this phenomenon.
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ Also by modeling the sentence alignment of the articles globally, we were able to show a substantial improvement in task accuracy.
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ Furthermore, even articles created through translations may later diverge due to independent edits in either language.
Many websites are available in multiple languages, and unlike other potential sources — such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al., 2010) — it is common to find document pairs that are direct translations of one another. $$$$$ For German-English and Spanish-English, we extracted data with the null loss adjusted to achieve an estimated precision of 95 percent, and for English-Bulgarian a precision of 90 percent.

For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet contain many words and phrases that are good translations of each other.
For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al (2010), in order to determine how the effect of such data compares with the effect of web mined data. $$$$$ This ranking approach sidesteps the problematic class imbalance issue, resulting in improved average precision while retaining simplicity and clarity in the models.

Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ We also present results in the context of a full machine translation system to evaluate the potential utility of this data.
Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ A global sentence alignment model is able to capture this phenomenon.
Unfortunately, it is difficult to obtain meaningful results on some open domain test sets such as the Wikipedia dataset used by Smith et al (2010). $$$$$ Furthermore we found that the extracted Wikipedia sentences substantially improved translation quality on held-out Wikipedia articles.

For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity.
For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ This is surprising, given that Wikipedia contains annotated article alignments, and much work has been done on extracting bilingual lexicons on this dataset.
For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model.
For example, Smith et al (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. $$$$$ This new translation table is used to define another HMM word-alignment model (together with distortion probabilities trained from parallel data) for use in the sentence extraction models.

As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ The gains from adding the lexicon-based features can be dramatic as in the case of Bulgarian (the CRF model average precision increased by nearly 9 points).
As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles.
As this is computationally intensive, most studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting "inter-wiki" links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). $$$$$ Also, images in Wikipedia are often stored in a central source across different languages; this allows identification of captions which may be parallel (see Figure 1).

While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ It is difficult to say whether such improvement will carry over to other comparable corpora with less document structure and meta-data.
While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ The sheer volume of extracted parallel sentences within Wikipedia is a somewhat surprising result in the light of Wikipedia’s construction.
While several recent works on dealing with large bilingual collections of texts, e.g. (Smith et al., 2010), seek for extracting parallel sentences from comparable corpora, we present PARADOCS, a system designed to recognize pairs of parallel documents in a (large) bilingual collection of texts. $$$$$ One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages.

Smith et al (2010) extended these previous lines of work in several directions. $$$$$ As a summary measure of the performance of the models at different levels of recall we use average precision as defined in (Ido et al., 2006).
Smith et al (2010) extended these previous lines of work in several directions. $$$$$ Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented.
Smith et al (2010) extended these previous lines of work in several directions. $$$$$ We plan to address this question in future work.
Smith et al (2010) extended these previous lines of work in several directions. $$$$$ We plan to address this question in future work.

To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ The set of source and target sentences are observed.
To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ It is difficult to say whether such improvement will carry over to other comparable corpora with less document structure and meta-data.
To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ The sheer volume of extracted parallel sentences within Wikipedia is a somewhat surprising result in the light of Wikipedia’s construction.
To create our dataset, we followed Smith et al (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. $$$$$ A global sentence alignment model is able to capture this phenomenon.

Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ One is a sentence length feature taken from (Moore, 2002), which models the length ratio between the source and target sentences with a Poisson distribution.
Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs.
Wikipedia has become an attractive source of comparable documents in more recent work (Smith et al, 2010). $$$$$ This feature corresponds more closely to context similarity measures used in previous work on lexicon induction.
