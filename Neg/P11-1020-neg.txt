We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ Machine paraphrasing has many applications for natural language processing tasks, including machine translation (MT), MT evaluation, summary evaluation, question answering, and natural language generation.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ For the test set, we used each sentence once as the source sentence with all parallel descriptions as references (there were 16 references on average, with a minimum of 10 and a maximum of 31.)
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ The best correlation was achieved by taking the product of PINC and a sigmoid function of BLEU.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ The data thus has some similarities to parallel news descriptions of the same event, while avoiding much of the noise inherent in news.

The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Deploying the framework on Mechanical Turk over a two-month period yielded 85K English descriptions for 2K videos, one of the largest paraphrase data resources publicly available.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Machine paraphrasing has many applications for natural language processing tasks, including machine translation (MT), MT evaluation, summary evaluation, question answering, and natural language generation.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ In addition to paraphrasing, our data collection framework could also be used to produces useful data for machine translation and computer vision.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ We are grateful to everyone in the NLP group at Microsoft Research and Natural Language Learning group at UT Austin for helpful discussions and feedback.

Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ By pairing up descriptions of the same video in different languages, we obtain parallel data without requiring any bilingual skills.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.

Figure 2 $$$$$ This is consistent with our requirement that a good paraphrase should be lexically dissimilar from the source sentence while preserving its semantics.
Figure 2 $$$$$ (2010).
Figure 2 $$$$$ In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
Figure 2 $$$$$ However, a significant drawback of this approach is that PEM requires substantial in-domain bilingual data to train the semantic adequacy evaluator, as well as sample human judgments to train the overall metric.

3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ Professional translators produce large volumes of bilingual data according to a more or less consistent specification, indirectly fueling work on machine translation algorithms.
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ Also, more abstract actions such as reducing the deficit or fighting for justice cannot be easily captured by our method.
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ We are grateful to everyone in the NLP group at Microsoft Research and Natural Language Learning group at UT Austin for helpful discussions and feedback.
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ Thus, an automatic method for filtering those sentences could allow us to utilize even more of the data.

The STS task data includes five subtasks with text pairs from different sources $$$$$ We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
The STS task data includes five subtasks with text pairs from different sources $$$$$ Despite the similarities between paraphrasing and translation, several major differences have prevented researchers from simply following standards that have been established for machine translation.
The STS task data includes five subtasks with text pairs from different sources $$$$$ On average, annotators completed each task within 80 seconds, including the time required to watch the video.
The STS task data includes five subtasks with text pairs from different sources $$$$$ In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.

MSR video data (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
MSR video data (Chen and Dolan, 2011). $$$$$ We introduced a data collection framework that produces highly parallel data by asking different annotators to describe the same video segments.
MSR video data (Chen and Dolan, 2011). $$$$$ Second, we define a new evaluation metric, PINC (Paraphrase In N-gram Changes), that relies on simple BLEU-like n-gram comparisons to measure the degree of novelty of automatically generated paraphrases.
MSR video data (Chen and Dolan, 2011). $$$$$ The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates.

To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ For example, sentences from the Tier-2 tasks could be used as positive examples to train a string classifier to determine whether a noisy sentence belongs in the same cluster or not.
To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ However, other more advanced MT metrics that have shown higher correlation with human judgments could also be used.
To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ Finally, we also introduced a new metric, PINC, to measure the lexical dissimilarity between the source sentence and the paraphrase.
To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ Another method for collecting monolingual paraphrase data involves aligning semantically parallel sentences from different news articles describing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).

However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ Part of the issue is that a good paraphrase has the additional constraint that it should be lexically dissimilar to the source sentence while preserving the meaning.
However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ Deploying the framework on Mechanical Turk over a two-month period yielded 85K English descriptions for 2K videos, one of the largest paraphrase data resources publicly available.

Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ (2010).
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ We introduced a data collection framework that produces highly parallel data by asking different annotators to describe the same video segments.
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ We introduced a data collection framework that produces highly parallel data by asking different annotators to describe the same video segments.

Consider the following excerpts from a video description corpus (Chen and Dolan, 2011) $$$$$ While BLEU is typically not reliable at the single sentence level, our large number of reference sentences makes BLEU more stable even at this granularity.
Consider the following excerpts from a video description corpus (Chen and Dolan, 2011) $$$$$ To validate the results suggested by the automatic metrics, we asked two fluent English speakers to rate the generated paraphrases on the following categories: semantic, dissimilarity, and overall.
Consider the following excerpts from a video description corpus (Chen and Dolan, 2011) $$$$$ Section 4 discusses automatic evaluations of paraphrases and introduces the novel metric PINC.

Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ On average, 41 descriptions were produced for each video, with at least 27 for over 95% of the videos.
Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
