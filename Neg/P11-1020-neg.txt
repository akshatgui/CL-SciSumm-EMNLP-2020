We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ We use N = 4 in our evaluations.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ PEM had the worst correlation with human judgments of all the metrics.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ Another application for our data is to apply it to computer vision tasks such as video retrieval.
We used the Multiple Video Description Corpus (Chen and Dolan, 2011) obtained from multiple descriptions of short videos. $$$$$ For the test set, we used each sentence once as the source sentence with all parallel descriptions as references (there were 16 references on average, with a minimum of 10 and a maximum of 31.)

The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Section 4 discusses automatic evaluations of paraphrases and introduces the novel metric PINC.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Since PEM was trained on newswire data, its poor adaptation to this domain is expected.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
The dataset consists of 1,500 pairs of short video descriptions collected using crowd sourcing (Chen and Dolan, 2011) and subsequently annotated for the STS task (Agirre et al, 2012). $$$$$ Despite the similarities between paraphrasing and translation, several major differences have prevented researchers from simply following standards that have been established for machine translation.

Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ Almost any simple combination, such as taking the average of the two, yielded decent correlation with the human ratings.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ It might be possible to achieve similar effects using images or panels of images as the stimulus (von Ahn and Dabbish, 2004; Fei-Fei et al., 2007; Rashtchian et al., 2010), but we believed that videos would be more engaging and less ambiguous in their focus.
Such corpora have also been created manually through crowd sourcing (Chen and Dolan, 2011). $$$$$ Results are shown in Table 4.

Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). $$$$$ Finally, we also introduced a new metric, PINC, to measure the lexical dissimilarity between the source sentence and the paraphrase.
Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). $$$$$ While BLEU is typically not reliable at the single sentence level, our large number of reference sentences makes BLEU more stable even at this granularity.
Figure 2: Definition and instructions for annotation to provide a one-sentence description of the main action or event in the video (Chen and Dolan, 2011). $$$$$ We are grateful to everyone in the NLP group at Microsoft Research and Natural Language Learning group at UT Austin for helpful discussions and feedback.

3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ In addition, the highly parallel nature of the data allows us to use standard MT metrics such as BLEU to evaluate semantic adequacy reliably.
3. video descriptions Descriptions of short YouTube videos obtained via Mechanical Turk (Chen and Dolan, 2011). $$$$$ We computed the inter-annotator agreement as well as the correlation between BLEU, PINC, PEM (Liu et al., 2010) and the average human ratings on the sentence level.

The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). $$$$$ We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). $$$$$ The rest of the paper is organized as follows.
The STS task data includes five subtasks with text pairs from different sources: the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) (MSRpar), The Microsoft Research Video corpus (Chen and Dolan, 2011) (MSRvid), statistical machine translation output of parliament proceedings (Koehn, 2005) (SMT-eur). $$$$$ Examples of this kind of data include the Multiple-Translation Chinese (MTC) Corpus 2 which consists of Chinese news stories translated into English by 11 translation agencies, and literary works with multiple translations into English (e.g.

MSR video data (Chen and Dolan, 2011). $$$$$ We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
MSR video data (Chen and Dolan, 2011). $$$$$ In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
MSR video data (Chen and Dolan, 2011). $$$$$ The highly parallel nature of our data suggests a simpler solution to this problem.
MSR video data (Chen and Dolan, 2011). $$$$$ Many of them annotated all the available videos we had.

To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ We are grateful to everyone in the NLP group at Microsoft Research and Natural Language Learning group at UT Austin for helpful discussions and feedback.
To encourage quality contributions, we use a tiered payment structure (Chen and Dolan, 2011) that rewards the good workers. $$$$$ Another method for collecting monolingual paraphrase data involves aligning semantically parallel sentences from different news articles describing the same event (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).

However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ Finally, we also introduced a new metric, PINC, to measure the lexical dissimilarity between the source sentence and the paraphrase.
However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ A good paraphrase, according to our evaluation metric, has few n-gram overlaps with the source sentence but many n-gram overlaps with the reference sentences.
However, as pointed out by (Chenand Dolan, 2011), there is the lack of automatic metric that is capable to measure all the three criteria in paraphrase generation. $$$$$ Watch and describe a short segment of a video You will be shown a segment of a video clip and asked to describe the main action/event in that segment in ONE SENTENCE.

Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ It is also similar in spirit to the ‘Pear Stories’ film used by Chafe (1997).
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ We thank Chris Brockett, Raymond Mooney, Katrin Erk, Jason Baldridge and the anonymous reviewers for helpful comments on a previous draft.
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
Examples of groundings include pictures (Rashtchianu et al., 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases of the same sentence (Barzilay and Lee, 2003). $$$$$ Deploying the framework on Mechanical Turk over a two-month period yielded 85K English descriptions for 2K videos, one of the largest paraphrase data resources publicly available.

Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor. $$$$$ Examples of the outputs produced by the models trained on single parallel sentences and on all parallel sentences are shown in Table 2.
Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor. $$$$$ Of particular relevance are the paraphrasing work by Buzek et al. (2010) and Denkowski et al.
Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor. $$$$$ Each category is rated from 1 to 4, with 4 being the best.
Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): A man is sliding a cat on the floor. $$$$$ While most work on evaluating paraphrase systems has relied on human judges (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Bannard and Callison-Burch, 2005) or indirect, task-based methods (Lin and Pantel, 2001; Callison-Burch et al., 2006), there have also been a few attempts at creating automatic metrics that can be more easily replicated and used to compare different systems.

Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ In addition to the lack of standard datasets for training and testing, there are also no standard metrics like BLEU (Papineni et al., 2002) for evaluating paraphrase systems.
Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ However, other more advanced MT metrics that have shown higher correlation with human judgments could also be used.
Given a large set of videos and a number of descriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. $$$$$ Section 4 discusses automatic evaluations of paraphrases and introduces the novel metric PINC.
