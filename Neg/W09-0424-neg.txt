We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. $$$$$ Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al.
We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. $$$$$ We have described a scalable toolkit for parsingbased machine translation.
We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. $$$$$ More details on the MERT method and the implementation can be found in Zaidan (2009).4
We used Joshua (Li et al., 2009), a syntax-based decoder with a suffix array implementation, and rule induction via the standard Hiero grammar extraction heuristics (Chiang, 2007) for the TMs. $$$$$ The item also maintains backpointers to antecedent items, which are used for k-best extraction.

Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that approaches such as Hiero (Chiang et al, 2005) and Joshua (Li et al, 2009), enhanced by dissimilarity based re-ranking, would improve over our current system. $$$$$ The decoder achieves state of the art translation performance.
Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that approaches such as Hiero (Chiang et al, 2005) and Joshua (Li et al, 2009), enhanced by dissimilarity based re-ranking, would improve over our current system. $$$$$ We hope the release of the toolkit will greatly contribute the progress of the syntax-based machine translation research.'
Although the comparison against the Zhu system, which uses syntax-driven machine translation, shows no clear benefit for syntax-based machine translation, it may still be the case that approaches such as Hiero (Chiang et al, 2005) and Joshua (Li et al, 2009), enhanced by dissimilarity based re-ranking, would improve over our current system. $$$$$ The views and findings are the authors’ alone.

We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). $$$$$ Additionally, parallel and distributed computing techniques are exploited to make it scalable.
We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). $$$$$ In principle, the goodness of a string is measured by the total probability of its many derivations.
We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). $$$$$ We filtered and de-duplcated the resulting parallel corpus.
We see promising improvements over an n-gram LM for a solid Joshua-based baseline system (Li et al, 2009). $$$$$ This results in a vastly smaller rule set than techniques which extract all rules from the training set.

We use the Joshua implementation of the method for decoding (Li et al, 2009). $$$$$ This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare.
We use the Joshua implementation of the method for decoding (Li et al, 2009). $$$$$ This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare.
We use the Joshua implementation of the method for decoding (Li et al, 2009). $$$$$ (2006), and Liu et al. (2006)) has made remarkable progress in the last few years.
We use the Joshua implementation of the method for decoding (Li et al, 2009). $$$$$ In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation.

Our hybrid machine translation system combines translation output from $$$$$ This Java implementation is able to read the standard ARPA backoff n-gram models, and thus the decoder can be used independently from the SRILM toolkit.3 We also provide a native code bridge that allows the decoder to use the SRILM toolkit to read and score n-grams.
Our hybrid machine translation system combines translation output from $$$$$ Grammar formalism: Our decoder assumes a probabilistic synchronous context-free grammar (SCFG).

Decoding is carried out with Joshua (Li et al, 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional error correction and filtering steps to every system. $$$$$ The toolkit also implements suffix-array grammar extraction and minimum error rate training.
Decoding is carried out with Joshua (Li et al, 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional error correction and filtering steps to every system. $$$$$ We split the sentences and paragraphs in these documents, performed sentence-aligned them using software that IBM Model 1 probabilities into account (Moore, 2002).
Decoding is carried out with Joshua (Li et al, 2009), an open-source platform for SCFG-based MT. Due to engineering limitations in decoding with a large grammar, we apply three additional error correction and filtering steps to every system. $$$$$ In this paper, we describe Joshua, a general-purpose open source toolkit for parsing-based machine translation, serving the same role as Moses (Koehn et al., 2007) does for regular phrase-based machine translation.

A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. $$$$$ It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.
A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. $$$$$ Additionally, parallel and distributed computing techniques are exploited to make it scalable (Li and Khudanpur, 2008b).
A Hiero-style decoder Joshua (Li et al, 2009) is also used in our experiments. $$$$$ HR0011-06-2-0001 and the National Science Foundation under grants No.

However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models $$$$$ Additionally, parallel and distributed computing techniques are exploited to make it scalable.
However, it would be artificial to ignore dictionary resources when they exist. We experiment with two translation models $$$$$ The views and findings are the authors’ alone.

Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). $$$$$ The views and findings are the authors’ alone.
Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). $$$$$ Chart parsing: Given a source sentence to decode, the decoder generates a one-best or k-best translations using a CKY algorithm.
Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). $$$$$ That is, the probability of an output string is split among many distinct derivations (e.g., trees or segmentations).
Yet, our best system exhibits Hiero-level performance on French-English Europarl data using an SCFG-based decoder (Li et al, 2009). $$$$$ This research was supported in part by the Defense Advanced Research Projects Agency’s GALE program under Contract No.

We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). $$$$$ Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction.
We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). $$$$$ Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction.
We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). $$$$$ However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source.
We use our learned stochastic SCFG grammar with the decoding component of the Joshua SCFG toolkit (Liet al, 2009). $$$$$ The toolkit also implements suffix-array grammar extraction and minimum error rate training.

Joshua (Li et al, 2009) $$$$$ This native implementation is more scalable than the basic Java LM implementation.
Joshua (Li et al, 2009) $$$$$ The current code requires suffix array rule extraction to be run as a pre-processing step to extract the rules needed to translate a particular test set.
Joshua (Li et al, 2009) $$$$$ Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding.
Joshua (Li et al, 2009) $$$$$ Instead, we develop a variational approximation, which considers all the derivations but still allows tractable decoding.

The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. $$$$$ The toolkit also implements suffixarray grammar extraction (Callison-Burch et al., 2005; Lopez, 2007) and minimum error rate training (Och, 2003).
The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. $$$$$ The decoder achieves state of the art translation performance.
The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. $$$$$ Additionally, parallel and distributed computing techniques are exploited to make it scalable.
The 2011 WMT Tunable Metrics task consists of using Z-MERT (Zaidan, 2009) to tune a pre-built Urdu-English Joshua (Li et al, 2009) system to a new evaluation metric on a tuning set with 4 reference translations and decoding a test set using the resulting parameter set. $$$$$ This native implementation is more scalable than the basic Java LM implementation.

Decoding was carried out in Joshua (Li et al, 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (ClarkandLavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines. $$$$$ This will allow the decoder at runtime to efficiently extract exactly those rules needed to translate a particular sentence, without the need for a rule extraction pre-processing step.
Decoding was carried out in Joshua (Li et al, 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (ClarkandLavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines. $$$$$ Joshua implements all of the algorithms required for synchronous context grammars (SCFGs): chart-parsing, gram language model integration, beamcube-pruning, and extraction.
Decoding was carried out in Joshua (Li et al, 2009), an open-source framework for parsing-based MT. We managed our experiments with LoonyBin (ClarkandLavie, 2010), an open-source tool for defining, modifying, and running complex experimental pipelines. $$$$$ 0713448 and 0840112.

The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. $$$$$ One of the great difficulties with current MT pipelines is that these diverse components are often designed by separate groups and have different file format and interaction requirements.
The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. $$$$$ However, we are currently extending the decoder to directly access the suffix array.
The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. $$$$$ It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.
The unlabeled data was subsampled (Li et al, 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. $$$$$ Our toolkit is written in Java and implements all the essential algorithms described in Chiang (2007): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.

We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. $$$$$ The translation scores for four different systems are reported in Table 1.5 Baseline: In this system, we use the GIZA++ toolkit (Och and Ney, 2003), a suffix-array architecture (Lopez, 2007), the SRILM toolkit (Stolcke, 2002), and minimum error rate training (Och, 2003) to obtain word-alignments, a translation model, language models, and the optimal weights for combining these models, respectively.
We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. $$$$$ Large scale parsing-based statistical machine translation (e.g., Chiang (2007), Quirk et al. (2005), Galley et al.
We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. $$$$$ We re-score the top 300 translations to minimize expected loss under the Bleu metric.
We perform experiments using the open-source MT toolkit Joshua (Li et al, 2009a), and show that adding unsupervised data to the traditional supervised training setup improves performance. $$$$$ More details on the MERT method and the implementation can be found in Zaidan (2009).4

We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). $$$$$ The views and findings are the authors’ alone.
We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). $$$$$ In this system, we have used both deterministic annealing (for training) and variational decoding (for decoding).
We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). $$$$$ This yielded 2 million French documents paired with their English equivalents.
We perform experiments with the syntax-based MT system Joshua (Li et al, 2009a), which implements dynamic programming algorithms for second-order expectation semi rings (Li and Eisner, 2009) to efficiently compute the gradients needed for optimizing (8). $$$$$ We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words.

While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x $$$$$ This results in a high barrier to entry for other researchers, and makes experiments difficult to duplicate and compare.
While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x $$$$$ We re-score the top 300 translations to minimize expected loss under the Bleu metric.
While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x $$$$$ However, most of the systems mentioned above employ tailor-made, dedicated software that is not open source.
While (a) is not true in our setting because Xi is a hyper graph (which is ambiguous), Li et al (2009b) show how to approximate a hyper graph representation of p (x $$$$$ HR0011-06-2-0001 and the National Science Foundation under grants No.

We report results on Chinese-to-English translation tasks using Joshua (Li et al, 2009a), an open-source implementation of Hiero (Chiang, 2007). $$$$$ HR0011-06-2-0001 and the National Science Foundation under grants No.
We report results on Chinese-to-English translation tasks using Joshua (Li et al, 2009a), an open-source implementation of Hiero (Chiang, 2007). $$$$$ It uses parallel and distributed computing techniques for scalability.
We report results on Chinese-to-English translation tasks using Joshua (Li et al, 2009a), an open-source implementation of Hiero (Chiang, 2007). $$$$$ The toolkit has been used to translate roughly a million sentences in a parallel corpus for largescale discriminative training experiments (Li and Khudanpur, 2008a).

We use Joshua, a Java-based open source implementation of the hierarchical decoder (Li et al, 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrase model.perl script packed with Moses2 (Koehn et al., 2007). $$$$$ It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.
We use Joshua, a Java-based open source implementation of the hierarchical decoder (Li et al, 2009), release 1.1.1 Word alignment was computed using the first three steps of the train-factored-phrase model.perl script packed with Moses2 (Koehn et al., 2007). $$$$$ It is written in Java and implements all the essential algorithms described in Chiang (2007) and Li and Khudanpur (2008b): chart-parsing, n-gram language model integration, beam- and cube-pruning, and k-best extraction.

In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ The toolkit also implements suffixarray grammar extraction (Callison-Burch et al., 2005; Lopez, 2007) and minimum error rate training (Och, 2003).
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ HR0011-06-2-0001 and the National Science Foundation under grants No.
In modern machine translation systems such as Joshua (Li et al, 2009) and cdec (Dyer et al, 2010), a translation model is represented as a synchronous context-free grammar (SCFG). $$$$$ We re-score the top 300 translations to minimize expected loss under the Bleu metric.
