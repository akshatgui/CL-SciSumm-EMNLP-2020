IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). $$$$$ For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position.
IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000). $$$$$ The different alignment models we present provide different decompositions of Pr(fil ,41e{).

The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). $$$$$ We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.
The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). $$$$$ In this paper, we present and compare various single-word based alignment models for statistical machine translation.
The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al, 2003). $$$$$ In order to keep the training fast we can take into account only a small fraction of all alignments.

We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. $$$$$ The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991).
We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. $$$$$ In this paper, we present and compare various single-word based alignment models for statistical machine translation.
We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stolcke, 2002), and risk-based deterministic annealing (Smith and Eisner, 2006) to obtain word alignments, translation models, language models, and the optimal weights for combining these models, respectively. $$$$$ In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models.

The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). $$$$$ ,S .
The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). $$$$$ For the lexicon parameters the counts are: Correspondingly, the alignment and fertility probabilities can be estimated.
The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
The development of techniques in all these areas would be facilitated by automatic performance metrics, and alignment and translation quality metrics have been proposed (Och and Ney, 2000b; Papineni et al, 2002). $$$$$ In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models.

The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment. $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
The Alignment Error Rate (AER) introduced by Och and Ney (2000b) measures the fraction of links by which the automatic alignment differs from the reference alignment. $$$$$ We present different methods to combine alignments.

The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). $$$$$ The models IBM-1, IBM-2 and HMM have a particularly simple mathematical form so that the EM algorithm can be performed exactly, i.e. in the E-step it is possible to efficiently consider all alignments.
The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). $$$$$ For the lexicon parameters the counts are: Correspondingly, the alignment and fertility probabilities can be estimated.
The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
The IBM-3 models were trained on a subset of the Canadian Hansards French-English data which consisted of 50,000 parallel sentences (Och and Ney, 2000b). $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).

The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)). $$$$$ So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.
The GIZA++ toolkit (Och and Ney, 2000a) was used for training the IBM-3 models (as in (Och and Ney, 2000b)). $$$$$ In the Estep the counts for one sentence pair (f, e) are calculated.

Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b). $$$$$ We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.
Our unseen test data consisted of 207 French English sentence pairs from the Hansards corpus (Och and Ney, 2000b). $$$$$ As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.

The performance of the four decoders was measured with respect to the alignments provided by human experts (Och and Ney, 2000b). $$$$$ In this paper, we present and compare various single-word based alignment models for statistical machine translation.

Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). $$$$$ For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments.
Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). $$$$$ We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.
Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).
Once the training data was preprocessed, a word-to-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000). $$$$$ This allows an automatic evaluation, once a reference alignment has been produced.

The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003). $$$$$ Using translation quality is problematic, as translation quality is not well defined and as there are additional influences such as language model or decoder properties.
The approach was first presented by Brown et al (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamadaand Knight, 2000), (Vogel et al, 2003). $$$$$ All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment.

In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. $$$$$ An alignment Ã¢ for which holds = arg max Pr(fil for a specific model is called Viterbi alignment of this model.
In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. $$$$$ In order to keep the training fast we can take into account only a small fraction of all alignments.
In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. $$$$$ For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position.
In order to assess the quality of the word alignment, we randomly selected from the training corpus 350 sentences, and a manual gold standard alignment has been done with the criterion of Sure and Possible links, in order to compute Alignment Error Rate (AER) as described in (Och and Ney, 2000) and widely used in literature, together with appropriately redefined Recall and Precision measures. $$$$$ In this paper we use the models IBM-1 to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).

We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). $$$$$ In the HMM alignment model we allow for a dependence from the class E = C(ea,_,).
We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). $$$$$ We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.
We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). $$$$$ The models IBM-1, IBM-2 and HMM have a particularly simple mathematical form so that the EM algorithm can be performed exactly, i.e. in the E-step it is possible to efficiently consider all alignments.

We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. $$$$$ So far, no well established evaluation criterion exists in the literature for these alignment models.
We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. $$$$$ We discuss the five IBM alignment models, the Hidden- Markov alignment model, smoothing techniques and various modifications.

We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. $$$$$ All models include lexicon parameters p(f le) and additional parameters describing the probability of an alignment.
We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. $$$$$ In order to keep the training fast we can take into account only a small fraction of all alignments.
We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. $$$$$ For the lexicon parameters the counts are: Correspondingly, the alignment and fertility probabilities can be estimated.
We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. $$$$$ In the Estep the counts for one sentence pair (f, e) are calculated.

 $$$$$ We present different methods to combine alignments.
 $$$$$ We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).
 $$$$$ The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991).

Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4. $$$$$ The alignment a may contain alignments ai = 0 with the 'empty' word co to account for French words that are not aligned to any English word.
Table 2 compares the results of our algorithm with the results in (Och and Ney, 2000), where an HMM model is used to bootstrap IBM Model 4. $$$$$ The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991).

This demonstrates that we are competitive with the methods described in (Och and Ney, 2000). $$$$$ In order to keep the training fast we can take into account only a small fraction of all alignments.
This demonstrates that we are competitive with the methods described in (Och and Ney, 2000). $$$$$ For various reasons (nonunique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling.
This demonstrates that we are competitive with the methods described in (Och and Ney, 2000). $$$$$ In the HMM alignment model we allow for a dependence from the class E = C(ea,_,).

 $$$$$ Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).
 $$$$$ Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EMalgorithm, the counts are collected only over a subset of promising alignments.
 $$$$$ In the Estep the counts for one sentence pair (f, e) are calculated.

The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004). $$$$$ Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).
The first feature is the absolute difference between ai and ai-1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrase translation models (Koehn, 2004). $$$$$ For the lexicon parameters the counts are: Correspondingly, the alignment and fertility probabilities can be estimated.
