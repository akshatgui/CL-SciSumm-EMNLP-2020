One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ To generate a summary, it is necessary to find a sequence of words that maximizes the probability, under the content selection and summary structure models, that it was generated from the document to be summarized.
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ For each news article, the maximum overlap between the actual headline and the generated headline was noted; the length at which this overlap was maximal was also taken into account.
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ Our approach applies statistical models of the term selection and term ordering processes to produce short summaries, shorter than those reported previously.

Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ Once the parameters of a content selection model have been estimated from a suitable document/summary corpus, the model can be used to compute selection scores for candidate summary terms, given the terms occurring in a particular source document.
Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ Generating effective summaries requires the ability to select, evaluate, order and aggregate items of information according to their relevance to a particular subject or for a particular purpose.
Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ Sample output for the article in Figure 3, using both lexical and POS/positional information can be seen in Figure 4.

The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ Recently, we have extended the model to generate multi-sentential summaries as well: for instance, given an initial sentence such as “Clinton to meet visit MidEast.” and words that are related to nouns (“Clinton” and “mideast”) in the first sentence, the system biases the content selection model to select other nouns that have high mutual information with these nouns.
The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ This paper reviews the framework, discusses some of the pros and cons of this approach using examples from our corpus of news wire stories, and presents an initial evaluation.
The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ This approach was based on a statistical translation model that mapped between sets of words in a source language and sets of words in a target language, at the same time using an ordering model to constrain possible token sequences in a target language based on likelihood.

Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.
Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ Table 1 shows the results of these term selection schemes.
Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ In this case, the probability of any particular summarycontent candidate can be calculated simply as the product of the probabilities of the terms in the candidate set.

Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ The percentage of complete matches indicates how many of the summaries of a given length had all their terms included in the target headline. lap between the generated headlines and the test standards (both the actual headline and the summary sentence) was the metric of performance.
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ We used a mixture model (McLachlan and Basford, 1988) – combining the lexical and the POS probabilities – for both the content selection and the linearization tasks.

Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ Furthermore, our initial experiments have suffered from a lack of good training and testing corpora; few of the news stories we have in our corpora contain multi-sentential headlines.
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ Our approach applies statistical models of the term selection and term ordering processes to produce short summaries, shorter than those reported previously.
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ This paper presents results on experiments using this approach, in which statistical models of the term selection and term ordering are jointly applied to produce summaries in a style learned from a training corpus.
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.

Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.
Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ The work reported in this paper is most closely related to work on statistical machine translation, particularly the ‘IBM-style’ work on CANDIDE (Brown et al., 1993).
Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ As can be seen, even with such an impoverished language model, the system does quite well: when the generated headlines are four words long almost one in every five has all of its words matched in the article s actual headline.
Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ (We have experience doing much more laborious human evalPresident Clinton met with his top Mideast advisers, including Secretary of State Madeleine Albright and U.S. peace envoy Dennis Ross, in preparation for a session with Israel Prime Minister Benjamin Netanyahu tomorrow.

Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.
Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ Actual headlines are often, also, ungrammatical, incomplete phrases.
Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.
Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ Probabilities for sequences that have not been seen in the training data are estimated using back-off weights (Katz, 1987).

This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ This can be a problem, because in many situations, a short headline style indicative summary is desired.
This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ This paper describes an alternative approach to summarization capable of generating summaries shorter than a sentence, some examples of which are given in Figure 1.
This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ Recently, we have extended the model to generate multi-sentential summaries as well: for instance, given an initial sentence such as “Clinton to meet visit MidEast.” and words that are related to nouns (“Clinton” and “mideast”) in the first sentence, the system biases the content selection model to select other nouns that have high mutual information with these nouns.

For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.
For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.
For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ While the results so far can only be seen as indicative, this breed of non-extractive summarization holds a great deal of promise, both because of its potential to integrate many types of information about source documents and intended summaries, and because of its potential to produce very brief coherent summaries.

Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ The simplest model is a bigram language model, where the probability of a word sequence is approximated by the product of the probabilities of seeing each term given its immediate left context.
Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation.
Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ This paper describes an alternative approach to summarization capable of generating summaries shorter than a sentence, some examples of which are given in Figure 1.
Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ The headline length given is that a which the overlap between the terms in the target headline and the generated summary was maximized.

Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ For the discussions in this paper, this will be the model chosen.
Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ The documents were preprocessed before training: formatting and mark-up information, such as font changes and SGML/HTML tags, was removed; punctuation, except apostrophes, was also removed.
Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ This can be through the use of more sophisticated models, such as additional language models that take into account the signed distance between words in the original story to condition the probability that they should appear separated by some distance in the headline.
Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ It is likely that further processing, such as lemmatization, might be useful, producing smaller and better language models, but this was not evaluated for this paper.

In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ In the example sentence, this generated the subsequent sentence “US urges Israel plan.” This model currently has several problems that we are attempting to address: for instance, the fact that the words co-occur in adjacent sentences in the training set is not sufficient to build coherent adjacent sentences (problems with pronominal references, cue phrases, sequence, etc. abound).
In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ We then tested mixtures of the lexical and POS models, lexical and positional models, and all three models combined together.
In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ Recently, we have extended the model to generate multi-sentential summaries as well: for instance, given an initial sentence such as “Clinton to meet visit MidEast.” and words that are related to nouns (“Clinton” and “mideast”) in the first sentence, the system biases the content selection model to select other nouns that have high mutual information with these nouns.
In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ Generating effective summaries requires the ability to select, evaluate, order and aggregate items of information according to their relevance to a particular subject or for a particular purpose.

From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ For each of these stories, headlines were generated for a variety of lengths and compared against the (i) the actual headlines, as well as (ii) the sentence ranked as the most important summary sentence.
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ In this case, the probability of any particular summarycontent candidate can be calculated simply as the product of the probabilities of the terms in the candidate set.
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ Zero Level–Performance Evaluation: The zero-level model, that we have discussed so far, works surprisingly well, given its strong independence assumptions and very limited vocabulary.
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ Specific subsets of terms, representing the core summary content of an article, can then be compared for suitability in generating a summary.

For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ Most previous work on summarization has focused on extractive summarization: selecting text spans - either complete sentences or paragraphs – from the original document.
For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ We will briefly discuss the use of two additional sources of information: (i) part of speech (POS) information, and (ii) positional information.
For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ This paper describes an alternative approach to summarization capable of generating summaries shorter than a sentence, some examples of which are given in Figure 1.
