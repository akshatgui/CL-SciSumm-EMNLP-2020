One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ As we will discuss in this paper, we built several models of varying complexity;1 even the simplest one did reasonably well at summarization, whereas it would have been severely deficient at (traditional) translation.
One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate βi's from the document to be abstracted and rely on recent information ordering techniques to sort the βi fragments (Lapata, 2003). $$$$$ This paper has presented an alternative to extractive summarization: an approach that makes it possible to generate coherent summaries that are shorter than a single sentence and that attempt to conform to a particular style.

Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ This approach was based on a statistical translation model that mapped between sets of words in a source language and sets of words in a target language, at the same time using an ordering model to constrain possible token sequences in a target language based on likelihood.
Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ Generating effective summaries requires the ability to select, evaluate, order and aggregate items of information according to their relevance to a particular subject or for a particular purpose.
Some researchers (Banko et al, 2000) have developed simple statistical models for aligning documents and headlines. $$$$$ More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.

The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ To generate a summary, it is necessary to find a sequence of words that maximizes the probability, under the content selection and summary structure models, that it was generated from the document to be summarized.
The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ It does so by building statistical models for content selection and surface realization.
The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).
The second baseline is based on the noisy-channel generative (flat generative, FG) model proposed by Banko et al, (2000). $$$$$ Given good training corpora, this approach can also be used to generate headlines from a variety of formats: in one case, we experimented with corpora that contained Japanese documents and English headlines.

Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ Some of these errors are shown in Table 3.
Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ The simplest model for document length is a fixed length based on document genre.
Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ For the discussions in this paper, this will be the model chosen.
Our method for estimation of selection and ordering preferences is based on the technique described in (Banko et al, 2000). $$$$$ Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.

Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ Our approach applies statistical models of the term selection and term ordering processes to produce short summaries, shorter than those reported previously.
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ Extractive summarization techniques cannot generate document summaries shorter than a single sentence, something that is often required.
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ Furthermore, our initial experiments have suffered from a lack of good training and testing corpora; few of the news stories we have in our corpora contain multi-sentential headlines.
Even a larger beam size such as 80 (as used by Banko et al (2000)) does not match the title quality of the optimal decoder. $$$$$ In the simplest, zerolevel model that we have discussed, since each summary term is selected independently, and the summary structure model is first order Markov, it is possible to use Viterbi beam search (Forney, 1973) to efficiently find a near-optimal summary.

Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ Work on combining an information extraction phase followed by generation has also been reported: for instance, the FRUMP system (DeJong, 1982) used templates for both information extraction and presentation.
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ This can be modeled by estimating the likelihood of some token appearing in a summary given that some tokens (one or more, possibly different tokens) appeared in the document to be summarized.
Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al, 2000). $$$$$ Training a POS model for both these tasks requires far less data than training a lexical model, since the number of POS tags is much smaller.

Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ They could also, of course, be extended to use higher order n-grams, providing that sufficient numbers of training headlines were available to estimate the probabilities.
Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ A quick manual scan of some of the failures that might have been scored as successes 7Unlike the data in Table 1, these headlines contain only six words or fewer. in a subjective manual evaluation indicated that some of these errors could not have been avoided without adding knowledge to the system, for example, allowing the use of alternate terms for referring to collective nouns.
Banko et al (2000) propose a bag-of-words model for headline generation. $$$$$ As we will discuss in this paper, we built several models of varying complexity;1 even the simplest one did reasonably well at summarization, whereas it would have been severely deficient at (traditional) translation.

Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ The issue then becomes one of generating a target document in a more concise language from a source document in a more verbose language.
Following Banko et al (2000), we approximated the length distribution with a Gaussian. $$$$$ We expect to improve both the quality and scope of the summaries produced in future work.

This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ In the example sentence, this generated the subsequent sentence “US urges Israel plan.” This model currently has several problems that we are attempting to address: for instance, the fact that the words co-occur in adjacent sentences in the training set is not sufficient to build coherent adjacent sentences (problems with pronominal references, cue phrases, sequence, etc. abound).
This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ Most previous work on summarization has focused on extractive summarization: selecting text spans - either complete sentences or paragraphs – from the original document.
This approach has been explored in (Zajic et al, 2002) and (Banko et al, 2000). $$$$$ An ideal summarization system would understand each document and generate an appropriate summary directly from the results of that understanding.

For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ In the example sentence, this generated the subsequent sentence “US urges Israel plan.” This model currently has several problems that we are attempting to address: for instance, the fact that the words co-occur in adjacent sentences in the training set is not sufficient to build coherent adjacent sentences (problems with pronominal references, cue phrases, sequence, etc. abound).
For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ Most previous work on summarization has focused on extractive summarization: selecting text spans - either complete sentences or paragraphs – from the original document.
For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ It is likely that more sophisticated language models, such as structure models (Chelba, 1997; Chelba and Jelinek, 1998), or longer ngram models would lead to the system generating headlines that were more similar in phrasing to real headlines because longer range dependencies summary sentences, respectively, of the article.
For example, Banko et al (2000) draw inspiration from Machine Translation and generate headlines using statistical models for content selection and sentence realization. $$$$$ The target documents – the summaries – that the system needed to learn the translation mapping to, were the headlines accompanying the news stories.

Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ More recently, summarizers using sophisticated postextraction strategies, such as revision (McKeown et al., 1999; Jing and McKeown, 1999; Mani et al., 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented.
Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ This paper reviews the framework, discusses some of the pros and cons of this approach using examples from our corpus of news wire stories, and presents an initial evaluation.
Another approach, presented by (Banko et al, 2000), consists in generating coherent summaries that are shorter than a single sentence. $$$$$ The latter is interesting because it helps suggest the degree to which headlines used a different vocabulary from that used in the story itself.6 Term overcal model for content selection on 1000 Reuters news articles.

Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ Recently, we have extended the model to generate multi-sentential summaries as well: for instance, given an initial sentence such as “Clinton to meet visit MidEast.” and words that are related to nouns (“Clinton” and “mideast”) in the first sentence, the system biases the content selection model to select other nouns that have high mutual information with these nouns.
Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).
Banko et al (2000) uses beam search to identify approximate solutions. $$$$$ It does so by building statistical models for content selection and surface realization.

In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ However, it is worth emphasizing that many of the headlines generated by the system were quite good, but were penalized because our evaluation metric was based on the word-error rate and the generated headline terms did not exactly match the original ones.
In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ A more practical approach to this problem results in the use of an approximation: viewing summarization as a problem analogous to statistical machine translation.
In subsequent work to Witbrock and Mittal (1999), Banko et al (2000) describe the use of information about the position of words within four quarters of the source document. $$$$$ To generate a summary, it is necessary to find a sequence of words that maximizes the probability, under the content selection and summary structure models, that it was generated from the document to be summarized.

From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ Cross-validation is used to learn weights , and for a particular document genre.
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ However, it is hard to computationally evaluate coherence and phrasing effectiveness, so we have, to date, restricted ourselves to the content aspect, which is more amenable to a quantitative analysis.
From early in the field, it was pointed out that a purely extractive approach is not good enough to generate headlines from the body text (Banko et al, 2000). $$$$$ Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al., 1973), probabilistic measures for token salience (Salton et al., 1997), and the use of implicit discourse structure (Marcu, 1997).

For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ Furthermore, our initial experiments have suffered from a lack of good training and testing corpora; few of the news stories we have in our corpora contain multi-sentential headlines.
For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ Therefore, the overall probability of a candidate summary, , consisting of words , under the simplest, zero-level, summary model based on the previous assumptions, can be computed as the product of the likelihood of (i) the terms selected for the summary, (ii) the length of the resulting summary, and (iii) the most likely sequencing of the terms in the content set.
For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ The length of the summary can also be learned as a function of the source document.
