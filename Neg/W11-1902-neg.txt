In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. $$$$$ In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).
In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. $$$$$ For example, given the following ordered list of mentions, {mi, m2, m3, m4, m1�, m6}, where the subscript indicates textual order and the superscript indicates cluster id, our model will attempt to resolve only m2 and m4.
In fact, the Stanford coreference resolver (Lee et al 2011), which won the CoNLL-2011 shared task on coreference resolution, adopts the once-popular rule-based approach, resolving pronouns simply with rules that encode the aforementioned traditional linguistic constraints on coreference, such as the Binding constraints and gender and number agreement. The infrequency of occurrences of difficult pronouns in these standard evaluation corpora by no means undermines their significance, however. $$$$$ In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).

 $$$$$ Finally, Table 4 lists our results on the held-out testing partition.
 $$$$$ Our approach starts with a high-recall mention detection component, which identifies mentions using only syntactic information and named entity boundaries, followed by a battery of high-precision deterministic coreference sieves, applied one at a time from highest to lowest precision.
 $$$$$ For example, Haghighi and Klein (2010) compare four state-of-the-art systems on three different corpora and report B3 scores between 63 and 77 points.
 $$$$$ We also list results with gold and predicted linguistic annotations (i.e., syntactic parses and named entity recognition).

Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). $$$$$ In Section 3 we show the results of several experiments, which compare the impact of the various features in our system, and analyze the performance drop as we switch from gold mentions and annotations (named entity mentions and parse trees) to predicted information.
Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
Our second baseline is the Stanford resolver (Lee et al2011), which achieves the best performance in the CoNLL 2011 shared task (Pradhan et al2011). $$$$$ The table shows that the recall of our approach is 92.8% (if gold annotations are used) or 87.9% (with predicted annotations).

Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. $$$$$ We show results before and after coreference resolution and post-processing (when singleton mentions are removed).
Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. $$$$$ Using gold mentions, our system scored 60.7 in the closed track in 61.4 in the open track.
Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. $$$$$ Our results demonstrate that, despite their simplicity, deterministic models for coreference resolution obtain competitive results, e.g., we obtained the highest scores in both the closed and open tracks (57.8 and 58.3 respectively).
Lee et al (2011) use rules to extract appositions for co reference resolution, selecting only those that are explicitly flagged using commas or parentheses. $$$$$ Due to this boundary mismatch, all mentions found to be coreferent with this predicted mention are counted as precision errors, and all mentions in the same coreference cluster with the gold mention are counted as recall errors.

After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). $$$$$ While the corpora used in (Haghighi and Klein, 2010) are different from the one in this shared task, our result of 68 B3 suggests that our system’s performance is competitive.
After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). $$$$$ In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).
After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). $$$$$ Each tier builds on the entity clusters constructed by previous models in the sieve, guaranteeing that stronger features are given precedence over weaker ones.
After obtaining all the extracted noun phrases, we also use a rule-based method to remove some erroneous candidates based on previous studies (e.g. Lee et al (2011), Uryupina et al (2011)). $$$$$ Our system was ranked first in both tracks, with a score of 57.8 in the closed track and 58.3 in the open track.

They also outperform the learning-based systems of Sapena et al (2011) and Chang et al (2011), and perform competitively with Lee's system (Lee et al 2011). $$$$$ This sieve correctly links Britain with country, and plane with aircraft.
They also outperform the learning-based systems of Sapena et al (2011) and Chang et al (2011), and perform competitively with Lee's system (Lee et al 2011). $$$$$ These models incorporate lexical, syntactic, semantic, and discourse information, and have access to document-level information (i.e., we share mention attributes across clusters as they are built).

In this paper, we have chosen two coreference resolution systems $$$$$ Our approach starts with a high-recall mention detection component, which identifies mentions using only syntactic information and named entity boundaries, followed by a battery of high-precision deterministic coreference sieves, applied one at a time from highest to lowest precision.
In this paper, we have chosen two coreference resolution systems $$$$$ Our approach starts with a high-recall mention detection component, which identifies mentions using only syntactic information and named entity boundaries, followed by a battery of high-precision deterministic coreference sieves, applied one at a time from highest to lowest precision.
In this paper, we have chosen two coreference resolution systems $$$$$ This analysis indicates that using gold linguistic annotation yields an improvement of only 2 points.
In this paper, we have chosen two coreference resolution systems $$$$$ However, we believe we are the first to show that this high-recall/high-precision strategy yields competitive results for the complete task of coreference resolution, i.e., including mention detection and both nominal and pronominal coreference.

which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. $$$$$ The table shows that the recall of our approach is 92.8% (if gold annotations are used) or 87.9% (with predicted annotations).
which, along with the absence of Froggy in the name gazetteer for the system (Lee et al, 2011), would lead to both precision and recall errors for Froggy, as we observed. $$$$$ For example, the mention selected from the cluster {President George W. Bush, president, he} is President George W. Bush.

attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. $$$$$ We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.
attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. $$$$$ On the other hand, the semantic sieves do not help (row 3 versus 4).
attributes and features such as ANIMACY in the 2 According to Lee et al (2011), Stanforddcoref correctly. $$$$$ Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information.

The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). $$$$$ In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).
The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). $$$$$ Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information.
The Stanford tools perform part of speech tagging (Toutanova et al, 2003), constituent and dependency parsing (Klein and Manning, 2003), named entity recognition (Finkel et al, 2005), and coreference resolution (Lee et al, 2011). $$$$$ We thank the shared task organizers for their effort.

We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). $$$$$ First, we added five additional sieves, the majority of which address the semantic similarity between mentions, e.g., using WordNet distance, and shallow discourse understanding, e.g., linking speakers to compatible pronouns.
We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
We use the part-of speech (POS) tagger, the named-entity recognizer, the parser (Klein and Manning, 2003), and the coreference resolution system (Leeetal., 2011). $$$$$ In this work we showed how a competitive end-toend coreference resolution system can be built using only deterministic models (or sieves).

Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. $$$$$ Using gold mentions, our system scored 60.7 in the closed track in 61.4 in the open track.
Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. $$$$$ “Semantics” stands for the sieves presented in Section 2.3.2.
Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. $$$$$ On the other hand, using gold mentions raises the overall score by 15 points.
Our system is an extension of Stanford's multi-passsieve system, (Raghunathan et al,2010) and (Lee et al, 2011), by adding novel constraints and sieves. $$$$$ This material is based upon work supported by the Air Force Research Laboratory (AFRL) under prime contract no.

Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. $$$$$ While the corpora used in (Haghighi and Klein, 2010) are different from the one in this shared task, our result of 68 B3 suggests that our system’s performance is competitive.
Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. $$$$$ For this reason, we did not use the semantic sieves for our submission.
Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. $$$$$ We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.
Another example which omits today in the phrase for the predicted mention is mentioned in (Lee et al, 2011) and this boundary mismatch also accounts for precision and recall errors. $$$$$ Since these two sieves use 3We initialize the clusters as singletons and grow them progressively in each sieve.

For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent. $$$$$ FA8750-09-C-0181.
For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent. $$$$$ Finally, Table 4 lists our results on the held-out testing partition.
For Proper HeadWordMatch mentioned in (Lee et al, 2011), the Pronoun distance which indicates sentence distance limit between a pronoun and its antecedent. $$$$$ Post-processing is performed to adjust our output to the task specific constraints, e.g., removing singletons.

 $$$$$ For this shared task, we extended our existing system with new sieves that model shallow discourse (i.e., speaker identification) and semantics (lexical chains and alias detection).
 $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).
 $$$$$ FA8750-09-C-0181.

For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. $$$$$ We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.
For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. $$$$$ Our approach starts with a high-recall mention detection component, which identifies mentions using only syntactic information and named entity boundaries, followed by a battery of high-precision deterministic coreference sieves, applied one at a time from highest to lowest precision.
For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. $$$$$ This suggests that a different tuning of the sieve parameters is required for the predicted mention scenario.
For this purpose, existing work on coreference resolution (Lee et al, 2011) may prove to be useful. $$$$$ For example, [Lebanon] and [southern Lebanon] are not coreferent.

We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). $$$$$ The second stage implements the actual coreference resolution of the identified mentions.
We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). $$$$$ Furthermore, this model propagates global information by sharing attributes (e.g., gender and number) across mentions in the same cluster.
We filter arcs by simply adapting the sieves method proposed in (Lee et al, 2011). $$$$$ Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).

Sieves 2 to 7 are obtained from (Lee et al, 2011). $$$$$ This material is based upon work supported by the Air Force Research Laboratory (AFRL) under prime contract no.
Sieves 2 to 7 are obtained from (Lee et al, 2011). $$$$$ We thank the shared task organizers for their effort.
Sieves 2 to 7 are obtained from (Lee et al, 2011). $$$$$ This sieve addresses name aliases, which are detected as follows.
Sieves 2 to 7 are obtained from (Lee et al, 2011). $$$$$ Bare plurals - bare plurals are generic and cannot have a coreferent antecedent.

Note that this and several other rules rely on coreference information, which we obtain from two sources $$$$$ These were the top scores in both tracks.
Note that this and several other rules rely on coreference information, which we obtain from two sources $$$$$ FA8750-09-C-0181.

In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011). $$$$$ These models incorporate lexical, syntactic, semantic, and discourse information, and have access to document-level information (i.e., we share mention attributes across clusters as they are built).
In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011). $$$$$ FA8750-09-C-0181.
In particular, the top performing system in the CoNLL 2011 shared task (Pradhan et al, 2011) is a multi-pass system that applies tiers of deterministic co reference sieves from highest to lowest precision (Lee et al, 2011). $$$$$ We participated in both the open and closed tracks and submitted results using both predicted and gold mentions.
