Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ We presented the description, evaluation framework and assessment of systems participating in the SemEval-2010 sense induction task.
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ Hence, it can potentially favour systems generating a high number of homogeneous clusters.

Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ However in an imperfect situation, H(S|K) depends on the size of the dataset and the distribution of class sizes.
Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ Homogeneity.
Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ In the first one, unsupervised evaluation, systems’ answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009).
Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ We gratefully acknowledge the support of the EU FP7 INDECT project, Grant No.

Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ The first baseline, Most Frequent Sense (MFS), groups all testing instances of a target word into one cluster.
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ In the next step, participating systems were asked to disambiguate unseen instances of the same words using their learned senses.
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.

Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ The answers of the systems were then sent to organisers for evaluation.
Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ In this section, we present the results of the 26 systems along with two baselines.
Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ The reduction of the mapping corpus had a minimal impact on its performance.

The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ The evaluation has shown that the current state-of-the-art lacks unbiased measures that objectively evaluate clustering.
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ In the next step, participating systems were asked to disambiguate unseen instances of the same words using their learned senses.
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ 218086, the National Science Foundation Grant NSF-0715078, Consistent Criteria for Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No.

As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ More importantly, the definitions of hand-crafted lexical databases often do not reflect the exact meaning of a target word in a given context (V´eronis, 2004).
As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ Completeness.

Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ The main difference of the SemEval-2010 as compared to the SemEval-2007 sense induction task is that the training and testing data are treated separately, i.e the testing data are only used for sense tagging, while the training data are only used for sense induction.
Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ The target word dataset consists of 100 words, 50 nouns and 50 verbs.
Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.

In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ Homogeneity.
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ It seems that depending on the part-of-speech of the target word, different algorithms, features and parameters’ tuning have different impact.
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ As can be observed, the task consisted of three separate phases.
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ Systems generating a skewed distribution, in which a small number of homogeneous clusters tag the majority of instances and a larger number of clusters tag only a few instances, are likely to have a better performance than systems that produce a more uniform distribution.

However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ Homogeneity refers to the degree that each cluster consists of data points primarily belonging to a single GS class, while completeness refers to the degree that each GS class consists of data points primarily assigned to a single cluster (Rosenberg and Hirschberg, 2007).
However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ For each target word, participants were provided with a training set in order to learn the senses of that word.

We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010). $$$$$ When there is only a single class (H(S) = 0), any clustering would produce a perfectly homogeneous solution.
We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010). $$$$$ This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems.
We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010). $$$$$ The results of systems have shown that their performance in the unsupervised and supervised evaluation settings depends on cluster granularity along with the distribution of instances within the clusters.
