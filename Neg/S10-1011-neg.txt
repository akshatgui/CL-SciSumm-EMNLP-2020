Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ Participants were then asked to tag (disambiguate) each testing instance with the senses induced during the training phase.
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007).
Finally in terms of evaluation, our future work also focuses on evaluating HRGs using a fine grained sense inventory, extending the evaluation on the SemEval-2010 WSI task dataset (Manandhar et al., 2010) as well as applying HRGs to other related tasks such as taxonomy learning. $$$$$ The target word dataset consists of 100 words, 50 nouns and 50 verbs.

Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.
Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems.
Similar to Manandhar et al (2010), we use WordNet to first randomly select one synset of the first word, we then construct a set of words in various relations to the first word's chosen synset, including hypernyms, hyponyms, holonyms, meronyms and attributes. $$$$$ The evaluation has shown that the current state-of-the-art lacks unbiased measures that objectively evaluate clustering.

Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ 218086, the National Science Foundation Grant NSF-0715078, Consistent Criteria for Word Sense Disambiguation, and the GALE program of the Defense Advanced Research Projects Agency, Contract No.
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ The primary aim of SemEval-2010 WSI task is to allow comparison of unsupervised word sense induction and disambiguation systems.
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ For example, if we produce the row cluster vector [C1 = 0.8,C2 = 0.1, C3 = 0.1, C4=0.0],and multiply it with the normalised matrix of Table 3, then we would get a row sense vector in which G3 would be the winning sense with a score equal to 0.43.
Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al, 2010), which are used in our evaluation. $$$$$ Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.

Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ The target word dataset consists of 100 words, 50 nouns and 50 verbs.
Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ Despite that, none of the systems outperform the MFS baseline.
Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ In this task, participants were required to induce the senses of 100 target words using a training set, and then disambiguate unseen instances of the same words using the induced senses.
Our primary WSI evaluation is based on the dataset provided by the SemEval-2010 WSI shared task (Manandhar et al 2010). $$$$$ In the first phase, training phase, participating systems were provided with a training dataset that consisted of a set of target word (noun/verb) instances (sentences/paragraphs).

The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ More importantly, the definitions of hand-crafted lexical databases often do not reflect the exact meaning of a target word in a given context (V´eronis, 2004).
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ In the next step, participating systems were asked to disambiguate unseen instances of the same words using their learned senses.
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.
The original task also made use of V-measure and Paired F-score to evaluate the induced word sense clusters, but have degenerate behaviour in correlating strongly with the number of senses induced by the method (Manandhar et al 2010), and are hence omitted from this paper. $$$$$ Table 4 shows the V-Measure (VM) performance of the 26 systems participating in the task.

As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ More importantly, the definitions of hand-crafted lexical databases often do not reflect the exact meaning of a target word in a given context (V´eronis, 2004).
As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ In the second phase, testing phase, participating systems were provided with a testing dataset that consisted of a set of target word (noun/verb) instances (sentences/paragraphs).
As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ Unsupervised Word Sense Induction (WSI) aims to overcome these limitations of handconstructed lexicons by learning the senses of a target word directly from text without relying on any hand-crafted resources.
As a second experiment, we analyze incorrect sense assignments on SemEval-2 Task 14 (Manandhar et al., 2010) to measure whether sense-relatedness biases which sense was incorrectly selected. $$$$$ Precision can be defined as the number of common instance pairs between the two sets to the total number of pairs in the clustering solution (Equation 7), while recall can be defined as the number of common instance pairs between the two sets to the total number of pairs in the gold standard (Equation 8).

Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ The homogeneity, h, of a clustering solution is defined in Formula 1, where H(S|K) is the conditional entropy of the class distribution given the proposed clustering and H(S) is the class entropy.
Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ It seems that systems generating a smaller number of clusters than the GS number of senses are biased towards the MFS, hence they are not able to perform better.
Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ Table 6 shows the results of this evaluation for a 80-20 test set split, i.e.
Our word sense induction and disambiguation model is trained and tested on the dataset of the SEMEVAL-2010 WSI/WSD task (Manandhar et al, 2010). $$$$$ The evaluation has shown that the current state-of-the-art lacks unbiased measures that objectively evaluate clustering.

In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ Systems’ answers were evaluated in: (1) an unsupervised manner by using two clustering evaluation measures, and (2) a supervised manner in a WSD task.
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ This paper presents the description and evaluation framework of SemEval-2010 Word Sense Induction & Disambiguation task, as well as the evaluation results of 26 participating systems.
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ The reduction of the mapping corpus had a larger impact in this case.
In the paired F-Score (Artiles et al, 2009) evaluation, the clustering problem is transformed into a classification problem (Manandhar et al, 2010). $$$$$ The results of systems have shown that their performance in the unsupervised and supervised evaluation settings depends on cluster granularity along with the distribution of instances within the clusters.

However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ For that reason, we applied a second testing set split, where 60% of the testing corpus was used for mapping and 40% for evaluation.
However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ In the same vein, we can generate �36 total, the GS classes contain 5820 instance pairs.
However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ The answers of the systems were then sent to organisers for evaluation.
However, this induction step has proven to be greatly challenging, in the most recent shared tasks, induction systems either appear to perform poorly or fail to outperform the simple Most Frequent Sense baseline (Agirre and Soroa, 2007a; Manandhar et al, 2010). $$$$$ Our future work will focus on the assessment of sense induction on a task-oriented basis as well as on clustering evaluation.

We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010). $$$$$ The target word dataset consists of 100 words, 50 nouns and 50 verbs.
We first propose evaluating ensemble configurations of Word Sense Induction models using the standard shared tasks from SemEval-1 (Agirre and Soroa, 2007a) and SemEval-2 (Manandhar et al, 2010). $$$$$ More importantly, the definitions of hand-crafted lexical databases often do not reflect the exact meaning of a target word in a given context (V´eronis, 2004).
