UNN-WePS achieved an average purity of 0.6, and inverse purity of 0.73 in Semeval Task 13, achieving seventh position out of sixteen competing systems (Artiles et al 2007). $$$$$ 10 SWAT-IV ,58 ,64 ,55 ,71.
UNN-WePS achieved an average purity of 0.6, and inverse purity of 0.73 in Semeval Task 13, achieving seventh position out of sixteen competing systems (Artiles et al 2007). $$$$$ A couple of differences make our problem different.
UNN-WePS achieved an average purity of 0.6, and inverse purity of 0.73 in Semeval Task 13, achieving seventh position out of sixteen competing systems (Artiles et al 2007). $$$$$ This paper does not necessarily reflect the po sition of the U.S. Government.

We have described a system, UNN-WePS that disambiguates individuals in web pages as required for Semeval task 13 (Artiles et al 2007). $$$$$ 2.3 Baselines.
We have described a system, UNN-WePS that disambiguates individuals in web pages as required for Semeval task 13 (Artiles et al 2007). $$$$$ Training data included the downloaded webpages, their associated metadata and the human clustering of each document set, providing a develop ment test-bed for the participant?s systems.
We have described a system, UNN-WePS that disambiguates individuals in web pages as required for Semeval task 13 (Artiles et al 2007). $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.

We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. $$$$$ This paper does not necessarily reflect the po sition of the U.S. Government.
We used three datasets in our experiments, WePS1 Training and Testing (Artiles et al 2007), WePS2 Testing (Javier et al 2009). These datasets collected names from three different resources including Wikipedia names, program committee of a computer science conference and US census. $$$$$ Person names, however, are highly ambigu ous.

We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). $$$$$ 11 UA-ZSA ,58 ,60 ,58 ,64.
We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). $$$$$ This paper does not necessarily reflect the po sition of the U.S. Government.
We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). $$$$$ This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.
We adopt the same evaluation process as (Han and Zhao, 2009), and evaluating these models using Purity, Inverse Purity and the F-measure (also used in WePS Task Artiles et al 2007)). $$$$$ In addition, all the collected and annotated dataset will be publicly available 5 as a benchmark for Web People Search systems.At the same time, it is clear that building a re liable test-bed for the task is not simple.

We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009). $$$$$ Secondly, it is probably necessary to think about specific evaluation measures beyond standard clustering metrics such as purity and inverse purity,which are not tailored to the task and do not be have well when multiple classification is allowed.We hope to address these problems in a forthcom ing edition of the WEPS task.
We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009). $$$$$ This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.
We adopted the standard data sets used in the First Web People Search Clustering Task (WePS1) (Artiles et al, 2007) and the Second Web People Search Clustering Task (WePS2) (Artiles et al, 2009). $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.

We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al, 2007). $$$$$ The WEPS task ended with considerable success in terms of participation, and we believe that a careful analysis of the contributions made by participants(which is not possible at the time of writing this report) will be an interesting reference for future re search.
We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al, 2007). $$$$$ This research was supported in part by the National Science Foundation of United States under GrantIIS-00325657 and by a grant from the Spanish government under project Text-Mess (TIN2006-15265C06).
We consider the problem of disambiguating person names in a Web searching scenario as described by the Web People Search Task in SemEval 2007 (Artiles et al, 2007). $$$$$ The WEPS task ended with considerable success in terms of participation, and we believe that a careful analysis of the contributions made by participants(which is not possible at the time of writing this report) will be an interesting reference for future re search.

Two different evaluation measures are reported as described by the task $$$$$ The second differenceis that WSD usually operates with a dictionary con taining a relatively small number of senses that can be assigned to each word.
Two different evaluation measures are reported as described by the task $$$$$ First ofall, the variability across test cases is large and un predictable, and a system that works well with the 5http://nlp.uned.es/wepsnames in our test bed may not be reliable in practi cal, open search situations.
Two different evaluation measures are reported as described by the task $$$$$ This research was supported in part by the National Science Foundation of United States under GrantIIS-00325657 and by a grant from the Spanish government under project Text-Mess (TIN2006-15265C06).
Two different evaluation measures are reported as described by the task $$$$$ 11 UA-ZSA ,58 ,60 ,58 ,64.

Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. $$$$$ This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.
Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. $$$$$ The task description and the initial trial data set were publicly released before the start of the official evaluation.The official evaluation period started with the simultaneous release of both training and test data, to gether with a scoring script with the main evaluation measures to be used.
Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. $$$$$ The user is then forced ei ther to add terms to the query (probably losing recall and focusing on one single aspect of the person), orto browse every document in order to filter the infor mation about the person he is actually looking for.
Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval 2007 WEPS task (Artiles et al, 2007) and Semeval-. $$$$$ 3.1 Results and discussion.

In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). $$$$$ Partly because of that,our test-bed happened to be unintentionally challenging for systems, with a large difference be tween the average ambiguity in the training and test datasets.
In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). $$$$$ An early work in name disambiguation (Baggaand Baldwin, 1998) uses the similarity between doc uments in a Vector Space using a ?bag of words?
In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). $$$$$ In both cases, the problem addressed is the resolution of the ambiguity in a natural language expression.
In this section we will explain in more detail how we implemented the general schema described in the previous section to the Web People Search task (Artiles et al, 2007). $$$$$ 14 DFKI2 ,50 ,63 ,39 ,83.

The data we have used for training our system were made available in the framework of the SemEval (task 13 $$$$$ We found that, in many cases, different entities were mentioned using the ambiguous name within asingle document.
The data we have used for training our system were made available in the framework of the SemEval (task 13 $$$$$ 15 WIT ,49 ,66 ,36 ,93.

For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. $$$$$ The WEB03 Corpus has the lowest ambiguity (5,9 entities per name), for two reasons: first, randomly picked names belong predominantly to the long tail of unfrequent person names which, per se, have low ambiguity.
For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. $$$$$ Person names, however, are highly ambigu ous.
For both categories the number of target output clusters equals (number of RIPPER output clusters+ the number of documents*0.2). Although the clustering results with the best set tings for hierarchical and agglomerative clustering were very close with regard to F-score (combining purity and inverse purity, see (Artiles et al, 2007) for a more detailed description), manual inspection of the content of the clusters has revealed big differences between the two approaches. $$$$$ In most cases, the results for a person name search are a mix of pages about different peoplesharing the same name.

We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al, 2007). $$$$$ Partly because of that,our test-bed happened to be unintentionally challenging for systems, with a large difference be tween the average ambiguity in the training and test datasets.
We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al, 2007). $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.
We evaluate our methods using the benchmark test collection from the ACL SemEval-2007 web person search task (WePS hereafter) (Artiles et al, 2007). $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.

Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). $$$$$ Person names, however, are highly ambigu ous.
Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). $$$$$ The interpretation of micro-average F is less clear.
Hence the performance reported here is comparable to the official evaluation results (Artiles et al, 2007). $$$$$ This research was supported in part by the National Science Foundation of United States under GrantIIS-00325657 and by a grant from the Spanish government under project Text-Mess (TIN2006-15265C06).

The goal of the Web People Search task (Artiles et al 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. $$$$$ The interpretation of micro-average F is less clear.
The goal of the Web People Search task (Artiles et al 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.
The goal of the Web People Search task (Artiles et al 2007) is to assign Web pages to groups, where each group contains all (and only those) pages that refer to one unique entity. $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.

In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). $$$$$ This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.
In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). $$$$$ 18 ALL-IN-ONE ,40 ,58 ,29 1,00.
In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). $$$$$ (actual people) is unknown a priori, and it is in average much higher than in the WSD task(there are 90,000 different names shared by 100 mil lion people according to the U.S. Census Bureau).
In this paper, we described our participating system in the SemEval-2007 Web People Search Task (Artiles et al, 2007). $$$$$ This paper presents the task definition, resources, participation, and comparative re sults for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise.

The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008). $$$$$ 10 SWAT-IV ,58 ,64 ,55 ,71.
The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008). $$$$$ Secondly, it is probably necessary to think about specific evaluation measures beyond standard clustering metrics such as purity and inverse purity,which are not tailored to the task and do not be have well when multiple classification is allowed.We hope to address these problems in a forthcom ing edition of the WEPS task.
The research on cross-document entity coreference resolution can be traced back to the Web People Search task (Artiles et al, 2007) and ACE2008 (e.g. Baron and Freedman, 2008). $$$$$ This research was supported in part by the National Science Foundation of United States under GrantIIS-00325657 and by a grant from the Spanish government under project Text-Mess (TIN2006-15265C06).

Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al, 2007) $$$$$ 2.3 Baselines.
Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al, 2007) $$$$$ Finally, ten additional names were ran domly selected from the Program Committee listing of a Computer Science conference (ECDL 2006).This set offers a scenario of potentially low am biguity (computer science scholars usually have a stronger Internet presence than other professionalfields) with the added value of the a priori knowl edge of a domain specific type of entity (scholar) present in the data.
Here, we concentrate on the following SemEval 2007 Web People Search Task (Artiles et al, 2007) $$$$$ This paper does not necessarily reflect the po sition of the U.S. Government.

Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. $$$$$ There are some ranking swaps, but gen erally only within close pairs.The good performance of the ONE-IN-ONE baseline system is indicative of the abundance of singleton entities (entities represented by only one doc ument).
Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. $$$$$ Secondly, it is probably necessary to think about specific evaluation measures beyond standard clustering metrics such as purity and inverse purity,which are not tailored to the task and do not be have well when multiple classification is allowed.We hope to address these problems in a forthcom ing edition of the WEPS task.
Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. $$$$$ Following the pre vious example, in documents for the name ?Edward Fox?
Recently, there is significant research interest in a related task called Web Person Search (WePS) (Artiles et al, 2007), which seeks to determine whether two documents refer to the same person given a person name search query. $$$$$ This paper does not necessarily reflect the po sition of the U.S. Government.

The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. $$$$$ 16 UC3M 13 ,48 ,66 ,35 ,95.
The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. $$$$$ 2 IRST-BP ,75 ,77 ,75 ,80.
The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. $$$$$ In addition, all the collected and annotated dataset will be publicly available 5 as a benchmark for Web People Search systems.At the same time, it is clear that building a re liable test-bed for the task is not simple.
The more recent Web Person Search (WePS) task (Artiles et al, 2007) has created a benchmark dataset which is also used in this work. $$$$$ 2.1.3 Test data For the test data we followed the same process described for the training.

Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007). $$$$$ There are some ranking swaps, but gen erally only within close pairs.The good performance of the ONE-IN-ONE baseline system is indicative of the abundance of singleton entities (entities represented by only one doc ument).
Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007). $$$$$ This paper does not necessarily reflect the po sition of the U.S. Government.
Similar IR features are also used by other WePS systems as they are more robust to the variety of web pages (Artiles et al, 2007). $$$$$ 16 UC3M 13 ,48 ,66 ,35 ,95.
