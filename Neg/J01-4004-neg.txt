Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0). $$$$$ The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.
Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0). $$$$$ The first step is to remove all postmodifiers such as Corp. and Ltd. Then, the acronym function considers each word in turn, and if the first letter is capitalized, it is used to form the acronym.
Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0). $$$$$ There is a long tradition of work on coreference resolution within computational linguistics, but most of it was not subject to empirical evaluation until recently.
Decision tree algorithms were used for reference resolution by Aone and Bennett (1995, C4.5), McCarthy and Lehnert (1995, C4.5) and Soon et al (2001, C5.0). $$$$$ There is a long tradition of work on coreference resolution within computational linguistics, but most of it was not subject to empirical evaluation until recently.

It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain. $$$$$ In our system, we defined the following semantic classes: &quot;female,&quot; &quot;male,&quot; &quot;person,&quot; &quot;organization,&quot; &quot;location,&quot; &quot;date,&quot; &quot;time,&quot; &quot;money,&quot; &quot;percent,&quot; and &quot;object.&quot; These semantic classes are arranged in a simple ISA hierarchy.
It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain. $$$$$ This paper is an expanded version of a preliminary paper that appeared in the Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.
It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain. $$$$$ It is an important subtask in natural language processing systems.
It was criticized (Soon et al, 2001) that the features used by McCarthy and Lehnert (1995) are highly idiosyncratic and applicable only to one particular domain. $$$$$ Other baseline systems that are used are ONE_CHAIN, ONE_WRD, and HD_WRD (Cardie and Wagstaff 1999).

Soon et al (2001) use twelve features (see Table 1). $$$$$ The MUC-6 and MUC-7 coreference task definitions are slightly different.
Soon et al (2001) use twelve features (see Table 1). $$$$$ The contribution of our work lies in showing that a learning approach, when evaluated on common coreference data sets, is able to achieve accuracy competitive with that of state-of-the-art systems using nonlearning approaches.
Soon et al (2001) use twelve features (see Table 1). $$$$$ Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.
Soon et al (2001) use twelve features (see Table 1). $$$$$ are true, false, or unknown.

Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data. $$$$$ It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types.
Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data. $$$$$ Coreference resolution is the process of determining whether two expressions in natural language refer to the same entity in the world.
Soon et al (2001) include all noun phrases returned by their NP identifier and report an F-measure of 62.6% for MUC-6 data and 60.4% for MUC-7 data. $$$$$ One advantage of using a decision tree learning algorithm is that the resulting decision tree classifier can be interpreted by humans.

Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs. $$$$$ Also, we do not place any restriction on the possible candidate markables; that is, all markables, whether they are &quot;organization,&quot; &quot;person,&quot; or other entity types, are considered.
Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs. $$$$$ The possible antecedents of His cannot be His daughter or His daughter's eyes, but can be Mr. Tom or Mr. Tom's daughter.
Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs. $$$$$ 5.1.5 Errors in Alias Determination.
Features like the string ident and substring match features were used by other researchers (Soon et al, 2001), while the features ante med and ana med were used by Strube et al (2002) in order to improve the performance for definite NPs. $$$$$ For example, our named entity recognizer could not identify the two named entities USAir and Piedmont in the expression USAir and Piedmont but instead treat them as one single named entity.

However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001). $$$$$ RESOLVE does not use the APPOSITIVE feature.
However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001). $$$$$ In this paper, we presented a learning approach to coreference resolution of noun phrases in unrestricted text.
However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001). $$$$$ We evaluated our approach on common data sets, namely, the MUC-6 and MUC-7 coreference corpora.
However, reference resolution algorithms based on these classifiers achieve reasonable performance of about 60 to 63% F-measure (Soon et al, 2001). $$$$$ Also, we do not place any restriction on the possible candidate markables; that is, all markables, whether they are &quot;organization,&quot; &quot;person,&quot; or other entity types, are considered.

While the second best system (Bjorkelund and Farkas, 2012) followed the widely used baseline of Soon et al (2001), the winning system (Fernandes et al, 2012) proposed the use of a tree representation. $$$$$ The task definition states that a coreference chain must contain at least one element that is a head noun or a name; that is, a chain containing only prenominal modifiers is removed by the filtering module.
While the second best system (Bjorkelund and Farkas, 2012) followed the widely used baseline of Soon et al (2001), the winning system (Fernandes et al, 2012) proposed the use of a tree representation. $$$$$ The work of Aone and Bennett (1995), McCarthy and Lehnert (1995), Fisher et al. (1995), and McCarthy (1996) employed decision tree learning.

For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3). $$$$$ There are also 30 annotated training documents from MUC-7.
For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3). $$$$$ System architecture of natural language processing pipeline.
For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3). $$$$$ Among the papers that have reported quantitative evaluation results, most are not based on learning from an annotated corpus (Baldwin 1997; Kameyama 1997; Lappin and Leass 1994; Mitkov 1997).
For instance, the popular pairwise instance creation method suggested by Soon et al (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., he b 2 is the antecedent of Gary Wilber b 3). $$$$$ Other baseline systems that are used are ONE_CHAIN, ONE_WRD, and HD_WRD (Cardie and Wagstaff 1999).

By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set. $$$$$ Finally, the work of Cardie and Wagstaff (1999) also falls under the machine learning approach.
By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set. $$$$$ Their clustering method achieved a balanced Fmeasure of only 53.6% on MUC-6 test data.
By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set. $$$$$ On the other hand, the markables extracted by our system include nested noun phrases, MUC-style named entity types (money, percent, date, etc.
By using a simple co-reference resolution tool adapted from (Soon et al, 2001), we add all the mentions referring to the target into the extended target set. $$$$$ The following two subsections describe the errors in more detail.

Instances are created following Soon et al (2001). $$$$$ We obtained encouraging results, indicating that on the general noun phrase coreference task, the learning approach achieves accuracy comparable to that of nonlearning approaches.
Instances are created following Soon et al (2001). $$$$$ We obtained encouraging results, indicating that on the general noun phrase coreference task, the learning approach achieves accuracy comparable to that of nonlearning approaches.
Instances are created following Soon et al (2001). $$$$$ This paper is an expanded version of a preliminary paper that appeared in the Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.
Instances are created following Soon et al (2001). $$$$$ In addition, these features must be generic enough to be used across different domains.

Following Ng & Cardie (2002), our baseline system reimplements the Soon et al (2001) system. $$$$$ In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text.
Following Ng & Cardie (2002), our baseline system reimplements the Soon et al (2001) system. $$$$$ For MUC-7, because of slight changes in the coreference task definition, we include a filtering module to remove certain coreference chains.
Following Ng & Cardie (2002), our baseline system reimplements the Soon et al (2001) system. $$$$$ It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of &quot;organization,&quot; &quot;person,&quot; or other types.

We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE). $$$$$ We also thank Beth Sundheim for helpful comments on an earlier version of this paper, and Hai Leong Chieu for his implementation of the HMM-based named entity recognition module.
We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE). $$$$$ 5.2.1 Inadequacy of Current Surface Features.
We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE). $$$$$ The recall level here indirectly measures how effective the noun phrase identification module is.
We start with a baseline system using all the features from Soon et al (2001) that were not removed in the feature selection process (i.e. DISTANCE). $$$$$ Although Aone and Bennett's (1995) system also made use of decision tree learning for coreference resolution, they dealt with Japanese texts, and their evaluation focused only on noun phrases denoting organizations, whereas our evaluation, which dealt with English texts, encompassed noun phrases of all types, not just those denoting organizations.

It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. $$$$$ In MUC-6, j needs to be a definite noun phrase to be an appositive, while both indefinite and definite noun phrases are acceptable in MUC-7.
It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. $$$$$ All the unary features score an F-measure of 0.
It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. $$$$$ We would like to thank the MUC organizers who made available to us the MUC-6 and MUC-7 data sets, without which this work would have been impossible.
It supports both local (Soon et al (2001)-style) and global (ILP, Denis and Baldridge (2007)-style) models of coreference. $$$$$ For MUC-6, 30 dry-run documents annotated with coreference information were used as the training documents for our coreference engine.

Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set. $$$$$ In particular, information extraction (IE) systems like those built in the DARPA Message Understanding Conferences (Chinchor 1998; Sundheim 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (MUC-6 1995).
Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set. $$$$$ We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches.
Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set. $$$$$ Council on Foreign Relations, is expected to be named (undersecretary)1 for political affairs.... Former Sen. Tim Wirth is expected to get a newly created (undersecretary)2 post for global affairs, which would include refugees, drugs and environmental issues. when the surface strings of two markables match and thus, by the C5 decision tree in Figure 2, they are treated as coreferring.
Our local model of coreference is a reimplementation of the algorithm, proposed by Soon et al (2001) with an extended feature set. $$$$$ The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.

PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. $$$$$ Our system's scores are in the upper region of the MUC-6 and MUC-7 systems.
PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. $$$$$ On the other hand, the markables extracted by our system include nested noun phrases, MUC-style named entity types (money, percent, date, etc.
PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. $$$$$ It is an important subtask in natural language processing systems.
PAIRWISE: as in the work by Soon et al (2001), antecedent identification and anaphoricity determination are simultaneously executed by a single classifier. $$$$$ We would like to thank the MUC organizers who made available to us the MUC-6 and MUC-7 data sets, without which this work would have been impossible.

Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a). $$$$$ We evaluated our approach on common data sets, namely, the MUC-6 and MUC-7 coreference corpora.
Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a). $$$$$ It should be noted that the accuracy of our coreference resolution engine depends to a large extent on the performance of the NLP modules that are executed before the coreference engine.
Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a). $$$$$ The distance feature has different effects on different noun phrases.
Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al, 2001) and (Ng and Cardie, 2002a). $$$$$ This makes our coreference engine a domain-independent module.

We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al, 2001). $$$$$ Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.
We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al, 2001). $$$$$ System architecture of natural language processing pipeline.
We construct this entity-mention graph by learning to decide for each mention which preceding mention, if any, belongs in the same equivalence class; this approach is commonly called the pairwise coreference model (Soon et al, 2001). $$$$$ As explained in McCarthy (1996), the reason for this low recall is that RESOLVE takes only the &quot;relevant entities&quot; and &quot;relevant references&quot; as input, where the relevant entities and relevant references are restricted to &quot;person&quot; and &quot;organization.&quot; In addition, because of limitations of the noun phrase detection module, nested phrases are not extracted and therefore do not take part in coreference.

Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. $$$$$ We evaluated our approach on common data sets, namely, the MUC-6 and MUC-7 coreference corpora.
Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. $$$$$ In particular, information extraction (IE) systems like those built in the DARPA Message Understanding Conferences (Chinchor 1998; Sundheim 1995) have revealed that coreference resolution is such a critical component of IE systems that a separate coreference subtask has been defined and evaluated since MUC-6 (MUC-6 1995).
Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. $$$$$ .
Soon et al (2001) use the Closest-Link method: They select as an antecedent the closest preceding mention that is predicted coreferential by a pairwise coreference module; this is equivalent to choosing the closest mention whose pc value is above a threshold. $$$$$ The precision scores obtained when using the APPOSITIVE feature alone are shown in Table 3 and Table 4, which suggest that the module can be improved further.

Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001). $$$$$ The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.
Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001). $$$$$ The RESOLVE system is presented in McCarthy and Lehnert (1995), Fisher et al. (1995), and McCarthy (1996).
Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001). $$$$$ This paper is an expanded version of a preliminary paper that appeared in the Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.
Distance features are important for a system that makes links based on the best pairwise coreference value rather than implicitly incorporating distance by linking only the closest pair whose score is above a threshold, as done by e.g. Soon et al (2001). $$$$$ The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases.

The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001). $$$$$ The ability to link coreferring noun phrases both within and across sentences is critical to discourse analysis and language understanding in general.
The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001). $$$$$ (her boss)75 is not tested because (her)76 is its nested noun phrase.
The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001). $$$$$ In (6), the two instances of chief executive officer refer to two different persons, namely, Allan Laufgraben and Milton Petrie, and, again, should not corefer. made by the noun phrase identification module.
The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. Soon et al., 2001). $$$$$ For our system, the errors due to the inadequacy of surface features and semantic class determination problems account for about 75% of the missing links.
