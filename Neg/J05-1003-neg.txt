Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ This contrasts with the simple updates in the improved boosting algorithm (W+k = W+k + D and Wk = Wk + D).
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.

Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ First, we show that LogLossðUpdð¯a, k, dÞÞ < LogLossð¯a — Wþk — Wk þ Wþk e~d þ Wk edÞ ðB.4Þ Equation (B.6) can be derived from equation (B.5) through the bound logð1 + xÞ < x for all x.
Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ This was in spite of various heuristics that were implemented in an attempt to speed up LogLoss: for example, selecting multiple features at each round or recalculating the statistics for only the best K features for some small K at the previous round of feature selection.
Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ McCallum (2003) and Riezler and Vasserman (2004) describe approaches that add k features at each step, where k is some constant greater than one.

In our work, we included all features described in (Collins and Koo, 2005). $$$$$ The three nonterminals (for example, NP, NP, PP) identify the parent of the entire phrase, the nonterminal of the head of the phrase, and the nonterminal label for the PP.
In our work, we included all features described in (Collins and Koo, 2005). $$$$$ The approach gives a 13% relative reduction in error on parsing Wall Street Journal data.

We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ (2002) suggest training log-linear models (i.e., the LogLoss function in equation (9)) for parsing problems.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ This is around 1,000 times as much computation as that required for the efficient boosting algorithm applied to our data, suggesting that the feature selection methods in Berger, Della Pietra, and Della Pietra (1996), Ratnaparkhi (1998), and Della Pietra, Della Pietra, and Lafferty (1997) are not sufficiently efficient for the parsing task.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ Steve Abney and Fernando Pereira gave useful feedback on earlier drafts of this work.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ The three nonterminals (for example, NP, NP, PP) identify the parent of the entire phrase, the nonterminal of the head of the phrase, and the nonterminal label for the PP.

In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.
In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.
In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ Until now, we have defined BestLossðk, ¯aÞ to be the minimum of the loss given that the kth feature is updated an optimal amount: BestLossðk, ¯aÞ 1/4 min LogLossðUpdð¯a,k, dÞÞ d In this section we sketch a different approach, based on results from Collins, Schapire, and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4.
In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ The learning curve is initially steep, eventually flattening off, but reaching its peak value after a large number (90,386) of rounds of feature selection.

Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ This may be important in some domains, for example, those in which there are a very large number of features and this large number leads to difficulties in terms of memory requirements or computation time.
Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before.
Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ McCallum (2003) uses a value of k = 1,000.

A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.
A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ We apply the boosting method to parsing the Wall Street Journal treebank.
A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ This article has introduced a new algorithm, based on boosting approaches in machine learning, for ranking problems in natural language processing.
A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler case of classification problems; this work forms the basis for the reranking methods.

There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ The statistical justification for boosting approaches is quite different.
There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.
There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution.
There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.

These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ The new model achieved 89.75% a 13% relative decrease in measure error over the baseline model’s score of 88.2%.
These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ The fact that the boosting approach does not update the entire model at each round of feature selection may be a disadvantage in terms of the number of features or the test data accuracy of the final model.
These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ As a result the model can take into account quite a rich set of features in the history.

Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ Note that this is now an approximation, in that BestLossðk, ¯a) is an upper bound on the log-likelihood which may or may not be tight.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ Initially the graph is higher than that for & = 0.0025, but on later rounds the performance starts to decrease.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation which takes these features into account.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ Steve Abney and Fernando Pereira gave useful feedback on earlier drafts of this work.

As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ Until now, we have defined BestLossðk, ¯aÞ to be the minimum of the loss given that the kth feature is updated an optimal amount: BestLossðk, ¯aÞ 1/4 min LogLossðUpdð¯a,k, dÞÞ d In this section we sketch a different approach, based on results from Collins, Schapire, and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4.
As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ Parameter estimation becomes a problem of learning how to combine these different sources of information.
As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ In addition, the article introduced a new algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data that we use.
As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ Parsing can be framed as a supervised learning task, to induce a function f : X--+Y given training examples (xi, yi), where xi Z X, yi Z Y.

The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ Finally, thanks to the anonymous reviewers for several useful comments.
The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ “LR”is labeled recall; “LP”is labeled precision; “CBs”is the average number of crossing brackets per sentence; “0 CBs”is the percentage of sentences with 0 crossing brackets; “2 CBs”is the percentage of sentences with two or more crossing brackets.
The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ See Collins (2002a, 2002b) and Collins and Duffy (2001, 2002) for applications of the perceptron algorithm.
The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.

We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set.
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ This is the only essential difference between the new algorithm and the ExpLoss method.
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ We define GEN(x)ÎY to be the set of candidates for a given input x.
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.

For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001).
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ Future work on reranking approaches might consider other approaches—such as boosting of decision trees—which can effectively consider more complex features.
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ This article considers approaches which rerank the output of an existing probabilistic parser.

As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ We apply the boosting method to parsing the Wall Street Journal treebank.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ Results on section 23 of the WSJ Treebank.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.

We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ Appendix B: An Alternative Method for LogLoss In this appendix we sketch an alternative approach for feature selection in LogLoss that is potentially an efficient method, at the cost of introducing an approximation in the feature selection method.
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.

At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ Finally, thanks to the anonymous reviewers for several useful comments.
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ For a full derivation of the modified updates and for quite technical convergence proofs, see Collins, Schapire and Singer (2002).
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ One contribution of our research is to draw similar connections between the two approaches to ranking problems.
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ Each ai can take any value in the reals.

Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.
Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ Figures 8 and 9 show graphs of Work(n) and Savings(n) versus n. The savings from the improved algorithm are dramatic.
Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ One contribution of our research is to draw similar connections between the two approaches to ranking problems.
Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.

Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ From these results, the algorithms in Figures 3 and 4 could be altered to take the revised definitions of Wþk and Wk into account.
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation.
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ This function is f(z) = log (1 + e—z), f(z) = e—z, or f (z) = Qz < 01 for LogLoss, ExpLoss, and Error, respectively.
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.

In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ Machine-learning approaches to natural language parsing have recently shown some success in complex domains such as news wire text.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ This is a problem for parameter estimation, in which an estimate of the denominator is required, and Monte Carlo methods have been proposed (Della Pietra, Della Pietra, and Lafferty 1997; Abney 1997; Rosenfeld 1997) as a technique for estimation of this value.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ A key insight of Lafferty, McCallum, and Pereira (2001) is that when features are of a significantly local nature, the gradient of the function in equation (28) can be calculated efficiently using dynamic programming, even in cases in which the set of candidates involves all possible tagged sequences and is therefore exponential in size.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ The key characteristics of the approach are the use of global features and of a training criterion (optimization problem) that is discriminative and closely related to the task at hand (i.e., parse accuracy).
