Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ One contribution of our research is to draw similar connections between the two approaches to ranking problems.
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models.
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models.
Previous work in statistical parsing (Collins and Koo, 2005) has shown that applying reranking techniques to the n-best output of a base parser can improve parsing performance. $$$$$ The ranking error rate is the number of times a lower-scoring parse is (incorrectly) ranked above the best parse: where again, gpÄ is one if p is true, zero otherwise.

Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ It can be considered to be a type of PCFG, where the rules are lexicalized.
Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ Feature selection methods can probably be motivated either from a Bayesian perspective (through a prior favoring models with a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research.
Previous approaches (e.g., (Collins and Koo, 2005)) have used a linear model to combine the log probability under a base parser with arbitrary features derived from parse trees. $$$$$ The new model achieved 89.75% a 13% relative decrease in measure error over the baseline model’s score of 88.2%.

In our work, we included all features described in (Collins and Koo, 2005). $$$$$ , ni, as Thus Mij(¯a) is the difference in ranking score between the correct parse of a sentence and a competing parse xi,j.
In our work, we included all features described in (Collins and Koo, 2005). $$$$$ 5.4.1 The Effect of the a and N Parameters.
In our work, we included all features described in (Collins and Koo, 2005). $$$$$ In terms of simplicity, the methods in McCallum (2003) and Riezler and Vasserman (2004) require selection of a number of free parameters governing the behavior of the algorithm: the value for k, the value for a regularizer constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the precision with which the model is optimized at each stage of feature selection (McCallum [2003] describes using “just a few BFGS iterations”at each stage).
In our work, we included all features described in (Collins and Koo, 2005). $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.

We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ We split the training sample into three sets depending on this value: Aþk 1/4 fði,jÞ : 1/2hkðxi,1Þ — hkðxi,jÞ] 1/4 1g To find the value of d that minimizes this loss, we set the derivative of (A.1) with respect to d to zero, giving the following solution: where Z = ExpLoss(¯a) = Pi Pni 2 Si,je—Mi,j(¯a) is a constant (for constant ¯a) which appears in the BestLoss for all features and therefore does not affect their ranking.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ Boosting may even be applied in situations in which the number of features is infinite.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ Sections 2–21 inclusive (around 40,000 sentences) were used as training data, section 23 was used as the final test set.
We have found that features in (Collins and Koo, 2005), initially developed for English parsing, also give appreciable gains in accuracy when applied to Spanish. $$$$$ The methods we consider are greedy, at each iteration picking the feature hk with additive weight d which has the most impact on the loss function.

In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces.
In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ Abney (1997) describes the application of log-linear models to stochastic headdriven phrase structure grammars (HPSGs).
In the reranking experiments, we follow the procedure described in (Collins and Koo, 2005) for creation of a training set with n-best parses for each sentence. $$$$$ As an example, say the nonterminal dominating the example rule is S. The example rule would contribute (Left, S, VP, VBD, PP, adj = 1), (Right, S, VP, VBD, NP, adj = 1), (Right, S, VP, VBD, NP, adj = 0), and (Right, S, VP, VBD, SBAR, adj = 0).

Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning.
Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ The new model achieved 89.75% a 13% relative decrease in measure error over the baseline model’s score of 88.2%.
Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ The strength of our approach is that it allows a tree to be represented as an arbitrary set offeatures, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account.
Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule. $$$$$ In this article we give considerably more detail in terms of the algorithms involved, their justification, and their performance in experiments on natural language parsing.

A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ In terms of simplicity, the methods in McCallum (2003) and Riezler and Vasserman (2004) require selection of a number of free parameters governing the behavior of the algorithm: the value for k, the value for a regularizer constant (used in both McCallum [2003] and Riezler and Vasserman [2004]), and the precision with which the model is optimized at each stage of feature selection (McCallum [2003] describes using “just a few BFGS iterations”at each stage).
A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ Steve Abney and Fernando Pereira gave useful feedback on earlier drafts of this work.
A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998).
A well-studied subject (e.g. the work of Charniak and Johnson (2005) and of Collins and Koo (2005)), parse reranking is concerned with the reordering of n-best ranked parse trees output by a syntactic parser. $$$$$ Until now, we have defined BestLossðk, ¯aÞ to be the minimum of the loss given that the kth feature is updated an optimal amount: BestLossðk, ¯aÞ 1/4 min LogLossðUpdð¯a,k, dÞÞ d In this section we sketch a different approach, based on results from Collins, Schapire, and Singer (2002), which leads to an algorithm very similar to that for ExpLoss in Figures 3 and 4.

There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ In this section, we note that there is an alternative view of boosting in which it is described as a method for combining multiple models, for example, as a method for forming a linear combination of decision trees.
There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ The model in Charniak (2000) is quite different, however.
There is also work on discriminative models for parse reranking (Collins and Koo, 2005). $$$$$ “LR”is labeled recall; “LP”is labeled precision; “CBs”is the average number of crossing brackets per sentence; “0 CBs”is the percentage of sentences with 0 crossing brackets; “2 CBs”is the percentage of sentences with two or more crossing brackets.

These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.
These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data.
These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ The algorithm in Figure 4 recalculates the values of A k and A—k only for those features which co-occur with the selected feature k*.
These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al, 2005) and so on. $$$$$ A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence.

Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ The approach gives a 13% relative reduction in error on parsing Wall Street Journal data.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ Even with large values of k in the approach of McCallum (2003) and Riezler and Vasserman (2004) (e.g., k = 1,000), the approach we describe is likely to be at least as efficient as these alternative approaches.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.
Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005). $$$$$ Thanks to Rob Schapire and Yoram Singer for useful discussions on boosting algorithms and to Mark Johnson for useful discussions about linear models for parse ranking.

As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ Thus log-linear models with a Gaussian prior are likely to be comparable in terms of efficiency to the feature selection approach described in this article (in the experimental section, we showed that for the parsereranking task, the efficient boosting algorithm requires computation that is equivalent to around 40 passes over the training data).
As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ Section 5 gives experimental results, investigating the performance improvements on parsing, efficiency issues, and the effect of various parameters of the boosting algorithm.
As explained in Collins and Koo (2005), the decoding score plays an important role in reranking the candidate sentences. $$$$$ In feature selection approaches, as described in this article, the set of possible features hkðxÞ for k = 1, ... , m is taken to be a fixed set of relatively simple functions.

The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ In practice, however, adding new features to a generative or history-based model can be awkward: The derivation in the model must be altered to take the new features into account, and this can be an intricate task.
The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ The denominator terms in the qi,j definitions in equation (B.1) may complicate the algorithms somewhat, but it should still be possible to derive relatively efficient algorithms using the technique.
The n-best list for training data is produced using multi fold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). $$$$$ Other NLP tasks are likely to have similar characteristics in terms of sparsity.

We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ Finally, thanks to the anonymous reviewers for several useful comments.
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ The key characteristics of the approach are the use of global features and of a training criterion (optimization problem) that is discriminative and closely related to the task at hand (i.e., parse accuracy).
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ 6.5 Boosting, Perceptron, and Support Vector Machine Approaches for Ranking Problems Freund et al. (1998) introduced a formulation of boosting for ranking problems.
We compare our models with a boosting-based discriminative approach (Collins and Koo, 2005) and its regularized version (Huang et al, 2007). $$$$$ Steve Abney and Fernando Pereira gave useful feedback on earlier drafts of this work.

For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ The derivation is very close to that in Schapire and Singer (1999).
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ This article considers approaches which rerank the output of an existing probabilistic parser.
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.
For comparison purposes, we also showed the results of Collins and Koo (2005) its regularized versions with n-gram features. $$$$$ It follows that The ranking error is zero if all margins are positive.

As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ Our intention is to use the training examples to pick parameter values which improve upon this initial ranking.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ It is difficult to estimate the value for p, but assuming (very conservatively) that p = 2, selecting 10,000 features would require 30,000 passes over the training set.
As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). $$$$$ The earlier methods for maximum-entropy feature selection methods (Ratnaparkhi, Roukos, and Ward 1994; Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004).

We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses.
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ This article considers approaches which rerank the output of an existing probabilistic parser.
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ 7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman et al. (2000) and Duffy and Helmbold (1999).
We use the boosting approach of (Collins and Koo, 2005) to perform feature selection and identify good weight values. $$$$$ Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.

At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ The algorithm can be viewed as a feature selection method, optimizing a particular loss function (the exponential loss function) that has been studied in the boosting literature.
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ The methods we consider are greedy, at each iteration picking the feature hk with additive weight d which has the most impact on the loss function.
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ It can be seen that the performance gains are significantly larger in later rounds of feature selection, presumably because in later stages relatively infrequent features are being selected.
At test time, we choose an ordering using a maximum entropy reranking approach (Collins and Koo, 2005). $$$$$ Take the following definitions (note the similarity to the definitions in equations (13), (14), (15), and (16), with only the definitions for Wk+ and Wk~ being altered): Note that the ExpLoss computations can be recovered by replacing qi,j in equation (B.1) with qi,j 1/4 emi,jð¯aÞ.

Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ They are important for a few reasons.
Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ In contrast, our method requires a single parameter to be chosen (the value for the e smoothing parameter) and makes a single approximation (that only a single feature is updated at each round of feature selection).
Instead of examining and comparing rules in their entirety, this method abstracts a rule to its component parts, similar to features using information about n-grams of daughter nodes in parse reranking models (e.g., Collins and Koo, 2005). $$$$$ Previous work (Friedman, Hastie, and Tibshirani 1998) has drawn connections between log-linear models and boosting for classification problems.

Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ The parameters ak represent the influence of each feature on the score of a tree.
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ This section describes further experiments investigating various aspects of the boosting algorithm: the effect of the & and N parameters, learning curves, the choice of the Si,j weights, and efficiency issues.
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ As Schapire and Singer (1999) point out, the updates in equation (15) can be problematic, as they are undefined (infinite) when either Wþk or Wk is zero.
Collins and Koo (2005) proposed a reranking method for phrase structure parsing with which any type of global features in a parse tree can be used. $$$$$ Section 4 describes how these approaches can be generalized to ranking problems.

In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ We would argue that the improved boosting algorithm is a natural alternative to maximum-entropy or (conditional) log-linear models.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ This appendix gives a derivation of the optimal updates for ExpLoss.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ Experiments show an efficiency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting approach.
In the discriminative reranking method (Collins and Koo, 2005), first, a set of candidates is generated using a base model (GEN). $$$$$ We apply the boosting method to parsing the Wall Street Journal treebank.
