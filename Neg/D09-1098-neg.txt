Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ Finally, we release to the community a testbed for experimentally analyzing automatic set expansion, which includes a large collection of nearly random entity sets extracted from Wikipedia and over 22,000 randomly sampled seed expansion trials.
Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ An example observation is that good quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class.
Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ Given the set of seeds S = {Volvic, San Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our task is to find all other members of this class, such as {Agua Vida, Apenta, Culligan, Dasani, Ethos Water, Iceland Pure Spring Water, Imsdal, ...} Our goal is not to propose a new set expansion algorithm, but instead to test the effect of using our Web-scale term similarity matrix (enabled by the algorithm proposed in Section 3) on a state-of-theart distributional set expansion algorithm, namely (Sarmento et al. 2007).

 $$$$$ Since the gold standard sets vary significantly in size, we also provide the R-precision metric to normalize for set size: For the above metrics, 95% confidence bounds are computed using the randomly generated samples described in Section 5.2.
 $$$$$ Error analysis on the Web100 corpus shows that once our model has seen 10-20 seeds, the distributional similarity model seems to have enough statistics to discover as many new correct instances as it could ever find.
 $$$$$ Overall, corpus size and quality are both found to be important for extraction.
 $$$$$ Our implementation employs the MapReduce model by using the Map step to start M×N Map tasks in parallel, each caching 1/Mth part of A as an inverted index and streaming 1/Nth part of B through it.

KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ On average they only find 73% 5 To avoid biasing our Wikipedia corpus with the test sets, Wikipedia “List of” pages were omitted from our statistics as were any page linked to gold standard list members from “List of” pages. of the top-1000 similar terms of a random term whereas we find all of them.
KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.
KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ We consider S as a set of prototypical examples of the underlying entity set.
KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ For sets that contained fewer than 200 items, we only generated trials for seed sizes smaller than the set size.

Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ Figure 2 also illustrates that throughout the full precision/recall curve, Web100 significantly outperforms Web020, which in turn significantly outperforms Web004.
Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ Intuitively, some seeds are better than others.
Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ The pairwise similarity between 500 million terms was computed in 50 hours using 200 quad-core nodes.

To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ Given the set of seeds S = {Volvic, San Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our task is to find all other members of this class, such as {Agua Vida, Apenta, Culligan, Dasani, Ethos Water, Iceland Pure Spring Water, Imsdal, ...} Our goal is not to propose a new set expansion algorithm, but instead to test the effect of using our Web-scale term similarity matrix (enabled by the algorithm proposed in Section 3) on a state-of-theart distributional set expansion algorithm, namely (Sarmento et al. 2007).
To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ Unsupervised approaches rely on no labeled data and use either bootstrapped class-specific extraction patterns (Etzioni et al. 2005) to find new elements of a given class (for targeted extraction) or corpusbased term similarity (Pantel and Lin 2002) to find term clusters (in an open extraction framework).
To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ For each corpus, R-precision increased sharply from seed size 1 to 10 and the curve flattened out for seed sizes larger than 20 (figure omitted for lack of space).

Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ The actual inputs are read by the tasks Input: Two matrices A and B of feature vectors.
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ Why are the precision scores so low?
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ We evaluated the impact of the large similarity matrix on a set expansion task and found that the Web similarity matrix gave a large performance boost over a state-of-the-art expansion algorithm using Wikipedia.
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ Distinguishing between the various data series is not important, however important to notice is the very large gap between the precision/recall curves of the best and worst performing random seed sets.

Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ The Reduce step is used to group the output by bi.
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ Semi-supervised approaches are predominantly adopted since they allow targeted expansions while requiring only small sets of seed entities.
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ The resulting similarity matrix between 500 million terms is applied to the task of expanding lists of named entities (automatic set expansion).
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.

Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ In our work, we limited the total number of system expansions, per trial, to 1000.
Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ We evaluated the impact of the large similarity matrix on a set expansion task and found that the Web similarity matrix gave a large performance boost over a state-of-the-art expansion algorithm using Wikipedia.

As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ In our reported experiments, we expanded over 22,000 seed sets using our Web similarity model from Section 3.
As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ We proposed a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed over a 200 billion word crawl of the Web.
As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ Starting with a set of seed entities, semisupervised extraction methods use either classspecific patterns to populate an entity class or distributional similarity to find terms similar to the seed set (Paşca’s work also examines the advantages of combining these approaches).

According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ Upon inspection of a random sample of the “List of” pages, we found that several lists were compositions or joins of concepts, for example “List of World War II aces from Denmark” and “List of people who claimed to be God”.
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ For sets that contained fewer than 200 items, we only generated trials for seed sizes smaller than the set size.
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ Although this does not generate a perfect random sample, diversity is ensured by the random selection of nouns and relevancy is ensured by the author adjudication.

We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ The pairwise similarity between 500 million terms was computed in 50 hours using 200 quad-core nodes.
We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ Atterer and Schutze (2006) examine the influence of corpus size on combining a supervised approach with an unsupervised one for relative clause and PP-attachment.
We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ Creating lists of named entities is a critical problem at commercial engines such as Yahoo! and Google.
We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ Parallelization and optimizations are necessary.

Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Semi-supervised approaches are predominantly adopted since they allow targeted expansions while requiring only small sets of seed entities.
Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Figure 1 outlines our algorithm for computing the similarity between all elements of A and B.
Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ (2004) show the advantages of using large quantities of generic Web text over smaller corpora for extracting relations and named entities.

Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ Upon inspection of a random sample of the “List of” pages, we found that several lists were compositions or joins of concepts, for example “List of World War II aces from Denmark” and “List of people who claimed to be God”.

To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ For example, given the seed elements {Volvic, San Pellegrino, Gerolsteiner Brunnen, Bling H2O}, the resulting centroid consists of (details of the feature extraction protocol are in Section 6.1): brand, mineral water, monitor, lake, water, take over, ... Centroids are represented in the same space as terms allowing us to compute the similarity between centroids and all terms in our corpus.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ If a noun belonged to multiple lists, the authors chose the list that seemed most appropriate.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ Our task is to compute the similarity between all vectors in A and all vectors in B.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ We proposed a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed over a 200 billion word crawl of the Web.

The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size.
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ State-of-the-art techniques first compute term-term similarities for all available terms and then select candidates for set expansion from amongst the terms most similar to the seeds (Sarmento et al. 2007).
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.

Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ Among others, Etzioni et al. (2005) shows that a small pattern set can help bootstrap useful entity seed sets and reports on the impact of seed set noise on final performance.
Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ The gold standard is available for download at: http://www.patrickpantel.com/cgi-bin/Web/Tools/getfile.pl?type=data&id=ssegold/wikipedia.20071218.goldsets.tgz The 50 sets consist on average of 208 instances (with a minimum of 11 and a maximum of 1,116) for a total of 10,377 instances.
Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ Given the set of seeds S = {Volvic, San Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our task is to find all other members of this class, such as {Agua Vida, Apenta, Culligan, Dasani, Ethos Water, Iceland Pure Spring Water, Imsdal, ...} Our goal is not to propose a new set expansion algorithm, but instead to test the effect of using our Web-scale term similarity matrix (enabled by the algorithm proposed in Section 3) on a state-of-theart distributional set expansion algorithm, namely (Sarmento et al. 2007).

CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). $$$$$ Table 3 lists the resulting Rprecision along with the system precisions at ranks 25, 50, and 100 (see Figure 2 for detailed precision analysis).
CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). $$$$$ We propose a highly scalable implementation based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.
CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). $$$$$ For a given system, each of the 20,220 trials described in the previous section are expanded.
CL-Web: A state-of-the-art open domain method based on features extracted from the Web documents data set (Pantel et al, 2009). $$$$$ Finally, we release to the community a testbed for experimentally analyzing automatic set expansion, which includes a large collection of nearly random entity sets extracted from Wikipedia and over 22,000 randomly sampled seed expansion trials.

As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ Finally, we release to the community a testbed for experimentally analyzing automatic set expansion, which includes a large collection of nearly random entity sets extracted from Wikipedia and over 22,000 randomly sampled seed expansion trials.
As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size.
As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ In this paper, we adopt the following methodology for computing term similarity.

Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ Narrowing the focus to closely related work, Paşca (2007a; 2007b) and Paşca and Durme (2008) show the impact of varying the number of instances representative of a given class and the size of the attribute seed set on the precision of class attribute extraction.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ The Reduce step is used to group the output by bi.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ We addressed this issue by constructing a quasi-random sample as follows.

This combination was also used in (Pantel et al, 2009). $$$$$ We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
This combination was also used in (Pantel et al, 2009). $$$$$ We provide a detailed empirical analysis of the discovered named entities and quantify the effect on expansion accuracy of corpus size, corpus quality, seed composition, and seed set size.
This combination was also used in (Pantel et al, 2009). $$$$$ The pairwise similarity between 500 million terms was computed in 50 hours using 200 quad-core nodes.
This combination was also used in (Pantel et al, 2009). $$$$$ Computing the semantic similarity between terms has many applications in NLP including word classification (Turney and Littman 2003), word sense disambiguation (Yuret and Yatbaz 2009), contextspelling correction (Jones and Martin 1997), fact extraction (Paşca et al. 2006), semantic role labeling (Erk 2007), and applications in IR such as query expansion (Cao et al.
