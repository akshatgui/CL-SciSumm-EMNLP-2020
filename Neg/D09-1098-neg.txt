Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ Other currently unscalable smoothing techniques include Probabilistic Latent Semantic Analysis (Hofmann 1999), Iterative Scaling (Ando 2000), and Latent Dirichlet Allocation (Blei et al. 2003).
Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ 2007; Paşca et al. 2006; Paşca 2007a; Paşca 2007b; Paşca and Durme 2008).
Full algorithmic details are presented in (Pantel et al, 2009). $$$$$ In this case, calculating f1(x, y) is only required when both feature vectors have a shared non-zero feature, significantly reducing the cost of computation.

 $$$$$ We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
 $$$$$ Efficient computation of the similarity matrix can be achieved by leveraging the fact that is determined solely by the features shared by and (i.e., = = 0 for any x) an d that most of the feature vectors are very sparse (i.e., most possible contexts never occur for a given term).
 $$$$$ The distributional hypothesis (Harris 1954), which links the meaning of words to their contexts, has inspired many algorithms for computing term similarities (Lund and Burgess 1996; Lin 1998; Lee 1999; Erk and Padó 2008; Agirre et al. 2009).

KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ The pseudo-code in Figure 1 assumes that A can fit into memory, which for large A may be impossible.
KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ State-of-the-art techniques first compute term-term similarities for all available terms and then select candidates for set expansion from amongst the terms most similar to the seeds (Sarmento et al. 2007).
KE dis alone, a state-of-the-art distributional system implementing (Pantel et al, 2009), where the Ranker assigns scores to instances using the similarity score returned by KE dis alone. $$$$$ Semi-supervised approaches are predominantly adopted since they allow targeted expansions while requiring only small sets of seed entities.

Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ Term similarities are computed by comparing these pmi context vectors using measures such as cosine, Jaccard, and Dice.
Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ We evaluated the impact of the large similarity matrix on a set expansion task and found that the Web similarity matrix gave a large performance boost over a state-of-the-art expansion algorithm using Wikipedia.
Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al,2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc? on et al, 1999), and lexical inference (Kotlerman et al, 2012). $$$$$ The Reduce step is used to group the output by bi.

To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ Methods differ in their definition of a context (e.g., text window or syntactic relations), or by a means to weigh contexts (e.g., frequency, tf-idf, pointwise mutual information), or ultimately in measuring the similarity between two context vectors (e.g., using Euclidean distance, Cosine, Dice).
To score relevant words not appearing in the database (due to incompleteness of the database or lexical variations), GUSP uses DASH (Pantel et al, 2009) to provide additional word-pair scoring based on lexical distributional similarity computed over general text corpora (Wikipedia in this case). $$$$$ The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes.

Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ We found that a) very small seed sets of size 1 or 2 are not sufficient for representing the intended entity set; b) 520 seeds yield on average best performance; and c) surprisingly, increasing the seed set size beyond 20 or 30 on average does not find any new correct instances.
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ Set expansion systems consist of an expansion algorithm (such as the one described in Section 4.1) as well as a corpus (such as Wikipedia, a news corpus, or a web crawl).
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ Creating lists of named entities is a critical problem at commercial engines such as Yahoo! and Google.
Since case information is important for parsers and taggers, we first true cased the sentences using DASH (Pantel et al, 2009), which stores the case for each phrase in Wikipedia. $$$$$ Scaling this task to the Web requires parallelization and optimizations.

Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ Some entities could never be found by the distributional similarity model since they either do not occur or infrequently occur in the corpus or they occur in contexts that vary a great deal from other set elements.
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching.
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ State-of-the-art techniques first compute term-term similarities for all available terms and then select candidates for set expansion from amongst the terms most similar to the seeds (Sarmento et al. 2007).
Pantel et al (2009) discusses the issue of seed set size in detail, concluding that 5-20 seed words are often required for good performance. $$$$$ We evaluated the impact of the large similarity matrix on a set expansion task and found that the Web similarity matrix gave a large performance boost over a state-of-the-art expansion algorithm using Wikipedia.

Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ An example observation is that good quality class attributes can still be extracted using 20 or even 10 instances to represent an entity class.
Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ Inspection of the precision vs. rank graph (omitted for lack of space) revealed that from rank 1 thru 550, Wikipedia had the same precision as Web020.
Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ Building entity lexicons is a task of great interest for which structured, semi-structured and unstructured data have all been explored (GoogleSets; Sarmento et al. 2007; Wang and Cohen 2007; Bunescu and Mooney 2004; Etzioni et al.
Recently, (Vyaset al, 2009) proposed an automatic system for improving the seeds generated by editors (Pantel et al, 2009). $$$$$ Then, each expanded instance is judged as correct or incorrect automatically against the gold standard described in Section 5.1.

As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.
As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ Figure 3 illustrates the precision vs. recall tradeoff on our best performing corpus Web100 for 30 random seed sets of size 10 for each of our 50 gold standard sets (i.e., 1500 trials were tested.)
As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ On our datasets, we observed near linear running time in the corpus size.
As mentioned above, (Etzioni et al, 2005) report that seed set composition affects the correctness of the harvested instances, and (Pantel et al, 2009) observe an increment of 42% precision and 39% recall between the best and worst performing seed sets for the task of entity set expansion. $$$$$ We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.

According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ The resulting similarity matrix between 500 million terms is applied to the task of expanding lists of named entities (automatic set expansion).
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ We make public an experimental testbed for set expansion analysis that includes a large collection of diverse entity sets extracted from Wikipedia.
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.
According to (Pantel et al, 2009) 10 to 20 seeds are a sufficient starting set in a distributional similarity model to discover as many new correct instances as may ever be found. $$$$$ In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.

We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ In this paper, we propose a large-scale term similarity algorithm, based on distributional similarity, implemented in the MapReduce framework and deployed over a 200 billion word crawl of the Web.
We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ The resulting similarity matrix between 500 million terms is applied to the task of expanding lists of named entities (automatic set expansion).
We computed the distributional similarity between arguments using (Pantel et al, 2009) over a large crawl of the Web (described in Section 4.1). $$$$$ We proposed a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed over a 200 billion word crawl of the Web.

Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ The pairwise similarity between 500 million terms was computed in 50 hours using 200 quad-core nodes.
Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Banko and Brill (2001) show that Web-scale data helps with confusion set disambiguation while Lapata and Keller (2005) find that the Web is a good source of n-gram counts for unsupervised models.
Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Our optimization strategy follows a generalized sparse-matrix multiplication approach (Sarawagi and Kirpal 2004), which is based on the wellknown observation that a scalar product of two vectors depends only on the coordinates for which both vectors have non-zero values.
Our last feature is the distributional similarity scores of Pantel et al (2009), as trained over Wikipedia. $$$$$ Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks.

Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ For our largest corpus, Web100, we computed the pairwise similarity between over 500 million words in 50 hours using 200 four-core machines.
Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ On just the countries class, our R-Precision was 0.816 using Web100.
Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ Our goal is to study the performance gains on set expansion using our Web-scale term similarity algorithm from Section 3.
Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al (2009). $$$$$ Hadoop has been shown to scale to several thousands of machines, allowing users to write simple “map” and “reduce” code, and to seamlessly manage the sophisticated parallel execution of the code.

To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ On average they only find 73% 5 To avoid biasing our Wikipedia corpus with the test sets, Wikipedia “List of” pages were omitted from our statistics as were any page linked to gold standard list members from “List of” pages. of the top-1000 similar terms of a random term whereas we find all of them.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ Computing the pairwise semantic similarity between all words on the Web is a computationally challenging task.
To obtain examples of multiple semantic categories, we utilized selected Wikipedia listOf pages from (Pantel et al, 2009) and augmented these with our own manually defined categories, such that each list contained at least ten distinct examples occurring in our corpus. $$$$$ Given the set of seeds S = {Volvic, San Pellegrino, Gerolsteiner Brunnen, Bling H2O}, our task is to find all other members of this class, such as {Agua Vida, Apenta, Culligan, Dasani, Ethos Water, Iceland Pure Spring Water, Imsdal, ...} Our goal is not to propose a new set expansion algorithm, but instead to test the effect of using our Web-scale term similarity matrix (enabled by the algorithm proposed in Section 3) on a state-of-theart distributional set expansion algorithm, namely (Sarmento et al. 2007).

The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ We consider S as a set of prototypical examples of the underlying entity set.
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available.
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ More formally, commonly used similarity scores F(x�, y) can be expressed as: and f3 for some common similarity functions.
The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al, 2009). $$$$$ For sets that contained fewer than 200 items, we only generated trials for seed sizes smaller than the set size.

Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ Section 2.1 introduces several optimization techniques; below we propose an algorithm for large-scale term similarity computation which calculates exact scores for all pairs of terms, generalizes to several different metrics, and is scalable to a large crawl of the Web.
Some prior studies use every word in a document/sentence as the features, such as the distributional approaches (Pantel et al, 2009). $$$$$ Figure 2 illustrates the precision and recall tradeoff for our four corpora, with 95% confidence intervals computed over all 20,220 trials described in Section 4.2.

CL-Web $$$$$ The resulting similarity matrix between 500 million terms is applied to the task of expanding lists of named entities (automatic set expansion).
CL-Web $$$$$ On our Web crawl corpora, we observe that the full 200+ billion token crawl (Web100) has an average Rprecision 13% higher than 1/5th of the crawl (Web020) and 53% higher than 1/25th of the crawl.
CL-Web $$$$$ Previous work has examined the effect of using large, sometimes Web-size corpora, on system performance in the case of familiar NLP tasks.
CL-Web $$$$$ For sets that contained fewer than 200 items, we only generated trials for seed sizes smaller than the set size.

As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ For commercial engines such as Yahoo! and Google, creating lists of named entities found on the Web is critical for query analysis, document categorization, and ad matching.
As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ We found that a) very small seed sets of size 1 or 2 are not sufficient for representing the intended entity set; b) 520 seeds yield on average best performance; and c) surprisingly, increasing the seed set size beyond 20 or 30 on average does not find any new correct instances.
As (Pantel et al, 2009) show, picking seeds that yield high numbers of different terms is difficult. $$$$$ We apply the learned similarity matrix to the task of automatic set expansion and present a large empirical study to quantify the effect on expansion performance of corpus size, corpus quality, seed composition and seed size.

Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ On just the countries class, our R-Precision was 0.816 using Web100.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ For sets that contained fewer than 200 items, we only generated trials for seed sizes smaller than the set size.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ A representation for the meaning of S is computed by building a feature vector consisting of a weighted average of the features of its seed elements s1, s2, ..., sk, a centroid.
Given the seeds set S, a seeds centroid vector is produced using the surrounding word contexts (see below) of all occurrences of all the seeds in the corpus (Pantel et al 2009). $$$$$ A representation for the meaning of S is computed by building a feature vector consisting of a weighted average of the features of its seed elements s1, s2, ..., sk, a centroid.

This combination was also used in (Pantel et al, 2009). $$$$$ Although this does not generate a perfect random sample, diversity is ensured by the random selection of nouns and relevancy is ensured by the author adjudication.
This combination was also used in (Pantel et al, 2009). $$$$$ The pairwise similarity between 500 million terms is computed in 50 hours using 200 quad-core nodes.
This combination was also used in (Pantel et al, 2009). $$$$$ Semisupervised methods (including ours) are useful for extending finer grain entity classes, for which large unlabeled data sets are available.
This combination was also used in (Pantel et al, 2009). $$$$$ Semi-supervised approaches are predominantly adopted since they allow targeted expansions while requiring only small sets of seed entities.
