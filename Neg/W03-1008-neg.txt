The Gildea and Hockenmaier (2003) system uses features extracted from Combinatory Categorial Grammar (CCG) corresponding to the features that were used by G&J and G&P systems. $$$$$ Our system predicts all the roles, including core arguments as well as the ArgM labels and their function tags.
The Gildea and Hockenmaier (2003) system uses features extracted from Combinatory Categorial Grammar (CCG) corresponding to the features that were used by G&J and G&P systems. $$$$$ This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.

This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. $$$$$ Our results also indicate the importance of recovering long-range dependencies, either through the trace information in the Penn Treebank, or directly, as in the predicate-argument structures returned by the CCG parser.
This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. $$$$$ Correctly identifying the semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.
This analysis allows recovery of verbal arguments of nominalised raising and control verbs, a construction which both Gildea and Hockenmaier (2003) and Boxwell and White (2008) identify as a problem case when aligning Propbank and CCGbank. $$$$$ However, the performance of our CCG system is lowered by the fact that the syntactic analyses in its training corpus differ from those that underlie PropBank in important ways (in particular in the notion of heads and the complement-adjunct distinction).

Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003). $$$$$ Combinatory Categorial Grammar (CCG) (Steedman, 2000), is a grammatical theory which provides a completely transparent interface between surface syntax and underlying semantics, such that each syntactic derivation corresponds directly to an interpretable semantic representation which includes long-range dependencies that arise through control, raising, coordination and extraction.
Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003). $$$$$ The CCG parser returns the local and long-range word-word dependencies that express the predicateargument structure corresponding to the derivation.
Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003). $$$$$ The voice feature can be read off the CCG categories, since the CCG categories of past participles carry different features in active and passive voice (eg. sold can be (S[pt]\NP)/NP or S[pss]\NP).
Conventionally there are two kinds of methods for role assignments, one is using only statistical information (Gildea and Jurafsky, 2002) and the other is combining with grammar rules (Gildea and Hockenmaier, 2003). $$$$$ This feature is highly correlated with grammatical function, since subjects will generally appear before a verb, and objects after.

 $$$$$ Long-range dependencies are especially important for core arguments, shown by the fact that removing trace information from the Treebank parses results in a bigger drop for core arguments (83.5 to 76.3 F-score) than for all roles (74.1 to 70.2).
 $$$$$ Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999).
 $$$$$ That is, there may be no constituent in the CCG derivation corresponding to the same sequence of words as a particular constituent in the Treebank tree.
 $$$$$ Long-range dependencies can be projected through certain types of lexical categories or through rules such as coordination of functor categories.

Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. $$$$$ Our CCG-based system for automatically labeling verb arguments with PropBank-style semantic roles outperforms a system using a traditional Treebankbased parser for core arguments, which comprise 75% of the role labels, but scores lower on adjunctlike roles such as temporals and locatives.
Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. $$$$$ Roles for each verb are simply numbered Arg0, Arg1, Arg2, etc.
Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. $$$$$ In the CCG version, we replace the features above with corresponding features based on both the sentence’s CCG derivation tree (shown in Figure 1) and the CCG predicate-argument relations extracted from it (shown in Table 1).

However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. $$$$$ As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation.
However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. $$$$$ That is, there may be no constituent in the CCG derivation corresponding to the same sequence of words as a particular constituent in the Treebank tree.
However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. $$$$$ Combinatory Categorial Grammar (CCG) (Steedman, 2000), is a grammatical theory which provides a completely transparent interface between surface syntax and underlying semantics, such that each syntactic derivation corresponds directly to an interpretable semantic representation which includes long-range dependencies that arise through control, raising, coordination and extraction.
However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. $$$$$ The CCG parser returns predicate-argument structures that include long-range dependencies; therefore, it seems inherently better suited for this task.

In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b). $$$$$ (S[dcl]\NP1)/NP2, or S/S1, and indicate the wordword dependencies in the predicate-argument structure as tuples (wh, ch, i, wa), where ch is the lexical category of the head word wh, and wa is the head word of the constituent that fills the ith argument of ch.
In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b). $$$$$ We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b). $$$$$ For each role label for a verb’s argument in PropBank, we first find the head word for its constituent according to the the head rules of (Collins, 1999).
In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al, 2005b). $$$$$ We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.

For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. $$$$$ We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. $$$$$ The PropBank (Kingsbury and Palmer, 2002) and the FrameNet (Baker et al., 1998) projects both document the variation in syntactic realization of the arguments of predicates in general English text.
For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. $$$$$ In order to do this, we wish to correlate the CCG relations above with PropBank arguments.

As with a previous approach in CCG semantic role labeling (Gildea and Hockenmaier, 2003), this feature shows the exact nature of the syntactic dependency between the predicate and the word we are considering, if any such dependency exists. $$$$$ Our CCG-based system for automatically labeling verb arguments with PropBank-style semantic roles outperforms a system using a traditional Treebankbased parser for core arguments, which comprise 75% of the role labels, but scores lower on adjunctlike roles such as temporals and locatives.
As with a previous approach in CCG semantic role labeling (Gildea and Hockenmaier, 2003), this feature shows the exact nature of the syntactic dependency between the predicate and the word we are considering, if any such dependency exists. $$$$$ For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words.
As with a previous approach in CCG semantic role labeling (Gildea and Hockenmaier, 2003), this feature shows the exact nature of the syntactic dependency between the predicate and the word we are considering, if any such dependency exists. $$$$$ In particular, we assume that sets of roles appear independent of their linear order, and that the features F of a constituents are independent of other constituents’ features given the constituent’s role.
As with a previous approach in CCG semantic role labeling (Gildea and Hockenmaier, 2003), this feature shows the exact nature of the syntactic dependency between the predicate and the word we are considering, if any such dependency exists. $$$$$ Since the Collins parser does not provide trace information, its upper bound is given by the system tested on the gold-standard Treebank representation with traces removed.

We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank. $$$$$ (S[dcl]\NP1)/NP2, or S/S1, and indicate the wordword dependencies in the predicate-argument structure as tuples (wh, ch, i, wa), where ch is the lexical category of the head word wh, and wa is the head word of the constituent that fills the ith argument of ch.
We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank. $$$$$ While the CCGbank is derived from the Penn Treebank, in many cases the constituent structures do not correspond.
We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank. $$$$$ For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words.
We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003) , which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Tree bank. $$$$$ The CCG parser returns predicate-argument structures that include long-range dependencies; therefore, it seems inherently better suited for this task.

As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0,..., A5). $$$$$ In particular, for PPs, it depends on the “closely-related” (CLR) function tag, which is known to be unreliable.
As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0,..., A5). $$$$$ This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.
As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0,..., A5). $$$$$ The dataset contains annotations for 72,109 predicate-argument structures with 190,815 individual arguments (of which 75% are core, or numbered, arguments) and has includes examples from 2462 lexical predicates (types).
As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0,..., A5). $$$$$ Our results also have implications for parser evaluation, as the most frequently used constituent-based precision and recall measures do not evaluate how well long-range dependencies can be recovered from the output of a parser.

In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). $$$$$ In order to do this, we wish to correlate the CCG relations above with PropBank arguments.
In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). $$$$$ Correctly identifying the semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.
In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). $$$$$ For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words.
In contrast to the approach in (Punyakanok et al, 2008), which tags constituents directly, we tag headwords and then associate them with a constituent, as in a previous CCG-based approach (Gildea and Hockenmaier, 2003). $$$$$ Therefore, CCG assumes that not modifies might, whereas PropBank assumes it modifies gone.

This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for tree path-based features. $$$$$ For each verb appearing in the corpus, a set of semantic roles is defined.
This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for tree path-based features. $$$$$ The position feature simply indicates whether the constituent to be labeled occurs before or after the predicate.
This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for tree path-based features. $$$$$ As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation.
This feature has been shown (Gildea and Hockenmaier, 2003) to be an effective substitute for tree path-based features. $$$$$ This system performs at least as well as a system based on a traditional Treebank parser, and outperforms it on core argument roles.

 $$$$$ For the reasons described, the head words of the constituents that have PropBank roles are not necessarily the head words that stand in a predicate-argument relation in CCGbank.
 $$$$$ For this reason, we compute the correspondence between the CCG derivation and the PropBank labels at the level of head words.
 $$$$$ We present a system for automatically identifying PropBank-style semantic roles based on the output of a statistical parser for Combinatory Categorial Grammar.
 $$$$$ The Treebankparser returns skeletal phrase-structure trees without the traces or functional tags in the original Penn Treebank, whereas the CCG parser returns wordword dependencies that correspond to the underlying predicate-argument structure, including longrange dependencies arising through control, raising, extraction and coordination.

We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. $$$$$ The CCG parser returns predicate-argument structures that include long-range dependencies; therefore, it seems inherently better suited for this task.
We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. $$$$$ Correctly identifying the semantic roles of sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.
We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. $$$$$ As the CCG parser is trained and tested on a corpus of CCG derivations that have been obtained by automatic conversion from the Penn Treebank, we are able to compare performance using both goldstandard and automatic parses for both CCG and the traditional Treebank representation.
We follow a previous CCG based approach (Gildea and Hockenmaier, 2003) in using a feature to describe the PARG relationship between the two words, if one exists. $$$$$ Figure 1 shows the derivations of an ordinary sentence, a relative clause and a right-node-raising construction.

Using a version of Brutus incorporating only the CCG-based features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&H). $$$$$ In particular, for PPs, it depends on the “closely-related” (CLR) function tag, which is known to be unreliable.
Using a version of Brutus incorporating only the CCG-based features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&H). $$$$$ In all three sentences, the predicateargument relations between London and denied and plans and denied are the same, which in CCG is expressed by the fact that London fills the first (ie. subject) argument slot of the lexical category of denied, (S[dcl]\NP1)/NP2, and plans fills the second (object) slot.
Using a version of Brutus incorporating only the CCG-based features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&H). $$$$$ We refer to PropBank’s numbered arguments as “core” arguments.
Using a version of Brutus incorporating only the CCG-based features described above, we achieve better results than a previous CCG based system (Gildea and Hockenmaier, 2003, henceforth G&H). $$$$$ Roles for each verb are simply numbered Arg0, Arg1, Arg2, etc.
