The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ We will not discuss in detail the characteristics of the model or the parameter estimation procedure used — Improved Iterative Scaling.
The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ We thank Dan Klein and Michael Saunders for useful discussions, and the anonymous reviewers for many helpful comments.

 $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.
 $$$$$ In the Baseline model, of the unknown word error 41.3% is due to words being NNP and assigned to some other category, or being of other category and assigned NNP.
 $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
 $$$$$ The work presented in this paper explored just a few information sources in addition to the ones usually used for tagging.

In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ But often the modal can be several positions away from the current position — still obvious to a human, but out of sight for the baseline model.
In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ For example, the error on the proper noun category (NNP) accounts for a significantly larger percent of the total error for unknown words than for known words.
In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ However, all these methods use largely the same information sources for tagging, and often almost the same features as well, and as a consequence they also offer very similar levels of performance.

Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ Even when the accuracy figures for corpus-based part-of-speech taggers start to look extremely similar, it is still possible to move performance levels up.
Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ For instance, in (2), we find the exact same sequence of parts of speech, but (2a) is a particle use of on, while (2b) is a prepositional use.
Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ (2) a. Kim took on the monster. b. Kim sat on the monster.

The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ We thank Dan Klein and Michael Saunders for useful discussions, and the anonymous reviewers for many helpful comments.
The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ But other cases, such as VBNNBD and VBNBP/NN, represent systematic tag ambiguity patterns in English, for which the right answer is invariably clear in context, and for which there are in general good structural contextual clues that one should be able to use to disambiguate.
The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.

We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. $$$$$ Some, such as errors between NN/NNP/NNPS/NNS largely reflect difficulties with unknown words.

We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.
We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). $$$$$ Two of the most significant sources of classifier errors are the VBN/VBD ambiguity and the VBP/VB ambiguity.
We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). $$$$$ We do not at present have a good explanation for this phenomenon.

Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. $$$$$ The next table shows the final performance on the test set.
Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. $$$$$ Consequently, the accuracy on the rarer RP (particles) category is as low as 41.5% for the Baseline model (cf.

We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ We added a similar feature for resolving VBDNBN confusions.
We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ It is important to note that (2) is composed of information already 'known' to the tagger in some sense.
We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ All of these changes led to modest increases in tagging accuracy.
We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ The incorporation of the following two feature schemas greatly improved NNP accuracy: Conversely, empirically it was found that the prefix features for rare words were having a net negative effect on accuracy.

 $$$$$ For ease of comparison, the accuracies of all models on the test and development sets are shown in Table 7.
 $$$$$ We added two different feature templates to capture this information, consisting as usual of a predicate on the history h, and a condition on the tag t. The first predicate is true if the current word is often used as a particle, and if there is a verb at most 3 positions to the left, which is &quot;known&quot; to have a good chance of taking the current word as a particle.
 $$$$$ To help resolve a VB/VBP ambiguity in such cases, we can add a feature that looks at the preceding several words (we have chosen 8 as a threshold), but not across another verb, and activates if there is a to there, a modal verb, or a form of do, let, make, or help (verbs that frequently take a bare infinitive complement).
 $$$$$ The potential of maximum entropy methods has not previously been fully exploited for the task of assignment of parts of speech.

So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. $$$$$ This stands in contrast to the (manually-built) EngCG tagger, which achieves better performance by using lexical and contextual information sources and generalizations beyond those available to such statistical taggers, as Samuelsson and Voutilainen (1997) demonstrate. '
So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. $$$$$ In following experiments, we examined ways of using additional features to improve the accuracy of tagging unknown words.

Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.
Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ We note that accuracy is lower on the development set.
Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ We tried to improve the tagger's capability to resolve these ambiguities through adding information on verbs' preferences to take specific words as particles, or adverbs, or prepositions.
Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ However, all these methods use largely the same information sources for tagging, and often almost the same features as well, and as a consequence they also offer very similar levels of performance.

For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ We added two different feature templates to capture this information, consisting as usual of a predicate on the history h, and a condition on the tag t. The first predicate is true if the current word is often used as a particle, and if there is a verb at most 3 positions to the left, which is &quot;known&quot; to have a good chance of taking the current word as a particle.
For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ Table 8 shows the different number of feature templates of each kind that have been instantiated for the different models as well as the total number of features each model has.
For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.

We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ Two of the most significant sources of classifier errors are the VBN/VBD ambiguity and the VBP/VB ambiguity.
We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.
We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ The parameters Xi correspond to weights for the features fj.
We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ In the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.

The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ The form of these two feature templates was motivated by the structural rules of English and not induced from the training data, but it should be possible to look for &quot;predictors&quot; for certain parts of speech in the preceding words in the sentence by, for example, computing association strengths.
The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ They are a subset of the features used in Ratnaparkhi (1996).
The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ As seen in Table 5, VBNNBD confusions account for 6.9% of the total word error.
The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ In following experiments, we examined ways of using additional features to improve the accuracy of tagging unknown words.

It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ For ease of comparison, the accuracies of all models on the test and development sets are shown in Table 7.
It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ (2) a. Kim took on the monster. b. Kim sat on the monster.
It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ This stands in contrast to the (manually-built) EngCG tagger, which achieves better performance by using lexical and contextual information sources and generalizations beyond those available to such statistical taggers, as Samuelsson and Voutilainen (1997) demonstrate. '
It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ Two of the most significant sources of classifier errors are the VBN/VBD ambiguity and the VBP/VB ambiguity.

For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ We pay special attention to unknown words, because the markedly lower accuracy on unknown word tagging means that this is an area where significant performance gains seem possible.
For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ It can be seen that the features which help disambiguate verb forms, which look at capitalization and the first of the feature templates for particles are a very small number as compared to the features of the other kinds.

Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ We incorporated into a maximum entropy-based tagger more linguistically sophisticated features, which are non-local and do not look just at particular positions in the text.
Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ We also added features that model the interactions of previously employed predictors.
Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ (2) a. Kim took on the monster. b. Kim sat on the monster.

Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ Increasing the beam size did not result in improved accuracy.
Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ Two of the most significant sources of classifier errors are the VBN/VBD ambiguity and the VBP/VB ambiguity.
Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ There are now numerous systems for automatic assignment of parts of speech (&quot;tagging&quot;), employing many different machine learning methods.

This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ Table 6 shows part of the Baseline model's confusion matrix for just unknown words.
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ Rather than having a separate feature look at each preceding position, we define one feature that looks at the chosen number of positions to the left.
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ However, all these methods use largely the same information sources for tagging, and often almost the same features as well, and as a consequence they also offer very similar levels of performance.
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ The number of VBD/VBN confusions was reduced by 12.3%.
