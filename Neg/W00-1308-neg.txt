The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ One conclusion that we can draw is that at present the additional word features used in Ratnaparkhi (1996) — looking at words more than one position away from the current — do not appear to be helping the overall performance of the models.
The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ In the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.
The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ The number of VBD/VBN confusions was reduced by 12.3%.
The primary reason for the large disparity between the Brill tagger output and original Penn Treebank annotation is that it is notoriously difficult to differentiate between particles, prepositions and adverbs (Toutanova and Manning, 2000). $$$$$ We tried to improve the tagger's capability to resolve these ambiguities through adding information on verbs' preferences to take specific words as particles, or adverbs, or prepositions.

 $$$$$ We tried to improve the tagger's capability to resolve these ambiguities through adding information on verbs' preferences to take specific words as particles, or adverbs, or prepositions.
 $$$$$ Feature Type Template General feature templates can be instantiated by arbitrary contexts, whereas rare feature templates are instantiated only by histories where the current word wi is rare.
 $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.
 $$$$$ 10).

In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ This paper explores the notion that automatically built tagger performance can be further improved by expanding the knowledge sources available to the tagger.
In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ The results on the test set after adding these features are shown below: 96.76% 86.76% Unknown word error is reduced by 15% as compared to the Baseline model.
In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ In many cases it is easy for people (and for taggers) to determine the correct form.
In order to make a fair comparison between the human texts and our own, we used a part-of-speech (POS) tagger (Toutanova and Manning, 2000) to extract those grammatical categories that we aim to control within our framework, i.e. nouns, verbs, prepositions, adjectives and adverbs. $$$$$ In particular, we get improved results by incorporating these features: (i) more extensive treatment of capitalization for unknown words; (ii) features for the disambiguation of the tense forms of verbs; (iii) features for disambiguating particles from prepositions and adverbs.

Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.
Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ The number of VBD/VBN confusions was reduced by 12.3%.
Toutanova and Manning (2000), Toutanova et al (2003), Lafferty et al (2001) and Vadas and Curran (2005) used additional language-specific morphological or syntactic features. $$$$$ The percentage of the same type of error for known words is 16.2%.

The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ The accuracy on the RP category rose to 44.3%.
The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ In following experiments, we examined ways of using additional features to improve the accuracy of tagging unknown words.
The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning (2000). $$$$$ These features were some help in reducing the RB/IN/RP confusions.

We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. $$$$$ The accuracy on the RP category rose to 44.3%.
We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. $$$$$ In the future we hope to explore automatically discovering information sources that can be profitably incorporated into maximum entropy part-of-speech prediction.
We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. $$$$$ This stands in contrast to the (manually-built) EngCG tagger, which achieves better performance by using lexical and contextual information sources and generalizations beyond those available to such statistical taggers, as Samuelsson and Voutilainen (1997) demonstrate. '
We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. $$$$$ The data set sizes are shown below together with numbers of unknown words.

We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). $$$$$ This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.
We use the Google Web 1T data (Brants and Franz (2006)), and POS-tagged ngrams using Stanford POS Tagger (Toutanova and Manning (2000)). $$$$$ But often the modal can be several positions away from the current position — still obvious to a human, but out of sight for the baseline model.

Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. $$$$$ Even when the accuracy figures for corpus-based part-of-speech taggers start to look extremely similar, it is still possible to move performance levels up.
Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. $$$$$ The addition of the features (1) and (2) and the removal of the prefix features considerably improved the accuracy on unknown words and the overall accuracy.
Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. $$$$$ This feature can be viewed as the conjunction of two features, one of which is already in the baseline model, and the other of which is the negation of a feature existing in the baseline model — since for words at the beginning of a sentence, the preceding tag is the pseudo-tag NA, and there is a feature looking at the preceding tag.
Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al, 1992) to tag utterance words for part of speech. $$$$$ Namely, for regular verbs the -ed form can be either a VBD or a VBN and similarly the stem form can be either a VBP or VB.

We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ In the Baseline model, of the unknown word error 41.3% is due to words being NNP and assigned to some other category, or being of other category and assigned NNP.
We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ The addition of the two feature schemas helped reduce the VBNBP and VBD/VBN confusions.
We apply the Stanford POS tagger (Toutanova and Manning, 2000) on Twitter messages, and only select nouns and adjectives as valid candidates for user tags. $$$$$ For example, the accuracy on nouns is greater than the accuracy on adjectives.

 $$$$$ The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words.
 $$$$$ As seen in Table 5, VBNNBD confusions account for 6.9% of the total word error.
 $$$$$ We do not at present have a good explanation for this phenomenon.
 $$$$$ We do not at present have a good explanation for this phenomenon.

So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. $$$$$ 10).
So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. $$$$$ The incorporation of the following two feature schemas greatly improved NNP accuracy: Conversely, empirically it was found that the prefix features for rare words were having a net negative effect on accuracy.
So we have also used the Stanford POS tagger (Toutanova and Manning, 2000) to tag these transcripts before calculating the Discriminative TFIDF score. $$$$$ For ease of comparison, the accuracies of all models on the test and development sets are shown in Table 7.

Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ Even though our maximum entropy model does not require independence among the predictors, it provides for free only a simple combination of feature weights, and additional 'interaction terms' are needed to model non-additive interactions (in log-space terms) between features.
Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ This paper presents results for a maximumentropy-based part of speech tagger, which achieves superior performance principally by enriching the information sources used for tagging.
Next, we replace all nouns with their POS tag; we use the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ As seen in Table 1 the features are conjunctions of a boolean function on the history h and a boolean function on the tag t. Features whose first conjuncts are true for more than the corresponding threshold number of histories in the training data are included in the model.

For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ We note that accuracy is lower on the development set.
For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ These features were some help in reducing the RB/IN/RP confusions.
For POS-tagging, we used the Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ We used a tag dictionary for known words in testing.

We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ The statistics are expressed as the expected values of appropriate functions defined on the contexts h and tags t. In particular, the constraints demand that the expectations of the features for the model match the empirical expectations of the features over the training data.
We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ The verb-particle pairs that are known by the system to be very common were collected through analysis of the training data in a preprocessing stage.
We consider a word as a predicate if it is tagged as a verb by a Part-of-Speech tagger (Toutanova and Manning, 2000). $$$$$ While progress is slow, because each new feature applies only to a limited range of cases, nevertheless the improvement in accuracy as compared to previous results is noticeable, particularly for the individual decisions on which we focused.

The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ There are now numerous systems for automatic assignment of parts of speech (&quot;tagging&quot;), employing many different machine learning methods.
The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ The work presented in this paper explored just a few information sources in addition to the ones usually used for tagging.
The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ A maximum entropy approach has been applied to partof-speech tagging before (Ratnaparkhi 1996), but the approach's ability to incorporate nonlocal and non-HMM-tagger-type evidence has not been fully explored.
The Stanford POS tagger (Toutanova and Manning, 2000) and the Stanford parser (Klein and Manning, 2003) were used to produce the part of speech and dependency annotations. $$$$$ The incorporation of the following two feature schemas greatly improved NNP accuracy: Conversely, empirically it was found that the prefix features for rare words were having a net negative effect on accuracy.

It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.
It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ We added a similar feature for resolving VBDNBN confusions.
It uses a maximum-entropy approach to handle information diversity without assuming predictor independence (Toutanova and Manning, 2000). $$$$$ The improvement in classification accuracy therefore comes at the price of adding very few parameters to the maximum entropy model and does not result in increased model complexity.

For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ The accuracy of the baseline model is markedly lower for unknown words than for previously seen ones.
For part-of-speech (POS) tagging of the sentences, we used Stanford POS Tagger (Toutanova and Manning, 2000). $$$$$ No.

Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ The accuracy on the RP category rose to 44.3%.
Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ This paper has thus presented some initial experiments in improving tagger accuracy through using additional information sources.
Following Toutanova and Manning (2000) approximately, more information is defined for words that are considered rare (which we define here as words that occur fewer than fifteen times). $$$$$ For example, the error on the proper noun category (NNP) accounts for a significantly larger percent of the total error for unknown words than for known words.

Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ This paper describes the models that we developed and the experiments we performed to evaluate them.
Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ The number of VBNBP confusions was reduced by 23.1% as compared to the baseline.
Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ The best resulting accuracy for the tagger on the Penn Treebank is 96.86% overall, and 86.91% on previously unseen words.
Toutanova and Manning (2000) achieves 96.9% (on seen) and 86.9% (on unseen) with an MEMM. $$$$$ As previously discussed in Mikheev (1999), it is possible to improve the accuracy on capitalized words that might be proper nouns or the first word in a sentence, etc.

This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ Two of the most significant sources of classifier errors are the VBN/VBD ambiguity and the VBP/VB ambiguity.
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ In many cases it is easy for people (and for taggers) to determine the correct form.
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ (2) a. Kim took on the monster. b. Kim sat on the monster.
This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous log linear model in Toutanova and Manning (2000). $$$$$ While progress is slow, because each new feature applies only to a limited range of cases, nevertheless the improvement in accuracy as compared to previous results is noticeable, particularly for the individual decisions on which we focused.
