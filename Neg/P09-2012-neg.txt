 $$$$$ In contrast, the histogram of subtree sizes used in parsing with the sampled grammar matches the shape of the histogram from the grammar itself.
 $$$$$ We used the standard split for the Wall Street Journal portion of the Treebank, training on sections 2 to 21, and reporting results on sentences with no more than forty words from section 23.
 $$$$$ The sampled grammars outperform all of them.

 $$$$$ Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.
 $$$$$ They substantially outperform heuristically extracted grammars from previous work as well as our novel spinal grammar, and can do so with many fewer rules.
 $$$$$ In contrast, the histogram of subtree sizes used in parsing with the sampled grammar matches the shape of the histogram from the grammar itself.
 $$$$$ The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.

Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ Nearly all of the rules of the best single iteration sampled grammar (100, 0.8, 500) are lexicalized (50,820 of 60,633), and almost half of them have a height greater than one (27,328).
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn.
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ All DPs share parameters p$ and α.
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ Parsing with the “minimal subset” grammar uses highly lexicalized subtrees, but they do not improve accuracy.

A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.
A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.

 $$$$$ Past approaches have resorted to heuristics.
 $$$$$ The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.
 $$$$$ We used the standard split for the Wall Street Journal portion of the Treebank, training on sections 2 to 21, and reporting results on sentences with no more than forty words from section 23.

 $$$$$ One solution would be to manually annotate a treebank with TSG derivations, but in addition to being expensive, this task requires one to know what the grammar actually is.
 $$$$$ These problems are common in natural language processing tasks that search for a hidden segmentation.
 $$$$$ This approach is unsatisfying in some ways, however.

Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.
Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ TSG subtrees were flattened to CFG rules and reconstructed afterward, with identical mappings favoring the most probable rule.
Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work.

A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn.
A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ They substantially outperform heuristically extracted grammars from previous work as well as our novel spinal grammar, and can do so with many fewer rules.
A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ Part of the thinking motivating TSGs is to let the data determine the best set of subtrees.
A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.
