 $$$$$ TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size.
 $$$$$ For pruning, we binned nonterminals according to input span and degree of binarization, keeping the ten highest scoring items in each bin. the significantly larger “minimal subset” grammar.

 $$$$$ Instead of heuristic extraction we would prefer a model that explained the subtrees found in the grammar.
 $$$$$ The sampled grammars are model-based, are simple to specify and extract, and take the expected shape over subtree size.
 $$$$$ In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work.
 $$$$$ The inference (parsing) task for TSGs is NP-hard (Sima’an, 1996), and in practice the most probable parse is approximated (1) by sampling from the derivation forest or (2) from the top k derivations.

Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ All DPs share parameters p$ and α.
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ Under a regular CFG, each parse tree uniquely idenfifies a derivation.
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ They substantially outperform heuristically extracted grammars from previous work as well as our novel spinal grammar, and can do so with many fewer rules.
Similar models were developed independently by O'Donnell et al. (2009) and Post and Gildea (2009). $$$$$ In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size.

A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ We compare with three other grammars.
A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ Our standard CKY parser and Gibbs sampler were both written in Perl.
A more principled technique is to use a sparse nonparametric prior, as was recently presented by Cohn et al (2009) and Post and Gildea (2009). $$$$$ Collapsed Gibbs sampling with a DP prior fits nicely with the task of learning a TSG.

 $$$$$ The sampled grammars are model-based, are simple to specify and extract, and take the expected shape over subtree size.
 $$$$$ We examined sentence-level F1 scores and found that the use of larger subtrees did correlate with accuracy; however, the low overall accuracy (and the fact that there are so many of these large subtrees available in the grammar) suggests that such rules are overfit.
 $$$$$ They substantially outperform heuristically extracted grammars from previous work as well as our novel spinal grammar, and can do so with many fewer rules.
 $$$$$ Rule probabilities for all grammars were set with relative frequency.

 $$$$$ Acknowledgments This work was supported by NSF grants IIS-0546554 and ITR-0428020.
 $$$$$ We construct sampled grammars in two ways: by summing all subtree counts from the derivation states of the first i sampling iterations together with counts from the Treebank CFG rules (denoted (α, p$,≤i)), and by taking the counts only from iteration i (denoted (α, p$, i)).
 $$$$$ Gibbs sampling with a DP prior chooses smaller but more general rules.
 $$$$$ Since different TSG derivations can produce the same parse tree, learning procedures must guess the derivations, the number of which is exponential in the tree size.

Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ Past approaches have resorted to heuristics.
Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ This compels heuristic methods of subtree extraction, or maximum likelihood estimators which tend to extract large subtrees that overfit the training data.
Of these approaches, work in Bayesian learning of TSGs produces intuitive grammars in a principled way, and has demonstrated potential in language modeling tasks (Post and Gildea, 2009b; Post, 2010). $$$$$ Collapsed Gibbs sampling with a DP prior fits nicely with the task of learning a TSG.

A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ These problems are common in natural language processing tasks that search for a hidden segmentation.
A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ We construct sampled grammars in two ways: by summing all subtree counts from the derivation states of the first i sampling iterations together with counts from the Treebank CFG rules (denoted (α, p$,≤i)), and by taking the counts only from iteration i (denoted (α, p$, i)).
A Bayesian-learned tree substitution grammar (Post and Gildea, 2009a). $$$$$ Rule probabilities for all grammars were set with relative frequency.
