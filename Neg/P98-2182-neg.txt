We chose three clusters produced by a program similar to Roark and Charniak (1998) except that it is based on a generative probability model and tries to classify all nouns rather than just those in pre-selected clusters. $$$$$ Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an &quot;enhancer&quot; of existing broad-coverage resources.
We chose three clusters produced by a program similar to Roark and Charniak (1998) except that it is based on a generative probability model and tries to classify all nouns rather than just those in pre-selected clusters. $$$$$ In Riloff and Shepherd (1997), noun co-occurrence statistics were used to indicate nominal category membership, for the purpose of aiding in the construction of semantic lexicons.
We chose three clusters produced by a program similar to Roark and Charniak (1998) except that it is based on a generative probability model and tries to classify all nouns rather than just those in pre-selected clusters. $$$$$ Instead, all noun compounds with a head that is included in our final ranked list, are evaluated for inclusion in a second list.

This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). $$$$$ All compound nouns in the former constructions are represented by the head of the compound.
This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). $$$$$ We have outlined an algorithm in this paper that, as it stands, could significantly speed up the task of building a semantic lexicon.
This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). $$$$$ For the final ranking, we chose the log likelihood statistic outlined in Dunning (1993), which is based upon the co-occurrence counts of all nouns (see Dunning for details).
This problem is addressed by Riloff and Shepherd (1997), Roark and Charniak (1998) and more recently by Widdows and Dorow (2002). $$$$$ Effective lexicons must often include many domainspecific terms, so that available broad coverage resources, such as Wordnet (Miller, 1990), are inadequate.

Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used co occurrence statistics in local context to discover sibling relations. $$$$$ Why would one expect that members of the same semantic category would co-occur in discourse?
Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used co occurrence statistics in local context to discover sibling relations. $$$$$ semantic lexicons semiautomatically could be a great time saver, relative to creating them by hand.
Inspired by the conjunction and appositive structures, Riloff and Shepherd (1997), Roark and Charniak (1998) used co occurrence statistics in local context to discover sibling relations. $$$$$ All compound nouns in the former constructions are represented by the head of the compound.

Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures. $$$$$ A simple probability is a much more conservative statistic, insofar as it selects far fewer words with the potential for infection, it limits the extent of any infection that does occur, and it includes rare words.
Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures. $$$$$ If it does not fall below the cutoff, then it is kept (provided its head noun is not later omitted).
Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures. $$$$$ To this end, parsing is invaluable.
Rilo and Shepherd (Rilo and Shepherd, 1997) developed a bootstrap ping algorithm that exploits lexical co-occurrence statistics, and Roark and Charniak (Roark and Charniak, 1998) re ned this algorithm to focus more explicitly on certain syntactic structures. $$$$$ Table 1 shows the seed words that were used for some of the categories tested.

The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.
The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ The method for evaluating whether or not to include a noun compound in the second list is intended to exclude constructions such as government plane and include constructions such as fighter plane.
The main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in a particular category, such as vehicles or weapons (Riloff and Shepherd, 1997) (Roarkand Charniak, 1998). $$$$$ Another option would be to use head nouns identified in Wordnet, which, as a set, should include the most common members of the category in question.

Roark and Charniak describe a "generic algorithm" for extracting such lists of similar words using the notion of semantic similarity, as follows (Roark and Charniak, 1998). $$$$$ Because the relationship between nouns in a compound is quite different than that between nouns in the other constructions, the algorithm consists of two separate components: one to deal with conjunctions, lists, and appositives; and the other to deal with noun compounds.

Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ Semantic lexicons play an important role in many natural language processing tasks.
Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ Another option would be to use head nouns identified in Wordnet, which, as a set, should include the most common members of the category in question.
Algorithms of this type were used by Riloff and Shepherd (1997) and Roark and Charniak (1998), reporting accuracies of 17% and 35% respectively. $$$$$ For instance, either pickup truck or pickup is a legitimate vehicle, whereas cargo plane is legitimate, but cargo is not.

Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ Two head nouns co-occur in this algorithm if they meet the following four conditions: In contrast, R&S counted the closest noun to the left and the closest noun to the right of a head noun as co-occuring with it.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ To this end, parsing is invaluable.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.
Since lists are usually comprised of objects which are similar in some way, these relationships have been used to extract lists of nouns with similar properties (Riloff and Shepherd, 1997) (Roark and Charniak, 1998). $$$$$ Two head nouns co-occur in this algorithm if they meet the following four conditions: In contrast, R&S counted the closest noun to the left and the closest noun to the right of a head noun as co-occuring with it.

Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. $$$$$ Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area.
Our results are an order of magnitude better than those reported by Riloff and Shepherd (1997) and Roark and Charniak (1998), who report average accuracies of 17% and 35% respectively. $$$$$ We will also present some experimental results from two corpora, and discuss criteria for judging the quality of the output.

The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ Nouns are matched with their plurals in the corpus, and a single representation is settled upon for both, e.g. car(s).
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ Most common adjectives are dropped in our compound noun analysis, since they occur with a wide variety of heads.
The experiments in (Riloff and Shepherd, 1997) were performed on the 500,000 word MUC-4 corpus, and those of (Roark and Charniak, 1998) were performed using MUC-4 and the Wall Street Journal corpus (some 30 million words). $$$$$ We have outlined an algorithm in this paper that, as it stands, could significantly speed up the task of building a semantic lexicon.

The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable. $$$$$ The relation is stipulated to be transitive, so that all head nouns in a list co-occur with each other (e.g. in the phrase planes, trains, and automobiles all three nouns are counted as co-occuring with each other).
The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable. $$$$$ Nouns are matched with their plurals in the corpus, and a single representation is settled upon for both, e.g. car(s).
The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable. $$$$$ Nouns are matched with their plurals in the corpus, and a single representation is settled upon for both, e.g. car(s).
The high accuracy achieved thus questions the conclusion drawn by Roark and Charniak (1998) that parsing is invaluable. $$$$$ Co-Occurrence bigrams are collected for head nouns according to the notion of co-occurrence outlined above.

Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. $$$$$ Our algorithm focuses exclusively on these constructions.
Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. $$$$$ This can be seen in the slope of the graphs in figure 1.
Roark and Charniak (1998) used the co-occurrence of words as features to classify nouns. $$$$$ Other categories that we investigated were crimes, people, commercial sites, states (as in static states of affairs), and machines.

The goal of extracting semantic information from text is well-established, and has encouraged work on lexical acquisition (Roark and Charniak, 1998), information extraction (Cardie, 1997), and ontology engineering (Hahn and Schnattinger, 1998). $$$$$ For instance, either pickup truck or pickup is a legitimate vehicle, whereas cargo plane is legitimate, but cargo is not.
The goal of extracting semantic information from text is well-established, and has encouraged work on lexical acquisition (Roark and Charniak, 1998), information extraction (Cardie, 1997), and ontology engineering (Hahn and Schnattinger, 1998). $$$$$ For example, both Escort and Chinook are (among other things) types of vehicles (a car and a helicopter, respectively), but neither are cited as so in Wordnet.

This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). $$$$$ In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars.
This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). $$$$$ We determined three ways to evaluate the output of the algorithm for usefulness.
This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). $$$$$ Their figure of merit was simply the ratio of the times the noun coocurs with a noun in the seed list to the total frequency of the noun in the corpus.
This may be explained by the fact that words appearing in conjunctions are often taxonomically similar (Roark and Charniak, 1998) and that taxonomic information is particularly useful for compound interpretation, as evidenced by the success of WordNet-based methods (see Section 5). $$$$$ More generally, the relative success of the algorithm demonstrates the potential benefit of narrowing corpus input to specific kinds of constructions, despite the danger of compounding sparse data problems.

To select seed words, we used the procedure proposed by Roark and Charniak (1998), ranking all of the head nouns in the training corpus by frequency and manually selecting the first 10 nouns that unambiguously belong to each category. $$$$$ Semantic lexicons play an important role in many natural language processing tasks.

 $$$$$ Co-Occurrence bigrams are collected for head nouns according to the notion of co-occurrence outlined above.
 $$$$$ The algorithm generates many words not included in broad coverage resources, such as Wordnet, and could be thought of as a Wordnet &quot;enhancer&quot; for domain-specific applications.
 $$$$$ (In the case that two nouns have the same probability, the rightmost noun is chosen.)

For example, Hearst (Hearst, 1992) learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as NP, NP, and other NPs, and Roarkand Charniak (Roark and Charniak, 1998) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions. $$$$$ Nouns are matched with their plurals in the corpus, and a single representation is settled upon for both, e.g. car(s).
For example, Hearst (Hearst, 1992) learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as NP, NP, and other NPs, and Roarkand Charniak (Roark and Charniak, 1998) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions. $$$$$ The algorithm generates many words not included in broad coverage resources, such as Wordnet, and could be thought of as a Wordnet &quot;enhancer&quot; for domain-specific applications.
For example, Hearst (Hearst, 1992) learned hyponymy relationships by collecting words in lexico-syntactic expressions, such as NP, NP, and other NPs, and Roarkand Charniak (Roark and Charniak, 1998) generated semantically related words by applying statistical measures to syntactic contexts involving appositives, lists, and conjunctions. $$$$$ Table 2 presents these numbers for the categories vehicle and weapon.

In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet. $$$$$ Tables 2 and 5 give the relevant data for the categories that we investigated.
In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet. $$$$$ This last category was run because of the sparse data for the category weapon in the Wall Street Journal.
In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet. $$$$$ For example, if we are processing the category vehicle and the word artillery is selected as a seed word, a whole set of weapons that cooccur with artillery can now be selected in future iterations.
In previous research on semantic lexicon induction, Roark and Charniak (Roark and Charniak, 1998) showed that 3 of every 5 words learned by their system were not present in WordNet. $$$$$ As an illustration of this last condition, neither Galileo Probe nor gray plane is a valid entry, the former because it denotes an individual and the latter because it is a class of planes based upon an incidental feature (color).

Roark and Charniak (Roark and Charniak, 1998) followed up on this work by using a parser to explicitly capture these structures. $$$$$ Manually building domain-specific lexicons can be a costly, time-consuming affair.
Roark and Charniak (Roark and Charniak, 1998) followed up on this work by using a parser to explicitly capture these structures. $$$$$ Manually building domain-specific lexicons can be a costly, time-consuming affair.

Roark and Charniak (1998) applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives. $$$$$ This is a sensible approach in any case, since it provides the broadest coverage of category occurrences, from which to select additional likely category members.
Roark and Charniak (1998) applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives. $$$$$ Generically, their algorithm can be outlined as follows: Our algorithm uses roughly this same generic structure, but achieves notably superior results, by changing the specifics of: what counts as co-occurrence; which figures of merit to use for new seed word selection and final ranking; the method of initial seed word selection; and how to manage compound nouns.
Roark and Charniak (1998) applied this idea to extraction of words which belong to the same categories, utilizing syntactic relations such as conjunctions and appositives. $$$$$ We have also examined in detail the reasons why it works, and have shown it to work well for multiple corpora and multiple categories.
