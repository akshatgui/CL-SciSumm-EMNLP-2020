Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ Table 3 summarizes the performance of the various models on the phrase similarity dataset.
Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011a).
Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ The neural language model (NLM) gives best results for the recursive autoencoder (RAE), although the other two representations come close.

Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ The network is given a list of word vectors as input and a binary tree representing their syntactic structure.
Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ Even though nMSRPC may be large, the computer files storing our feature vectors do not explode in size because wdCount contains many zeros and the classifier allows a sparse notation of (non-zero) feature values.
Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ The problem of finding the right methods of vector composition cannot be pursued independent of the choice of lexical representation.

The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ The sentences considered in Section 4 are too large for this approach and all word vectors must be members of the same vector space.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010).
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Note that this is the cosine of the angle between senVeci1 and senVeci2.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder.

Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ This model is more sophisticated than the one we used in our experiments (see Table 4 and 5).
Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ We simply sum the existing word vectors, that is, vectors obtained via the respective corpus for words that are not on our stoplist: And do the same with point-wise multiplication: The multiplication model in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008).
Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ All models in Table 3 are significantly correlated with the human similarity judgments (p < 0.01).
Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.

 $$$$$ With regard to NLM, vector addition yields overall better results.
 $$$$$ Again these methods differ in terms of how they implement compositionality: addition and multiplication are commutative and associative operations and thus ignore word order and, more generally, syntactic structure.

Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues $$$$$ The simple semantic space may not take word order or sentence structure into account, but nevertheless achieves considerable semantic expressivity: it is on par with the third-order tensor without having access to as much data (3 billion words) or a syntactically parsed corpus.
Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues $$$$$ In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces.
Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues $$$$$ The problem of finding the right methods of vector composition cannot be pursued independent of the choice of lexical representation.

Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ One way of assessing how well p represents its direct children is to decode their vectors in a reconstruction layer: During training, the goal is to minimize the reconstruction errors of all input pairs at nonterminal nodes p in a given parse tree by computing the square of the Euclidean distance between the original input and its reconstruction: Socher et al. (2011a) extend the standard recursive autoencoder sketched above in two ways.
Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ Regarding the last four features, we measured the similarity between sentences the same way as we did with phrases in section 3.
Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ Rows in the table correspond to different vector representations: the simple distributional semantic space (SDS) from Mitchell and Lapata (2010), Baroni and Lenciâ€™s (2010) distributional memory tensor (DM) and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj).
