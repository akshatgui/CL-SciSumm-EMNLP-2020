Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ The model concatenates the learned embeddings of the n words and predicts a score for the n-gram sequence using the learned embeddings as features.
Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995).
Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al, 2011) and (Blacoe and Lapata, 2012). $$$$$ The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010).

Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.
Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods.
Blacoe and Lapata (2012) compare count and predict representations as input to composition functions. $$$$$ In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication.

The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ Using the composition models described above, we compute the cosine similarity of phr1 and phr2: Model similarities were evaluated against the human similarity ratings using Spearman’s p correlation coefficient.
The cw approach is very popular (for example both Huang et al (2012) and Blacoe and Lapata (2012) used it in the studies we discussed in Section 1). $$$$$ The corpus consists of sentence pairs Seni1,Seni2 and labels indicating whether they are in a paraphrase relationship or not.

Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ It involves entire sentences exhibiting varied syntactic constructions and in the limit involves genuine natural language undertanding.
Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ The elementary objects that we operate on are vectors associated with words.
Blacoe and Lapata (2012) have an extensive comparison of the performance of various vector based models on this data set to which we compare our model in Table 5. $$$$$ The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011a).

 $$$$$ Their values are set to the ratio of the probability of the context word given the target word to the probability of the context word overall.
 $$$$$ This model is more sophisticated than the one we used in our experiments (see Table 4 and 5).
 $$$$$ Our first experiment focused on modeling similarity judgments for short phrases gathered in human experiments.

Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. $$$$$ Using the composition models described above, we compute the cosine similarity of phr1 and phr2: Model similarities were evaluated against the human similarity ratings using Spearman’s p correlation coefficient.
Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. $$$$$ We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication.
Following standard practice in paraphrase detection studies (e.g., Blacoe and Lapata (2012)), we use cosine similarity between sentence pairs as computed by one of our systems together with two shallow similarity cues: word overlap between the two sentences and difference in sentence length. $$$$$ Different matrices are then generated from the tensor, and their rows and columns give rise to different semantic spaces appropriate for capturing different semantic problems.

Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication.
Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ The n-gram is paired with a corrupted n-gram x˜ = (w1,..., ˜wn) where ˜wn =6 wn is chosen uniformly from the vocabulary.
Our result stands in contrast with Blacoe and Lapata (2012), the only study we are aware of that compared a sophisticated composition model (Socher et al's 2011 model) to add and mult on realistic sentences, which attained the top performance with the simple models for both figures of merit they used. $$$$$ Our first experiment focused on modeling similarity judgments for short phrases gathered in human experiments.
