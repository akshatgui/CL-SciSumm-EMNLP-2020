This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser. $$$$$ At this beam size the best possible F1 constituent score is 98.5.
This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser. $$$$$ 6/21/1998.
This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser. $$$$$ The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater.
This strategy is similar to the one employed by Carreras et al (2008) to prune the search space of the actual parser. $$$$$ Finally, other recent work (Petrov et al., 2007; Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing.

 $$$$$ To deal with this problem, we use a simple initial model to prune the search space of the more complex model.
 $$$$$ 1 requires O(n3G) time, where n is the length of the sentence, and G is a grammar constant.
 $$$$$ KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. derivations from the Penn treebank trees.

This approach can be seen as trade-off between phrase based reranking experiments (Collins, 2000) and the approach of Carreras et al (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. $$$$$ sections 23 and 24 of the treebank, using the method of (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model.
This approach can be seen as trade-off between phrase based reranking experiments (Collins, 2000) and the approach of Carreras et al (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. $$$$$ 1 requires O(n3G) time, where n is the length of the sentence, and G is a grammar constant.
This approach can be seen as trade-off between phrase based reranking experiments (Collins, 2000) and the approach of Carreras et al (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. $$$$$ The method allows relatively easy incorporation of features; future work should leverage this in producing more accurate parsers, and in applying the parser to different languages or domains.

Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text). $$$$$ The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG).
Our system also compares favourably with the system of Carreras et al (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al (2009) that makes use of external data (unannotated text). $$$$$ For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work.

Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar. $$$$$ We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.
Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar. $$$$$ As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations.
Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar. $$$$$ This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm.
Carreras et al (2008) use coarse-to-fine pruning with dependency parsing, but in that case, a graph based dependency parser provides the coarse pass, with the fine pass being a far-more-expensive tree adjoining grammar. $$$$$ Any of these feature types can be combined with surface features of the sentence x, in a similar way to the use of surface features in conditional random fields (Lafferty et al., 2001).

 $$$$$ A critical problem when training a GLM for parsing is the computational complexity of the inference problem.
 $$$$$ Other similar trees, but with more VP levels, will give the same set D. However, this issue appears to be benign in the Penn WSJ treebank.

2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9 $$$$$ Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features.
2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9 $$$$$ As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations.
2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9 $$$$$ As described shortly, we will use this model to compute marginal scores for dependencies in both training and test sentences.
2.2? Petrov& amp; Klein (2007) 6.2 Carreras et al (2008) Unk This Paper Baseline 100.7 Baseline+Padding 89.5 Baseline+Padding+Semi 46.8 Table 9 $$$$$ At this beam size the best possible F1 constituent score is 98.5.

 $$$$$ Sections 2-21 of the Penn Wall Street Journal treebank were used as training data in our experiments, and section 22 was used as a development set.
 $$$$$ The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features.
 $$$$$ This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm.
 $$$$$ The features could potentially look at any information in the labels, which are of the form (POS, A, ηh, ηm, L), but in our experiments, we map labels to a pair (GRM((POS, A, ηh, ηm, L)), A).

Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008). $$$$$ The features take into account the identity of the labels l used in the derivations.
Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008). $$$$$ We make use of this idea of automata, and also make direct use of the method described in section 4.2 of (Eisner, 2000) that allows a set of possible senses for each word in the input string.
Suzuki et al (2009) and phrase-structure annotations in the case of Carreras et al (2008). $$$$$ As described shortly, we will use this model to compute marginal scores for dependencies in both training and test sentences.

 $$$$$ A driving motivation for our approach comes from the flexibility of the feature-vector representations f(x, y) that can be used in the model.
 $$$$$ Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.
 $$$$$ Thanks to Jenny Rose Finkel for suggesting that we evaluate dependency parsing accuracies.
 $$$$$ The method allows relatively easy incorporation of features; future work should leverage this in producing more accurate parsers, and in applying the parser to different languages or domains.

 $$$$$ KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006).
 $$$$$ A marginal score µ(x, h, m, l) is a value between 0 and 1 that reflects the plausibility of a dependency for sentence x with head-word xh, modifier word xm, and label l. In the first-stage pruning model the labels l are triples of non-terminals representing grammatical relations, as described in section 2.1 of this paper—for example, one possible label would be (VP VBD NP), and in general any triple of nonterminals is possible.
 $$$$$ HR0011-06-C-0022.
 $$$$$ By a previous modifier, we mean a modifier m′ that was adjoined from the same direction as m (i.e., such that h < m′ < m or m < m′ < h).

 $$$$$ Table 1 shows the results for the method.
 $$$$$ We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach.
 $$$$$ M. Collins was funded by NSF grant IIS-0347631 and DARPA contract No.
 $$$$$ HR0011-06-C-0022.

(Carreras et al., 2008) and edge annotation (Huang, 2008). $$$$$ A simple “first-order” model would define � d(x, (h, m, l)) (2) Figure 3: An example tree, formed by a combina(h,m,l)ED(y) tion of the two structures in figure 2.
(Carreras et al., 2008) and edge annotation (Huang, 2008). $$$$$ We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees.

Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008). $$$$$ The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater.
Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008). $$$$$ (For test or development data, we used the part-of-speech tags generated by the parser of (Collins, 1997).
Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008). $$$$$ We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism.
Many edges can be ruled out beforehand, either based on the distance in the sentence between the two words (Eisner and Smith, 2010), the predictions of a local ranker (Martins et al2009), or the marginals computed from a simpler parsing model (Carreras et al2008). $$$$$ We make use of this idea of automata, and also make direct use of the method described in section 4.2 of (Eisner, 2000) that allows a set of possible senses for each word in the input string.

The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al, 2008)). $$$$$ The function d maps dependencies within y to feature vectors.
The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al, 2008)). $$$$$ We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing.
The key idea in our approach is to allow highly flexible reordering operations, in combination with a discriminative model that can condition on rich features of the source-language input string. Our approach builds on a variant of tree adjoining grammar (TAG; (Joshi and Schabes, 1997)) (specifically, the formalism of (Carreras et al, 2008)). $$$$$ In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003; Taskar et al., 2004)), the optimal label y* for an input x is where Y(x) is the set of possible labels for the input x; f(x, y) E Rd is a feature vector that represents the pair (x, y); and w is a parameter vector.

Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). $$$$$ The complexity of training and applying such a model is again O(Gn3), where G is the number of possible labels, and the number of possible labels (triples of non-terminals) is around G = 1000 in the case of treebank parsing; this value for G is still too large for the method to be efficient.
Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). $$$$$ To deal with this problem, we use a simple initial model to prune the search space of the more complex model.
Our work builds on the variant of tree adjoining grammar (TAG) introduced by (Carreras et al., 2008). $$$$$ With the beam setting used in our experiments (α = 10−6), only 0.34% of possible dependencies are considered by the TAG-based model, but 99% of all correct dependencies are included.

To continue our example, the resulting entry would be as follows $$$$$ The results in this work (88.3%/88.0% F1) are lower than our F1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach.
To continue our example, the resulting entry would be as follows $$$$$ These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing.

 $$$$$ A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibility in the features which can be included in the definition of f(x, y).
 $$$$$ By a previous modifier, we mean a modifier m′ that was adjoined from the same direction as m (i.e., such that h < m′ < m or m < m′ < h).
 $$$$$ A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved.
 $$$$$ Instead, we train three separate models µ1, µ2, and µ3 for the three different positions in the non-terminal triples.
