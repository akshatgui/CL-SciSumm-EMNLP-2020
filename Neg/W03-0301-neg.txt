 $$$$$ Two teams made use of syntactic structure in the text to be aligned.
 $$$$$ We are grateful to all the participants in the shared task, for their hard work and involvement in this evaluation exercise.
 $$$$$ The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world.
 $$$$$ Annotators were not asked to assign a Sure or Probable label.

We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). $$$$$ However, these come from parallel text that has been shallowly parsed via a method known as bilingual bracketing.
We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). $$$$$ ... 18 1 0 18 2 0 18 3 0 18 4 0 ... since the words do not correspond one to one, and yet the two phrases mean the same thing in the given context, the phrases should be linked as wholes, by linking each word in one to each word in another.
We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). $$$$$ Increased performance is therefore expected when larger training data sets are available.
We use the data provided forthe French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003). $$$$$ Data and evaluation software used in this exercise are available online at http://www.cs.unt.edu/Ëœrada/wpt.

The evaluation is done using the performance measures described in (Mihalcea and Pedersen, 2003): precision, recall and F-score on the probable and sure alignments, as well as the Alignment Error Rate (AER), which in our case is a weighted average of the recall on the sure alignments and the precision on the probable. $$$$$ We are grateful to all the participants in the shared task, for their hard work and involvement in this evaluation exercise.
The evaluation is done using the performance measures described in (Mihalcea and Pedersen, 2003): precision, recall and F-score on the probable and sure alignments, as well as the Alignment Error Rate (AER), which in our case is a weighted average of the recall on the sure alignments and the precision on the probable. $$$$$ We would also like to thank the student volunteers from the Department of English, Babes-Bolyai University, Cluj-Napoca, Romania who helped creating the Romanian-English word aligned data.

HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). $$$$$ The fourth measure was originally introduced by (Och and Ney, 2000), and proposes the notion of quality of word alignment.
HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). $$$$$ The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world.
HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). $$$$$ Out of 28 different evaluation figures, 20 top ranked figures are provided by systems with limited resources.
HLT-03 best F is Ralign.EF.1 and best AER is XRCE.Nolem.EF.3 (Mihalcea and Pedersen, 2003). $$$$$ The fourth measure was originally introduced by (Och and Ney, 2000), and proposes the notion of quality of word alignment.

HLT-03 best is Ralign.EF.1 (Mihalcea and Pedersen, 2003). $$$$$ Each word alignment submission was evaluated in terms of the above measures.
HLT-03 best is Ralign.EF.1 (Mihalcea and Pedersen, 2003). $$$$$ Similarly, AER for English-French (5.71% highest error reduction) is clearly better than the AER for Romanian-English (28.86% highest error reduction).

hlt-03 best is xrce.nolem (mihalcea and pedersen, 2003). $$$$$ There are many people who contributed greatly to making this word alignment evaluation task possible.

We evaluated the alignment performance of the proposed models with two tasks: Japanese English word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al, 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ There are many people who contributed greatly to making this word alignment evaluation task possible.
We evaluated the alignment performance of the proposed models with two tasks: Japanese English word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al, 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ UMD was a straightforward implementation of IBM Model 2, BiBr employed a boosting procedure in deriving an IBM Model 1 lexicon, Ralign used IBM Model 2 as a foundation for their recursive splitting procedure, and XRCE used IBM Model 4 as a base for alignment with lemmatized text and bilingual lexicons.
We evaluated the alignment performance of the proposed models with two tasks: Japanese English word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al, 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ The shared task included Romanian-English and English-French sub-tasks, and drew the participation of seven teams from around the world.

As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). $$$$$ In particular, we would like to thank Dan Melamed for suggesting the two different subtasks (limited and unlimited resources), and Michael Carl and Phil Resnik for initiating interesting discussions regarding phrase-based evaluations.
As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). $$$$$ A manual verification of the alignment was also performed.
As a point of comparison, the SMT community has been evaluating performance of word-alignment systems on an even smaller dataset of 447 pairs of non-overlapping sentences (Mihalcea and Pedersen, 2003). $$$$$ Evaluations were performed with respect to four different measures.

(Mihalcea and Pedersen, 2003). $$$$$ In this paper, we presented the task definition, and resources involved, and shortly described the participating systems.
(Mihalcea and Pedersen, 2003). $$$$$ For unlimited resources, ProAlign.RE.1 has the best performance.
(Mihalcea and Pedersen, 2003). $$$$$ This set of evaluations pertains to full coverage word alignments.

This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). $$$$$ The task of word alignment consists of finding correspondences between words and phrases in parallel texts.
This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). $$$$$ Out of 28 different evaluation figures, 20 top ranked figures are provided by systems with limited resources.
This is the same training data set as used in the 2003 word alignment evaluation (Mihalcea and Pedersen, 2003). $$$$$ The word alignment result files had to include one line for each word-to-word alignment.

The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). $$$$$ The gold standard for the two language pair alignments were produced using slightly different alignment procedures, which allowed us to study different schemes for producing gold standards for word aligned data.
The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). $$$$$ The results obtained for the English-French Sure alignments are significantly higher (80.54% best Fmeasure) than those for Romanian-English Sure alignments (71.14% best F-measure).
The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). $$$$$ Similarly, AER for English-French (5.71% highest error reduction) is clearly better than the AER for Romanian-English (28.86% highest error reduction).
The evaluation was run with respect to precision, recall, F-measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003). $$$$$ We are grateful to all the participants in the shared task, for their hard work and involvement in this evaluation exercise.

For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). $$$$$ </s> and he said from the English sentence has no corresponding translation in French, and therefore all these words are aligned with the token id 0.
For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). $$$$$ A total of 447 English-French aligned sentences (Och and Ney, 2000), and 248 Romanian-English aligned sentences were released one week prior to the deadline.
For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). $$$$$ Systems making use of any additional resources (e.g. bilingual dictionaries, additional parallel corpora, and others) were classified under the Unlimited Resources category.
For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). $$$$$ We are grateful to all the participants in the shared task, for their hard work and involvement in this evaluation exercise.

To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). $$$$$ Similarly, AER for English-French (5.71% highest error reduction) is clearly better than the AER for Romanian-English (28.86% highest error reduction).
To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). $$$$$ Instead, we had an arbitration phase, where a third annotator judged the cases where the first two annotators disagreed.
To choose the regularization strength and the initial learning rate 0,3 we trained several models on a 10,000-sentence-pair subset of the French English Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). $$$$$ Participating teams were asked to produce word alignments, following a common format as specified below, and submit their output by a certain deadline.

This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). $$$$$ Data and evaluation software used in this exercise are available online at http://www.cs.unt.edu/Ëœrada/wpt.
This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). $$$$$ Tables 2 and 3 show all of the submissions for each team in the two subtasks, and provide a brief description of their approaches.
This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003). $$$$$ Participants were required to run their word alignment systems on these two sets, and submit word alignments.

In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). $$$$$ Fourday combines several intuitive baselines via a nearest neighbor classifier, RACAI carries out a greedy alignment based on an automatically extracted dictionary of translations, and UMDâ€™s implementation of IBM Model 2 provides an experimental platform for their future work incorporating prior knowledge about cognates.
In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). $$$$$ All data were pre-tokenized.
In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). $$$$$ Without them, all these comparative analyses of word alignment techniques would not be possible.
In this section, we report experiments conducted with Canadian Hansards data from the 2003 HLT NAACL word-alignment workshop (Mihalcea and Pedersen, 2003). $$$$$ This suggests that perhaps using a large number of additional resources does not seem to improve a lot over the case when only parallel texts are employed.

We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ Given an alignment A, and a gold standard alignment ~, each such alignment set eventually consisting of two sets As, .Ap, and 9s, 9p corresponding to Sure and Probable alignments, the following measures are defined (where T is the alignment type, and can be set to either S or P).
We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.
We applied this matching algorithm to word level alignment using the English-French Hansards data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). $$$$$ These are systems that are based on GIZA++, with or without additional resources (lemmatizers and lexicons).

We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). $$$$$ Tables 4 and 5 list the results obtained by participating systems in the Romanian-English task.
We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). $$$$$ Participating teams were asked to produce word alignments, following a common format as specified below, and submit their output by a certain deadline.
We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). $$$$$ The fourth measure was originally introduced by (Och and Ney, 2000), and proposes the notion of quality of word alignment.
We used the same training and test data as in our previous work, a subset of the Canadian Hansards bilingual corpus supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003). $$$$$ All three of these systems were developed within a short period of time before and during the shared task.

The French/English data were those used by Mihalcea and Pedersen (2003). $$$$$ Three teams approached the shared task with baseline or prototype systems.
The French/English data were those used by Mihalcea and Pedersen (2003). $$$$$ A shared task on word alignment was organized as part of the HLT/NAACL 2003 Workshop on Building and Using Parallel Texts.
The French/English data were those used by Mihalcea and Pedersen (2003). $$$$$ Identical tokenization procedures were used for training, trial, and test data.
The French/English data were those used by Mihalcea and Pedersen (2003). $$$$$ We would also like to thank the student volunteers from the Department of English, Babes-Bolyai University, Cluj-Napoca, Romania who helped creating the Romanian-English word aligned data.

The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). $$$$$ A total of 447 English-French aligned sentences (Och and Ney, 2000), and 248 Romanian-English aligned sentences were released one week prior to the deadline.
The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). $$$$$ All three of these systems were developed within a short period of time before and during the shared task.
The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). $$$$$ We believe however that these figures are not directly comparable, since the English-French Probable alignments were obtained as a reunion of the alignments assigned by two different annotators, while for the Romanian-English Probable set two annotators had to reach an agreement (that is, an intersection of their individual alignment assignments).
The alignments produced by MEBA were compared to the ones produced by YAWA and evaluated against the Gold Standard (GS) annotations used in the Word Alignment Shared Tasks (Romanian-English track) organized at HLT-NAACL2003 (Mihalcea and Pedersen 2003). $$$$$ We would also like to thank the student volunteers from the Department of English, Babes-Bolyai University, Cluj-Napoca, Romania who helped creating the Romanian-English word aligned data.

The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks. $$$$$ However, these come from parallel text that has been shallowly parsed via a method known as bilingual bracketing.
The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks. $$$$$ The only evaluation set where Romanian-English data leads to better performance is the Probable alignments set.
The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks. $$$$$ The annotators did not produce any NULL alignments.
The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the word alignment task, and established standard performance measures and some benchmark tasks. $$$$$ Data and evaluation software used in this exercise are available online at http://www.cs.unt.edu/Ëœrada/wpt.
