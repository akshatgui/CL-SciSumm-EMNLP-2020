 $$$$$ The difference between these studies on text simplification and our system is that a text simplification system usually does not remove anything from an original sentence, although it may change its structure or words, but our system removes extraneous phrases from the extracted sentences.
 $$$$$ The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.
 $$$$$ IRI 96-19124 and IRI 96-18797.
 $$$$$ For example, whether a determiner is obligatory is relative to the noun phrase it is in; whether a prepositional phrase is obligatory is relative to the sentence or the phrase it is in.

In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). $$$$$ They are then stored in a table and loaded at running time.
In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). $$$$$ IRI 96-19124 and IRI 96-18797.
In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). $$$$$ The system first parsed the sentences in the corpus using ESG parser.
In fact, professional abstractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (Jing, 2000). $$$$$ 3.2 Evaluation result In the evaluation, we used 400 sentences in the corpus to compute the probabilities that a phrase is removed, reduced, or unchanged.

For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http $$$$$ We present a novel sentence reduction system which removes extraneous phrases from sentences that are extracted from an article in text summarization.
For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http $$$$$ We tested the program on the rest 100 sentences.
For example, Jing (2000) trained her system on a set of 500 sentences from the Benton Foundation (http $$$$$ Each following step annotates each node in the parse tree with additional information, such as syntactic or context importance, which are used later to determine which phrases (they are represented as subtrees in a parse tree) can be considered extraneous and thus removed.

Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). $$$$$ The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.
Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). $$$$$ The system first parsed the sentences in the corpus using ESG parser.

Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ It should also be able to interact with the modules that run after it, such as the sentence combination module, so that it can revise reduction decisions according to the feedback from these modules.
Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase.
Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.

Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). $$$$$ (Grefenstette, 1998) proposed to remove phrases in sentences to produce a telegraphic text that can be used to provide audio scanning service for the blind.
Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). $$$$$ The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.
Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). $$$$$ This material is based upon work supported by the National Science Foundation under Grant No.
Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). $$$$$ The deleted phrases can be prepositional phrases, clauses, to-infinitives, or gerunds, and multiple phrases can be removed form a single sentence.

Jing and McKeown (H. Jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. $$$$$ The information in the lexicon is used to mark the obligatory arguments of verb phrases.
Jing and McKeown (H. Jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. $$$$$ This material is based upon work supported by the National Science Foundation under Grant No.
Jing and McKeown (H. Jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. $$$$$ In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information.
Jing and McKeown (H. Jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. $$$$$ This material is based upon work supported by the National Science Foundation under Grant No.

Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). $$$$$ The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.
Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). $$$$$ IRI 96-19124 and IRI 96-18797.
Sentence compression is the task of producing a shorter form of a single given sentence, so that the new form is grammatical and retains the most important information of the original one (Jing, 2000). $$$$$ It can also be followed by a noun phrase and a to-infinitive phrase (e.g., he convinced me to go to the party).

Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). $$$$$ IRI 96-19124 and IRI 96-18797.
Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). $$$$$ We can tailor the reduction system to queries-based summarization.
Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). $$$$$ One reason for this is that our probability model can hardly capture the dependencies between a particular adjective and the head noun since the training corpus is not large enough, while the other sources of information, including grammar or context information, provide little evidence on whether an adjective or an adverb should be removed.

Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). $$$$$ The probabilities we computed from the training corpus covered 58% of instances in the test corpus.
Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). $$$$$ Figure 4: Reduced form by the program We can see that along five edges (they are D—)T, D—*G, B-4A, B—>C), both the human and the program made decisions.
Sentence compression is the task of producing a shorter form of a grammatical source (input) sentence, so that the new form will still be grammatical and it will retain the most important information of the source (Jing, 2000). $$$$$ The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.

The evaluation of sentence reduction (see (Jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts. $$$$$ (Corston-Oliver and Dolan, 1999) proposed to remove clauses in sentences before indexing documents for Information Retrieval.

To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). $$$$$ This material is based upon work supported by the National Science Foundation under Grant No.
To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). $$$$$ IRI 96-19124 and IRI 96-18797.
To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). $$$$$ Some researchers suggested removing phrases or clauses from sentences for certain applications.

Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ In this step, the system decides which components in the sentence are most related to the main topic being discussed.
Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ IRI 96-19124 and IRI 96-18797.
Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ Therefore, we take this into account when marking the obligatory components using subcategorization knowledge from the lexicon (step 2) — we not only look at the PPs that are attached to a verb phrase, but also PPs that are next to the verb phrase but not attached, in case it is part of the verb phrase.
Jing (2000) was perhaps the first to tackle the sentence compression problem. $$$$$ If a node is removed, the subtree with that node as the root is removed as a whole, thus no decisions are needed for the descendants of the removed node.

Table 5 shows a 5 sentence summary created using algorithm 1 for the paper A00-1043 (Jing, 2000). $$$$$ For instance, for a sentence, the main verb, the subject, and the object(s) are essential if they exist, but a prepositional phrase is not; for a noun phrase, the head noun is essential, but an adjective modifier of the head noun is not.
Table 5 shows a 5 sentence summary created using algorithm 1 for the paper A00-1043 (Jing, 2000). $$$$$ In the last step of reduction when the system makes the final decision, the relevance of a phrase to the query is taken into account, together with syntactic, context, and corpus information.
Table 5 shows a 5 sentence summary created using algorithm 1 for the paper A00-1043 (Jing, 2000). $$$$$ In that case, the task of the reduction is not to remove phrases that are extraneous in terms of the main topic of an article, but phrases that are not very relevant to users' queries.

To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). In these approaches, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries. $$$$$ We also wrote a preprocessor to deal with particular structures that the parser often has problems with, such as appositions.
To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). In these approaches, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries. $$$$$ For the decisions on removing or keeping a clause, the system has a success rate of 78.1%; for the decisions on removing or keeping a to-infinitive, the system has a success rate of 85.2%.
To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). In these approaches, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries. $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
To overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of Knight and Marcu (2000) and Jing (2000). In these approaches, decisions about which material to include/delete in the sentence summaries do not rely on relative frequency information on words, but rather on probability models of subtree deletions that are learned from a corpus of parses for sentences and their summaries. $$$$$ The evaluation shows that 81.3% of reduction decisions made by the system agreed with those of humans.

In addition, an automatic evaluation method based on context-free deletion decisions has been proposed by Jing (2000). $$$$$ Ideally, the sentence reduction module should interact with other modules in a summarization system.
In addition, an automatic evaluation method based on context-free deletion decisions has been proposed by Jing (2000). $$$$$ Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase.
In addition, an automatic evaluation method based on context-free deletion decisions has been proposed by Jing (2000). $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not

Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). $$$$$ At each node of the parse tree, both the human and the program make a decision whether to remove the node or to keep it.
Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). $$$$$ Ideally, the sentence reduction module should interact with other modules in a summarization system.
Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). $$$$$ Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.
Sentence compression produces a summary of a single sentence that retains the most important information while remaining grammatical (Jing, 2000). $$$$$ Our system makes intelligent reduction decisions based on multiple sources of knowledge, including syntactic knowledge, context, and probabilities computed from corpus analysis.

A syntactic approach considers the alignment over parse trees (Jing, 2000), and a similar technique has been used with dependency trees to evaluate the quality of sentence fusions (Marsi and Krahmer, 2005). $$$$$ Both studies removed phrases based only on their syntactic categories, while the focus of our system is on deciding when it is appropriate to remove a phrase.
