For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ They observed that source domains closer to the target helped more.
For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ On the polarity dataset, this model matches the results reported by Pang et al. (2002).
For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ To show how SCL deals with those domain mismatches, we look at the adaptation from book reviews to reviews of kitchen appliances.
For evaluation we selected two domain adaptation datasets :spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline.

Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. $$$$$ We thank Nikhil Dinesh for helpful advice throughout the course of this work.
Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. $$$$$ Adapting classifiers from books to DVDs, for instance, is easier than adapting them from books to kitchen appliances.
Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. $$$$$ Table 1 shows the set-symmetric differences between the two methods for pivot selection when adapting a classifier from books to kitchen appliances.
Blitzer et al (2007) used structural correspondence learning to train a classifier on source data with new features induced from target unlabeled data. $$$$$ In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking.

For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). $$$$$ He used the difference in mutual information with two human-selected features (the words “excellent” and “poor”) to score features in a completely unsupervised manner.
For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). $$$$$ The domains are ordered clockwise from the top left: books, DVDs, electronics, and kitchen.
For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). $$$$$ We refer throughout the rest of this work to our method for selecting pivots as SCL-MI.
For these experiments we use the Multi-Domain Sentiment Dataset, introduced by Blitzer et al (2007). $$$$$ We discuss related work in Section 7 and conclude in Section 8.

We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). $$$$$ Here we make use of the A-distance (BenDavid et al., 2006).
We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). $$$$$ For reasons of space, for each target domain we show adaptation from only the two domains on which SCL-MI performed the worst relative to the supervised baseline.
We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). $$$$$ For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains.
We split the labeled data 80/20 following Blitzer et al (2007) (cf. Chen et al. (2012) train on all "labeled" data and test on the "unlabeled" data). $$$$$ After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset (Pang et al., 2002).

Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. $$$$$ Using the notation of Ando and Zhang (2005), we can write the supervised training objective of SCL on the source domain as where y is the label.
Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National BusinessCenter (DOI-NBC).
Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. $$$$$ After this conversion, we had 1000 positive and 1000 negative examples for each domain, the same balanced composition as the polarity dataset (Pang et al., 2002).
Empirical work on NLP domain shifts has focused on the former. For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. We continue in this tradition by making two assumptions about our setting. $$$$$ The Eth pivot predictor is characterized by its weight vector wt; positive entries in that weight vector mean that a non-pivot feature (like “fast dualcore”) is highly correlated with the corresponding pivot (like “excellent”).

We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). $$$$$ On the polarity dataset, this model matches the results reported by Pang et al. (2002).
We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). $$$$$ While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006).
We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). $$$$$ We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006).
We selected three data sets commonly used in domain adaptation: spam (Jiang and Zhai, 2007), ACE 2005 named entity recognition (Jiang and Zhai, 2007), and sentiment (Blitzer et al, 2007). $$$$$ Let 0 E Rkxd be the top k left singular vectors of W (here d indicates the total number of features).

In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. $$$$$ The thick horizontal bars are the accuracies of the in-domain classifiers for these domains.
In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National BusinessCenter (DOI-NBC).
In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. $$$$$ We are also actively searching for a larger and more varied set of domains on which to test our techniques.
In a batch setting this corresponds to learning a linear classifier to discriminate the domains, and Blitzer et al (2007) showed correlations with the error from domain adaptation. $$$$$ For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline.

However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). $$$$$ Now we learn a predictor for the augmented instance (x, 0x).
However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). $$$$$ This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.
However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). $$$$$ We can observe from these results that there is a rough grouping of our domains.
However, such methods require the existence of either a parallel corpus/machine translation engine for projecting/translating annotations/lexica from a resource-rich language to the target language (Banea et al., 2008; Wan, 2008), or a domain that is "similar" enough to the target domain (Blitzer et al, 2007). $$$$$ The unsupervised A-distance measure of divergence between domains correlates well with loss due to adaptation.

We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem.
We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation.
We use five sentiment classification datasets, including the widely-used movie review dataset [MOV] (Pang et al, 2002) as well as four datasets containing reviews of four different types of products from Amazon [books (BOO), DVDs (DVD), electronics (ELE), and kitchen appliances (KIT)] (Blitzer et al, 2007). $$$$$ We apply the projection 0x to obtain k new real-valued features.

We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. $$$$$ Since we were able to obtain labels for all of the reviews, we also ensured that they were balanced between positive and negative examples, as well.
We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. $$$$$ With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter.
We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. $$$$$ In the future, we wish to include some of the more recent advances in sentiment classification, as well as addressing the more realistic problem of ranking.
We show that this constraint is effective on the sentiment classification task (Pang et al, 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al, 2007) without the need to engineer auxiliary tasks. $$$$$ However, this approach raises two important questions.

We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). $$$$$ We apply the projection 0x to obtain k new real-valued features.
We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). $$$$$ We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.
We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). $$$$$ While many of the features of a good cell phone review are the same as a computer review – the words “excellent” and “awful” for example – many words are totally new, like “reception”.
We evaluate our approach on adapting sentiment classifiers on 4 domains: books, DVDs, electronics and kitchen appliances (Blitzer et al, 2007). $$$$$ SCL addresses both of these issues simultaneously by aligning features from the two domains.

Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. $$$$$ On the polarity dataset, this model matches the results reported by Pang et al. (2002).
Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. $$$$$ Sections 2-5 focused on how to adapt to a target domain when you had a labeled source dataset.
Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. $$$$$ First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006).
Both the achieved error reduction and the absolute score match the results reported in (Blitzer et al, 2007) for the best version of the SCL method (SCL-MI, 36%), suggesting that our approach is a viable alternative to SCL. $$$$$ At the beginning of Section 2 we gave examples of how features can change behavior across domains.

To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). $$$$$ Choosing pivots by mutual information allows us to further reduce the error to 36%.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). $$$$$ When it is 0, the two domains are indistinguishable using a linear classifier.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). $$$$$ While the polarity dataset is a popular choice in the literature, we were unable to use it for our task.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCL method (Blitzer et al, 2007). $$$$$ We stress that our method improves a supervised baseline.

To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). $$$$$ Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be able to adequately cover electronics or kitchen appliances.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). $$$$$ Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). $$$$$ He used the difference in mutual information with two human-selected features (the words “excellent” and “poor”) to score features in a completely unsupervised manner.
To evaluate our approach, we consider the same dataset as the one used to evaluate the SCLmethod (Blitzer et al, 2007). $$$$$ Our key intuition is that even when “good-quality reception” and “fast dual-core” are completely distinct for each domain, if they both have high correlation with “excellent” and low correlation with “awful” on unlabeled data, then we can tentatively align them.

Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. $$$$$ We set k, the number of singular vectors of the weight matrix, to 50.
Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. $$$$$ Now we learn a predictor for the augmented instance (x, 0x).
Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. $$$$$ Structural correspondence learning reduces the error due to transfer by 21%.
Instead of using the full set of bigram and unigram counts as features (Blitzer et al, 2007), we use a frequency cut-off of 30 to remove infrequent ngrams. $$$$$ The Adistance can be measured from unlabeled data, and it was designed to take into account only divergences which affect classification accuracy.

In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). $$$$$ On the polarity dataset, this model matches the results reported by Pang et al. (2002).
In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). $$$$$ All the experiments use a classifier trained on the training set of one domain and tested on the test set of a possibly different domain.
In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). $$$$$ Each review consists of a rating (0-5 stars), a reviewer name and location, a product name, a review title and date, and the review text.
In Table 1, we also compare the results of our method with the results of the best version of the SCL method (SCL-MI) reported in Blitzer et al (2007). $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA or the Department of Interior-National BusinessCenter (DOI-NBC).

Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. $$$$$ Figure 1 gives accuracies for all pairs of domain adaptation.
Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. $$$$$ For reasons of space, for each target domain we show adaptation from only the two domains on which SCL-MI performed the worst relative to the supervised baseline.
Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. $$$$$ We also note that while Florian et al. (2004) and Blitzer et al.
Second, the absolute scores achieved in Blitzer et al (2007) are slightly worse than those demonstrated in our experiments both for supervised and semi-supervised methods. $$$$$ For classification, we use linear predictors on unigram and bigram features, trained to minimize the Huber loss with stochastic gradient descent (Zhang, 3For a description of the construction of the polarity dataset, see http://www.cs.cornell.edu/people/ pabo/movie-review-data/.

Our approach results in competitive domain adaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCLmethod (Blitzer et al, 2007). $$$$$ First, we showed that for a given source and target domain, we can significantly improve for sentiment classification the structural correspondence learning model of Blitzer et al. (2006).
Our approach results in competitive domain adaptation performance on the sentiment classification task, rivalling that of the state-of-the-art SCLmethod (Blitzer et al, 2007). $$$$$ This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.

On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). $$$$$ We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.
On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). $$$$$ We chose pivot features using not only common frequency among domains but also mutual information with the source labels.
On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). $$$$$ This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains.
On a separate note, previous research has explicitly studied sentiment analysis as an application of transfer learning (Blitzer et al, 2007). $$$$$ Table 2 illustrates one row of the projection matrix 0 for adapting from books to kitchen appliances; the features on each row appear only in the corresponding domain.

We use the dataset from (Blitzer et al, 2007) for sentiment classification. $$$$$ Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4, µ = 0, which we have used in our results so far.
We use the dataset from (Blitzer et al, 2007) for sentiment classification. $$$$$ When we report results with SCL and SCL-MI, we require that pivots occur in more than five documents in each domain.
We use the dataset from (Blitzer et al, 2007) for sentiment classification. $$$$$ Now we learn a predictor for the augmented instance (x, 0x).
We use the dataset from (Blitzer et al, 2007) for sentiment classification. $$$$$ Thus we can use the Adistance to select source domains to label which will give low target domain error.
