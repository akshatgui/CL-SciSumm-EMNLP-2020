For evaluation the parser returns dependency structures, but we have also developed a module which builds first order semantic representations from the derivations, which can be used for inference (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

CCG-based syntactic parsing (Bos et al, 2004). $$$$$ Hence providing a compositional semantics for CCG simply amounts to assigning semantic representations to the lexical en tries and interpreting the combinatory rules.
CCG-based syntactic parsing (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.
Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. $$$$$ The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.
Bos et al (2004) present an algorithm that learns CCG lexicons with semantics but requires fully specified CCG derivations in the training data. $$$$$ Andthird, there exist highly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.

This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). $$$$$ However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plication in hand.There is some existing work with a similar motivation to ours.
This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). $$$$$ Briscoe and Carroll (2002) gen erate underspecified semantic representations fromtheir robust parser.
This leads a traditional parsing system with a direct mapping from the parse tree to a semantic representation to fail to achieve a parse on 35% percent of the stories, and as such could not be used (Bos et al, 2004). $$$$$ The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.

The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. $$$$$ However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plication in hand.There is some existing work with a similar motivation to ours.
The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.
The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).
The nlp tools used by ASKNet are the C& amp; C parser (Clark and Curran, 2007) and the semantic analysis program Boxer (Bos et al, 2004), which operates on the ccg derivations output by the parser to produce a first-order representation. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. $$$$$ The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999),Charniak (2000)) have led to their use in a num ber of NLP applications, such as question-answering(Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplifica tion (Carroll et al, 1999), and a linguist?s search engine (Resnik and Elkiss, 2003).
It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. $$$$$ analysis of certainsyntactic phenomena such as coordination and ex traction, allowing the logical form to be obtained for such cases in a straightforward way.
It makes use of two NLP tools, the Clark and Curran parser (Clark and Curran, 2004) and the semantic analysis tool Boxer (Bos et al, 2004), both of which are part of the C& amp; C Toolkit1. The parser is based on Combinatory Categorial Grammar (CCG) and has been trained on 40,000 manually annotated sentences of the WSJ. $$$$$ However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.

Bos et al (2004) derive semantic interpretations from a wide-coverage categorial grammar. There are several differences between this and RASP-RMRS, but the most important arise from the differences between CCG and RASP. $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.

On the practical side, we have corpora with CCG derivations for each sentence (Hockenmaier and Steed man, 2007), a wide-coverage parser trained on that corpus (Clark and Curran, 2007) and a system for converting CCG derivations into semantic representations (Bos et al, 2004). However, despite being treated as a single unified grammar formalism, each of these authors use variations of CCG which differ primarily on which combinators are included in the grammar and the restrictions that are put on them. $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.

One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). $$$$$ However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plication in hand.There is some existing work with a similar motivation to ours.
One advantage of the CCG parser is that it is able to assign rich structural descriptions to sentences, from a variety of representations ,e.g. CCG derivations, CCG dependency structures, grammatical relations (Carroll et al, 1998), and first-order logical forms (Bos et al, 2004). $$$$$ However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plication in hand.There is some existing work with a similar motivation to ours.

Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.
Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). $$$$$ However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.
Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
Recently, the state of the art in wide coverage parsing has made wide-coverage semantic processing come into the reach of research in computational semantics (Bos et al., 2004). $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.

For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).
For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. $$$$$ However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plication in hand.There is some existing work with a similar motivation to ours.
For example, Bos et al (2004) build semantic representations from the parse derivations of a CCG parser, and the English Resource Grammar (ERG) (Toutanovaet al, 2005) provides a semantic representation using minimal recursion semantics. $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.

Additionally, Bos et al (2004) consider the challenging problem of constructing broad-coverage semantic representations with CCG, but do not learn the lexicon. $$$$$ However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.

Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004). $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.
Despite some recent advances in this direction (Bos et al, 2004), it is still the case that it is hard to obtain deep semantic analyses which are accurate enough to support logical inference (Lev et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). $$$$$ Such parsers typically return phrase-structure trees in the styleof the Penn Treebank, but without traces and co indexation.
As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
As mentioned at the beginning of this paper, the conversion of unrestricted text to some logical form has experienced a recent revival recently (Bos et al, 2004). $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.

At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. $$$$$ The use of the ?-calculusis integral to our method.
At the other end of the spectrum, Bos et al (Bos et al., 2004) have developed a broad-coverage parser to translate sentences to a logic based on discourse representation theory. $$$$$ Toutanova et al (2002) and Ka plan et al (2004) combine statistical methods with a linguistically motivated grammar formalism (HPSG and LFG respectively) in an attempt to achieve levels of robustness and accuracy comparable to the Penn Treebank parsers (which Kaplan et al do achieve).

Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data. $$$$$ Such parsers typically return phrase-structure trees in the styleof the Penn Treebank, but without traces and co indexation.
Bos et al (2004) present an algorithm for building semantic representations from CCG parses but requires fully specified CCG derivations in the training data. $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.
For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). $$$$$ There are a number of ad vantages to using CCG for this task.
For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
For example, a system that translates the output of a robust CCG parser into semantic representations has been developed (Bosetal., 2004). $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.

In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). $$$$$ However, the usefulness of this outputis limited, since the underlying meaning (as repre sented in a predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.In this paper we demonstrate how a widecoverage statistical parser using Combinatory Categorial Grammar (CCG) can be used to generate semantic representations.
In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
In this section, we present a method to derive TDL semantic representations from HPSG parse trees, adopting, in part, a previous method (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.

We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.
We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). $$$$$ There are a number of ad vantages to using CCG for this task.
We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). $$$$$ This paper shows how to construct semantic representations from the derivations producedby a wide-coverage CCG parser.
We used the Penn Treebank (Marcus, 1994) Section 22 (1,527 sentences) to develop and evaluate the proposed method and Section 23 (2,144 sentences) as the final test set. We measured the coverage of the construction of TDL semantic representations, in the manner described in a previous study (Bos et al, 2004). $$$$$ We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text.We believe this is a major step towards wide coverage semantic interpretation, one of the key objectives of the field of NLP.

Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences. $$$$$ Briscoe and Carroll (2002) gen erate underspecified semantic representations fromtheir robust parser.
Although this number is slightly less than 92.3%, as reported by Bos et al, (2004), it seems reasonable to say that the proposed method attained a relatively high coverage, given the expressive power of TDL. The construction of TDL semantic representations failed for 11.7% of the sentences. $$$$$ Unlike the dependency structures returned by the parser itself, these can be used directly for semantic in terpretation.
