Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ We must consider not only where the learner pushes the mass, but also from where the mass is taken.
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ 3 plots tagging accuracy (on ambiguous words) for each dictionary on the 24K dataset.
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ Hence their B (Eq.
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.

Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ One might do the same for the local maxima of any CE objective, though theoretical and experimental support for this idea remain for future work.
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ When the vocabulary E is the set of words in a natural language, it is never fully known; approximations for defining LENGTH = Em include using observed E from the training set (as we do) or adding a special OOV symbol.
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ Natural language is a delicate thing.

 $$$$$ They advocate running EM many times and selecting the local maximum that maximizes entropy.
 $$$$$ For sequence models like WFSAs it is computed using a dynamic programming algorithm (the forward algorithm for WFSAs).
 $$$$$ We can combine DEL1WORD and TRANS1 by taking their union; this gives a larger neighborhood, DELORTRANS1.4 The DEL1SUBSEQ neighborhood allows the deletion of any contiguous subsequence of words that is strictly smaller than the whole sequence.
 $$$$$ For any plausible sentence, there are many slight perturbations of it that will make it implausible.

We compare the output to two annotation schemes $$$$$ We introduce there the notion of designing neighborhoods to guide learning for particular tasks.
We compare the output to two annotation schemes $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.
We compare the output to two annotation schemes $$$$$ To train on we require methods for log-linear models; few exist.
We compare the output to two annotation schemes $$$$$ For them, the Markov random field is over labeling configurations for all examples, not, as in our case, complex structured labels for a particular example.

Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ 2 summarizes some concrete examples; see also §3.1–3.2.
Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ Consider first the neighborhood consisting of all sequences generated by deleting a single symbol from the m-length sequence xm1 : DEL1WORD(xi ) = {xl−1xm`+1 1 < ` < m} U {xm1 } This set consists of m + 1 strings and can be compactly represented as a lattice (see Fig.
Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ 2 shows the Viterbi accuracy of each criterion trained on the 96K-word dataset as smoothing was varied; the table shows, for each (criterion, dataset) pair the performance of the selected A or u2 and the one chosen by an oracle.

The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ Models selected using unlabeled development data are circled.
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ Each graph corresponds to a different level of dilution.
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ We refer to the mapping N : X —* 21 as the neighborhood function, and the optimization of Eq.
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ One strategy is to incorporate domain knowledge into the model’s structure.

Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ 1a).
Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ We seek a model, parameterized by 0, such that the (unknown) correct analysis yi is the best analysis for xi (under the model).
Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ The LENGTH neighborhood is as close to loglinear EM as it is practical to get.

We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ When the vocabulary E is the set of words in a natural language, it is never fully known; approximations for defining LENGTH = Em include using observed E from the training set (as we do) or adding a special OOV symbol.
We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ The parameter vector B E Rn specifies a weight for each of the n transitions in the automaton. y is a hidden path through the automaton (determining a POS sequence), and x is the string it emits. u(x, y  |0) is defined by applying exp to the total weight of all transitions in y.
We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ This is good for the LENGTH objective, but not for learning good POS tag sequences.

 $$$$$ As in EM, p(xi ..., 0) is found by marginalizing over hidden variables (Eq.
 $$$$$ One can envision a mixed objective function that tries to fit the labeled examples while discriminating unlabeled examples from their neighborhoods.8 Regardless of how much (if any) data are labeled, the question of good smoothing techniques requires more attention.
 $$$$$ Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).
 $$$$$ 1), but yi is unknown, so none apply.

Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ When the vocabulary E is the set of words in a natural language, it is never fully known; approximations for defining LENGTH = Em include using observed E from the training set (as we do) or adding a special OOV symbol.
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ 1) are available.
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ This 8Zhu and Ghahramani (2002) explored the semi-supervised classification problem for spatially-distributed data, where some data are labeled, using a Boltzmann machine to model the dataset.

The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ We seek a model, parameterized by 0, such that the (unknown) correct analysis yi is the best analysis for xi (under the model).
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic.
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).

First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ If yz were observed, a variety of training criteria would be available (see Tab.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ CRFs allowing the incorporation of arbifeatures into the model.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ 3) are not comparable to each other because each is measured on a different set of ambiguous words. leads to a need for more efficient tuning of the prior parameters on development data.

 $$$$$ Note that the different plots are not comparable, because their y-axes are based on different sets of ambiguous words.
 $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.
 $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
 $$$$$ We must consider not only where the learner pushes the mass, but also from where the mass is taken.

System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ Put another way, the learner assumes not only that xi is good, but that xi is locally optimal in example space (X), and that alternative, similar examples (from the neighborhood) are inferior.
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ We have not yet specified the form of our probabilistic model, only that it is parameterized by B� E Rn.
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ All the objective functions in this paper take the form where Ai C Bi (for each i).
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).

Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ One might do the same for the local maxima of any CE objective, though theoretical and experimental support for this idea remain for future work.
Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ Good neighborhoods, rather, perform well in their own right.
Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ The improvement from adding spelling features is striking: DELORTRANS1 and TRANS1 recover nearly completely (modulo the model selection problem) from the diluted dictionaries.

Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ Log-linear models can be so crafted and have already achieved excellent performance when trained on annotated data, where they are known as “maximum entropy” models (Ratnaparkhi et al., 1994; Rosenfeld, 1994).
Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ Effectiveness of the approach on POS tagging using unlabeled data is demonstrated (§5).
Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.

The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ CRFs allowing the incorporation of arbifeatures into the model.
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic.
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.

Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ We must consider not only where the learner pushes the mass, but also from where the mass is taken.
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ The parameter vector B E Rn specifies a weight for each of the n transitions in the automaton. y is a hidden path through the automaton (determining a POS sequence), and x is the string it emits. u(x, y  |0) is defined by applying exp to the total weight of all transitions in y.
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ We discuss future work (§6) and conclude (§7).
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ For them, the Markov random field is over labeling configurations for all examples, not, as in our case, complex structured labels for a particular example.

One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ We must consider not only where the learner pushes the mass, but also from where the mass is taken.
One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.
One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ Effectiveness of the approach on POS tagging using unlabeled data is demonstrated (§5).

This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ CE hypothesizes that each positive example in training implies a domain-specific set of examples which are (for the most part) degraded (§2).
This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ For this corpus and tag set, on average, a tagger must decide between 2.3 tags for a given token.
This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ One might do the same for the local maxima of any CE objective, though theoretical and experimental support for this idea remain for future work.
