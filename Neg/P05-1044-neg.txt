Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ Hence their B (Eq.
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ In all cases, training stopped when the relative change in the criterion fell below 10−4 between steps (typically G 100 steps).
Recent work by Smith and Eisner (2005) on contrastive estimation suggests similar techniques to generate local neighborhoods of a parse; however, the purpose in their work is to define an approximation to the partition function for log-linear estimation (i.e., the normalization factor in a MaxEnt model). $$$$$ We have presented contrastive estimation, a new probabilistic estimation criterion that forces a model to explain why the given training data were better than bad data implied by the positive examples.

Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ To check this intuition, we built local normalized models p(word I tag) from the parameters learned by TRANS1 and LENGTH.
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ 5), though very large, was finite and could be sampled. with spelling features.
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ We next consider some non-classical neighborhood functions for sequences.
Smith and Eisner (2005a; 2005b) generate negative evidence for their contrastive estimation method by moving or removing a word in a sentence. $$$$$ Models selected using unlabeled development data are circled.

 $$$$$ DEL1SUBSEQ and DEL1WORD are poor because they do not give helpful classes of negative evidence: deleting a word or a short subsequence often does very little damage.
 $$$$$ Of course, the validity of this hypothesis will depend on the form of the neighborhood function.

We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). $$$$$ In this paper, we describe an alternative to EM: contrastive estimation (CE), which (unlike EM) explicitly states the source of the probability mass that is to be given to an example.1 One reason is to make normalization efficient.
We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). $$$$$ We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.

Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).
Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ To check this intuition, we built local normalized models p(word I tag) from the parameters learned by TRANS1 and LENGTH.
Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ Alternatives to exact com2These are exemplified by CRFs (Lafferty et al., 2001), which can be viewed alternately as undirected dynamic graphical models with a chain topology, as log-linear models over entire sequences with local features, or as WFSAs.
Smith and Eisner (2005) initialized with all weights equal to zero (uninformed, deterministic initialization) and performed unsupervised model selection across smoothing parameters by evaluating the training criterion on unseen, unlabeled development data. $$$$$ A final neighborhood we will consider is LENGTH, which consists of Em.

The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ In this paper, we describe an alternative to EM: contrastive estimation (CE), which (unlike EM) explicitly states the source of the probability mass that is to be given to an example.1 One reason is to make normalization efficient.
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
The settings of the various experiments vary in terms of the exact gold annotation scheme used for evaluation (the full WSJ set was used by all authors except Goldwater and Griffiths (2007) and the GGTP-17 model which used the set of 17coarse grained tags proposed by (Smith and Eisner, 2005)) and the size of the test set. $$$$$ CE with the LENGTH neighborhood is very similar to EM; it is equivalent to using EM to estimate the parameters of a model defined by Eq.

Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ The effectiveness of CE (and different neighborhoods) for dependency grammar induction is explored in Smith and Eisner (2005) with considerable success.
Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ We discuss future work (§6) and conclude (§7).
Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ A final neighborhood we will consider is LENGTH, which consists of Em.
Evaluation was done against the POS-tag annotations of the 45-tag PTB tag set (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tag set (hereafter PTB17). $$$$$ Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).

We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).
We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ In this paper, the expectations in Eq.
We follow this method, but also attempt to identify negative examples that are semantically similar to the positive ones in order to improve the discriminative power of the classifier (Smith and Eisner, 2005). $$$$$ We must consider not only where the learner pushes the mass, but also from where the mass is taken.

 $$$$$ In this paper, we describe an alternative to EM: contrastive estimation (CE), which (unlike EM) explicitly states the source of the probability mass that is to be given to an example.1 One reason is to make normalization efficient.
 $$$$$ Models selected using unlabeled development data are circled.
 $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
 $$$$$ This class of implicit negative evidence provides the source of probability mass for the observed example.

Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ Why is this?
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ To check this intuition, we built local normalized models p(word I tag) from the parameters learned by TRANS1 and LENGTH.
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ 1), but yi is unknown, so none apply.
Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. $$$$$ In contrast, DELORTRANS1 and TRANS1 do not allow the learner to manipulate emission weights for words not in the sentence.

The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ An often-used alternative to EM is a class of socalled Viterbi approximations, which iteratively find the probabilistically-best y� and then, on each iteration, solve a supervised problem (see Tab.
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic.
The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. $$$$$ When the vocabulary E is the set of words in a natural language, it is never fully known; approximations for defining LENGTH = Em include using observed E from the training set (as we do) or adding a special OOV symbol.

First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ When X = E+ for some symbol alphabet E, certain kinds of neighborhoods have natural, compact representations.
First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). $$$$$ 1).

 $$$$$ 7), we apply a standard numerical optimization method (L-BFGS) that iteratively climbs the function using knowledge of its value and gradient (Liu and Nocedal, 1989).
 $$$$$ While a POS lexicon might be available for a new language, certainly it will not give exhaustive information about all word types in a corpus.

System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ For a k-state WFSA, this equates to solving a linear system of k equations in k variables (Tarjan, 1981).
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ 4 and 6 where fj(x, y) is the number of times the path y takes the jth transition.
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999)). $$$$$ CE with lattice neighborhoods is not confined to the WFSAs of this paper; when estimating weighted CFGs, the key algorithm is the inside algorithm for lattice parsing (Smith and Eisner, 2005).

Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ Riezler (1999) did so, then resorted to an approximation because the true objective function was hard to normalize.
Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ Given an input string x = (x1, x2, ..., xm), we write xji for the substring (xi, xi+1, ..., xj) and xm1 for the whole string.
Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
Smith and Eisner (2005) design a contrastive estimation technique which yields a higher accuracy of 88.6%. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).

Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM’s criterion (Smith and Eisner, 2004).
Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ Riezler (1999) did so, then resorted to an approximation because the true objective function was hard to normalize.
Contrastive estimation (CE) (Smith and Eisner, 2005a) is another log-linear framework for primarily unsupervised structured prediction. $$$$$ We have presented contrastive estimation, a new probabilistic estimation criterion that forces a model to explain why the given training data were better than bad data implied by the positive examples.

The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ CE hypothesizes that each positive example in training implies a domain-specific set of examples which are (for the most part) degraded (§2).
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ Consider, as a concrete example, learning natural language syntax.
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ Each graph corresponds to a different level of dilution.
The model has been shown to work in unsupervised tasks such as POS induction (Smith and Eisner, 2005a), grammar induction (Smith and Eisner, 2005b), and morphological segmentation (Poon et al, 2009), where good neighborhoods can be identified. $$$$$ 4 and 6 where fj(x, y) is the number of times the path y takes the jth transition.

Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ We seek a model, parameterized by 0, such that the (unknown) correct analysis yi is the best analysis for xi (under the model).
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features—that EM can’t—to largely recover from the loss of knowledge.
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ Then if q is any fixed (untrained) distribution over neighborhoods, CE equates to running EM on the model defined by CE may also be viewed as an importance sampling approximation to EM, where the sample space X is replaced by N(xi).
Smith and Eisner (2005) use neighborhoods of related instances to figure out what makes found instances good. $$$$$ For log-linear models, both CL and JL estimation (Tab.

One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ Finding linguistic structure in raw text is not easy.
One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ 3) are not comparable to each other because each is measured on a different set of ambiguous words. leads to a need for more efficient tuning of the prior parameters on development data.
One example of the kind of operator used is the transposition operator proposed by Smith and Eisner (2005). $$$$$ An often-used alternative to EM is a class of socalled Viterbi approximations, which iteratively find the probabilistically-best y� and then, on each iteration, solve a supervised problem (see Tab.

This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ This could indicate that the learner was trapped in a local maximum, suggesting that, since other criteria did not exhibit this behavior, LENGTH might be a bumpier objective surface.
This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ This lattice is similar to that of DEL1WORD, but adds some arcs (Fig.
This shares the same form as the contrastive estimation proposed by (Smith and Eisner, 2005). $$$$$ Hence their B (Eq.
