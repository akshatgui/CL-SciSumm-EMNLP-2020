Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. $$$$$ The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parser’s model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art.
Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. $$$$$ Recall the standard MaxEnt conditional probability model for a parse y E Y: The loss function LD proposed in Riezler et al. (2002) is just the negative log conditional likelihood of the best parses Y+(s) relative to the n-best parser output Y(s): The partial derivatives of this loss function, which are required by the numerical estimation procedure, are: In the experiments reported here, we used a Gaussian or quadratic regularizer R(w) = cPmj=1 w2j, where c is an adjustable parameter that controls the amount of regularization, chosen to optimize the reranker’s f-score on the development set (section 24 of the treebank).
Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. $$$$$ (s1), ... , y?(sn).
Niu et al (2009) also use the reranker (RP) of Charniak and Johnson (2005) as a stronger baseline, but the results are missing. $$$$$ However, turning off dynamic programming results in a loss in efficiency.

Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. $$$$$ This method generates 50-best lists that are of substantially higher quality than previously obtainable.
Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. $$$$$ (s1), ... , y?(sn).
Features extracted from the output of three probabilistic parsers of English (Charniak and Johnson, 2005), one trained on Wall Street Journal trees (Marcus et al, 1993), one trained on a distorted version of the tree bank obtained by automatically creating grammatical error and adjusting the parse trees, and the third trained on the union of the original and distorted versions. $$$$$ We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002).

Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). $$$$$ Dynamic programming parsing algorithms for PCFGs require O(m2) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2).
Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). $$$$$ The reranker selects the best parse from this set of parses using a wide variety of features.
Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). $$$$$ Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient.
Reranking has been used in many tasks to find better global solutions, such as machine translation (Wang et al, 2007), parsing (Charniak and Johnson, 2005), and disfluency detection (Zwarts and Johnson, 2011). $$$$$ We use a MaxEnt estimator to find the feature weights ˆ0, where L is the loss function and R is a regularization penalty term: The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?

 $$$$$ We use a MaxEnt estimator to find the feature weights ˆ0, where L is the loss function and R is a regularization penalty term: The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?
 $$$$$ We n-best trees f-score New 0.9102 Collins 0.9037 best trees, with weights estimated from sections 2– 21 and the regularizer constant c adjusted for optimal f-score on section 24 and evaluated on sentences of length less than 100 in section 23. trained the n-best parser on sections 2–21 of the Penn Treebank, and used section 24 as development data to tune the mixing parameters of the smoothing model.
 $$$$$ We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002).
 $$$$$ The results are presented in Table 3.

 $$$$$ Dynamic programming parsing algorithms for PCFGs require O(m2) dynamic programming states, where m is the length of the sentence, so an n-best parsing algorithm requires O(nm2).
 $$$$$ A feature extractor, described in section 3, is a vector of m functions f = (fl, ... , fm), where each fj maps a parse y to a real number fj(y), which is the value of the jth feature on y.

We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). $$$$$ We evaluated the performance of our reranking parser using the standard PARSEVAL metrics.
We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). $$$$$ This schema was inspired by a similar schema in Collins and Koo (in submission).
We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). $$$$$ A feature extractor, described in section 3, is a vector of m functions f = (fl, ... , fm), where each fj maps a parse y to a real number fj(y), which is the value of the jth feature on y.
We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). $$$$$ The reranker selects the best parse from this set of parses using a wide variety of features.

This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. $$$$$ Now comes the easy part.
This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. $$$$$ Heads (208,599) The instances of this schema are tuples of head-to-head dependencies, as mentioned above.
This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. $$$$$ Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).
This is similar to the pruning described in Charniak and Johnson (2005) where edges in a coarse-grained parse forest are pruned to allow full evaluation with fine grained categories. $$$$$ The number n of parses was set to 50 for the experiments described here, but some simple sentences actually received fewer than 50 parses (so n is actually a function of s).

 $$$$$ We n-best trees f-score New 0.9102 Collins 0.9037 best trees, with weights estimated from sections 2– 21 and the regularizer constant c adjusted for optimal f-score on section 24 and evaluated on sentences of length less than 100 in section 23. trained the n-best parser on sections 2–21 of the Penn Treebank, and used section 24 as development data to tune the mixing parameters of the smoothing model.
 $$$$$ The n-best parser’s most probable parses are already of state-of-the-art quality, but the reranker further improves the f-score.
 $$$$$ More to the point, however, is that the system we describe is reasonably efficient so it can be used for the kind of routine parsing currently being handled by the Charniak or Collins parsers.

Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. $$$$$ The algorithm was originally described for hidden Markov models.
Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. $$$$$ The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.
Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. $$$$$ From the 1-best result we see that the base accuracy of the parser is 89.7%.1 2-best and 10-best show dramatic oracle-rate improvements.
Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. $$$$$ We use a MaxEnt estimator to find the feature weights ˆ0, where L is the loss function and R is a regularization penalty term: The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?

We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively. $$$$$ As noted above, the fine-grained model conditions on information that is not available in the coarse-grained model.
We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively. $$$$$ (s) for each sentence s in the training data.
We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively. $$$$$ This schema was inspired by a similar schema in Collins and Koo (in submission).
We experimented with three scenarios; in two of them we trained using the gold standard trees and then tested on gold standard parse trees (GoldGold), and text annotated using a state-of-the-art statistical parser (Charniak and Johnson, 2005) (Gold Charniak), respectively. $$$$$ Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper.

 $$$$$ We use a MaxEnt estimator to find the feature weights ˆ0, where L is the loss function and R is a regularization penalty term: The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?
 $$$$$ This section explains how we estimate the feature weights 0 = (01, ... , 0m) for the feature functions f = (f1, ... , fm).
 $$$$$ We evaluated the performance of our reranking parser using the standard PARSEVAL metrics.

Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser. $$$$$ In the experiments described below t = 5, though we also experimented with t = 2.
Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser. $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser. $$$$$ Finally thanks to the National Science Foundation for its support (NSF IIS-0112432, NSF 9721276, and NSF DMS-0074276).
Charniak and Johnson (2005) showed accuracy improvements from composed local tree features on top of a lexicalized base parser. $$$$$ A prime example of this idea is from Goodman (1997), who describes amethod for producing a simple but crude approximate grammar of a standard context-free grammar.

 $$$$$ Our reranking parser associates a parse with a score vθ(y), which is a linear function of the feature values f(y).
 $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.
 $$$$$ We did this both on the trees supplied to us by Michael Collins, and on the output of the n-best parser described in this paper.
 $$$$$ We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less.

We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. $$$$$ We list each feature schema’s name, followed by the number of features in that schema with a count of at least 5, together with a brief description of the instances of the schema and the schema’s parameters.
We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. $$$$$ The reranker selects the best parse from this set of parses using a wide variety of features.
We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. $$$$$ Because the coarse-to-fine approach prunes the set of possible parse edges beforehand, a simple approach which enumerates the n-best analyses of each parse edge is not only practical but quite efficient.
We adapt the maximum entropy reranker from Charniak and Johnson (2005) by creating a customized feature extractor for event structures - in all other ways, the reranker model is unchanged. $$$$$ We evaluated the performance of our reranking parser using the standard PARSEVAL metrics.

To improve performance and robustness, features are pruned as in Charniak and Johnson (2005) $$$$$ Acknowledgements We would like to thanks Michael Collins for the use of his data and many helpful comments, and Liang Huang for providing an early draft of his paper and very useful comments on our paper.
To improve performance and robustness, features are pruned as in Charniak and Johnson (2005) $$$$$ The parser then removes all constituents nij,k whose probability falls below some preset threshold.
To improve performance and robustness, features are pruned as in Charniak and Johnson (2005) $$$$$ We used the Limited Memory Variable Metric optimization algorithm from the PETSc/TAO optimization toolkit (Benson et al., 2004) to find the optimal feature weights θˆ because this method seems substantially faster than comparable methods (Malouf, 2002).
To improve performance and robustness, features are pruned as in Charniak and Johnson (2005) $$$$$ We use a MaxEnt estimator to find the feature weights ˆ0, where L is the loss function and R is a regularization penalty term: The training data D = (s1, ... , sn,) is a sequence of sentences and their correct parses y?

To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. $$$$$ This result is only slightly higher than the highest reported result for this test-set, Bod’s (.907) (Bod, 2003).
To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. $$$$$ (s1), ... , y?(sn).
To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. $$$$$ At the end one has a beam-width’s number of best parses (Roark, 2001).
To train the classifiers, we used parse trees from the Charniak and Johnson (2005) parser with the same feature representation as in the original system. $$$$$ Further, the nth-best parse can only involve at most n suboptimal parsing decisions, and all but one of these must be involved in one of the second through the n−1th-best parses.

Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). $$$$$ (The results are for all sentences of section 23 of the WSJ tree-bank of length ≤ 100.)
Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). $$$$$ This method generates 50-best lists that are of substantially higher quality than previously obtainable.
Other training algorithms include perceptron-style algorithms (Liang et al, 2006), MaxEnt (Charniak and Johnson, 2005), and boosting variants (Kudo et al, 2005). $$$$$ The system we described here has an f-score of 0.91 when trained and tested using the standard PARSEVAL framework.

We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). $$$$$ (s1), ... , y?(sn).
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). $$$$$ This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000).
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and HPSG parser (Miyao and Tsujii, 2005). $$$$$ Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).

Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. $$$$$ Heads (208,599) The instances of this schema are tuples of head-to-head dependencies, as mentioned above.
Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. $$$$$ Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000).
Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. $$$$$ The other features are integer valued; informally, each feature is associated with a particular configuration, and the feature’s value fj(y) is the number of times that the configuration that fj indicates.
Given a tree pair (f, c), whose respective parses (pif ,pic) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. $$$$$ A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning.

The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. $$$$$ We did this both on the trees supplied to us by Michael Collins, and on the output of the n-best parser described in this paper.
The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. $$$$$ This result is only slightly higher than the highest reported result for this test-set, Bod’s (.907) (Bod, 2003).
The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. $$$$$ The answer seems to be yes.
