In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. $$$$$ We partition the corpus into different genres, defined as being roughly identical to corpus source.
In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. $$$$$ In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.
In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. $$$$$ We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
In this respect our approach is similar to that of Foster and Kuhn (2007), however we used a probabilistic classifier to determine a vector of probabilities representing class-membership, rather than distance based weights. $$$$$ Set mixture weights as a function of the distances from corpus components to the current source text.

Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. $$$$$ The results of the final experiment, to determine the effects of source granularity on dynamic adaptation, are shown in table 8.
Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. $$$$$ However, it is not necessary for cross-domain adaptation, where the genre of the development corpus is assumed to represent the test domain.
Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. $$$$$ Our approach to mixture-model adaptation can be summarized by the following general algorithm: from several different domains.
Both Yamamoto and Sumita (2007) and Foster and Kuhn (2007), extended this to include the translation model. $$$$$ Ueffing (2006) describes a self-training approach that also uses a two-pass algorithm.

Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. $$$$$ To derive the joint counts c(g, t) from which p(s|� and p(�t|s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).
Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. $$$$$ Given a source sentence s, this tries to find the target sentence t� that is the most likely translation of s, using the Viterbi approximation: where alignment a = (91, �t1, j1), ..., (sK, tK, jK); tk are target phrases such that t = �t1 ... tK; sk are source phrases such that s = gj1 ... sjK; and 9k is the translation of the kth target phrase tk.
Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. $$$$$ A more sophisticated approach that attempts to transform and combine multiple distance metrics did not yield positive results, probably due to an unsucessful optmization procedure.
Early efforts focus on building separate models (Foster and Kuhn, 2007) and adding features (Matsoukas et al, 2009) to model domain information. $$$$$ Our baseline is a standard phrase-based SMT system (Koehn et al., 2003).

Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). $$$$$ We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). $$$$$ Table 4 shows the performance of the parameterized weighting function described by (4), with source-side EM and LSA metrics as inputs.
Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). $$$$$ A more sophisticated approach that attempts to transform and combine multiple distance metrics did not yield positive results, probably due to an unsucessful optmization procedure.
Besides many works addressing holistic LM domain adaptation for SMT, e.g. Foster and Kuhn (2007), recently methods were also proposed to explicitly adapt the LM to the discourse topic of a talk (Ruiz and Federico, 2011). $$$$$ Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments.

Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. $$$$$ This is compared to direct weight optimization, as both these techniques use Downhill Simplex for parameter tuning.
Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. $$$$$ The most commonly-used framework for mixture models is a linear one: where p(x|h) is either a language or translation model; pc(x|h) is a model trained on component c, and λc is the corresponding weight.
Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. $$$$$ This is especially the case for bilingual applications, because parallel training corpora are relatively rare and tend to be drawn from specific domains such as parliamentary proceedings.
Both we restudied in (Foster and Kuhn, 2007), which concluded that the best approach was to combine submodels of the same type (for instance, several different TMs or several different LMs) linearly, while combining models of different types (for instance, a mixture TM with a mixture LM) log linearly. $$$$$ The perTable 7 shows results for the hybrid approach described at the end of section 3.5.2: global weights are learned on NIST04-nw, but linear weights are derived dynamically from the current test file.

Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. $$$$$ Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters.
Thus, the variant of VSM adaptation tested here bears a superficial resemblance to domain adaptation based on mixture models for TMs, as in (Foster and Kuhn, 2007), in that both approaches rely on information about the subcorpora from which the data originate. $$$$$ This is quite surprising, because these results are on the development set: the loglinear model tunes its component weights on this set, whereas the linear model only adjusts global LM and TM weights.

For details, refer to (Foster and Kuhn, 2007). $$$$$ We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
For details, refer to (Foster and Kuhn, 2007). $$$$$ Set mixture weights as a function of the distances from corpus components to the current source text.
For details, refer to (Foster and Kuhn, 2007). $$$$$ Approaches developed for the two settings can be complementary: an in-domain development corpus can be used to make broad adjustments, which can then be fine tuned for individual source texts.
For details, refer to (Foster and Kuhn, 2007). $$$$$ Combine weighted component models into a single global model, and use it to translate as described in the previous section.

In (Foster and Kuhn, 2007), two kinds of linear mixture were described $$$$$ The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
In (Foster and Kuhn, 2007), two kinds of linear mixture were described $$$$$ We use two different estimates for the conditional probabilities p(�t|g) and p(g|�t): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004).
In (Foster and Kuhn, 2007), two kinds of linear mixture were described $$$$$ In cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly.

In (Foster and Kuhn, 2007) two basic settings are investigated $$$$$ A baseline system generates translations that, after confidence filtering, are used to construct a parallel corpus based on the test set.
In (Foster and Kuhn, 2007) two basic settings are investigated $$$$$ We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
In (Foster and Kuhn, 2007) two basic settings are investigated $$$$$ Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments.

Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. $$$$$ We have investigated a number of approaches to mixture-based adaptation using genres for Chinese to English translation.
Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. $$$$$ We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components.
Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. $$$$$ In both cases, the “forward” phrase probabilities p(�t|s) are not used as features, but only as a filter on the set of possible translations: for each source phrase s� that matches some ngram in s, only the 30 top-ranked translations t according to p(�t|g) are retained.
Although dynamic adaptation is closely related to static domain adaptation (Foster and Kuhn, 2007), in this scenario we are not interested in the quality of the final model. $$$$$ We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.

Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice. $$$$$ We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice. $$$$$ Given a source sentence s, this tries to find the target sentence t� that is the most likely translation of s, using the Viterbi approximation: where alignment a = (91, �t1, j1), ..., (sK, tK, jK); tk are target phrases such that t = �t1 ... tK; sk are source phrases such that s = gj1 ... sjK; and 9k is the translation of the kth target phrase tk.
Foster and Kuhn (2007) interpolated the in and general-domain phrase tables together, assigning either linear or log-linear weights to the entries in the tables before combining overlapping entries; this is now standard practice. $$$$$ The remainder of the paper is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes.

In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1. $$$$$ To model p(t, a|s), we use a standard loglinear approach: where each fi(s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus.
In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1. $$$$$ We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
In this work, we directly compare the approaches of (Foster and Kuhn, 2007) and (Koehn and Schroeder, 2007) on the systems generated from the methods mentioned in Section 2.1. $$$$$ Each source sentence is then decoded using the language model trained on the cluster that assigns highest likelihood to that sentence.

(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. $$$$$ Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample).
(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. $$$$$ We have investigated a number of approaches to mixture-based adaptation using genres for Chinese to English translation.
(Foster and Kuhn, 2007) applied a mixture model approach to adapt the system to a new domain by using weights that depend on text distances to mixture components. $$$$$ The work we present here is complementary to both the IR approaches and Ueffing’s method because it provides a way of exploiting a preestablished corpus division.

Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). $$$$$ A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation.
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). $$$$$ We use two different estimates for the conditional probabilities p(�t|g) and p(g|�t): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004).
Specifically, we will interpolate the translation models as in Foster and Kuhn (2007), including a maximum a posteriori combination (Bacchiani et al 2006). $$$$$ The most successful is to weight component models in proportion to maximum-likelihood (EM) weights for the current text given an ngram language model mixture trained on corpus components.

Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. $$$$$ The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. $$$$$ We use two different estimates for the conditional probabilities p(�t|g) and p(g|�t): relative frequencies and “lexical” probabilities as described in (Zens and Ney, 2004).
Foster and Kuhn (2007) presented an approach that resembles more to our work, in which they divided the training corpus into different components and integrated models trained on each component using the mixture modeling. $$$$$ A simple approach to weighting is to choose a single metric Di, and set the weights in (2) to be proportional to the corresponding distances: Because different distance metrics may capture complementary information, and because optimal weights might be a non-linear function of distance, we also experimented with a linear combination of metrics transformed using a sigmoid function: where Qi reflects the relative predictive power of Di, and the sigmoid parametes ai and bi can be set to selectively suppress contributions from components that are far away.

Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). $$$$$ In general, these parameters are also specific to the particular model being adapted, ie the LM or the TM.
Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). $$$$$ The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
Among other applications, language model perplexity has been used for domain adaptation (Foster and Kuhn, 2007). $$$$$ The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.

Mixture-modelling for language models is well established (Foster and Kuhn, 2007). $$$$$ A second contribution is a fairly broad investigation of the large space of alternatives defined by the mixture-modeling framework, using a simple genrebased corpus decomposition.
Mixture-modelling for language models is well established (Foster and Kuhn, 2007). $$$$$ We assume two basic settings.
Mixture-modelling for language models is well established (Foster and Kuhn, 2007). $$$$$ Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments.
Mixture-modelling for language models is well established (Foster and Kuhn, 2007). $$$$$ We will also look at bilingual metrics for crossdomain adaptation, and investigate better combinations of cross-domain and dynamic adaptation.

Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. $$$$$ Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample).
Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. $$$$$ Other conclusions are: linear mixtures are more tractable than loglinear ones; LM-based metrics are better than VS-based ones; LM adaptation works well, and adding an adapted TM yields no improvement; cross-domain adaptation is optimal, but dynamic adaptation is a good fallback strategy; and source granularity at the genre level is better than the document or test-set level.
Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. $$$$$ To model p(t, a|s), we use a standard loglinear approach: where each fi(s, t, a) is a feature function, and weights αi are set using Och’s algorithm (Och, 2003) to maximize the system’s BLEU score (Papineni et al., 2001) on a development corpus.
Foster and Kuhn (2007) find that both TM and LM adaptation are effective, but that combined LM and TM adaptation is not better than LM adaptation on its own. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. $$$$$ A baseline system generates translations that, after confidence filtering, are used to construct a parallel corpus based on the test set.

Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ Table 5 shows results for cross-domain adaptation, using the source-side EM metric for linear weighting.
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ Perplexity (Jelinek, 1997) is a standard way of evaluating the quality of a language model on a test text.
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ Our baseline is a standard phrase-based SMT system (Koehn et al., 2003).

They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). $$$$$ We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). $$$$$ Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters.
They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). $$$$$ The most successful is to weight component models in proportion to maximum-likelihood (EM) weights for the current text given an ngram language model mixture trained on corpus components.
They used sampling without replacement to create a number of base models whose phrase-tables are combined with that of the baseline (trained on the full training-set) using linear mixture models (Foster and Kuhn, 2007). $$$$$ Phrase translation model probabilities are features of the form: log p(s|t,a) ≈ EKk=1 log p(gk|�tk).
