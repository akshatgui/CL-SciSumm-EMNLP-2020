USR is the weakly supervised system of Naseem et al (2010). $$$$$ Our experiments showed that encoding a compact, well-accepted set of language-independent constraints significantly improves accuracy on multiple languages compared to the current state-of-the-art in unsupervised parsing.
USR is the weakly supervised system of Naseem et al (2010). $$$$$ As shown by Grac¸a et al. (2007), the update in (3) is a constrained optimization problem and can be solved by performing gradient search on its dual: For a fixed value of A the optimal q(z) ∝ q'(z) exp(−ATf(z)).
USR is the weakly supervised system of Naseem et al (2010). $$$$$ For each tree node i generated in context c by parent symbol s' and parent subsymbol z': and parses.
USR is the weakly supervised system of Naseem et al (2010). $$$$$ Specifically, for a single s, each distribution πss0z0c over subsymbols is drawn from a DP with concentration parameter α and base distribution βs over subsymbols.

 $$$$$ In this paper we demonstrated that syntactic universals encoded as declarative constraints improve grammar induction.
 $$$$$ Furthermore, these universal rules are compact and well-understood, making them easy to manually construct.
 $$$$$ Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages.
 $$$$$ Generating the Tree Structure We now consider how the structure of the tree arises.

Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ Across six languages our approach outperforms state-of-theart unsupervised methods by a significant mar
Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ Recall from the previous section that the update for q(z) is performed via gradient search on the dual of a constrained minimization problem of the form: where s0 varies over the set of unique symbols in the observed tags, z0 denotes subsymbols for each symbol, c varies over context values comprising a pair of direction (left or right) and valence (first, second, or third or higher) values, and s corresponds to child symbols.
Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ We also automatically refine the syntactic categories given in our coarsely tagged input.

The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ Each node of the dependency tree is comprised of three random variables: an observed coarse symbol s, a hidden refined subsymbol z, and an observed word x.
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ In this paper we demonstrated that syntactic universals encoded as declarative constraints improve grammar induction.
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ In this paper, we present an alternative grammar induction approach that exploits these structural correspondences by declaratively encoding a small set of universal dependency rules.
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ We address this challenge by requiring the universal constraints to only hold in expectation rather than absolutely, i.e., we permit a certain number of violations of the constraints.

 $$$$$ Thanks to Taylor Berg-Kirkpatrick, Sabine Iatridou, Ramesh Sridharan, and members of the MIT NLP group for their suggestions and comments.
 $$$$$ These results are summarized in Table 6 and discussed in detail below; line numbers refer to entries in Table 6.
 $$$$$ We also automatically refine the syntactic categories given in our coarsely tagged input.
 $$$$$ Thanks to Taylor Berg-Kirkpatrick, Sabine Iatridou, Ramesh Sridharan, and members of the MIT NLP group for their suggestions and comments.

Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ More closely related to our work is the position paper by Bender (2009), which advocates the use of manually-encoded cross-lingual generalizations for the development of NLP systems.
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ We test the effectiveness of our grammar induction model on six Indo-European languages from three language groups: English, Danish, Portuguese, Slovene, Spanish, and Swedish.
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ These results demonstrate that a single threshold is broadly applicable across languages.
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ Across six languages our approach outperforms state-of-theart unsupervised methods by a significant mar

In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ A more detailed discussion of the threshold’s empirical impact is presented in Section 7.1.
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ We also provide the performance of two baselines — the dependency model with valence (DMV) (Klein and Manning, 2004) and the phylogenetic grammar induction (PGI) model (Berg-Kirkpatrick and Klein, 2010).
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ This setup is more relevant for learning with universals since individual rule frequencies vary greatly between languages.
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ This objective expresses constraints in the form of preferences over model expectations.

We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ In contrast to prior work on rule-driven dependency induction (Druck et al., 2009), where each rule has a separately specified expectation, we only set a single minimum expectation for the proportion of all dependencies that must match one of the rules.
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ Our model can also optionally refine common high-level syntactic categories into per-language categories by inducing a clustering of words using Dirichlet Processes (Ferguson, 1973).
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ These rules are used as expectation constraints on the posterior distribution over dependency structures.
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ We address this challenge by requiring the universal constraints to only hold in expectation rather than absolutely, i.e., we permit a certain number of violations of the constraints.

 $$$$$ The final output metric is directed dependency accuracy.
 $$$$$ However, our goal is to apply category refinement to dependency parsing, rather than to PCFGs, requiring a substantially different model formulation.
 $$$$$ For English we use the Penn Treebank (Marcus et al., 1993), transformed from CFG parses into dependencies with the Collins head finding rules (Collins, 1999); for the other languages we use data from the 2006 CoNLL-X Shared Task (Buchholz and Marsi, 2006).

Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. $$$$$ While our present work has yielded substantial gains over previous unsupervised methods, a large gap still remains between our method and fully supervised techniques.

We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. $$$$$ Our method uses a single set of manually-specified language-independent rules that identify syntactic dependencies between pairs of syntactic categories that commonly occur across languages.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. $$$$$ As with prior work (Liang et al., 2009b), we assume a degenerate q(β) - δ0∗(β) for tractability reasons, i.e., all mass is concentrated on some single β∗.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. $$$$$ We address this challenge by requiring the universal constraints to only hold in expectation rather than absolutely, i.e., we permit a certain number of violations of the constraints.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered: USR: The weakly supervised system of Naseem et al (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. $$$$$ During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.

For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ Similarly, the updates for q(θ) and q(φ) are given by: where Cs0z0c(s) is the count of child symbol s being generated by the parent symbol s0 and subsymbol z0 in context c and Cs0z0x is the count of word x being generated by symbol s0 and subsymbol z0. where N is the total number of sentences, len(n) is the length of sentence n, and index h(nj) refers to the head of the jth node of sentence n. Given this q0(z) a gradient search is performed using (6) to find the optimal λ and thus the primal solution for updating q(z).
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ In addition to these universal dependencies, each specific language typically possesses its own idiosyncratic set of dependencies.
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ We now describe how to augment our generative model of dependency structure with constraints derived from linguistic knowledge.
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0904684, and a Graduate Research Fellowship).

Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ We also automatically refine the syntactic categories given in our coarsely tagged input.
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ This objective expresses constraints in the form of preferences over model expectations.
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ Variational inference transforms this problem into an optimization problem where we try to find a distribution q(B, z) from a restricted set Q that minimizes the KL-divergence between q(B, z) and p(B, z  |x): KL(q(B, z) k p(B, z  |x)) Thus F is a lower bound on likelihood.

The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ The objective is penalized by the square distance between model expectations and the prespecified values of the expectation.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ Instead, following the approach of Grac¸a et al. (2007), we constrain the posterior to satisfy the rules in expectation during inference.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ To make this maximization tractable we make a mean field assumption that q belongs to a set Q of distributions that factorize as follows: We further constrain q to be from the subset of Q that satisfies the expectation constraint Eq[f(z)] ≤ b where f is a deterministically computable function of the hidden structures.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ In addition to these universal dependencies, each specific language typically possesses its own idiosyncratic set of dependencies.

 $$$$$ In contrast to prior work on rule-driven dependency induction (Druck et al., 2009), where each rule has a separately specified expectation, we only set a single minimum expectation for the proportion of all dependencies that must match one of the rules.
 $$$$$ Across six languages our approach outperforms state-of-theart unsupervised methods by a significant mar
 $$$$$ Finally, we update the degenerate factor q(βs) with the projected gradient search algorithm used by Liang et al. (2009b).
 $$$$$ Though these languages share a high-level Indo-European ancestry, they cover a diverse range of syntactic phenomenon.

Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ Our experiments showed that encoding a compact, well-accepted set of language-independent constraints significantly improves accuracy on multiple languages compared to the current state-of-the-art in unsupervised parsing.
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ We test the effectiveness of our grammar induction model on six Indo-European languages from three language groups: English, Danish, Portuguese, Slovene, Spanish, and Swedish.
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ English-specific Dependency Rules For English, we also consider a small set of hand-crafted dependency rules designed by Michael Collins3 for deterministic parsing, shown in Table 3.
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ An additional point of comparison is the lexicalized unsupervised parser of Headden III et al. (2009), which yields the current state-of-the-art unsupervised accuracy on English at 68.8%.

 $$$$$ The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0904684, and a Graduate Research Fellowship).
 $$$$$ For English we use the Penn Treebank (Marcus et al., 1993), transformed from CFG parses into dependencies with the Collins head finding rules (Collins, 1999); for the other languages we use data from the 2006 CoNLL-X Shared Task (Buchholz and Marsi, 2006).

In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ Specifically, PGI reduces induction ambiguity by connecting language-specific parameters via phylogenetic priors.
In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ In our model, for example, f counts the dependency edges that are an instance of one of the declaratively specified dependency rules, while b is the proportion of the total dependencies that we expect should fulfill this constraint.2 With the mean field factorization and the expectation constraints in place, solving the maximization of F in (1) separately for each factor yields the following updates: where We can solve (2) by setting q(B) to q'(B) — since q(z) is held fixed while updating q(B), the expectation function of the constraint remains constant during this update.
In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ Learning to Refine Syntactic Categories Recent research has demonstrated the usefulness of automatically refining the granularity of syntactic categories.
In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ We also automatically refine the syntactic categories given in our coarsely tagged input.

Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ By updating q(B) and q(z) as in (2) and (3) we are effectively maximizing the lower bound F. 2Constraints of the form E9[f(z)] > b are easily imposed by negating f(z) and b.
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ While our present work has yielded substantial gains over previous unsupervised methods, a large gap still remains between our method and fully supervised techniques.
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ This threshold value was chosen based on minimal tuning on a single language and ruleset (English with universal rules) and carried over to each other experimental condition.
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ These rules are used as expectation constraints on the posterior distribution over dependency structures.

Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ Our choice of language is motivated by the fact that a wide range of prior parsing algorithms were developed for and tested exclusively on English.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ Leveraging these universal rules has the potential to improve parsing performance for a large number of human languages; this is particularly relevant to the processing of low-resource languages.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0904684, and a Graduate Research Fellowship).
