USR is the weakly supervised system of Naseem et al (2010). $$$$$ We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
USR is the weakly supervised system of Naseem et al (2010). $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.
USR is the weakly supervised system of Naseem et al (2010). $$$$$ The factors are updated one at a time holding all other factors fixed.

 $$$$$ We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
 $$$$$ We also test model performance when no linguistic rules are available, i.e., performing unconstrained variational inference.
 $$$$$ We also argue that cross-language universals are beneficial for automatic language processing; however, our focus is on learning language-specific adaptations of these rules from data.

Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ Furthermore, we find that our method outperforms the generalized expectation approach using corpus-specific constraints.
Other orthogonal dependency grammar induction techniques - including ones based on universal rules (Naseem et al2010) - may also benefit in combination with DBMs. $$$$$ For each experiment we run 50 iterations of variational updates; for each iteration we perform five steps of gradient search to compute the update for the variational distribution q(z) over dependency structures.

The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors, and do not necessarily reflect the views of the funding organizations.
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ This is easily effected during inference by setting the HDP variational approximation truncation level to one.
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rulesets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al. (2009)) to benefit from their complementary strengths.
The second class of techniques assumes knowledge about identities of part-of-speech tags (Naseem et al., 2010), i.e., which word tokens are verbs, which ones are nouns, etc. $$$$$ Our results demonstrate that universal rules greatly improve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010).

 $$$$$ This setup is more relevant for learning with universals since individual rule frequencies vary greatly between languages.
 $$$$$ We arrived at this threshold by tuning on the basis of English only.
 $$$$$ While our present work has yielded substantial gains over previous unsupervised methods, a large gap still remains between our method and fully supervised techniques.

Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ We also test model performance when no linguistic rules are available, i.e., performing unconstrained variational inference.
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ We are especially grateful to Michael Collins for inspiring us toward this line of inquiry and providing deterministic rules for English parsing.
Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al, 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). $$$$$ We formulate a generative Bayesian model that explains the observed data while accounting for declarative linguistic rules during inference.

In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ This result suggests that some rules are harder to learn than others regardless of their frequency, so their presence in the specified ruleset yields stronger performance gains.
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ We are especially grateful to Michael Collins for inspiring us toward this line of inquiry and providing deterministic rules for English parsing.
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ Our experiments showed that encoding a compact, well-accepted set of language-independent constraints significantly improves accuracy on multiple languages compared to the current state-of-the-art in unsupervised parsing.
In particular it builds on the idea that unsupervised parsing can be informed by universal dependency rules (Naseem et al, 2010). $$$$$ We are especially grateful to Michael Collins for inspiring us toward this line of inquiry and providing deterministic rules for English parsing.

We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0904684, and a Graduate Research Fellowship).
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ Generating the Tree Structure We now consider how the structure of the tree arises.
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ We restrict q(θs0z0c) and q(φs0z0) to be Dirichlet distributions and q(z) to be multinomial.
We reformulate the universal dependency rules used in Naseem et al (2010) in terms of the universal tags provided in the shared task (Figure 2), but unlike them, we do not engage in grammar induction. $$$$$ Learning to Refine Syntactic Categories Recent research has demonstrated the usefulness of automatically refining the granularity of syntactic categories.

 $$$$$ Variational approximations to the HDP are truncated at 10.
 $$$$$ The final output metric is directed dependency accuracy.
 $$$$$ During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.
 $$$$$ Finally, we generate the word x from a finite multinomial with parameters φsz, where s and z are the symbol and subsymbol of the current node.

Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. $$$$$ Each run of HDP-DEP below is with syntactic refinement enabled.
Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. $$$$$ We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
Naseem et al (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert hand written rules as constraints. $$$$$ Thanks to Taylor Berg-Kirkpatrick, Sabine Iatridou, Ramesh Sridharan, and members of the MIT NLP group for their suggestions and comments.

We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered $$$$$ Variational approximations to the HDP are truncated at 10.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered $$$$$ In particular, the HDP generation of z is obviated and word x is drawn from a word distribution 0s indexed solely by coarse symbol s. The resulting simplified model closely resembles DMV (Klein and Manning, 2004), except that it 1) explicitly generate words x rather than only partof-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution B.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered $$$$$ In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rulesets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al. (2009)) to benefit from their complementary strengths.
We compare to the following three systems that do not augment the tree banks and report results for some of the languages that we considered $$$$$ The final output metric is directed dependency accuracy.

For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ In standard variational inference, an intractable true posterior is approximated by a distribution from a tractable set (Bishop, 2006).
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ They can potentially help disambiguate structural ambiguities that are difficult to learn from data alone — for example, our rules prefer analyses in which verbs are dependents of auxiliaries, even though analyzing auxiliaries as dependents of verbs is also consistent with the data.
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ Datasets and Evaluation We test the effectiveness of our grammar induction approach on English, Danish, Portuguese, Slovene, Spanish, and Swedish.
For instance, Naseem et al (2010) explicitly encode these similarities in the form of universal rules which guide grammar induction in the target language. $$$$$ In addition to these universal dependencies, each specific language typically possesses its own idiosyncratic set of dependencies.

Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ This result attests to how the expectation constraints consistently guide inference toward high-accuracy areas of the search space.
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ Our approach also expresses constraints as expectations on the posterior; we utilize the machinery of their framework within a variational inference algorithm with a mean field approximation.
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ In the following let the parent of the current node have symbol s' and subsymbol z'; the root node is generated from separate root-specific distributions.
Among several available fine-to-coarse mapping schemes, we employ the one of Naseem et al (2010) that yields consistently better performance for our method and the baselines than the mapping proposed by Petrov et al (2011). $$$$$ In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac¸a et al., 2007; Ganchev et al., 2009; Grac¸a et al., 2009; Ganchev et al., 2010).

The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ For each non-terminal grammar symbol, the model posits a Hierarchical Dirichlet Process over its refinements (subsymbols) to automatically learn the granularity of syntactic categories.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ These structural correspondences, referred to as syntactic universals, have been extensively studied in linguistics (Baker, 2001; Carnie, 2002; White, 2003; Newmeyer, 2005) and underlie many approaches in multilingual parsing.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ They can potentially help disambiguate structural ambiguities that are difficult to learn from data alone — for example, our rules prefer analyses in which verbs are dependents of auxiliaries, even though analyzing auxiliaries as dependents of verbs is also consistent with the data.
The second approach takes the universal rules of Naseem et al (2010) but rather than estimating a probabilistic model with these rules, a rule based heuristic is used to select a parse rather. $$$$$ The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0904684, and a Graduate Research Fellowship).

 $$$$$ The prior (base distribution) for each θs0z0c is a symmetric Dirichlet with hyperparameter θ0.
 $$$$$ Table 4 shows the performance of both our full model (HDP-DEP) and its No-Split version using universal dependency rules across six languages.
 $$$$$ This is computed based on the Viterbi parses produced using the final unnormalized variational distribution q(z) over dependency structures.

Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ The factors are updated one at a time holding all other factors fixed.
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ Universal Dependency Rules We compile a set of 13 universal dependency rules consistent with various linguistic accounts (Carnie, 2002; Newmeyer, 2005), shown in Table 1.
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ In the above GEM, DP, Dir, and Mult refer respectively to the stick breaking distribution, Dirichlet process, Dirichlet distribution, and multinomial distribution.
Gillenwater et al (2010) is a fully unsupervised extension of the approach described in Klein and Manning (2004), whereas Naseem et al (2010) rely on hand-written cross-lingual rules. $$$$$ During inference of the probabilistic model, we use posterior expectation constraints to require that a minimum proportion of the dependencies we infer be instances of these rules.

 $$$$$ Finally, we generate the word x from a finite multinomial with parameters φsz, where s and z are the symbol and subsymbol of the current node.
 $$$$$ Thus they allow us to judge whether the model is able to improve upon a human-engineered deterministic parser.
 $$$$$ As shown by Grac¸a et al. (2007), the update in (3) is a constrained optimization problem and can be solved by performing gradient search on its dual: For a fixed value of A the optimal q(z) ∝ q'(z) exp(−ATf(z)).

In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ Across six languages our approach outperforms state-of-theart unsupervised methods by a significant mar
In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ Interestingly, setting the threshold value independently for each language to its “true” proportion based on the gold dependencies (denoted as the “Gold” case in Figure 2) does not achieve optimal Table 6: Directed accuracy of our model (HDP-DEP) on sentences of length 10 or less and 20 or less from WSJ with different rulesets and with no rules, along with various baselines from the literature.
In fact, the universal tag sets manually induced by Petrov et al (2011) and by Naseem et al (2010) disagree on 10% of the tags. $$$$$ Our experiments showed that encoding a compact, well-accepted set of language-independent constraints significantly improves accuracy on multiple languages compared to the current state-of-the-art in unsupervised parsing.

Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ Impact of Rules Selection We compare the performance of HDP-DEP using the universal rules versus a set of rules designed for deterministically parsing the Penn Treebank (see Section 5 for details).
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ In contrast to prior work on rule-driven dependency induction (Druck et al., 2009), where each rule has a separately specified expectation, we only set a single minimum expectation for the proportion of all dependencies that must match one of the rules.
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rulesets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al. (2009)) to benefit from their complementary strengths.
Naseem et al (2010) obtain slightly better results, but only evaluate on six languages. $$$$$ In this paper we demonstrated that syntactic universals encoded as declarative constraints improve grammar induction.

Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ We present an approach to grammar induction that utilizes syntactic universals to improve dependency parsing across a range of languages.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ The factors are updated one at a time holding all other factors fixed.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ We are especially grateful to Michael Collins for inspiring us toward this line of inquiry and providing deterministic rules for English parsing.
Naseem et al (2010) report better results than ours on Portuguese, Slovene, Spanish and Swedish, but worse on Danish. $$$$$ To incorporate the constraints, we further restrict the set to only include distributions that satisfy the specified expectation constraints over hidden variables.
