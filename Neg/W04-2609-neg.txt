The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ One important application is Question Answering.
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ More recently, (Rosario and Hearst 2001), (Rosario, Hearst, and Fillmore 2002), (Lapata 2002) have proposed automatic methods that analyze and detect noun compounds relations from text.
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ This approach stemmed from our desire to answer questions such as: It is well understood and agreed in linguistics that concepts can be represented in many ways using various constructions at different syntactic levels.
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ Instead of manually annotating a corpus for each semantic relation, we do it only for each syntactic pattern and get a clear view of its semantic space.

It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ In order to classify a given set of examples (members of ), one needs some kind of measure of the similarity (or the difference) between any two given members of .
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ We consider these applications for future work.
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ 1 2 3 6 7 11 13 15 16 21 25 the rest 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0 Table 5: Sample row from the conditional probability table where the feature pair is entity-entity.
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations.

The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations.
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ Table 1 lists these relations, their definitions, examples, and some references.
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ This paper is about the automatic labeling of semantic relations in noun phrases (NPs).
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ We have analyzed the sources of errors in each case and found out that most of them are due to (in decreasing order of importance): (1) errors in automatic sense disambiguation, (2) missing combinations of features that occur in testing but not in the training data, (3) levels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others.

We have compared the performance of SVM with three other learning algorithms $$$$$ The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds.
We have compared the performance of SVM with three other learning algorithms $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.

We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ CNs have been studied intensively in linguistics, psycho-linguistics, philosophy, and computational linguistics for a long time.
We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ The K coefficient is 1 if there is a total agreement among the annotators, and 0 if there is no agreement other than that expected to occur by chance.
We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.
We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure.

Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ Instead of manually annotating a corpus for each semantic relation, we do it only for each syntactic pattern and get a clear view of its semantic space.
Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ A powerful method of answering more difficult questions is to associate to each question the semantic relation that reflects the meaning of that question and then search for that semantic relation over the candidates of semantically tagged paragraphs.

Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ Another popular approach focuses on the interpretation of the underlying semantics.
Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure.
Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ Relation no.

Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Besides the work on semantic roles, considerable interest has been shown in the automatic interpretation of complex nominals, and especially of compound nominals.
Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Semantically speaking, prepositional constructions can encode various semantic relations, their interpretations being provided most of the time by the underlying context.
Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ One pattern can express a number of semantic relations, its disambiguation being provided by the context or world knowledge.
Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.

In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement $$$$$ The verb make tells us to look for a MAKE/PRODUCE relation which is found in the complex nominal “car factory”-MAKE/PRODUCE of the text: “The car factory in Howell Michigan closed on Dec 22, 1991” which leads to answer car.
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement $$$$$ The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure.
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement $$$$$ We can draw from here the following conclusions: Given each NP syntactic construction considered, the goal is to develop a procedure for the automatic labeling of the semantic relations they encode.
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement $$$$$ Whenever the annotators found an example encoding a semantic relation other than those provided or they didn’t know what interpretation to give, they had to tag it as “OTHERS”.

We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ We have assembled a corpus from two sources: Wall Street Journal articles from TREC-9, and eXtended WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu).
We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ The focus here is to determine the semantic relations that hold between different concepts within the same phrase, and to analyze the meaning of these compounds.
We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ There are several semantic relations at the noun phrase level: (1) Saturday’s snowfall is a genitive encoding a TEMPORAL relation, (2) one-day record is a TOPIC noun compound indicating that record is about one-day snowing - an ellipsis here, (3) record in Hartford is an adjective phrase in a LOCATION relation, (4) total of 12.5 inches is an of-genitive that expresses MEASURE, (5) weather service is a noun compound in a TOPIC relation, (6) car which was driven by a college student encodes a THEME semantic role in an adjectival clause, (7) college student is a compound nominal in a PART-WHOLE/MEMBER-OF relation, (8) interstate overpass is a LOCATION noun compound, (9) mountains of Virginia is an of-genitive showing a PART-WHOLE/PLACE-AREA and LOCATION relation, (10) concrete barrier is a noun compound encoding PART-WHOLE/STUFF-OF.
We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ (Semmelmeyer and Bolander 1992)) in which the modifier noun is expressed by a prepositional phrase which functions as an adjective (eg toy in the box - a LOCATION relation).

Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure.
Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ In 28(3).
Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.
Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ We cast this as a supervised learning problem where input/ output pairs are available as training data.

Moldovan et al (2004) also use WordNet. $$$$$ Adjective Phrases are prepositional phrases attached to nouns acting as adjectives (cf.
Moldovan et al (2004) also use WordNet. $$$$$ There are 9 noun hierarchies, thus only 81 possible combinations at the most general level.
Moldovan et al (2004) also use WordNet. $$$$$ An important way of improving the performance of a system is to do a detailed error analysis of the results.

The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ The numbers in the top row identify the semantic relations (as in Table 4).
The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ The semantic interpretation of genitive constructions is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations; for example: “John’s car”-POSSESSOR-POSSESSEE, “Mary’s brother”-KINSHIP, “last year’s exhibition”-TEMPORAL, “a picture of my nice”-DEPICTION-DEPICTED, and “the desert’s oasis”-PART-WHOLE/PLACE-AREA.

Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ Prepositions play an important role both syntactically and semantically.
Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ Most importantly for us, each sequence of nouns, or possibly adjectives and nouns, has a particular meaning as a whole carrying an implicit semantic relation; for example, “spoon handle” (PART-WHOLE) or “musical clock” (MAKE/PRODUCE).
Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ (Rosario and Hearst 2001) focused on the medical domain making use of a lexical ontology and standard machine learning techniques.

A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ Moreover, the annotators were asked to indicate if the instance was lexicalized or not.
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ The problem is to decide which semantic relation to assign to a new, unseen example .
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ Example: “Saturday’s snowfall topped a one-day record in Hartford, Connecticut, with the total of 12.5 inches, the weather service said.
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ The verb make tells us to look for a MAKE/PRODUCE relation which is found in the complex nominal “car factory”-MAKE/PRODUCE of the text: “The car factory in Howell Michigan closed on Dec 22, 1991” which leads to answer car.

Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ Details about this approach are provided in (Girju et al. 2004)).
Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations.
Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ We have analyzed the sources of errors in each case and found out that most of them are due to (in decreasing order of importance): (1) errors in automatic sense disambiguation, (2) missing combinations of features that occur in testing but not in the training data, (3) levels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others.

Moldovan et al (2004) also use WordNet. $$$$$ This is in contrast to our prior approach ( (Girju, Badulescu, and Moldovan 2003a)) when we studied one relation at a time, and learned constraints to identify only that relation.
Moldovan et al (2004) also use WordNet. $$$$$ What did the factory in Howell Michigan make?
Moldovan et al (2004) also use WordNet. $$$$$ Often semantic relations are not disjoint or mutually exclusive, two or more appearing in the same lexical construct.
Moldovan et al (2004) also use WordNet. $$$$$ The verb make tells us to look for a MAKE/PRODUCE relation which is found in the complex nominal “car factory”-MAKE/PRODUCE of the text: “The car factory in Howell Michigan closed on Dec 22, 1991” which leads to answer car.

Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ Semantic relations occur with high frequency in open text, and thus, their discovery is paramount for many applications.
Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ For example, the expression “Texas city” contains both a LOCATION as well as a PART-WHOLE relation.

From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ The semantic interpretation of genitive constructions is considered problematic by linguists because they involve an implicit relation that seems to allow for a large variety of relational interpretations; for example: “John’s car”-POSSESSOR-POSSESSEE, “Mary’s brother”-KINSHIP, “last year’s exhibition”-TEMPORAL, “a picture of my nice”-DEPICTION-DEPICTED, and “the desert’s oasis”-PART-WHOLE/PLACE-AREA.
From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ The compound nominal semantics is distinctly specified by the feature pair , written shortly as .
From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ Q.
From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ These errors could be substantially decreased with more research effort.

For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ In 28(3).
For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ Roles.
For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ Model Formulation.
