The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ One important application is Question Answering.
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations.
The annotation of each example consisted of specifying its feature vector and the most appropriate semantic relation as defined in (Moldovan et al 2004). $$$$$ (1) Sometimes the meaning changes with the head (eg “musical clock” MAKE/PRODUCE, “musical creation” THEME), other times with the modifier (eg “GM car” MAKE/PRODUCE, “family car” POSSESSION).

It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ The semantic relations are the underlying relations between two concepts expressed by words or phrases.
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ This process has the advantage of reducing the annotation effort, a time consuming activity.
It also provides a mapping from the FrameNet deep semantic roles to general thematic roles (list defined in (Moldovan et al 2004)), and use cases for VerbNet. $$$$$ From the question stem word where, we know the question asks for a LOCATION which is found in the complex nominal “Three Mile Island”-LOCATION of the sentence “The Three Mile Island nuclear incident caused a DOE policy crisis”, leading to the correct answer “Three Mile Island”.

The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations.
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ An important first step is to map the characteristics of each NP construction (usually not numerical) into feature vectors.
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ These errors could be substantially decreased with more research effort.
The list of 35 semantic relations was presented in (Moldovan et al 2004). $$$$$ A further analysis of the data led us to consider a different criterion of classification that splits the examples into nominalizations and non-nominalizations.

We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. $$$$$ Thus, the features that contribute to the semantic interpretation of genitives are: the nouns’ semantic classes, the type of genitives, discourse and pragmatic information.
We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. $$$$$ As a result, I missed my appointment (CAUSE)).
We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. $$$$$ There are several semantic relations at the noun phrase level: (1) Saturday’s snowfall is a genitive encoding a TEMPORAL relation, (2) one-day record is a TOPIC noun compound indicating that record is about one-day snowing - an ellipsis here, (3) record in Hartford is an adjective phrase in a LOCATION relation, (4) total of 12.5 inches is an of-genitive that expresses MEASURE, (5) weather service is a noun compound in a TOPIC relation, (6) car which was driven by a college student encodes a THEME semantic role in an adjectival clause, (7) college student is a compound nominal in a PART-WHOLE/MEMBER-OF relation, (8) interstate overpass is a LOCATION noun compound, (9) mountains of Virginia is an of-genitive showing a PART-WHOLE/PLACE-AREA and LOCATION relation, (10) concrete barrier is a noun compound encoding PART-WHOLE/STUFF-OF.
We have compared the performance of SVM with three other learning algorithms: (1) semantic scattering (Moldovan et al 2004), (2) decision trees (a C4.5 implementation), and (3) Naive Bayes. $$$$$ Last but not least, the discovery of text semantic relations can improve syntactic parsing and even WSD which in turn affects directly the accuracy of other NLP modules and applications.

We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ In 28(3).
We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ For example, “apple juice seat” can be defined as “seat with apple juice on the table in front of it” (cf.
We considered as baseline semantic scattering which is a new learning model (Moldovan et al 2004) developed in-house for the semantic classification of noun noun pairs in NP constructions. $$$$$ Relation no.

Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ Relation no.
Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.
Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ Many researchers that followed this approach relied mostly on hand-coded rules (Finin 1980), (Uanderwende 1994).
Recent work on the automatic/semi automatic interpretation of NCs (e.g., Lapata (2002), Rosario and Marti (2001), Moldovan et al (2004) and Kim and Baldwin (2005)) has made assumptions about the scope of semantic relations or restricted the domain of interpretation. $$$$$ We distinguish here between semantic relations and semantic roles.

Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ Moreover, they were provided with the sentence in which the pairs occurred along with their corresponding WordNet senses.
Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ An important way of improving the performance of a system is to do a detailed error analysis of the results.
Moldovan et al (2004) used the word senses of nouns based on the do main or range of interpretation of an NC, leading to questions of scalability and portability to noveldomains/NC types. $$$$$ (Downing 1977)).

Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Example: “musical clock” - MAKE/PRODUCE, and “electric clock”- INSTRUMENT.
Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations.
Rosario and Marti (2001) achieved about 60% using a neural net work in a closed domain, Moldovan et al (2004) achieved 43% using word sense disambiguation of the head noun and modifier over open domain data, and Kim and Baldwin (2005) produced 53% using lexical similarities of the head noun and modifier (using the same relation set, but evaluated over a different dataset). $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.

In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). $$$$$ We study the distribution of the semantic relations across different NP patterns and analyze the similarities and differences among resulting semantic spaces.
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). $$$$$ Table 1 lists these relations, their definitions, examples, and some references.
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). $$$$$ Last but not least, the discovery of text semantic relations can improve syntactic parsing and even WSD which in turn affects directly the accuracy of other NLP modules and applications.
In this study, we selected three semantic similarity based models which had been found to perform strongly in previous research, and which were easy to re-implement: SENSE COLLOCATION (Moldovan et al, 2004), CONSTITUENT SIMILARITY (Kim and Baldwin, 2005) and CO-TRAINING ,e.g. using Sense COLLOCATION or CONSTITUENT SIMILAR ITY (Kim and Baldwin, 2007). $$$$$ The semantic relation derives from the lexical, syntactic, semantic and contextual features of each NP construction.

We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ Also, the judges tagged the NP nouns in the training corpus with their corresponding WordNet senses.
We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ For example, in order to interpret correctly “GM car” we have to know that GM is a car-producing company.
We tested the original methods of Moldovan et al (2004) and Kim and Baldwin (2005), and combined them with the co-training methods of Kim and Baldwin (2007) to come up with six different hybrid systems for evaluation, as detailed in Table 1. $$$$$ An important way of improving the performance of a system is to do a detailed error analysis of the results.

Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ 1 2 3 6 7 11 13 15 16 21 25 the rest 0.06103 0.11268 0.00939 0.04225 0.39437 0.01878 0.03286 0.25822 0.04694 0.01878 0.00469 0 Table 5: Sample row from the conditional probability table where the feature pair is entity-entity.
Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ In 28(3).
Moldovan et al (2004) propose a 35 class scheme to classify relations in various phrases; the same scheme has been applied to noun com pounds and other noun phrases (Girju et al, 2005). $$$$$ Thus, in a sense, semantic relations are more general than semantic roles and many semantic role types will appear on our list of semantic relations.

Moldovan et al (2004) also use WordNet. $$$$$ We distinguish here between semantic relations and semantic roles.

The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ Table 2 shows for each syntactic category the number of randomly selected sentences from each corpus, the number of instances found in these sentences, and finally the number of instances that our group managed to annotate by hand.
The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ Q.
The first method uses sense collocations as proposed by Moldovan et al (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). $$$$$ Here is an example.

Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ The reason is that nominalization noun phrases seem to call for a different set of learning features than the non-nominalization noun phrases, taking advantage of the underlying verbargument structure.
Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ The semantic relations are the underlying relations between two concepts expressed by words or phrases.
Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ Table 1 lists these relations, their definitions, examples, and some references.
Moldovan et al (2004) proposed a method called semantic scattering for interpreting NCs. $$$$$ Let be the training set of examples or instances where is the number of examples each accompanied by its semantic relation label .

A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ One important application is Question Answering.
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ In order to classify a given set of examples (members of ), one needs some kind of measure of the similarity (or the difference) between any two given members of .
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ As a result, I missed my appointment (CAUSE)).
A wide variety of features are used by different algorithms, ranging from simple bag-of-words frequencies to WordNet-based features (Moldovan et al., 2004). $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.

Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ One important application is Question Answering.
Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ Let be the training set of examples or instances where is the number of examples each accompanied by its semantic relation label .
Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ Q.
Moldovan et al (2004) proposed a different scheme with 35 classes. $$$$$ We distinguish here between semantic relations and semantic roles.

Moldovan et al (2004) also use WordNet. $$$$$ Here is an example.
Moldovan et al (2004) also use WordNet. $$$$$ We have analyzed the sources of errors in each case and found out that most of them are due to (in decreasing order of importance): (1) errors in automatic sense disambiguation, (2) missing combinations of features that occur in testing but not in the training data, (3) levels of specialization are too high, (4) errors caused by metonymy, (6) errors in the modifier-head order, and others.
Moldovan et al (2004) also use WordNet. $$$$$ We have assembled a corpus from two sources: Wall Street Journal articles from TREC-9, and eXtended WordNet glosses (XWN) (http://xwn.hlt.utdallas.edu).

Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ The semantic interpretation of CNs proves to be very difficult for a number of reasons.
Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ Roles.
Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.
Moldovan et al (2004) use SVMs as well as a novel algorithm (i.e., semantic scattering). $$$$$ In case the modifier is a denominal adjective, we take the synset of the noun from which the adjective is derived.

From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ Level Level 1 Level 2 Level 3 Level 4 Number of 9 52 70 122 features Number 9 46 47 47 features No. of feature pairs 57 out of 81 189 out of 2392 204 out of 3290 250 out of 5734 Number of 1 152 181 225 with only one relation Average number 2.7692 1.291 1.1765 1.144 non-zero relations per line Table 6: Statistics for the semantic class features by level of specialization.
From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ Details about this approach are provided in (Girju et al. 2004)).
From these, 80% were used for training and 20% for test ing. Each genitive instance was tagged with the corresponding semantic relations by two annotators, based on a list of 35 most frequently used semantic relations proposed by (Moldovan et al, 2004) and shown in Table 1. $$$$$ The numbers in the top row identify the semantic relations (as in Table 4).

For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ These errors could be substantially decreased with more research effort.
For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ So far we have experimented with three models: (1) semantic scattering, (2) decision trees, and (3) naive Bayes.
For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ The semantic interpretation of CNs proves to be very difficult for a number of reasons.
For this study, we adopt a revised version of these mantic relation set proposed by (Moldovan et al, 2004). $$$$$ Q.
