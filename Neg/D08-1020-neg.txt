In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.
In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.
In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ Briefly, an expansion relation means that the second clause continues the theme of the first clause, a comparison relation indicates that something in the two clauses is being compared, contingency means that there is a causal relation between the clauses, and temporal means they occur either at the same time or sequentially.

In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ Also unexpectedly, two of the superficial cohesion features appear in the larger models: F10 is the average word overlap over nouns and pronouns and F11 is the average number of pronouns per sentence.
In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.
In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.

We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ R2 is the proportion of variance in readability ratings explained by the model.
We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.
We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.

Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability.
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.

Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ This setting is also beneficial in terms of data use, because each pair of articles with different average readability scores now becomes a data point for the classification task.
Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ Standard indices were developed based on the link between word frequency/length and readability, such as Flesch-Kincaid (Kincaid, 1975), Automated Readability Index (Kincaid, 1975), Gunning Fog (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975).
Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text.
Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.

 $$$$$ These are, in decreasing order of association strength: Vocabulary and discourse relations are the strongest predictors of readability, followed by average number of verb phrases and length of the text.
 $$$$$ The predictors are the differences between the two articles’ features.
 $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.
 $$$$$ Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoiding rough shifts, that is, sequences of sentences that share no entities in common.

When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ This is evidence for the fact that there is a complex interplay between readability factors: the entity grid factors which individually have very weak correlation with readability combine well, while adding the three additional discourse features to the likelihood of discourses relations actually worsens performance slightly.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ We compensate for this by performing linear regressions with the unigram log likelihood and with the number of words in the article as an additional variable.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ But even for this type of task other factors besides vocabulary use are at play in determining readability.

 $$$$$ Similar indication for interplay between features is provided by the class ablation classification results, in which classes of features are removed.
 $$$$$ It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list.
 $$$$$ More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language.

Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002).
Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ For classification, we used WEKA’s linear support vector implementation (SMO) and performance was evaluated using 10-fold cross-validation.
Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ We use the leaps package in R to find the best subset of features for linear regression, for subsets of size one to eight.

Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ Instead, F1—the depth of the parse tree—appears in the best model with more than four features.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ We have investigated which linguistic features correlate best with readability judgments.

This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ We use the leaps package in R to find the best subset of features for linear regression, for subsets of size one to eight.
This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ The top level relations are Expansion, Comparison, Contingency, and Temporal.
This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ Note that the sequence of just the implicit relations is also not sufficient.

Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ The length of the article can serve as a baseline feature—longer articles are ranked lower by the assessors, so this feature can be taken as baseline indicator of readability.
Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ Entity grid features also make their way into the best models when more features are used for prediction: S-X, O-O, O-X, N-O transitions (F19, F22, F23, F30).
Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ This setting is also beneficial in terms of data use, because each pair of articles with different average readability scores now becomes a data point for the classification task.

To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ The correlations are positive: the more probable an article was based on its vocabulary, the higher it was generally rated.
To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition.
To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ Similar indication for interplay between features is provided by the class ablation classification results, in which classes of features are removed.

Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ We have investigated which linguistic features correlate best with readability judgments.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ Instead, F1—the depth of the parse tree—appears in the best model with more than four features.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ The linear regression results confirm the expectation that the combination of different factors is a rather complex issue.

Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.

Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005).
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007).
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels.

Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ We have investigated which linguistic features correlate best with readability judgments.
Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ In many applications such as text generation and summarization, systems need to decide the order in which selected sentences or generated clauses should be presented to the user.
Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976).
Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ Their combination gives the best result for regression with three predictors, and they explain half of the variance in readability ratings, R2 = 0.5029.

Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ The predictors are the differences between the two articles’ features.
Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text.
Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax, cohesion, entity coherence and discourse.

These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ Correlations are 0.48 and 0.54 respectively.
These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ If the consecutive sentences are only related by entity-based coherence (Knott et al., 2001) they are annotated with EntRel.
These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ We use the squared multiple correlation coefficient (R2) to assess the effectiveness of predictions.
These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.

An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ These results indicate that the most important individual factors in the readability ranking task, in decreasing order of importance, are log likelihood of discourse relations, number of discourse relations, N-O transitions, O-N transitions, average number of VPs per sentence and text probability under a general language model.
An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability.
An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.
