In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ The question then arises as to what to use as a background corpus.
In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ In this section we consider the problem of pairwise ranking of text readability.
In any case, this aspect of readability may be worth further investigation (Pitler and Nenkova, 2008). $$$$$ As a result, the vast majority of prior work on readability deals with labeling texts with the appropriate school grade level.

In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ Longer articles are perceived as less well-written and harder to read than shorter ones.
In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled.
In another related work, (Pitler and Nenkova, 2008) investigated the impact of certain surface linguistic features, syntactic, entity coherence and discourse features on the readability of Wall Street Journal (WSJ) Corpus. $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.

We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.
We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ Our study is novel in the use of gold-standard discourse features for predicting readability and the simultaneous analysis of various readability factors.
We use the syntactic features used in (Pitler and Nenkova, 2008) as baselines for our experiments on grammaticality in this paper. $$$$$ We thus create a classification problem: given two articles, is article 1 more readable than article 2?

Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability.
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ (Elhadad and Sutaria, 2007) classified words in medical texts as familiar or unfamiliar to a general audience based on their frequencies in corpora.
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
Pitler and Nenkova (2008) consider a different task of predicting text quality for an educated adult audience. $$$$$ This finding is in line with prescriptive clear writing advice (Gunning, 1952; Spandel, 2004), but is to our knowledge novel in the computational linguistics literature.

Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ The definition of what one might consider to be a well-written and readable text heavily depends on the intended audience (Schriver, 1989).
Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ But the other individually significant feature, average number of verb phrases per sentence (F3) never appears in the best models.
Pitler and Nenkova (2008) propose a unified framework composed of vocabulary, syntactic, elements of lexical cohesion, entity coherence and discourse relations to measure text quality, which resembles the composition of rubrics in the area of essay scoring (Burstein et al, 2003). $$$$$ This finding is in line with prescriptive clear writing advice (Gunning, 1952; Spandel, 2004), but is to our knowledge novel in the computational linguistics literature.

 $$$$$ Their combination gives the best result for regression with three predictors, and they explain half of the variance in readability ratings, R2 = 0.5029.
 $$$$$ This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text.
 $$$$$ This setting is also beneficial in terms of data use, because each pair of articles with different average readability scores now becomes a data point for the classification task.
 $$$$$ When all features are used for prediction, the accuracy is high, 88.88%.

When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ If the model predicts readability perfectly, R2 = 1, and if the model has no predictive capability, R2 = 0.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ Their combination gives the best result for regression with three predictors, and they explain half of the variance in readability ratings, R2 = 0.5029.
When readability is targeted towards adult competent language users a more prominent role is played by discourse features (Pitler and Nenkova, 2008). $$$$$ The average number of verb phrases in each sentence, the number of words in the article, the likelihood of the vocabulary, and the likelihood of the discourse relations all are highly correlated with humans’ judgments of how well an article is written.

 $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.
 $$$$$ Entity grid features also make their way into the best models when more features are used for prediction: S-X, O-O, O-X, N-O transitions (F19, F22, F23, F30).
 $$$$$ In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006).
 $$$$$ The predictors are the differences between the two articles’ features.

Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ The linear regression results confirm the expectation that the combination of different factors is a rather complex issue.
Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ These results indicate that the most important individual factors in the readability ranking task, in decreasing order of importance, are log likelihood of discourse relations, number of discourse relations, N-O transitions, O-N transitions, average number of VPs per sentence and text probability under a general language model.
Pitler and Nenkova (2008) used the Penn Discourse Treebank (Prasad et al, 2008) to examine discourse relations. $$$$$ Also unexpectedly, two of the superficial cohesion features appear in the larger models: F10 is the average word overlap over nouns and pronouns and F11 is the average number of pronouns per sentence.

Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ Most studies focus on a single factor contributing to readability for a given intended audience.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
Pitler and Nenkova (2008) used the same features to evaluate how well a text is written. $$$$$ Still, we do not have unified computational models that capture the interplay between various aspects of readability.

This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ Each relation is annotated for both its sense and how it is realized (implicit or explicit).
This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005).
This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ These results indicate that the most important individual factors in the readability ranking task, in decreasing order of importance, are log likelihood of discourse relations, number of discourse relations, N-O transitions, O-N transitions, average number of VPs per sentence and text probability under a general language model.
This approach was subsequently pursued by Pitler and Nenkova (2008) in their readability study. $$$$$ The average number of verb phrases in each sentence, the number of words in the article, the likelihood of the vocabulary, and the likelihood of the discourse relations all are highly correlated with humans’ judgments of how well an article is written.

Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability.
Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.
Measures of cohesion have also been used in a variety of NLP tasks such as measuring text readability (e.g. (Pitler and Nenkova, 2008)), measuring stylistic differences in text (Mccarthy et al, 2006), and for topic segmentation in tutorial dialog (Olney and Cai, 2005). $$$$$ We use the squared multiple correlation coefficient (R2) to assess the effectiveness of predictions.

To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.
To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ We have investigated which linguistic features correlate best with readability judgments.
To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952).
To answer this question, we performed an additional experiment on Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008). $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.

Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ This setting is also beneficial in terms of data use, because each pair of articles with different average readability scores now becomes a data point for the classification task.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ That is, rather than trying to predict the readability of a single document, we consider pairs of documents and predict which one is better.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008). $$$$$ This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487) and by NSF Grant IIS-07-05671.

Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax, cohesion, entity coherence and discourse.
Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.
Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008). $$$$$ As expected, the NEWS model that included more general news stories had a higher correlation with people’s judgments.

Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ As in the case of vocabulary features, the presence of more relations will lead to overall lower probabilities so we also consider the number of discourse relations (F14) and the log likelihood combined with the number of relations as features.
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ We thank Aravind Joshi, Bonnie Webber, and the anonymous reviewers for their many helpful comments.
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ While using any one out of syntactic, lexical, coherence, or discourse features is substantally better than the baseline surface features on the discrimination task, using a combination of entity coherence and discourse relations produces the best performance.
Pitler and Nenkova (2008) and Kate et al (2010), for example, average out results collected from different readers. $$$$$ This task may in fact be the more natural one, since in most applications the main concern is with the relative quality of articles rather than their absolute scores.

Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language.
Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ While surface measures such as the average number of words per sentence or the average number of characters per word are not good predictors, there exist syntactic, semantic, and discourse features that do correlate highly.
Nevertheless, recent works in the NLP community investigating the impact of entity grids (Barzilay and Lapata, 2008) or of discourse relations (Pitler and Nenkova, 2008) on text coherence and readability go in the same direction as research on Coh-Metrix, in that they aim at identifying the linguistic features that best express readability at syntactic, semantic and discourse level. $$$$$ We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus.

Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability.
Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976).
Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ Entity grid features also make their way into the best models when more features are used for prediction: S-X, O-O, O-X, N-O transitions (F19, F22, F23, F30).
Syntactic features from PCFG parse trees have also been used for gender attribution (Sarawgi et al 2011), genre identification (Stamatatos et al 2000), native language identification (Wong and Dras, 2011) and readability assessment (Pitler and Nenkova, 2008). $$$$$ When all features are used for prediction, the accuracy is high, 88.88%.

These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ We chose to experiment with two corpora: the entire Wall Street Journal corpus and a collection of general AP news, which is generally more diverse than the financial news found in the WSJ.
These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ In terms of classes of features, the 16 entity grid features perform the best, leading to an accuracy of 79.41%, followed by the combination of the four discourse features (77.36%), and syntax features (74.07%).
These methods identify regularities in words (Barzilay and Lee, 2004), entity coreference (Barzi lay and Lapata, 2008) and discourse relations (Pitler and Nenkova, 2008) from a large collection of articles and use these patterns to predict the coherence. $$$$$ We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability.

An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ The length of the article can serve as a baseline feature—longer articles are ranked lower by the assessors, so this feature can be taken as baseline indicator of readability.
An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ We have investigated which linguistic features correlate best with readability judgments.
An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r= 0.1637, p= 0.3874) and the mean number of characters per word (r= 0.0859, p= 0.6519). $$$$$ Only six features used by themselves lead to accuracies higher than the length baseline.
