Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. $$$$$ We are grateful to Claire Cardie and Vincent Ng for providing us the results of their coreference system on our data.
Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. $$$$$ Also note that the grid in Table 1 takes coreference resolution into account.
Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles. $$$$$ Evaluation results show that their system outperforms two commonly used reading level measures (the Flesch-Kincaid Grade Level index and Lexile).

We adopt Barzilay and Lapata (2008)'s entity based local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. $$$$$ To give a concrete example, the pronoun they is attested 173 times in the difficult corpus and only 73 in the easy corpus.
We adopt Barzilay and Lapata (2008)'s entity based local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. $$$$$ Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of the National Science Foundation or EPSRC.
We adopt Barzilay and Lapata (2008)'s entity based local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. $$$$$ For each occurrence of a discourse entity in the text, the corresponding grid cell contains information about its presence or absence in a sequence of sentences.

A prominent example is the entity-based model by Barzilay and Lapata (2008). $$$$$ Next, we describe ranking approaches to natural language generation and focus on coherence metrics used in current text planners.
A prominent example is the entity-based model by Barzilay and Lapata (2008). $$$$$ This observation suggests that coreference information is a good indicator of the level of reading difficulty and explains why its omission from the entity-based feature space yields inferior performance.
A prominent example is the entity-based model by Barzilay and Lapata (2008). $$$$$ From a design viewpoint, we emphasize automatic computation for both the underlying discourse representation and the inference procedure.
A prominent example is the entity-based model by Barzilay and Lapata (2008). $$$$$ We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.

Adapted from the introduction to Barzilay and Lapata (2008). $$$$$ Machine-generated summaries are markedly distinct from human texts even when these are incoherent (as in the case of our ordering experiment).
Adapted from the introduction to Barzilay and Lapata (2008). $$$$$ Givon’s (1987) and Hoey’s (1991) accounts of discourse continuity complement local measurements by considering global characteristics of entity distribution, such as the lifetime of an entity in discourse and the referential distance between subsequent mentions.
Adapted from the introduction to Barzilay and Lapata (2008). $$$$$ We investigated three important parameters for grid construction: the computation of coreferring entity classes, the inclusion of syntactic knowledge, and the influence of salience.

We follow Barzilay and Lapata (2008) and use the Fisher Sign test. $$$$$ These often do not overlap with the proper names found in the North American News Corpus used for training the LSA model.
We follow Barzilay and Lapata (2008) and use the Fisher Sign test. $$$$$ Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.
We follow Barzilay and Lapata (2008) and use the Fisher Sign test. $$$$$ Computational considerations prevent us from considering discourse representations that cannot be computed reliably by existing tools.
We follow Barzilay and Lapata (2008) and use the Fisher Sign test. $$$$$ Thanks to Eli Barzilay, Eugene Webber, and three anonymous reviewers for helpful comments and suggestions.

The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al, 1995). $$$$$ Similarly to the sentence ordering task, our training data includes pairs of summaries (xij,xik) of the same document(s) di, where xij is more coherent than xik.
The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al, 1995). $$$$$ The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168 and grant IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).
The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al, 1995). $$$$$ Thanks to Eli Barzilay, Eugene Webber, and three anonymous reviewers for helpful comments and suggestions.

Their local model of discourse coherence is based on the entity-grid (Barzilay and Lapata, 2008), as well as on the lexicalized IBM model (see Section 4.6 above); we have experimented with both, and showed that they have a minimal effect on grading performance with the FCE dataset. $$$$$ Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.
Their local model of discourse coherence is based on the entity-grid (Barzilay and Lapata, 2008), as well as on the lexicalized IBM model (see Section 4.6 above); we have experimented with both, and showed that they have a minimal effect on grading performance with the FCE dataset. $$$$$ In the following section, we provide an overview of entity-based theories of local coherence and outline previous work on its computational treatment.
Their local model of discourse coherence is based on the entity-grid (Barzilay and Lapata, 2008), as well as on the lexicalized IBM model (see Section 4.6 above); we have experimented with both, and showed that they have a minimal effect on grading performance with the FCE dataset. $$$$$ The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168 and grant IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).
Their local model of discourse coherence is based on the entity-grid (Barzilay and Lapata, 2008), as well as on the lexicalized IBM model (see Section 4.6 above); we have experimented with both, and showed that they have a minimal effect on grading performance with the FCE dataset. $$$$$ Entity-based accounts of local coherence have a long tradition within the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Halliday and Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi, and Weinstein 1995).

First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. $$$$$ In both experiments, our method yields improvements over state-of-the-art models.
First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. $$$$$ Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.
First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. $$$$$ Thanks to Eli Barzilay, Eugene Webber, and three anonymous reviewers for helpful comments and suggestions.

Barzilay and Lapata (2008) introduce the entity grid as a method of representing the coherence of a document. $$$$$ Several entity-based approaches assert that grammatical function is indicative of an entity’s prominence in discourse (Hudson, Tanenhaus, and Dell 1986; Kameyama 1986; Brennan, Friedman, and Pollard 1987; Grosz, Joshi, and Weinstein 1995).
Barzilay and Lapata (2008) introduce the entity grid as a method of representing the coherence of a document. $$$$$ The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences and records distributional, syntactic, and referential information about discourse entities.
Barzilay and Lapata (2008) introduce the entity grid as a method of representing the coherence of a document. $$$$$ Thanks to Eli Barzilay, Eugene Webber, and three anonymous reviewers for helpful comments and suggestions.

In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. $$$$$ Centering Theory formalizes fluctuations in topic continuity in terms of transitions between adjacent utterances.
In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. $$$$$ The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168 and grant IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).
In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. $$$$$ Recall from Section 3 that our approach captures local coherence by modeling patterns of entity distribution in discourse, without taking note of their lexical instantiations.
In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. $$$$$ Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.

We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). $$$$$ Thanks to Eli Barzilay, Eugene Webber, and three anonymous reviewers for helpful comments and suggestions.
We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). $$$$$ Note that considerable latitude is available when specifying the transition types to be included in a feature vector.
We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). $$$$$ We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.
We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). $$$$$ The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168 and grant IIS-0415865) and EPSRC (Lapata; grant GR/T04540/01).

This set is larger than the set that was used in Experiment 1 of Barzilay and Lapata (2008), which consists of 400 documents in two English subcorpora on earthquakes and accidents respectively. $$$$$ We are grateful to Claire Cardie and Vincent Ng for providing us the results of their coreference system on our data.
This set is larger than the set that was used in Experiment 1 of Barzilay and Lapata (2008), which consists of 400 documents in two English subcorpora on earthquakes and accidents respectively. $$$$$ The algorithm introduced in the article automatically abstracts a text into a set of entity transition sequences, a representation that reflects distributional, syntactic, and referential information about discourse entities.
This set is larger than the set that was used in Experiment 1 of Barzilay and Lapata (2008), which consists of 400 documents in two English subcorpora on earthquakes and accidents respectively. $$$$$ Both LSA and our entity-grid model are local—they model sentence-to-sentence transitions without being aware of global document structure.
This set is larger than the set that was used in Experiment 1 of Barzilay and Lapata (2008), which consists of 400 documents in two English subcorpora on earthquakes and accidents respectively. $$$$$ In our experiments, we built two content models, one for the Accidents corpus and one for the Earthquake corpus.

Barzilay and Lapata (2008) found that grammatical role improves performance in this task for an English corpus. $$$$$ We first summarize entity-based theories of discourse, and overview previous attempts for translating their underlying principles into computational coherence models.
Barzilay and Lapata (2008) found that grammatical role improves performance in this task for an English corpus. $$$$$ Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.
Barzilay and Lapata (2008) found that grammatical role improves performance in this task for an English corpus. $$$$$ It borders Marsamxett Harbor to the north and Grand Harbor to the south.
Barzilay and Lapata (2008) found that grammatical role improves performance in this task for an English corpus. $$$$$ Here, the expressions the same area, the remote region, and site all refer to Menglian county.

The results we obtain are higher than the results for the English corpora of Barzilay and Lapata (2008) (87.2% on the Earthquakes corpus and 90.4% on the Accidents corpus), but this is probably due to corpus differences as well as the availability of perfect coreference information in our experiments. $$$$$ In other theories, salience is defined in terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday and Hasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993).
The results we obtain are higher than the results for the English corpora of Barzilay and Lapata (2008) (87.2% on the Earthquakes corpus and 90.4% on the Accidents corpus), but this is probably due to corpus differences as well as the availability of perfect coreference information in our experiments. $$$$$ Barzilay and Lapata Modeling Local Coherence Although machine learning approaches to coreference resolution have been reasonably successful—state-of-the-art coreference tools today reach an F-measure2 of 70% when trained on newspaper texts—it is unrealistic to assume that such tools will be readily available for different domains and languages.
The results we obtain are higher than the results for the English corpora of Barzilay and Lapata (2008) (87.2% on the Earthquakes corpus and 90.4% on the Accidents corpus), but this is probably due to corpus differences as well as the availability of perfect coreference information in our experiments. $$$$$ We are grateful to Claire Cardie and Vincent Ng for providing us the results of their coreference system on our data.
The results we obtain are higher than the results for the English corpora of Barzilay and Lapata (2008) (87.2% on the Earthquakes corpus and 90.4% on the Accidents corpus), but this is probably due to corpus differences as well as the availability of perfect coreference information in our experiments. $$$$$ The candidate generation phase is followed by an assessment phase in which the candidates are ranked based on a set of desirable properties encoded in a ranking function.

Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ First, an automatic coreference resolution tool will be expected to be less accurate on our corpus, because it was trained on well-formed human-authored texts.
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ A great deal of research has been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich 2000; Hasler 2004; Karamanis et al. 2004).
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ Computational Modeling.

We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. $$$$$ When considering text generation applications, it is desirable to rank rather than classify instances: There is often no single coherent rendering of a given text but many different possibilities that can be partially ordered.
We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. $$$$$ The goal of text planning is to determine the content of a text by selecting a set of information-bearing units and arranging them into a structure that yields well-formed output.
We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. $$$$$ For more details on the grammatical relations extraction component we refer the interested reader to Barzilay (2003).
We extend the original entity-based coherence model (Barzilay and Lapata, 2008) by learning from more fine-grained coherence preferences in training data. $$$$$ Central to this approach is the entity-grid representation of discourse, which captures patterns of entity distribution in a text.

We show that our multiple-rank model outperforms B & L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). $$$$$ This framework for data acquisition enables large-scale automatic evaluation and is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003; Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004).
We show that our multiple-rank model outperforms B & L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). $$$$$ An optimal learner should return a ranking r∗ that orders the summaries according to their coherence.
We show that our multiple-rank model outperforms B & L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). $$$$$ In the next section, we present a method for coherence assessment that overcomes these limitations: We introduce an entity-based representation of discourse that is automatically computed from raw text; we argue that the proposed representation reveals entity transition patterns characteristic of coherent texts.
We show that our multiple-rank model outperforms B & L's basic model on two tasks, sentence ordering and summary coherence rating, evaluated on the same datasets as in Barzilay and Lapata (2008). $$$$$ Ties are resolved randomly.

For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference + and Coreference -. $$$$$ Not surprisingly, a variety of coherence theories have been developed over the years (e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found application in many symbolic text generation systems (e.g., Scott and de Souza 1990; Kibble and Power 2004).
For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference + and Coreference -. $$$$$ This is partly due to mismatches between training and testing conditions.
For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference + and Coreference -. $$$$$ Parameter estimation in such a space requires a large sample of training examples that is unavailable for most domains and applications.
For entity extraction, Barzilay and Lapata (2008) had two conditions: Coreference + and Coreference -. $$$$$ We are grateful to Claire Cardie and Vincent Ng for providing us the results of their coreference system on our data.

Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating. $$$$$ We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.
Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating. $$$$$ It is therefore not surprising that their inclusion in the feature set does not increase performance.
Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating. $$$$$ Using the proposed representation, we achieve good performance on text ordering, summary coherence evaluation, and readability assessment.
Two evaluation tasks for Barzilay and Lapata (2008)'s entity-based model are sentence ordering and summary coherence rating. $$$$$ Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily reflect the views of the National Science Foundation or EPSRC.

Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). $$$$$ Valletta was planned in the 16th century by the Italian architect Francesco Laparelli.
Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). $$$$$ A key requirement for any system that produces text is the coherence of its output.
Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). $$$$$ We re-conceptualize coherence assessment as a learning task and show that our entity-based representation is well-suited for ranking-based generation and text classification tasks.
Barzilay and Lapata (2008) experimented on two datasets: news articles on the topic of earthquakes (Earthquakes) and narratives on the topic of aviation accidents (Accidents). $$$$$ Moreover, we observe that linguistically impoverished models consistently perform worse than their linguistically elaborate counterparts.
