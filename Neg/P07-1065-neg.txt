Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ A lossless representation scheme (for example, a hash map, trie etc.) must depend on N since it assigns a distinct representation to each possible set drawn from the universe.
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ By taking the derivative we find that the number of functions k* that minimizes f is, m k* = ln 2 · . n which leads to the intuitive result that exactly half the bits in the filter will be set to 1 when the optimal number of hash functions is chosen.

Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.
Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ Bloom filters have been widely used in database applications for reducing communications overheads and were recently applied to encode word frequencies in information retrieval (Linari and Weikum, 2006) using a method that resembles the non-redundant scheme described above.
Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ Although the non-redundant scheme requires fewer items be stored in the filter and, therefore, has a lower underlying false positive rate (0.076 versus 0.
Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ Deploying such LMs requires either a cluster of machines (and the overheads of remote procedure calls), per-sentence filtering (which again, is slow) and/or the use of some other lossy compression (Goodman and Gao, 2000).

As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.
As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided $$$$$ The framework presented here shows that with some consideration for its workings, the randomised nature of the Bloom filter need not be a significant impediment to is use in applications.
As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided $$$$$ Each function is assumed to be independent from each other and to map items in the universe to the range 1 to m uniformly at random.

There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ The Boolean BF-LM is a standard BF containing all n-grams of a certain length in the training corpus, Strain.
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ Adding 3-grams drawn from the whole of the Gigaword corpus rather than simply the Agence France Press section results in slightly improved performance with signficantly less memory than the AFP-KN-3 model (see Figure 3).
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.

We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM $$$$$ In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.
We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM $$$$$ All of our experiments use publically available resources.

We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ Once a bit has been set to 1 it remains set for the lifetime of the filter.
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ Letting p be the proportion of bits that are still zero after these n elements have been inserted, this gives, As n items have been entered in the filter by hashing each k times, the probability that a bit is still zero is, which is the expected value of p. Hence the false positive rate can be approximated as, f = (1 − p)k : (1 − p�)k : (1 − e−kn m)k .
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ Deploying such LMs requires either a cluster of machines (and the overheads of remote procedure calls), per-sentence filtering (which again, is slow) and/or the use of some other lossy compression (Goodman and Gao, 2000).

Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ We take advantage of the monotonicity of the ngram event space to place upper bounds on the frequency of an n-gram prior to testing for it in the filter and potentially truncate the outer loop in Algorithm 2 when we know that the test could only return postive in error.
Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW).
Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ In our experiments we make use of both standard (i.e.

Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ Figure 1 shows the relationship between space allocated to the BF models and BLEU score (left) and false positive rate (right) respectively.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ While our main experiments use the Bloom filter models in conjunction with a conventional smoothed trigram model, we also present experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ The larger models improve somewhat on the baseline performance.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.

However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ A lossless representation scheme (for example, a hash map, trie etc.) must depend on N since it assigns a distinct representation to each possible set drawn from the universe.
However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.
However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ Although, the false positive rate for the BF containing 2-grams, in addition, to 3-grams (filtered) is higher than the false positive rate of the unfiltered BF containing only 3-grams, the actual error rate of the former is lower for models with less memory.
However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation.

Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.
Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ As shown in Section 3.1, the probability of overestimating an item’s frequency under the log-frequency BF scheme decays exponentially in the size of this overestimation error.
Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ We give the gzip compressed size as an optimistic lower bound on the size of any lossless representation of each model.2 2Note, in particular, that gzip compressed files do not support direct random access as required by our application.

Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.
Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ Adding 3-grams drawn from the whole of the Gigaword corpus rather than simply the Agence France Press section results in slightly improved performance with signficantly less memory than the AFP-KN-3 model (see Figure 3).
Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ While our main experiments use the Bloom filter models in conjunction with a conventional smoothed trigram model, we also present experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.

Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ This does not use any frequency information.
Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.
Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.

We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ We hope the present work will help establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.
We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.

All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ We conducted a range of experiments to explore the effectiveness and the error-space trade-off of Bloom filters for language modelling in SMT.
All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ We hope the present work will help establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.
All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ Our approach can complement all these techniques.

RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.
RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ Our first set of experiments examines the relationship between memory allocated to the BF and BLEU score.
RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ Figure 1 shows the relationship between space allocated to the BF models and BLEU score (left) and false positive rate (right) respectively.

For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ The one-sided error of the BF and the training scheme ensure that the actual quantised count cannot be larger than this value.
For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ Sub-sequence filtering can be performed by using the minimum value returned by lower-order models as an upper-bound on the higher-order models.

The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ The training and testing routines are given here as Algorithms 1 and 2 respectively.
The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ The log-frequency BF-LM implements a multinomial feature function in the decoder that returns the value associated with an n-gram by Algorithm 2.
The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ Language modelling (LM) is a crucial component in statistical machine translation (SMT).

RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ A BF represents a set S = {x1, x2, ..., xn} with n elements drawn from a universe U of size N. The structure is attractive when N » n. The only significant storage used by a BF consists of a bit array of size m. This is initially set to hold zeroes.
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ A standard BF can implement a Boolean ‘language model’ test: have we seen some fragment of language before?
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ Since the BF-LMs easily allow us to deploy very high-order n-gram models, we use them to evaluate the impact of different order n-grams on the translation process presenting results using the Boolean and log-frequency BF-LM in isolation for n-grams of order 1 to 10.

Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ The larger models improve somewhat on the baseline performance.
Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.
Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.

There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ The one-sided error of the BF and the training scheme ensure that the actual quantised count cannot be larger than this value.
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ We quantise raw frequencies, c(x), using a logarithmic codebook as follows, The precision of this codebook decays exponentially with the raw counts and the scale is determined by the base of the logarithm b; we examine the effect of this parameter in experiments below.
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ The procedure terminates as soon as any of the k hash functions hits a 0 and the previous count is reported.
