Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ In our experiments we create a range of models referred to by the corpus used (EP or GW), the order of the n-gram(s) entered into the filter (1 to 10), whether the model is Boolean (Bool-BF) or provides frequency information (FreqBF), whether or not sub-sequence filtering was used (FTR) and whether it was used in conjunction with the baseline trigram (+EP-KN-3).
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ Distinct items may not be hashed to k distinct locations in the filter; we ignore collisons.
Our framework makes use of the log-frequency Bloom filter presented in (Talbot and Osborne, 2007), and described briefly below, to compute smoothed conditional n-gram probabilities on the fly. $$$$$ Language modelling (LM) is a crucial component in statistical machine translation (SMT).

Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.
Recent work (Talbot and Osborne, 2007) presented a scheme for associating static frequency information with a set of n-grams in a BF efficiently. $$$$$ Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.

As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. $$$$$ 1 The space requirements of a Bloom filter are quite spectacular, falling significantly below informationtheoretic error-free lower bounds while query times are constant.
As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. $$$$$ We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams.
As noted in Talbot and Osborne (2007), errors for this log-frequency BF scheme are one-sided: frequencies will never be underestimated. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.

There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ We take advantage of the monotonicity of the ngram event space to place upper bounds on the frequency of an n-gram prior to testing for it in the filter and potentially truncate the outer loop in Algorithm 2 when we know that the test could only return postive in error.
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ The probability of a false postive, f, is clearly the probability that none of k randomly selected bits in the filter are still 0 after training.
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams.
There is a potential risk of redundancy if we represent related statistics using the log-frequency BF scheme presented in Talbot and Osborne (2007). $$$$$ We give the gzip compressed size as an optimistic lower bound on the size of any lossless representation of each model.2 2Note, in particular, that gzip compressed files do not support direct random access as required by our application.

We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.
We refer to (Talbot and Osborne, 2007) for empirical results establishing the performance of the log frequency BF-LM: overestimation errors occur with a probability that decays exponentially in the size of the overestimation error. $$$$$ The log-frequency BF-LM implements a multinomial feature function in the decoder that returns the value associated with an n-gram by Algorithm 2.

We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ We quantise raw frequencies, c(x), using a logarithmic codebook as follows, The precision of this codebook decays exponentially with the raw counts and the scale is determined by the base of the logarithm b; we examine the effect of this parameter in experiments below.
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ As the counts are quantised logarithmically, the counter will be incremented only a small number of times.
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ Although, the false positive rate for the BF containing 2-grams, in addition, to 3-grams (filtered) is higher than the false positive rate of the unfiltered BF containing only 3-grams, the actual error rate of the former is lower for models with less memory.
We hope the present work will, together with Talbot and Osborne (2007), establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics. $$$$$ If all bits referenced by the k hash values are 1 then we assume that the item was a member; if any of them are 0 then we know it was not.

Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.
Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ The k bits indexed by the hash values for each item are set to 1; the item is then discarded.
Work by Talbot and Osborne (2007), Van Durme and Lall (2009) and Goyal et al (2009) considered the problem of building very large language models via the use of randomized data structures known as sketches. $$$$$ Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models, e.g.

Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ All of our experiments use publically available resources.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ By taking the derivative we find that the number of functions k* that minimizes f is, m k* = ln 2 · . n which leads to the intuitive result that exactly half the bits in the filter will be set to 1 when the optimal number of hash functions is chosen.
Recent work (Talbot and Osborne, 2007b) has demonstrated that randomized encodings can be used to represent n-gram counts for LMs with signficant space-savings, circumventing information-theoretic constraints on lossless data structures by allowing errors with some small probability. $$$$$ We have shown that Bloom Filters can form the basis for space-efficient language modelling in SMT.

However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models, e.g.
However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.
However, if we are willing to accept that occasionally our model will be unable to distinguish between distinct n-grams, then it is possible to store each parameter in constant space independent of both n and the vocabulary size (Carter et al, 1978), (Talbot and Osborne, 2007a). $$$$$ The error analysis in Section 2 focused on the false positive rate of a BF; if we deploy a BF within an SMT decoder, however, the actual error rate will also depend on the a priori membership probability of items presented to it.

Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ Figure 4 shows the results of adding 5-grams drawn from the Gigaword corpus to the baseline.
Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
Recent work (Talbot and Osborne, 2007b) has used lossy encodings based on Bloom filters (Bloom, 1970) to represent logarithmically quantized corpus statistics for language modeling. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.

Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ We have shown that Bloom Filters can form the basis for space-efficient language modelling in SMT.
Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams.
Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).
Note that unlike the constructions in (Talbot and Osborne, 2007b) and (Church et al, 2007) no errors are possible for n grams stored in the model. $$$$$ Bits in the filter can, therefore, be shared by distinct items allowing significant space savings but introducing a non-zero probability of false positives at test time.

Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.
Following (Talbot and Osborne, 2007a) we can avoid unnecessary false positives by not querying for the longer n-gram in such cases. $$$$$ A Bloom filter (BF) is a randomised data structure for set membership queries.

We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.
We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ Adding 3-grams drawn from the whole of the Gigaword corpus rather than simply the Agence France Press section results in slightly improved performance with signficantly less memory than the AFP-KN-3 model (see Figure 3).
We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ The error rate Err is, This implies that, unlike a conventional lossless data structure, the model’s accuracy depends on other components in system and how it is queried.
We have also implemented a Bloom Filter LM in Joshua, following Talbot and Osborne (2007). $$$$$ A Bloom filter (BF) is a randomised data structure for set membership queries.

All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ The framework presented here shows that with some consideration for its workings, the randomised nature of the Bloom filter need not be a significant impediment to is use in applications.
All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ Table 2 shows the numbers of distinct n-grams in these corpora.
All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ Table 3 shows the results of the baseline (EP-KN3) and other conventional n-gram models trained on larger corpora (AFP-KN-3) and using higher-order dependencies (EP-KN-4).
All of them were estimated using the SRILM toolkit except the English News LM for which we applied RandLM (Talbot and Osborne, 2007) to cope with the large amount of training data. $$$$$ Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.

RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ This is known as a one-sided error.
RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ Deploying such LMs requires either a cluster of machines (and the overheads of remote procedure calls), per-sentence filtering (which again, is slow) and/or the use of some other lossy compression (Goodman and Gao, 2000).
RANDLM (Talbot and Osborne, 2007) performs well and scaled to the full data with improvement (resulting in our best overall system). $$$$$ Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.

For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ In this section, we give a brief overview of the Bloom filter (BF); refer to Broder and Mitzenmacher (2005) for a more in detailed presentation.
For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.
For language modeling, we use RandLM (Talbot and Osborne, 2007). $$$$$ Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.

The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ The larger models improve somewhat on the baseline performance.
The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.
The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.
The system we submitted corresponds to the GIZA++ and SBLITG (only news) system, but with RandLM (Talbot and Osborne, 2007) as language model rather than SRILM. $$$$$ Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.

RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ Bloom filters have been widely used in database applications for reducing communications overheads and were recently applied to encode word frequencies in information retrieval (Linari and Weikum, 2006) using a method that resembles the non-redundant scheme described above.
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ Figure 6 shows that for this task the most useful n-gram sizes are between 3 and 6.
RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. $$$$$ Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability.

Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.
Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ We also run experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.
Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.
Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple,2010) offer better memory consumption at the expense of CPU and accuracy. $$$$$ Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.

There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ A Bloom filter (BF) is a randomised data structure for set membership queries.
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ A Bloom filter (BF) is a randomised data structure for set membership queries.
There also have been prior work on maintaining approximate counts for higher-order language models (LMs) ((Talbot and Osborne, 2007a) operates under the model that the goal is to store a compressed representation of a disk-resident table of counts and use this compressed representation to answer count queries approximately. $$$$$ This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.
