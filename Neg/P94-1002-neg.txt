Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). $$$$$ It is possible that an additional pass through the text could be used to find structure of this kind.
Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). $$$$$ This is especially important in expository text in which the subject matter tends to structure the discourse more so than characters, setting, etc.
Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). $$$$$ The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes.
Our salience factors mirror those used by (Lappin and Leass, 1994), with the exception of Poss-s, discussed below, and CNTX-S, which is sensitive to the context in which a discourse referent appears, where a context is a topically coherent segment of text, as determined by a text-segmentation algorithm which follows (Hearst, 1994). $$$$$ Passonneau & Litman (1993) discuss at length considerations about evaluating segmentation algorithms according to reader judgment information.

TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors. $$$$$ For example, disambiguation algorithms that train on arbitrary-size text windows, e.g., Yarowsky (1992) and Gale et a/.
TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation word repetition and computes similarities between textual units based on the similarities of word space vectors. $$$$$ This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.

Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ Because the model of discourse structure is one in which text is partitioned into contiguous, nonoverlapping blocks, I call the general approach TextTiling.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ This work was sponsored in part by the Advanced Research Projects Agency under Grant No.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ This theoretical stance bears a close resemblance to Chafe's notion of The Flow Model of discourse (Chafe 1979), in description of which he writes (pp 179-180): Our data.., suggest that as a speaker moves from focus to focus (or from thought to thought) there are certain points at which there may be a more or less radical change in space, time, character configuration, event structure, or, even, world.

Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994). $$$$$ (The results for this text were fifth highest out of the 13 test texts.)
Messages are partitioned into multi-paragraph segments using TextTiling, which reportedly has an overall precision of 83% and recall of 78% (Hearst, 1994). $$$$$ Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.

This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). $$$$$ As another possible alternative Kozima (1993) has suggested using a (computationally expensive) semantic similarity metric to find similarity among terms within a small window of text (5 to 7 words).
This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). $$$$$ This paper has benefited from the comments of Graeme Hirst, Jan Pedersen, Penni Sibun, and Jeff Siskind.
This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). $$$$$ Since readers often disagree about where to draw a boundary marking for a topic shift, one can only use the general trends as a basis from which to compare different algorithms.
This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). $$$$$ This paper has benefited from the comments of Graeme Hirst, Jan Pedersen, Penni Sibun, and Jeff Siskind.

As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. $$$$$ Judgments were obtained from seven readers for each of thirteen magazine articles which satisfied the length criteria (between 1800 and 2500 words)5 and which contained little structural demarkation.
As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. $$$$$ One way to evaluate these segmentation algorithms is to compare against judgments made by human readers, another is to compare the algorithms against texts premarked by authors, and a third way is to see how well the results improve a computational task.
As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. $$$$$ This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
As TextTiling, the topic segmentation method of Hearst (Hearst, 1994), the topic segmenter we propose, called F06, first evaluates the lexical cohesion of texts and then finds their topic shifts by identifying breaks in this cohesion. $$$$$ These scores appear in Table 1 (results at 33% are also shown for comparison purposes).

Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). $$$$$ This work does not incorporate the notion of multiple simultaneous themes but instead just tries to find breaks in semantic similarity among a small number of terms.
Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). $$$$$ In Passonneau & Litman's (1993) data, if 4 or more out of 7 judges mark a boundary, the segmentation is found to be significant using a variation of the Q-test (Cochran 1950).
Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). $$$$$ I have used the results of TextTiling in a new paradigm for information access on fulltext documents (Hearst 1994).
Texts without adequate paragraph marking could be segmented using tools such as TextTiling (Hearst, 1994). $$$$$ I have found in this work that term repetition alone is a very useful indicator of subtopic structure, when analyzed in terms of multiple simultaneous information threads.

The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). $$$$$ Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.
The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). $$$$$ Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.
The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). $$$$$ Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.
The first task can be addressed by using the hierarchical structure readily available in the text (e.g., chapters, sections and subsections) or by employing existing topic segmentation algorithms (Hearst, 1994). $$$$$ The chains are used to structure texts according to the attentional/intentional theory of discourse structure (Grosz Sidner 1986), and the extent of the chains correspond to the extent of a segment.

This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). $$$$$ This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). $$$$$ This is an example of a breakdown caused by the assumptions about the subtopic structure.
This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). $$$$$ One way to evaluate these segmentation algorithms is to compare against judgments made by human readers, another is to compare the algorithms against texts premarked by authors, and a third way is to see how well the results improve a computational task.
This division can either be automatically computed using one of the many available text segmentation algorithms (Hearst, 1994), or it can be based on demarcations already present in the input (e.g., paragraph markers). $$$$$ Note that this moving window approach means that each tokensequence appears in k * 2 similarity computations.

For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. $$$$$ ... At points where all of these change in a maximal way, an episode boundary is strongly present.
For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. $$$$$ Figure 2 shows the distribution, by sentence number, of selected terms from the Stargazers text.
For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. $$$$$ Earlier work (Hearst 1993) incorporated thesaural information into the algorithms; surprisingly the latest experiments find that this information degrades the performance.
For example, the TextTiling algorithm, introduced by (Hearst, 1994), assumes that the local minima of the word similarity curve are the points of low lexical cohesion and thus the natural boundary candidates. $$$$$ I Additionally, (Passonneau 8.6 Litman 1993) concede the difficulty of eliciting hierarchical intentional structure with any degree of consistency from their human judges.

Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ This theoretical stance bears a close resemblance to Chafe's notion of The Flow Model of discourse (Chafe 1979), in description of which he writes (pp 179-180): Our data.., suggest that as a speaker moves from focus to focus (or from thought to thought) there are certain points at which there may be a more or less radical change in space, time, character configuration, event structure, or, even, world.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ In practice, a value of k = 6 works well for many texts.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ However, many expository texts consist of long sequences of paragraphs with very little structural demarcation.
Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). $$$$$ The algorithm uses domain-independent lexical frequency and distribution information to recognize the interactions of multiple simultaneous themes.

Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). $$$$$ The ultimate goal is to not only identify the extents of the subtopical units, but to label their contents as well.
Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). $$$$$ MDA972-92-J-1029 with the Corporation for National Research Initiatives (CNRI), and by the Xerox Palo Alto Research Center.
Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). $$$$$ Motivated segments can also be used as a more meaningful unit for indexing long texts.
Discourse segmentation of the documents composed of parallel parts is a novel and challenging problem, as previous research has mostly focused on the linear segmentation of isolated texts (e.g., (Hearst, 1994)). $$$$$ However, it isn't clear how useful this significance information is, since a simple majority does not provide overwhelming proof about the objective reality of the subtopic break.

Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). $$$$$ A simple way to segment the texts is to place boundaries randomly in the document, constraining the number of boundaries to equal that of the average number of paragraph gaps assigned by judges.
Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). $$$$$ Segment boundaries are assigned to the token-sequence gaps with the largest corresponding scores, adjusted as necessary to correspond to true paragraph breaks.
Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). $$$$$ The core algorithm has three main parts: Tokenization refers to the division of the input text into individual lexical units.
Fordocuments where hierarchical information is not explicitly provided, such as automatic speech transcripts, we can use automatic segmentation methods to induce such a structure (Hearst, 1994). $$$$$ Motivated segments can also be used as a more meaningful unit for indexing long texts.

In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. $$$$$ This section compares the algorithm against reader judgments, since author markups are fallible and are usually applied to text types that this algorithm is not designed for, and Hearst (1994) shows how to use TextTiles in a task (although it does not show whether or not the results of the algorithms used here are better than some other algorithm with similar goals).
In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. $$$$$ These new scores, called depth scores, corresponding to how sharp a change occurs on both sides of the tokensequence gap, are then sorted.
In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. $$$$$ Two fully-implemented versions of the algorithm are described and shown to produce segmentation that corresponds well to human judgments of the major subtopic boundaries of thirteen lengthy texts.
In this study we apply the methods of Foltz et al (1998), Hearst (1994, 1997), and a new technique utilizing an orthonormal basis to topic segmentation of tutorial dialogue. $$$$$ Therefore I do not feel the issue is closed, and instead consider successful grouping of related words as future work.

Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. $$$$$ The algorithm also incorporates the notion of &quot;chain returns&quot; — repetition of terms after a long hiatus — to close off an intention that spans over a digression.
Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. $$$$$ Judges were technical researchers.
Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. $$$$$ This section describes two algorithms for discovering subtopic structure using term repetition as a lexical cohesion indicator.
Both Hearst (1994, 1997) and Foltz et al (1998) use vector space methods discussed below to represent and compare units of text. $$$$$ For example, in the Stargazers text, a discussion of continental movement, shoreline acreage, and habitability gives way to a discussion of binary and unary star systems.

However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. $$$$$ This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.
However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. $$$$$ Figure 2 shows the distribution, by sentence number, of selected terms from the Stargazers text.
However, Hearst (1994, 1997) and Foltz et al (1998) differ on how text units are defined and on how to interpret the results of a comparison. $$$$$ This work was sponsored in part by the Advanced Research Projects Agency under Grant No.

The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. $$$$$ Since readers often disagree about where to draw a boundary marking for a topic shift, one can only use the general trends as a basis from which to compare different algorithms.
The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. $$$$$ In the test data, boundaries are placed in about 41% of the paragraph gaps.
The text unit's definition in Hearst (1994, 1997) and Foltz et al (1998) is generally task dependent, depending on what size gives the best results. $$$$$ (1992b), and algorithms that use lexical co-occurrence to determine semantic relatedness, e.g., Schiitze (1993), might benefit from using windows with motivated boundaries instead.

Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. $$$$$ Boundaries are determined by changes in the sequence of similarity scores.
Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. $$$$$ The chains are used to structure texts according to the attentional/intentional theory of discourse structure (Grosz Sidner 1986), and the extent of the chains correspond to the extent of a segment.
Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. $$$$$ One way to evaluate these segmentation algorithms is to compare against judgments made by human readers, another is to compare the algorithms against texts premarked by authors, and a third way is to see how well the results improve a computational task.
Hearst likewise chooses a large unit, 6 token-sequences of 20 tokens (Hearst, 1994), but varies these parameters dependent on the characteristics of the text to be segmented, e.g. paragraph size. $$$$$ Passonneau & Litman (1993) discuss at length considerations about evaluating segmentation algorithms according to reader judgment information.

Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. $$$$$ One way to evaluate these segmentation algorithms is to compare against judgments made by human readers, another is to compare the algorithms against texts premarked by authors, and a third way is to see how well the results improve a computational task.
Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. $$$$$ I would like to thank Anne Fontaine for her interest and help in the early stages of this work, and Robert Wilensky for supporting this line of research.
Hearst (1994, 1997) in contrast uses a relative comparison of cohesion, by recasting vector comparisons as depth scores. $$$$$ This paper describes TextTiling, an algorithm for partitioning expository texts into coherent multi-paragraph discourse units which reflect the subtopic structure of the texts.

Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware. $$$$$ Information retrieval algorithms can use subtopic structuring to return meaningful portions of a text if paragraphs are too short and sections are too long (or are not present).
Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware. $$$$$ The algorithm also incorporates the notion of &quot;chain returns&quot; — repetition of terms after a long hiatus — to close off an intention that spans over a digression.
Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware. $$$$$ For example, in the Stargazers text, a discussion of continental movement, shoreline acreage, and habitability gives way to a discussion of binary and unary star systems.
Hearst (1994, 1997) was replicated using the JTextTile (Choi, 1999) Java soft ware. $$$$$ However, it isn't clear how useful this significance information is, since a simple majority does not provide overwhelming proof about the objective reality of the subtopic break.
