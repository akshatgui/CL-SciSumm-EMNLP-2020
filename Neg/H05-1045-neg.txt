(Choi et al, 2005)) and topics of opinions (Stoyanov and Cardie, 2008). $$$$$ We thank the reviewers for their many helpful com ments, and the Cornell NLP group for their advice and suggestions for improvement.
(Choi et al, 2005)) and topics of opinions (Stoyanov and Cardie, 2008). $$$$$ Section 4 then presents a new variation of Au toSlog, AutoSlog-SE, which generates IE patterns toextract sources.
(Choi et al, 2005)) and topics of opinions (Stoyanov and Cardie, 2008). $$$$$ In particular, we consider Conditional Random Fields (Lafferty et al, 2001) and a variation of AutoSlog (Riloff, 1996a).CRFs have been used successfully for Named En tity recognition (e.g., McCallum and Li (2003),Sarawagi and Cohen (2004)), and AutoSlog has performed well on information extraction tasks in sev eral domains (Riloff, 1996a).
(Choi et al, 2005)) and topics of opinions (Stoyanov and Cardie, 2008). $$$$$ Section 5 describes the hybrid sys tem: we encode the IE patterns as additional features in the CRF model.

We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). $$$$$ We cast this problem as an information extraction task, using both CRFs and extraction patterns.
We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). $$$$$ We evaluate the CRF using the basic fea tures from Section 3, both with and without the IE pattern features from Section 5.
We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). $$$$$ 361
We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). $$$$$ pattern.Each extraction pattern has frequency and prob ability values produced by AutoSlog-SE, hence we create four IE pattern-based features for each token xi: SourcePatt-Freq, SourceExtr-Freq, SourcePatt-Prob, and SourceExtr-Prob, where the frequency values are divided into threeranges: {0, 1, 2+} and the probability values are di vided into five ranges of equal size.

The features we use (Table 5) are mostly inspired by Choi et al (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). $$$$$ We thank the reviewers for their many helpful com ments, and the Cornell NLP group for their advice and suggestions for improvement.
The features we use (Table 5) are mostly inspired by Choi et al (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). $$$$$ While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns.
The features we use (Table 5) are mostly inspired by Choi et al (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). $$$$$ is negated, which means that the verb phrase does not bear an opinion, but our system failed to recognize the negation.
The features we use (Table 5) are mostly inspired by Choi et al (2005) and by the ones used for plain support vector machines (SVMs) in (Wiegand and Klakow, 2010). $$$$$ 2.

Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wiegand and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence. $$$$$ Section 4 then presents a new variation of Au toSlog, AutoSlog-SE, which generates IE patterns toextract sources.
Choi et al (2005) and Choi et al (2006) explore conditional random fields, Wiegand and Klakow (2010) examine different combinations of convolution kernels, while Johansson and Moschitti (2010) present a re-ranking approach modeling complex relations between multiple opinions in a sentence. $$$$$ Consequently, the patterns learned by bothAutoSlog and AutoSlog-TS need to be manually re viewed by a person to achieve good accuracy.
