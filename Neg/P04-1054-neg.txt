More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. $$$$$ We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.
More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. $$$$$ The kernel function must be symmetric [K(x, y) = K(y, x)] and positivesemidefinite.
More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. $$$$$ We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences.
More recently, (Zelenko et al, 2003) have proposed extracting relations by computing kernel functions between parse trees and (Culotta and Sorensen, 2004) have extended this work to estimate kernel functions between augmented dependency trees. $$$$$ Here, C(vq, vr) might return α1 if vq = vr, and α2 if vq and vr are in the same category, where α1 > α2 > 0.

Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. $$$$$ We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. $$$$$ One could also perform latent semantic indexing to collapse feature values into similar “categories” — for example, the words “football” and “baseball” might fall into the same category.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. $$$$$ Zelenko et al. (2003) have shown the contiguous kernel to be computable in O(mn) and the sparse kernel in O(mn3), where m and n are the number of children in trees T1 and T2 respectively.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus. $$$$$ By positive-semidefinite, we require that the if x1, ... , xn E X, then the n x n matrix G defined by Gij = K(xi, xj) is positive semidefinite.

This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high precision but very low recall. $$$$$ As an example, consider a kernel over strings.
This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high precision but very low recall. $$$$$ Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.
This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high precision but very low recall. $$$$$ Haussler (1999) describes convolution kernels, which find the similarity between two structures by summing the similarity of their substructures.
This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high precision but very low recall. $$$$$ We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel.

 $$$$$ A support vector machine (SVM) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem (Cristianini and Shawe-Taylor, 2000).
 $$$$$ We reference a subset j of children of ti by ti[j] C_ ti[c].
 $$$$$ Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.
 $$$$$ We then threshold our output to achieve an optimal operating point.

Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis. $$$$$ To address this problem, we apply SVMs to the task of relation extraction.
Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis. $$$$$ We then threshold our output to achieve an optimal operating point.
Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis. $$$$$ Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously.
Culotta and Sorensen (2004) describe a slightly generalized version of this kernel based on dependency trees, in which a bag-of words kernel is used to compensate for errors in syntactic analysis. $$$$$ Given a labeled training set of potential relations, we define a tree kernel over dependency trees which we then use in an SVM to classify test instances.

 $$$$$ We then augment each node of the tree with a feature vector (Table 3).
 $$$$$ Note that for the purposes of this paper, we do not consider the link labels (e.g.
 $$$$$ Any method that provides a “soft” match between feature values will sharpen the granularity of the kernel and enhance its modeling power.
 $$$$$ The kernel function must be symmetric [K(x, y) = K(y, x)] and positivesemidefinite.

 $$$$$ Finally, we refer to the parent of node ti as ti.p.
 $$$$$ For example, in entity detection the original instance representation is generally a word vector corresponding to a sentence.
 $$$$$ Zelenko et al. (2003) have shown the contiguous kernel to be computable in O(mn) and the sparse kernel in O(mn3), where m and n are the number of children in trees T1 and T2 respectively.

Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. $$$$$ We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel.
Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. $$$$$ If 0m(ti) =� 0m(tj), then we declare the two nodes completely dissimilar.
Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction (Zelenko et al, 2003), named entity recognition (Culotta and Sorensen, 2004), semantic role labeling (Moschitti et al, 2008) and so on. $$$$$ Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.

For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. $$$$$ Using this kernel within a Support Vector Machine, we detect and classify relations between entities in the Automatic Content Extraction (ACE) corpus of news articles.
For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. $$$$$ Feature extraction and induction may result in features such as part-ofspeech, word n-grams, character n-grams, capitalization, and conjunctions of these features.
For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. $$$$$ Our contributions are a richer sentence representation, a more general framework to allow feature weighting, as well as the use of composite kernels to reduce kernel sparsity.
For example, (Culotta and Sorensen, 2004) add hypernyms of entities to features derived from WordNet. $$$$$ Any method that provides a “soft” match between feature values will sharpen the granularity of the kernel and enhance its modeling power.

Table 5 summarizes the results of a comparison between the latent topic feature and the features used by (Culotta and Sorensen, 2004). $$$$$ In the case of text, a more structured data source (e.g. a relational database) allows richer queries than does an unstructured data source (e.g. a collection of news articles).
Table 5 summarizes the results of a comparison between the latent topic feature and the features used by (Culotta and Sorensen, 2004). $$$$$ To address this problem, we apply SVMs to the task of relation extraction.

 $$$$$ (“NA” denotes the feature is not present for this node.)
 $$$$$ In the case of more structured objects, such as parse trees, features may include some description of the object’s structure, such as “has an NP-VP subtree.” Kernel methods can be particularly effective at reducing the feature engineering burden for structured objects.
 $$$$$ For example, current web search engines would not perform well on the query, “list all California-based CEOs who have social ties with a United States Senator.” Only a structured representation of the data can effectively provide such a list.
 $$$$$ To determine the similarity between two strings, string kernels (Lodhi et al., 2000) count the number of common subsequences in the two strings, and weight these matches by their length.

The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). $$$$$ A set of rules maps a parse tree to a dependency tree.
The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). $$$$$ Haussler (1999) describes convolution kernels, which find the similarity between two structures by summing the similarity of their substructures.
The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). $$$$$ Note that for the purposes of this paper, we do not consider the link labels (e.g.
The Dependency Tree Kernel (DTK) of Culottaand Sorensen (2004) is based on the work of Zelenko et al (2003). $$$$$ We examine the utility of different features such as Wordnet hypernyms, parts of speech, and entity types, and find that the dependency tree kernel achieves a 20% F1 improvement over a “bag-of-words” kernel.

To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)). $$$$$ While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.
To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)). $$$$$ While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.
To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)). $$$$$ A matching subsequence of children is a sequence of children a and b such that m(ai, bi) = 1 (bi < n).
To compare relations in two instance sentences X, Y Culotta and Sorensen (2004) proposes to compare the subtrees induced by the relation arguments x 1, x 2 and y 1, y 2, i.e. computing the node kernel between the two lowest common ancestors (lca) in the dependecy tree of the relation argument nodes K DTK (X, Y)=? (lca (x 1, x 2) ,lca (y 1, y 2)). $$$$$ To generate the parse tree of each sentence, we use MXPOST, a maximum entropy statistical parser1; we then convert this parse tree to a dependency tree.

 $$$$$ In the case of text, a more structured data source (e.g. a relational database) allows richer queries than does an unstructured data source (e.g. a collection of news articles).
 $$$$$ Here we are interested in detecting and classifying instances of relations, where a relation is some meaningful connection between two entities (Table 2).
 $$$$$ For example, in entity detection the original instance representation is generally a word vector corresponding to a sentence.

Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. $$$$$ We now define a kernel function for dependency trees.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. $$$$$ Suspecting that noise was introduced by the non-matching nodes allowed in the sparse tree kernel, we performed the experiment with different values for the decay factor A = {.9,.5,.1}, but obtained no improvement.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. $$$$$ To address this problem, we apply SVMs to the task of relation extraction.
Culotta and Sorensen (2004) extended this work to estimate similarity between augmented dependency trees. $$$$$ We describe a relation extraction technique based on kernel methods.

In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. $$$$$ The data consists of about 800 annotated text documents gathered from various newspapers and broadcasts.
In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. $$$$$ An improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations.
In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. $$$$$ While the dependency tree kernel appears to perform well at the task of classifying relations, recall is still relatively low.
In (Culotta and Sorensen, 2004) a dependency tree kernel is used to detect the Named Entity classes in natural language texts. $$$$$ Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.

 $$$$$ Note that the left-to-right ordering of the sentence is maintained in the dependency tree only among siblings (i.e. the dependency tree does not specify an order to traverse the tree to recover the original sentence).
 $$$$$ As an example, consider a kernel over strings.
 $$$$$ Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space.
 $$$$$ In addition to the contiguous and sparse tree kernels, we also implement a bag-of-words kernel, which treats the tree as a vector of features over nodes, disregarding any structural information.

This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). $$$$$ Also, Roth and Yih (2002) learn a Bayesian network to tag entities and their relations simultaneously.
This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). $$$$$ The task of the kernel function is to find these similarities.
This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). $$$$$ A simple kernel function takes the dot product of the vector representation of instances being compared.
This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004). $$$$$ These results contradict those given in Zelenko et al. (2003), where the sparse kernel achieves 2-3% better F1 performance than the contiguous kernel.

Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ Then, we use the SVM trained only on positive relation instances to classify each predicted relation.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ For example, current web search engines would not perform well on the query, “list all California-based CEOs who have social ties with a United States Senator.” Only a structured representation of the data can effectively provide such a list.

Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel.
Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ See Zelenko et al. (2003) for a proof that K is kernel function.
Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ We assume that entity tagging has been performed; so to generate potential relation instances, we iterate over all pairs of entities occurring in the same sentence.
Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). $$$$$ Five entities have been annotated (PERSON, ORGANIZATION, GEO-POLITICAL ENTITY, LOCATION, FACILITY), along with 24 types of relations (Table 2).
