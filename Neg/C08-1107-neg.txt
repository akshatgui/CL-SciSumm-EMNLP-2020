Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ (Pekar, 2006) learnsrules only between templates related by local dis course (information from different documents is ignored).
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ The evaluation assesses the correctness of all argument ex tractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application).
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ A rule application is considered correct if the matched argument is annotated by the corresponding ACE role.

Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ To better utilize the ACE dataset, we considered for a target event the annotations of other events that entail it as being correct as well.We note that each argument was considered sep arately.
Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ An entailmentrule specifies a directional inference relation be tween two templates, text patterns with variables, such as ?X win lawsuit against Y ? X sue Y ?.
Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.

Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ We tested the different approaches utilizing a standard IE test-set and compared them to binary rule learning.
Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ BInc?s re call increases moderately compared to other unarylearning approaches, but it is still substantially bet ter than not using rules (a relative recall increase of 50% already at K=10).

We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ This type of reasoning has been identified as a core semanticinference paradigm by the generic Textual Entail ment framework (Giampiccolo et al, 2007).
We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ Entail ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g.

DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ In addition, the Balanced-Inclusion measure outperformed all other tested methods.
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ We performed two adaptations to the ACE dataset to fit it better to our evaluation needs.
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.

Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ For example, a QA system has to deduce that ?SCO sued IBM?
Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ electX?
Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ The results show that the learned unary rule-sets outperform the binary rule-set.

argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).
argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.

ArgumentMappedWordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLex plus (Meyers et al, 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). $$$$$ In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.
ArgumentMappedWordNet (AmWN): A resource for entailment rules between verbal and nominal predicates (Szpektor and Dagan, 2009), including their argument mapping, based on WordNet and NomLex plus (Meyers et al, 2004), verified statistically through intersection with the unary-DIRT algorithm (Szpektor and Dagan, 2008). $$$$$ is the only argument of ?acqui sition?.

Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ (Lin and Pantel, 2001; Szpektor et al, 2004; Sekine, 2005).
Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ Some works focused on learningrules from comparable corpora, containing com parable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al, 2003).

 $$$$$ One reason for this is that incorrectunary rules may be derived even from correct bi nary rules.
 $$$$$ In the first approach, rules are directly learned based on distributionalsimilarity measures.
 $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
 $$$$$ In our main approach, rules are learned by measuring how similar the variable instantiations of two templates in a corpusare.

We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ In addition, the Balanced-Inclusion measure outperformed all other tested methods.
We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.
We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ In future work, we plan to explore additional unary template structures and similarity scores, and to improve rule application utilizing context matching methods such as (Szpektor et al, 2008).
We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ Some workstake the combination of the two variable instantiations in each template occurrence as a single complex feature, e.g. {X-Y =?SCO-IBM?}, and com pare between these complex features of t and t ?

Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). $$$$$ is the only argument of ?acqui sition?.
Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). $$$$$ to V in the above example.
Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). $$$$$ However, event mentions are annotated as correct in ACE only if they explicitly describe the targetevent.
Adjuncts (time and 6http: //projects.ldc.upenn.edu/ace/ 7 Only 26 frequent event types that correspond to a unique predicate were tested, following (Szpektor and Dagan, 2008). $$$$$ In many NLP applications, such as Question An swering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text.

Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ In a second approach, unary rules arederived from binary rules learned by state-of-the art binary rule learning methods.
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ?kill X ? injure X?.
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ In a second approach, unary rules arederived from binary rules learned by state-of-the art binary rule learning methods.

apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ In addition, the Balanced-Inclusion measure outperformed all other tested methods.
apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ In addition, the Balanced-Inclusion measure outperformed all other tested methods.

We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). $$$$$ Following (Szpektor et al, 2008), we found the ACE 2005 event training set 2useful for this pur pose.
We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). $$$$$ We tested the various unsupervised unary rule 849learning methods, as well as a binary rule learn ing method, on a test set derived from a standard IE benchmark.
We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). $$$$$ Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.

In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?.
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ to V in the above example.
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to theother algorithms.
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ 2 http://projects.ldc.upenn.edu/ace/ 852All event mentions are annotated in the corpus, in cluding the instantiated arguments of the predicate.

In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. $$$$$ To capture noun modifiers that act as predi-.
In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. $$$$$ As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules.
In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects: their goal is paraphrase extraction, while we are extracting directional entailment rules; as textual resources for pattern extraction they use parallel corpora (using patterns in another language as pivots), while we rely on monolingual Wikipedia revisions (taking benefit from its increasing size); the para phrases they extract are more similar to DIRT, while our approach allows to focus on the acquisition of rules for specific phenomena frequent in entailment pairs, and not covered by other resources. $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.

Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ Under the first approach we proposed a novel directional measure for scoring entailment rules, termed Balanced-Inclusion.
Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ For instance, a Divorce mention does entail a preceding marriage event but it does not ex plicitly describe it, and thus it is not annotated as a Marry event.
Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ?kill X ? injure X?.

Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ Reason % mentions Incorrect Rule learned 39.0 Context mismatch 27.0 Match error 19.0 Annotation problem 15.0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K=20.
Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ Xsolve?, ?solution to Y ? solve Y ? and ?find solu tion to Y ? solve Y ?.
Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ ACE guidelines specify for each event its possible arguments, each associated with a semantic role.
Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ Some rights reserved.infer ?X sue Y ? and identify ?IBM?, Y ?s instantiation, as the answer for the above question.

Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor rect mentions.
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ In addition, their template structure islimited to only verbs and their direct syntactic ar guments, which may yield incorrect rules, e.g. forlight verbs (see Section 5.2).
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ Thus, a recall upper bound for en tailment rules is 88%.
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ (Pekar, 2006) learnsrules only between templates related by local dis course (information from different documents is ignored).
