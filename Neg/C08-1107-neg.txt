Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ The results show that the learned unary rule-sets outperform the binary rule-set.
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules.
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules.
Since we acquire verb entailment pairs based on unary templates (Szpektor and Dagan, 2008) we used the Lin formula to acquire unary templates directly rather than using the DIRT formula, which is the arithmetic-geometric mean of Lin's similarities for two slots in a binary template. $$$$$ as the variable are: ?losing X?, ?X play?

Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ To capture noun modifiers that act as predi-.
Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ Acknowledgements This work was partially supported by ISF grant 1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex cellence IST-2002-506778 and the NEGEV project (www.negev-initiative.org).
Szpektor and Dagan (2008) proposed a directional similarity measure called BInc (Balanced Inclusion) that consists of Lin and Precision, as BInc (l, r)=? Lin (l, r)? Precision (l, r) 1173where l and r are the target templates. $$$$$ is the template ?X call Y indictable?, which is not a path between X and Y . For every noun node v in a parsed sentence, we generate templates with v as a variable as follows: 1.

Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable).
Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.
Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Szpektor and Dagan (2008) also proposed a unary template, which is defined as a template consisting of one argument slot and one predicate phrase. $$$$$ They consistently outperform Derived-Avg.

We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ and ?call Y indictable?
We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable).
We define a unary template as a template consisting of one argument slot and one predicate, following Szpektor and Dagan (2008). $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.

DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ For example, ?X acquire Y ? X own Y ? and ?countersuit against X ? lawsuit against X?.
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ We performed two adaptations to the ACE dataset to fit it better to our evaluation needs.
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ Some rights reserved.infer ?X sue Y ? and identify ?IBM?, Y ?s instantiation, as the answer for the above question.
DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al, 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a). Since our model can contextualize various distributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity. $$$$$ On the other hand, some correct rules were only learned by BInc, e.g. ?countersuit againstX?X sue?

Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ This behav ior generates high-score incorrect rules.
Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.
Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ Under the first approach we proposed a novel directional measure for scoring entailment rules, termed Balanced-Inclusion.
Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek tor and Dagan, 2008). $$$$$ features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al, 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al, 2002)and (Pekar, 2006) learn rules between unary tem plates.

argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.
argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations ?l, r?
argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ This provides the first comparisonbetween the performance of unary and binary rule sets.
argument mapping by decomposing templates with several arguments into unary ones (Szpektor and Dagan, 2008). $$$$$ For instance, the Transfer-Money event refers to both do nating and lending money, and thus annotations ofthis event cannot be mapped to a specific seed tem plate.

ArgumentMappedWordNet (AmWN) $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.
ArgumentMappedWordNet (AmWN) $$$$$ We are further motivated by the fact thatsome (mostly supervised) works in IE found learn ing unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al, 2003; Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail ment rules.This paper investigates acquisition of unary entailment rules from regular non-comparable cor pora.
ArgumentMappedWordNet (AmWN) $$$$$ For example, we marked a mention of a divorced person as entailing the marriage of that person, but did not consider the place and time of the divorce act to be those of the marriage .
ArgumentMappedWordNet (AmWN) $$$$$ electX?

Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.
Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ Yet,in such corpora it is harder to recognize varia tions of the same predicate.
Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ guments), binary rules either miss that mention, orextract both the correct argument and another in correct one.
Szpektor and Dagan (2008) use the distributional similarity of arguments to detect unary template entailment, whilst Berant et al (2010) apply it to binary relations in focused entailment graphs. $$$$$ If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates.

 $$$$$ Context mismatches occur when the entail ing template is matched in inappropriate contexts.
 $$$$$ This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.
 $$$$$ with ?party?
 $$$$$ Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.

We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ On the other hand, directional measures such as Weeds Precision tend to prefer directional rules inwhich the entailing template is infrequent.
We follow here the experimental setup presented in (Szpektor and Dagan, 2008), testing the generated rules on the ACE 2005 event dataset 6. This. $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.

Adjuncts (time and 6http $$$$$ is inferred from ?SCO won a lawsuit against IBM?
Adjuncts (time and 6http $$$$$ In addition to adapting state-of-the-art similar ity measures for unary rule learning, we propose a new measure, termed Balanced-Inclusion, which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity.
Adjuncts (time and 6http $$$$$ The performance of each acquired rule-base was measured for each ACE event.
Adjuncts (time and 6http $$$$$ Following this approach, we chose the structure of unary templates to be paths as well, where oneend of the path is the template?s variable.

Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg.
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ As an example for the procedure, the templates extracted from the sentence ?The losing party played it safe?
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including in formation extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ (Ravichandran and Hovy, 2002; Szpektor et al, 2004; Sekine, 2005).Directional Measures Most rule learning meth ods apply a symmetric similarity measure between two templates, viewing them as paraphrasing eachother.

apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules betweentemplates with a single variable.
apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ This section reviews relevant distributional simi larity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning.
apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ This provides the first comparisonbetween the performance of unary and binary rule sets.
apply BInc (Szpektor and Dagan, 2008), to compute for every verb pair a similarity score between each of the five count vectors. $$$$$ In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.

We consider two similarity functions $$$$$ If an in frequent template has common instantiations with another template, the coverage of its features istypically high, whether or not an entailment relation exists between the two templates.
We consider two similarity functions $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
We consider two similarity functions $$$$$ Entail ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g.
We consider two similarity functions $$$$$ Various measures wereproposed in the literature for assessing such simi larity between two words, u and v. Given a word q, its set of features F q and feature weights w q (f) for f ? F q , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Lin(u, v) = ? f?F u ?F v [w u (f) + w v (f)] ? f?F u w u (f) + ? f?F v w v (f) where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: w q (f) = log[ Pr(f |q) Pr(f) ].

In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform 855 better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ In this paper we investigate two approaches for unsupervised learning of such rules and com pare the proposed methods with a binary rule learning method.
In addition, we obtained similarity lists learned by Linand Pantel (2001), and replicated 3 similarity measures learned by Szpektor and Dagan (2008), over the RCV1corpus7. $$$$$ Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem plates with two variables, ignoring unary rules, rules between unary templates (templates withonly one variable).

In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects $$$$$ No threshold setting mechanism is suggested inthe literature for the scores of the different algo rithms, especially since rules for different right hand side templates have different score ranges.
In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects $$$$$ The results show that the learned unary rule-sets outperform the binary rule-set.
In (Szpektor and Dagan, 2008), two approaches for unsupervised learning of unary rules (i.e. between templates with a single variable) are investigated. In (Zhao et al, 2009), a pivot approach for extracting paraphrase patterns from bilingual parallel corpora is presented, while in (Callison-Burch,2008) the quality of paraphrase extraction from parallel corpora is improved by requiring that phrases and their paraphrases have the same syntactic type. Our approach is different from theirs in many respects $$$$$ We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2).

Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ We presented two approaches for unsupervised ac quisition of unary entailment rules from regular (non-comparable) corpora.
Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Recently, (Szpektor and Dagan, 2008) tried identifying the entailment relation between lexical-syntactic templates using WeedsPrec, but observed that it tends to promote unreliable relations involving infrequent templates. $$$$$ and ?X play safe?.

Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ In the first approach, rules are directly learned based on distributionalsimilarity measures.
Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ We presented two approaches for unsupervised ac quisition of unary entailment rules from regular (non-comparable) corpora.
Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure describedin Section 3.2).
Finally, we adopt the balancing approach in (Szpektor and Dagan, 2008), which, as explained in Section 2, penalizes similarity for infrequent words having fewer features (4 th property) (in our version, we truncated LIN similarity lists after top 1000 words). $$$$$ The second approach de rives unary rules from a given rule-base of binary rules.

Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ Several results rise from our evaluation: (a) while most work on unsupervised learning ignored unary rules, all tested unary methods outperformed the binary method; (b) it is better to learn unary rules directly than to derive them from a binary rule-base; (c) our proposed Balanced-Inclusion measure outperformed all other tested methods interms of F1 measure.
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ To capture noun modifiers that act as predi-.
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ Such approach is notfeasible for non-comparable corpora where statis tical measurement is required.
Last, a richer form of representation, termed unary, has been suggested where a different predicate is defined for each argument (Szpektor and Dagan, 2008). $$$$$ We tested the different approaches utilizing a standard IE test-set and compared them to binary rule learning.
