Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others. $$$$$ Implementing this search efficiently is therefore of prime importance.
Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others. $$$$$ The authors wish to thank Yoram Singer for his collaboration in an earlier phase of this research project, and Giorgio Satta for helpful discussions.
Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others. $$$$$ Vilain and Day (1996) identify (and classify) name phrases such as company names, locations, etc.
Of the several slightly different definitions of a base NP in the literature we use for the purposes of this work the definition presented in (Ramshaw and Marcus, 1995) and used also by (Argamon et al, 1998) and others. $$$$$ This research was supported in part by grant 498/95-1 from the Israel Science Foundation, and by grant 8560296 from the Israeli Ministry of Science.

SV phrases, following the definition suggested in (Argamon et al, 1998), are word phrases starting with the subject of the sentence and ending with the first verb, excluding modal verbs. $$$$$ It was used for learning linguistic structures other than shallow syntax.
SV phrases, following the definition suggested in (Argamon et al, 1998), are word phrases starting with the subject of the sentence and ending with the first verb, excluding modal verbs. $$$$$ The common practice for approaching this task is by tedious manual definition of possible pattern structures, often in the form of regular expressions or finite automata.
SV phrases, following the definition suggested in (Argamon et al, 1998), are word phrases starting with the subject of the sentence and ending with the first verb, excluding modal verbs. $$$$$ In future work we plan to investigate a datadriven approach for optimal selection and weighting of statistical features of candidate scores, as well as to apply the method to syntactic patterns of Hebrew and to domain-specific patterns for information extraction.
SV phrases, following the definition suggested in (Argamon et al, 1998), are word phrases starting with the subject of the sentence and ending with the first verb, excluding modal verbs. $$$$$ Each matching tile gives supporting evidence that a part of the candidate can be a part of a target instance.

As reported in (Argamon et al, 1998), most base NPs present in the data are less or equal than 4 words long. $$$$$ The MBSL scoring algorithm searches the training corpus for each subsequence of the sentence in order to find matching tiles.
As reported in (Argamon et al, 1998), most base NPs present in the data are less or equal than 4 words long. $$$$$ This research was supported in part by grant 498/95-1 from the Israel Science Foundation, and by grant 8560296 from the Israeli Ministry of Science.
As reported in (Argamon et al, 1998), most base NPs present in the data are less or equal than 4 words long. $$$$$ OSTIA (Onward Subsequential Transducer Inference Algorithm; Oncina, Garcia, and Vidal 1993), learns a subsequential transducer in the limit.
As reported in (Argamon et al, 1998), most base NPs present in the data are less or equal than 4 words long. $$$$$ Identifying local patterns of syntactic sequences and relationships is a fundamental task in natural language processing (NLP).

Argamon et al (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. $$$$$ The authors wish to thank Yoram Singer for his collaboration in an earlier phase of this research project, and Giorgio Satta for helpful discussions.
Argamon et al (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. $$$$$ However, writing rules is often tedious and time consuming.
Argamon et al (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. $$$$$ Surprisingly, though, rather little work has been devoted to learning local syntactic patterns, mostly noun phrases (Ramshaw and Marcus, 1995; Vilain and Day, 1996).
Argamon et al (1998) segmented the POS sequence of a multi-word into small POS tiles, counted tile frequency in the new word and non-new-word on the training set respectively, and detected new words using these counts. $$$$$ Applying the method to three syntactic patterns in English yielded positive results, suggesting its applicability for recognizing local linguistic patterns.

(Argamon et al, 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. $$$$$ In our instantiation of the MBSL schema, we define the score f(t) of a tile t as the ratio of its positive count pos(t) and its total count total(t): for a predefined threshold 0.
(Argamon et al, 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. $$$$$ This way, no information in the training is lost, as can happen in other learning systems that construct a single generalized model at the time of training.
(Argamon et al, 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. $$$$$ Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus.
(Argamon et al, 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks. $$$$$ The method utilizes a part-of-speech tagged training corpus in which all instances of the target pattern are marked (bracketed).

Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences. $$$$$ Figure 3 shows the learning curves by amount of training examples and number of words in the training data, for particular parameter settings.
Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences. $$$$$ This research was supported in part by grant 498/95-1 from the Israel Science Foundation, and by grant 8560296 from the Israeli Ministry of Science.
Ramshaw and Marcus (1995), Munoz et al (1999), Argamon et al (1998), Daelemans et al (1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences. $$$$$ We do so by encoding the training corpus using suffix trees (Edward and McCreight, 1976), which provide string searching in time which is linear in the length of the searched string.
