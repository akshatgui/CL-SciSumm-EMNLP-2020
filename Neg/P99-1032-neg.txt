Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). $$$$$ In addition, the average accuracy of the classifier is 81.5% on the sentences the judges tagged with certainty.
Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). $$$$$ Taking human performance as an upper bound, the system has room for improvement.
Specifically, each sentence was assigned a subjective or objective classitication, according to concensus lags derived by a stalistical analysis of the chisses assigned by three human judges (see (Wiebe et al, 1999) for further information). $$$$$ For example, sentences may fall on the borderline between two categories.

Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al 1999, Ipeirotis et al 2010). $$$$$ We successfully use bias-corrected tags for two purposes: to guide a revision of the coding manual, and to develop an automatic classifier.
Other approaches to annotator quality control include using EM-based algorithms for estimating annotator bias (Wiebe et al 1999, Ipeirotis et al 2010). $$$$$ The latent class model was first introduced by Lazarsfeld (1966) and was later made computationally efficient by Goodman (1974).

Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ We also present the results of a probabilistic classifier developed on the resulting annotations.
Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ We are grateful to Matthew T. Bell and Richard A. Wiebe for participating in the annotation study, and to the anonymous reviewers for their comments and suggestions.
Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ This paper presents a case study of analyzing and improving intercoder reliability in discourse tagging using the statistical techniques presented in (Bruce and Wiebe, 1998; Bruce and Wiebe, to appear).

We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. $$$$$ Intuitively, Aii represents the difference between the actual counts and those predicted by independence.
We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. $$$$$ A second corpus is annotated by the same four judges according to the new coding manual.
We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. $$$$$ Bias is measured by testing the fit of the model for marginal homogeneity: = 25+i for all i.
We followed (Wiebe et al, 1999) in rationalizing the subjective vs. the objective categories. $$$$$ In this work, model fit is reported in terms of the likelihood ratio statistic, G2, and its significance (Read and Cressie, 1988; Dunning, 1993).

Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. $$$$$ Table 2 shows the changes, from study 1 to study 2, in the Kappa values for pairwise agreement among the judges.
Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. $$$$$ Each spends about five hours.
Wiebe et al (1999) use statistical methods to automatically correct the biases in an notations of speaker subjectivity. $$$$$ Otherwise, the sentence is subjective.&quot;' We focus on sentences about private states, such as belief, knowledge, emotions, etc.

Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). $$$$$ Judges J and B, the first two authors of this paper, are NLP researchers.
Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). $$$$$ The corrected tags serve two purposes in this work.
Our experimental results show that the subjectivity classifier performs well (77% recall with 81% precision) and that the learned nouns improve upon previous state-of-the-art subjectivity results (Wiebe et al, 1999). $$$$$ Feature selection, model selection, and parameter estimation are performed anew on each fold.

Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al, 1999). $$$$$ However, the object of the sentence is not presented as material that is factual to the reporter, so the sentence is classified as subjective.
Row (2) is a Naive Bayes classifier that uses the WBO features, which performed well in prior research on sentence-level subjectivity classification (Wiebe et al, 1999). $$$$$ This paper demonstrates a procedure for automatically formulating a single best tag when there are multiple judges who disagree.

In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). $$$$$ In related work (Wiebe et al., in preparation), we found that article types, such as announcement and opinion piece, are significantly correlated with the subjective and objective classification.
In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). $$$$$ This research was supported in part by the Office of Naval Research under grant number N00014-95-1-0776.
In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). $$$$$ For each fold, we calculate the system's accuracy on the subset of the test set consisting of such sentences.
In contrast, our work classifies individual sentences, as does the research in (Wiebe et al, 1999). $$$$$ The coding manual and data from our experiments are available at: hap: / /www.cs.nmsu.edur wiebe/projects.

Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. $$$$$ Judge M is an undergraduate computer science student, and judge D has no background in computer science or linguistics.
Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. $$$$$ Judges J and B, the first two authors of this paper, are NLP researchers.
Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. $$$$$ In the remainder of this paper, we describe the classification being performed (in section 2), the statistical tools used to analyze the data and produce the bias-corrected tags (in section 3), the case study of improving intercoder agreement (in section 4), and the results of the classifier for automatic subjectivity tagging (in section 5).
Bruc eand Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position. $$$$$ In step 4, judge B was excluded from the interactive discussion for logistical reasons.

While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al (1999). $$$$$ In particular, we analyze patterns of agreement to identify systematic disagreements that result from relative bias among judges, because they can potentially be corrected automatically.
While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al (1999). $$$$$ The definitions of the categories in our coding manual are intention-based: &quot;If the primary intention of a sentence is objective presentation of material that is factual to the reporter, the sentence is objective.

Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ The revision of the coding manual results in as much as a 16 point improvement in pairwise Kappa values, and raises the average agreement among the judges to a Kappa value of over 0.87 for the sentences that can be tagged with certainty.
Wiebe et al (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. $$$$$ A second corpus is annotated by the same four judges according to the new coding manual.

Following (Wiebe et al, 1999), if the primary goal of a sentence is judged as the objective reporting of information, it was labeled as OBJ. $$$$$ For example, sentences may fall on the borderline between two categories.
Following (Wiebe et al, 1999), if the primary goal of a sentence is judged as the objective reporting of information, it was labeled as OBJ. $$$$$ Finally, a feature is included representing co-occurrence of word tokens and punctuation marks with the subjective and objective classification.4 There are many other features to investigate in future work, such as features based on tags assigned to previous utterances (see, e.g., (Wiebe et al., 1997; Samuel et al., 1998)), and features based on semantic classes, such as positive and negative polarity adjectives (Hatzivassiloglou and McKeown, 1997) and reporting verbs (Bergler, 1992).

Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. $$$$$ A binary feature is included for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not.
Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. $$$$$ The average pairwise percentage agreement between D, J, and M and the bias-corrected tags in the entire data set is 89.5%, while the system's percentage agreement with the bias-corrected tags (i.e., its accuracy) is 72.17%.
Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. $$$$$ Based on the judges' feedback, 22 of the 504 bias-corrected tags are changed, and a second draft of the coding manual is written.
Wiebe et al (1999) train a sentence-level probabilistic classifier on data from the WSJ to identify subjectivity in these sentences. $$$$$ We are aware of only one previous project reporting intercoder agreement results for similar categories, the switchboard-DAMSL project mentioned above.

Again, our feature set is richer than Wiebe et al (1999). $$$$$ We successfully use bias-corrected tags for two purposes: to guide a revision of the coding manual, and to develop an automatic classifier.
Again, our feature set is richer than Wiebe et al (1999). $$$$$ The EM algorithm takes as input the number of latent categories hypothesized, i.e., the number of values of L, and produces estimates of the parameters.
Again, our feature set is richer than Wiebe et al (1999). $$$$$ A binary feature is included for each of the following: the presence in the sentence of a pronoun, an adjective, a cardinal number, a modal other than will, and an adverb other than not.

Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity. $$$$$ In generation and machine translation, it is desirable to generate text that is appropriately subjective or objective (Hovy, 1987).
Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity. $$$$$ The larger the G2 value, the greater the bias.
Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity. $$$$$ The task addressed here comes to the fore in other genres, especially news reporting.
Previous work on sentence-level subjectivity classification (Wiebe et al, 1999) used training corpora that had been manually annotated for subjectivity. $$$$$ This model constrains the off-diagonal counts, i.e., the counts that correspond to disagreement.

 $$$$$ We address evidentiality in text (Chafe, 1986), which concerns issues such as what is the source of information, and whether information is being presented as fact or opinion.
 $$$$$ They are used to guide the revision of the coding manual, resulting in improved Kappa scores, and they serve as a gold standard for developing a probabilistic classifier.
 $$$$$ We are grateful to Matthew T. Bell and Richard A. Wiebe for participating in the annotation study, and to the anonymous reviewers for their comments and suggestions.

According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. $$$$$ The task addressed here comes to the fore in other genres, especially news reporting.
According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. $$$$$ Subjective and objective categories are potentially important for many text processing applications, such as information extraction and information retrieval, where the evidential status of information is important.
According to the coding manual (Wiebe et al, 1999), subjective sentences are those expressing evaluations, opinions, emotions, and speculations. $$$$$ There are 299/500 such sentences.

One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). $$$$$ Using only simple features, the classifier achieves an average accuracy 21 percentage points higher than the baseline, in 10-fold cross validation experiments.
One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). $$$$$ The task addressed here comes to the fore in other genres, especially news reporting.
One judge annotated all articles in four datasets of the Wall Street Journal Treebank corpus (Marcus et al, 1993) (W9-4, W9-10, W9-22, and W9 33, each approximately 160K words) as well as the corpus of Wall Street Journal articles used in (Wiebe et al, 1999) (called WSJ-SE below). $$$$$ The model selected is the one with the highest accuracy on a held-out portion of the training data. sets.

Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). $$$$$ Using only simple features, the classifier achieves an average accuracy 21 percentage points higher than the baseline, in 10-fold cross validation experiments.
Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). $$$$$ This research was supported in part by the Office of Naval Research under grant number N00014-95-1-0776.
Additionally, it may be possible to refine the classications automatically using methods such as those described in (Wiebe et al, 1999). $$$$$ The strong performance of the classifier and its consistency with the judges demonstrate the value of this approach to developing gold-standard tags.

During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999). $$$$$ Table 1 shows a four-category data configuration, in which certainty ratings 0 and 1 are combined and ratings 2 and 3 are combined.
During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999). $$$$$ The revision of the coding manual results in as much as a 16 point improvement in pairwise Kappa values, and raises the average agreement among the judges to a Kappa value of over 0.87 for the sentences that can be tagged with certainty.
During the past few years, the problem of polarity recognition has been usually faced as a step beyond the identification of the subjectivity or objectivity of texts (Wiebe et al, 1999). $$$$$ Our goal is to correct correlated disagreements automatically.
