The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. $$$$$ This initial partition is improved iteratively by moving a single word from one class to another.
The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. $$$$$ The classes BIL are determined by bilingually optimizing classes with Eq.
The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. $$$$$ In statistical natural language processing we always face the problem of sparse data.
The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. $$$$$ For optimization we used the exchange algorithm.

We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. $$$$$ A simple approximation of Pr(41) is to model it as a product of bigram probabilities: Pr (wPI) = HiN_, p(wi *2_1).
We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. $$$$$ One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. $$$$$ The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation.
We use the publicly available implementation MK CLS3 (Och, 1999) to train this model. $$$$$ Because of this it is necessary to perform an additional optimization process which determines the number of classes.

Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. $$$$$ If we insert this into Eq.
Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. $$$$$ (1).
Word alignment was estimated with GIZA++ tool (Och, 2003), coupled with mk cls3 (Och, 1999), which allows for statistical word clustering for better generalization. $$$$$ If we want to estimate the bigram probabilities p(wlw') using a realistic natural language corpus we are faced with the problem that most of the bigrams are rarely seen.

This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. $$$$$ In statistical natural language processing we always face the problem of sparse data.
This is the shared task baseline system for the 2006 NAACL/HLT workshop on statistical machine translation (Koehn and Monz, 2006) and consists of the Pharaoh decoder (Koehn, 2004), SRILM (Stolcke, 2002), GIZA++ (Och and Ney, 2003), mkcls (Och, 1999), Carmel,1 and a phrase model training code. $$$$$ The obtained word classes give a low translation lexicon perplexity and improve the quality of staProceedings of EACL '99 tistical machine translation.

The word classes for the class-based features are trained using the mkcls tool (Och, 1999). $$$$$ This initial partition is improved iteratively by moving a single word from one class to another.
The word classes for the class-based features are trained using the mkcls tool (Och, 1999). $$$$$ In statistical natural language processing we always face the problem of sparse data.
The word classes for the class-based features are trained using the mkcls tool (Och, 1999). $$$$$ In statistical natural language processing we always face the problem of sparse data.
The word classes for the class-based features are trained using the mkcls tool (Och, 1999). $$$$$ We define Mc, to be the average number of seen predecessor and successor word classes.

We use the publicly available implementation MKCLS (Och, 1999) to train this model. $$$$$ The grouping of words with a different meaning like today and tomorrow does not imply that these words should be translated by the same Spanish word, but it does imply that the translations of these words are likely to be in the same Spanish word class.
We use the publicly available implementation MKCLS (Och, 1999) to train this model. $$$$$ It can be seen that the resulting classes often contain words that are similar in their syntactic and semantic functions.
We use the publicly available implementation MKCLS (Och, 1999) to train this model. $$$$$ With the notation I for the number of iterations needed for convergence, B for the number of word bigrams, M for the number of classes and V for the vocabulary size the computational complexity of this algorithm is roughly I (B • log2 (B IV) +V M • Mo).
We use the publicly available implementation MKCLS (Och, 1999) to train this model. $$$$$ For the small EuTRANs-I task the word error rates reduce significantly.

The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). $$$$$ We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.
The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). $$$$$ We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.
The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). $$$$$ An efficient optimization algorithm for LPI is described in section 4.
The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). $$$$$ The target language of our experiments is English.

This feature implements a 5-gram language model of target statistical classes (Och, 1999). $$$$$ Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models.
This feature implements a 5-gram language model of target statistical classes (Och, 1999). $$$$$ In bilingual word clustering we are interested in classes F and E which form partitions of the vocabulary of two languages.
This feature implements a 5-gram language model of target statistical classes (Och, 1999). $$$$$ (2) and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL '99 criterion LP,.

This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. $$$$$ One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. $$$$$ The algorithm to determine bilingual classes is depicted in Figure 1.
This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999) has become a standard part of statistical machine translation systems. $$$$$ The quality of the resulting local optima can be improved if we accept a short-term degradation of the optimization criterion during the optimization process.

As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. $$$$$ One possibility to solve this problem is to partition the set of all words into equivalence classes.
As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. $$$$$ This initial partition is improved iteratively by moving a single word from one class to another.
As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. $$$$$ By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.
As a baseline we report the performance of mkcls (Och, 1999) on all test corpora. $$$$$ By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.

A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. $$$$$ For the optimization of LP2 we can use the same algorithm with small modifications.
A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. $$$$$ One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
A later study by (Och, 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. $$$$$ Our approach is simpler and computationally more efficient than (Wang et al., 1996).

This approach was shown to give the best results in (Och, 1999). $$$$$ It can be seen that Eq.
This approach was shown to give the best results in (Och, 1999). $$$$$ The grouping of words with a different meaning like today and tomorrow does not imply that these words should be translated by the same Spanish word, but it does imply that the translations of these words are likely to be in the same Spanish word class.
This approach was shown to give the best results in (Och, 1999). $$$$$ Because of this it is necessary to perform an additional optimization process which determines the number of classes.
This approach was shown to give the best results in (Och, 1999). $$$$$ We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.

Practically, we can use word alignment as used in (Och, 1999). $$$$$ We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.
Practically, we can use word alignment as used in (Och, 1999). $$$$$ An explanation for the superiority of BIL-2 over BIL is that by first optimizing the English classes mono-lingually, it is much more probable that longer sequences of classes occur more often thereby increasing the average alignment template size.
Practically, we can use word alignment as used in (Och, 1999). $$$$$ The algorithm to determine bilingual classes is depicted in Figure 1.
Practically, we can use word alignment as used in (Och, 1999). $$$$$ (2) and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL '99 criterion LP,.

First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. $$$$$ The quality of the resulting local optima can be improved if we accept a short-term degradation of the optimization criterion during the optimization process.
First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. $$$$$ We expect improvements in translation quality by allowing that words occur in more than one class and by performing a hierarchical clustering.
First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. $$$$$ We maximize the joint probability of a bilingual training corpus To perform the maximization of Eq.
First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. $$$$$ Because of this it is necessary to perform an additional optimization process which determines the number of classes.

Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. $$$$$ We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm.
Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. $$$$$ By applying a maximum-likelihood approach to the joint probability of a parallel corpus we obtained an optimization criterion for bilingual word classes which is very similar to the one used in monolingual maximum-likelihood word clustering.
Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. $$$$$ Various clustering techniques have been proposed (Brown et al., 1992; Jardino and Adda, 1993; Martin et al., 1998) which perform automatic word clustering optimizing a maximum-likelihood criterion with iterative clustering algorithms.

Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). $$$$$ We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). $$$$$ One way to reduce this problem is to group words into equivalence classes which is a standard method in statistical language modeling.
Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-jussa et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). $$$$$ Acknowledgements This work has been partially supported by the European Community under the ESPRIT project number 30268 (EuTrans).

For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999). $$$$$ If we insert this into Eq.
For the unsupervised tags, we used clustered word classes obtained using the mkcls software, which implements the approach of Och (1999). $$$$$ Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models.

Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. $$$$$ This initial partition is improved iteratively by moving a single word from one class to another.
Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. $$$$$ Our aim is to use word classes in statistical machine translation to allow for more robust statistical translation models.
Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. $$$$$ (9) with ng,1 = n9,2.
Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. $$$$$ To perform bilingual word clustering we use a maximum-likelihood approach as in the monolingual case.

However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ We chose the number of classes in such a way that the final performance of the translation system was optimal.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ In bilingual word clustering we are interested in classes F and E which form partitions of the vocabulary of two languages.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ We develop an optimization criterion based on a maximumlikelihood approach and describe a clustering algorithm.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ We define Mc, to be the average number of seen predecessor and successor word classes.

This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). $$$$$ In statistical natural language processing we always face the problem of sparse data.
This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). $$$$$ (2) and apply the negative logarithm and change the summation order we arrive at the following optimization Proceedings of EACL '99 criterion LP,.
This feature consists of a 5-gram model of words classes, which is trained from the target side of the bilingual corpus using the statistical classes from (Och, 1999). $$$$$ We will show that the usage of the bilingual word classes we get can improve statistical machine translation.
