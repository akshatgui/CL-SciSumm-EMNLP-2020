To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.
To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising.
To tackle this problem, in one of our recent work (Ng et al, 2003), we had gathered training data from parallel texts and obtained encouraging results in our evaluation on the nouns of SENSEVAL-2 English lexical sample task (Kilgarriff, 2001). $$$$$ If there are insufficient training examples for some sense of w from the parallel texts, then we just used as many parallel text training examples as we could find for that sense.

To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). $$$$$ To illustrate, we consider the noun channel where the difference between M3 and P2 is the largest.
To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.
To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.
To gather training examples from these parallel texts, we used the approach we described in (Ng et al, 2003) and (Chan and Ng, 2005b). $$$$$ The accuracy figures thus obtained for all the difficult nouns are listed in the column labeled P2 in Table 3.

Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. $$$$$ On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage.
Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. $$$$$ Ide et al. (2002) investigated word sense distinctions using parallel corpora.
Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. $$$$$ In general, such a practice results in higher test accuracy, since the test examples would look a lot closer to the training examples in this case.
Other similar work includes that in (Ng et al, 2003), where a sense-annotated corpus was automatically generated from a parallel corpus. $$$$$ For nature, the parallel text alignment approach gives better accuracy, and for sense the accuracy difference is only 0.014 (while there is a relatively large difference of 0.231 between P1 and P1-Baseline of sense).

For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. $$$$$ In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.
For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. $$$$$ This problem is particular severe for WSD, since sense-tagged data must be collected separately for each word in a language.
For example, Ng et al (2003) acquired sense examples using English-Chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using software. $$$$$ This is a relatively short time, especially when compared to the effort that we would otherwise need to spend to perform manual sense-tagging of training examples.

There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. $$$$$ The resulting parallel texts are then input to the GIZA++ software (Och and Ney 2000) for word alignment.
There is a growing number of methods that use data available in one language to build text processing tools for another language, for diverse tasks such as word sense disambiguation (Ng et al, 2003), syntactic parsing (Hwa et al, 2005), information retrieval (Monz and Dorr, 2005), subjectivity analysis (Mihalcea et al, 2007), and others. $$$$$ That is, eliminating the advantage that manually sense-tagged data have in their sense coverage would reduce the performance gap between the two approaches from 0.140 to 0.065.

For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. $$$$$ The lack of large-scale parallel corpora no doubt has impeded progress in this direction, although attempts have been made to mine parallel corpora from the Web (Resnik, 1999).
For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. $$$$$ A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
For the translation of ambiguous English words Ng et al (2003) made use of the fact that the various senses are often translated differently. $$$$$ In addition, they used an unsupervised method of noun group disambiguation, and evaluated on the English all-words task.

Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. $$$$$ A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. $$$$$ This set of nouns is relatively easy to disambiguate, since using the mostfrequently-occurring-sense baseline would have done well for most of these nouns.
Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. $$$$$ That is, eliminating the advantage that manually sense-tagged data have in their sense coverage would reduce the performance gap between the two approaches from 0.140 to 0.065.
Ng et al (2003) show that it is possible to use automatically word aligned parallel corpora to train accurate supervised WSD models. $$$$$ Each such occurrence of channel together with the 3-sentence context in English surrounding channel then forms a training example for a supervised WSD program in the next step.

Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data. $$$$$ Ideally, we would hope M1 and P1 to be close in value, since this would imply that WSD based on training examples collected from the parallel text alignment approach performs as well as manually sense-tagged training examples.
Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data. $$$$$ Li and Li (2002) investigated a bilingual bootstrapping technique, which differs from the method we implemented here.
Similarly, (Ng et al, 2003) report a research study which uses an English-Chinese parallel corpus in order to extract sense-tagged training data. $$$$$ Comparing the M1 and P1 figures, we observed that there is a set of nouns for which they are relatively close.

 $$$$$ The alignment result contains much noise, especially for words with low frequency counts.
 $$$$$ On a subset of the most difficult SENSEVAL-2 nouns, the accuracy difference between the two approaches is only 14.0%, and the difference could narrow further to 6.5% if we disregard the advantage that manually sense-tagged data have in their sense coverage.
 $$$$$ For two of the corpora, Hong Kong Hansards and Xinhua News, we gathered all English sentences containing the 29 SENSEVAL-2 noun occurrences (and their sentence-aligned Chinese sentence counterparts).

Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. $$$$$ In contrast, the P1 score of each noun is obtained by training the WSD classifier on a mixture of six parallel corpora, and tested on the official SENSEVAL-2 test set, and hence the training and test data come from dissimilar domains in this case.
Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.
Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. $$$$$ A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
Similarly, Ng et al (2003) employ English Chinese parallel word aligned corpora to identify a repository of senses for English. $$$$$ By manually examining a subset of about 1,000 examples, we estimate that the sense-tag error rate of training examples (tagged with lumped senses) obtained by our parallel text alignment approach is less than 1%, which compares favorably with the quality of manually sense tagged corpus prepared in SENSEVAL-2 (Kilgarriff, 2001).

For example, Ng et al (2003) proposed to train a classifier on sense examples acquired from word-aligned English-Chinese parallel corpora. $$$$$ Given a word-aligned parallel corpus, the different translations in a target language serve as the “sense-tags” of an ambiguous word in the source language.

Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. $$$$$ Different dictionaries define a different sense inventory.
Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. $$$$$ Our analysis also highlights the importance of the issue of domain dependence in evaluating WSD programs.
Ng et al (2003) address word sense disambiguation by manually annotating WordNet senses with their translation in the target language (Chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the IBM Models to a bilingual corpus. $$$$$ We then randomly split the data into a new training and a new test set such that no training and test examples come from the same document.

Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). $$$$$ The number of senses of each noun after sense lumping is given in column 3 of Table 3.
Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). $$$$$ For the remaining nouns (art, authority, channel, church, circuit, facility, grip, spade), the accuracy difference between M1 and P1 is at least 0.10.
Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al, 2003). $$$$$ Resnik and Yarowsky (2000) considered word sense disambiguation using multiple languages.

For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.
For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ We chose the same number of training examples for each sense as the official training data so that we can do a fair comparison between the accuracy of the parallel text alignment approach versus the manual sense-tagging approach.

Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. $$$$$ In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.
Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. $$$$$ In this paper, we reported an empirical study to evaluate an approach of automatically acquiring sense-tagged training data from English-Chinese parallel corpora, which were then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.
Ng et al (2003) extend this approach further and demonstrate that it is feasible for large scale WSD. $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.

Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. $$$$$ For example, some possible Chinese translations of the English noun channel are listed in Table 1.
Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. $$$$$ For example, for an English occurrence channel, both “频道” (sense 1 translation) and “途 径” (sense 5 translation) happen to appear in the aligned Chinese sentence.
Unlike Ng et al (2003) our algorithm works on monolingual corpora, which are much more abundant than parallel ones, and is fully automatic. $$$$$ In this paper, we evaluate an approach to automatically acquire sensetagged training data from English-Chinese parallel corpora, which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task.

To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ In manual sense tagging done in SENSEVAL-2, it is possible to assign two sense tags to church in this case, but in the parallel text setting, a particular translator will translate it in one of the two ways (教堂 or 教 会), and hence the sense tag found by parallel text alignment is only one of the two sense tags.
To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ This step could also be potentially automated if we have a suitable bilingual translation lexicon.
To gather examples from these parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ While it is encouraging to find out that the parallel text sense tags are of high quality, we are still left with the task of explaining the difference between M1 and P1 for the set of difficult nouns.

For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ Our investigation reveals that this method of acquiring sense-tagged data is promising and provides an alternative to manual sense tagging.
For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ The second column of Table 3 lists the number of senses of each noun as given in the WordNet 1.7 sense inventory (Miller, 1990).
For instance, Ng et al (2003) showed that it is possible to use word aligned parallel corpora to train accurate supervised WSD models. $$$$$ For example, some possible Chinese translations of the English noun channel are listed in Table 1.

To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ The second column of Table 3 lists the number of senses of each noun as given in the WordNet 1.7 sense inventory (Miller, 1990).
To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ We first lump together two senses s, and s2 of a noun if s, and s2 are translated into the same Chinese word.
To gather examples from parallel corpora, we followed the approach in (Ng et al, 2003). $$$$$ On average, about statistical methods in word sense disambiguation in the context of machine translation.

As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory. $$$$$ A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory. $$$$$ A central problem of word sense disambiguation (WSD) is the lack of manually sense-tagged data required for supervised learning.
As described in (Ng et al, 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory. $$$$$ The average number of senses before and after sense lumping is 5.07 and 3.52 respectively.
