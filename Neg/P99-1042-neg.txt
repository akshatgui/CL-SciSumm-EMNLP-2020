The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ • Encoding more semantic information in our representation for both question and document sentences.
The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ • Moving from whole sentence retrieval towards answer phrase retrieval.
The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ In addition, research into collaboration might lead to insights about intelligent tutoring.

The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Given that our current system can only respond with sentences from the text, this penalty is appropriate.
The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Our (WASHINGTON, D.C., 1964) - It was 150 years ago this year that our nation's biggest library burned to the ground.
The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Sem is the WordNet-based common noun semantic classification.

(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ We have argued that taking reading comprehension exams is a useful task for developing and evaluating natural language understanding systems.
(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ Stemming, on the other hand, produced a small but fairly consistent improvement.
(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ The latter was a combination of name coreference, as determined by Alembic, and a heuristic resolution of personal pronouns to the most recent prior named person.
(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ First, the evaluation should be automatic.

The Deep Read reading comprehension system (Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story. $$$$$ We created hand-tagged named entity data, which allowed us to measure the performance of Alembic: the accuracy (Fmeasure) was 76.5; see Chinchor and Sundheim (1993) for a description of the standard MUC scoring metric.
The Deep Read reading comprehension system (Hirschman et al, 1999) uses a statistical bag-of-words approach, matching the question with the lexically most similar sentence in the story. $$$$$ Crucially, the reading comprehension task is neither too easy nor too hard, as the performance of our pilot system demonstrates.

Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999) $$$$$ This is much better than chance, which would yield an average score of about 4-5% for the sentence metrics, given an average document length of 20 sentences.
Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999) $$$$$ It works by looking up all nouns of the text and adding person or location classes if any of a noun's senses is subsumed by the appropriate WordNet class.
Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999) $$$$$ Finally, the system contains an extension which substitutes the referent of personal pronouns for the pronoun in the bag representation.
Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999) $$$$$ This will allow us to improve answer word precision, which provides a good measure of how much extraneous material we are still returning.

We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ NameHum is hand-tagged named entity.
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ Finally, reading comprehension is a task that is sufficiently close to information extraction applications such as ad hoc question answering, fact verification, situation tracking, and document summarization, that improvements on the reading comprehension evaluations will result in improved systems for these applications.
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ • Adding new linguistic knowledge sources.

We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ For English written text, both of these tasks are relatively easy although not trivial—see Palmer and Hearst (1997).
We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ Such tests require us to deal with a wider variety of question types, e.g., What is this story about?
We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ We have acquired a corpus of 60 and 60 test stories of to grade material; each story is followed by short-answer questions (an answer key was also provided).
