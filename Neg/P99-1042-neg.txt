The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ Finally, we note that precision, as well as recall, will be useful to evaluate systems that can return clauses or phrases, possibly constructed, rather than whole sentence extracts as answers.
The Deep Read reading comprehension prototype system (Hirschman et al, 1999) achieves a level of 36% of the answers correct using a bag-of-words approach together with limited linguistic processing. $$$$$ The process of taking short-answer reading comprehension tests can be broken down into the following subtasks: A crucial component of all three of these subtasks is the representation of information in text.

The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Crucially, the reading comprehension task is neither too easy nor too hard, as the performance of our pilot system demonstrates.
The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ Similarly, the hand-tagged reference resolution data allowed us to evaluate automatic coreference resolution.
The RC task was first proposed by the MITRE Corporation which developed the Deep Read reading comprehension system (Hirschman et al, 1999). $$$$$ The question is still counted, meaning that the system receives a penalty in these cases.

(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ We have acquired a corpus of 60 and 60 test stories of to grade material; each story is followed by short-answer questions (an answer key was also provided).
(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ Initial experiments indicate that we can use essentially the same system architecture for both short-answer and multiple choice tests.
(Hirschman et al 1999) reported a HumSent accuracy of 36.6% on the Remedia test set. $$$$$ Our system represents the information content of a sentence (both question and text sentences) as the set of words in the sentence.


Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. $$$$$ At present, the stop-word list consists of forms of be, have, and do, personal and possessive pronouns, the conjunctions and, or, the prepositions to, in, at, of, the articles a and the, and the relative and demonstrative pronouns this, that, and which. who examined the texts and chose the sentence(s) that best answered the question, even where the sentence also contained additional (unnecessary) information.
Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. $$$$$ Finally, where question results are quite variable, perhaps because location expressions often do not include specific place names.
Moreover, the domain is another example of "found test material" in the sense of (Hirschman et al, 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA. $$$$$ Name identification provided consistent gains.

We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ For English written text, both of these tasks are relatively easy although not trivialâ€”see Palmer and Hearst (1997).
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ Finally, reading comprehension is a task that is sufficiently close to information extraction applications such as ad hoc question answering, fact verification, situation tracking, and document summarization, that improvements on the reading comprehension evaluations will result in improved systems for these applications.
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ We have acquired a corpus of 60 and 60 test stories of to grade material; each story is followed by short-answer questions (an answer key was also provided).
We call this set the MITRE corpus (Hirschman et al, 1999). $$$$$ Reading comprehension uses found material and provides humancomparable evaluations which can be computed automatically with a minimum of human annotation.

We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ The other adjacent performance differences in Figure 3 are suggestive, but not statistically significant.
We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ Finally, reading comprehension is a task that is sufficiently close to information extraction applications such as ad hoc question answering, fact verification, situation tracking, and document summarization, that improvements on the reading comprehension evaluations will result in improved systems for these applications.
We used the Remedia corpus (Hirschman et al, 1999) and ChungHwa corpus (Xu and Meng, 2005) in our experiments. $$$$$ We used these to construct and evaluate a baseline system that uses pattern matching (bag-of-words) techniques augmented with additional automated linguistic processing (stemming, name identification, semantic class identification, and pronoun resolution).
