We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993). $$$$$ Additional text from that domain would undoubtedly contain new words.
We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993). $$$$$ From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.
We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993). $$$$$ At the level of syntax (Section 3), an event is the use of a particular structure; the model predicts what the most likely rule is given a particular situation.
We were already using a generative statistical model for part-of-speech tagging (Weischedel et al 1993). $$$$$ 3.

Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993). $$$$$ For example, in morphological processing in English (Section 2), the events are the use of a word with a particular part of speech in a string of words.
Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993). $$$$$ Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text.
Word features are introduced primarily to help with unknown words, as in (Weischedel et al 1993). $$$$$ We have been comparing several differing algorithms from various sites to evaluate both the effectiveness of such a strategy in correctly predicting fragments.

Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging. $$$$$ One can similarly use probabilities for assigning semantic structure (Section 4).
Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging. $$$$$ The effectiveness of such models is well known (DeRose 1988; Church 1988; Kupiec 1989; Jelinek 1985), and they are currently in use in parsers (e.g. de Marcken 1990).
Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging. $$$$$ Kuhn, R., and De Mori, R. (1990).
Weischedel's group (Weischedel et al, 1993) examines unknown words in the context of part-of-speech tagging. $$$$$ Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text.

in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending. $$$$$ With supervised training, where a set of correct parse trees is provided as training, one estimates p(RHS) I LHS) by the number of times rule LHS RHSI appears in the training set divided by the number of times LHS appears in the trees.
in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending. $$$$$ This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning case frame information for verbs from example uses.
in (Weischedel et al, 1993) where an unknown word was guessed given the probabilities for an unknown word to be of a particular pos, its capitalisation feature and its ending. $$$$$ Identifying the part of speech of a word illustrates both the problem of ambiguity and the problem of unknown words.

For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ Additional text from that domain would undoubtedly contain new words.
For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ In trying to extract pre-specified data from open-ended text such as a newswire, it is clear that full semantic interpretation of such texts is not on the horizon.
For words that were unknown in our subtree set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models.

More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993). $$$$$ However, as we move from the application of understanding database queries in limited domains to applications of processing open-ended text, we found challenges that questioned our previous assumptions and suggested probabilistic models instead.
More recently, the natural language processing community has effectively employed these models for part-of speech tagging, as in the seminal (Church, 1988) and other, more recent efforts (Weischedel et al, 1993). $$$$$ In the next section, we describe finding their semantic category.

For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.
For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ 3.
For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ In this case we start with the explicit probability of the phrase PO attaching to the word X.
For words that were unknown in the training set, we guessed their categories by means of the method described in Weischedel et al (1993) which uses statistics on word-endings, hyphenation and capitalization. $$$$$ From spring 1990 through fall 1991, we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with probabilistic models.

More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model. $$$$$ Heights, NY.
More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model. $$$$$ In order to reduce the ambiguity further, we tested various ways to limit how many tags were returned based on their probabilities.
More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model. $$$$$ The disadvantage of this is that uses of a word that did not occur in the training set will be unknown to the system.
More advanced methods like those described by Weischedel et al (1993) incorporate the treatment of unknown words within the probability model. $$$$$ Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text.

The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al 1993) and concentrates on the simple noun phrase. $$$$$ On the other hand, there are many cases, including alternative subcategorization frames, where each of the encountered options needs to be included as a separate alternative.
The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al 1993) and concentrates on the simple noun phrase. $$$$$ We furthermore assume that every head noun and head verb has a lexical link to a unary predicate in a taxonomic domain model; that unary predicate is the most specific semantic class of entities denoted by the headword.
The output produced is in the tradition of partial parsing (Hindle 1983, McDonald 1992, Weischedel et al 1993) and concentrates on the simple noun phrase. $$$$$ This paper reports our experiments in predicting parts of speech of highly ambiguous words, predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints, and learning case frame information for verbs from example uses.

Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information. $$$$$ Though the probability model employed is not new, our empirical findings are novel.
Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information. $$$$$ From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.
Weischedel et al (1993) combine several heuristics in order to estimate the token generation prob ability according to various types of information. $$$$$ &quot;A cache-based natural language model for recognition.&quot; In Transactions on Pattern Analysis and Machine Intelligence, 12,570-583.

In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993). $$$$$ Guided by the past success of probabilistic models in speech processing, we have integrated probabilistic models into our language processing systems.
In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993). $$$$$ To see whether our four hypotheses (in italics above) effectively addressed the four concerns above, we chose to test the hypotheses on two well-known problems: ambiguity (both at the structural level and at the part-of-speech level) and inferring syntactic and semantic information about unknown words.
In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993). $$$$$ 3.
In our framework, we employ a simple HMM-based tagger, where the most probable tag sequence, given the words, is out put (Weischedel et al, 1993). $$$$$ Kupiec, J.

In addition to the ending, Weischedel et al (1993) exploit capitalisation. $$$$$ These required much detailed, handcrafted knowledge from several sources (e.g., acoustic and phonetic).
In addition to the ending, Weischedel et al (1993) exploit capitalisation. $$$$$ It is well known that a unification parser can process an unknown word by collecting the assumptions it makes while trying to find an interpretation for a sentence.
In addition to the ending, Weischedel et al (1993) exploit capitalisation. $$$$$ Using the semantic probabilities alone p' (X I P, 0) had poorer performance, a 34% error rate.
In addition to the ending, Weischedel et al (1993) exploit capitalisation. $$$$$ Additional text from that domain would undoubtedly contain new words.

 $$$$$ Since probability theory offers a general mathematical modeling tool for estimating how likely an event is, probability theory may be applied at all levels in natural language processing, because some set of events can be associated with each algorithm.
 $$$$$ Titles and bibliographies will cause similar distortions in a system trained on mixed case and using capitalization as a feature.
 $$$$$ For example, in the grammar used for this test, two different attachments for a prepositional phrase produced trees with the same set of rules, but differing in shape.

 $$$$$ From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.
 $$$$$ In fact, one should be able to improve the estimate of a tree's likelihood via p(S I W) -,-- p(S I T) * p(T I W).
 $$$$$ From these experiments, we are convinced that probabilistic models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpus, by supplementing knowledge-based techniques.
 $$$$$ The probabilistic algorithm is critical to selecting the appropriate generalizations to make from a set of examples.

The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case. $$$$$ &quot;Augmenting a hidden Markov model for phrase-dependent tagging.&quot; In Speech and Language Workshop.
The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case. $$$$$ Based on the results of those experiments, we have constructed a new natural language system (PLUM) for extracting data from text, e.g., newswire text.
The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case. $$$$$ For example, in the grammar used for this test, two different attachments for a prepositional phrase produced trees with the same set of rules, but differing in shape.
The practice of allowing only open-class tags for unknown words goes back a long way (Weischedel et al, 1993), and proved highly beneficial also in our case. $$$$$ Thus we estimate the probability of attachment as p/ (X I P,0) * p(d).
