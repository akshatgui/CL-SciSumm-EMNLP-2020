In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994). $$$$$ For instance, compounds occur very often in Chi- nese text, but none of the existing systems except ours pay much attention to identify them.
In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994). $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.

Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). $$$$$ Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words.
Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.

Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. $$$$$ In this paper, we propose the possible solutions for the above difficulties.
Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. $$$$$ Also there are many word segmentation techniques been developed.
Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. $$$$$ However the basic unit for sentence parsing and understanding is word.
Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. $$$$$ The statistical data supports that the maximal match- ing algorithm is the most effective heuristics.

In our segmentation system, a hybrid strategy is applied (Figure 1) $$$$$ Usually they use a lexicon with a large set of entries to match input sentences \[2,10,12,13,14,21\].

In our system, the well-known forward maximum matching algorithm (Chen and Liu, 1992) is implemented. $$$$$ However the basic unit for sentence parsing and understanding is word.
In our system, the well-known forward maximum matching algorithm (Chen and Liu, 1992) is implemented. $$$$$ In this paper, we like to raise the ptx~blems and the difficulties in identifying words and suggest the possible solutions.

(Chen and Liu,1992) and many subsequent works), or uses the position of characters in a word as the basis for segmentation (Xue, 2003). In terms of processing model, this is a contradiction since segmentation should be the pre-requisite of dictionary lookup and should not presuppose lexical information. $$$$$ The difficulties of identifying words include (l) the identification of com- plex words, such as Determinative-Measure, redupli- cations, derived words etc., (2) the identification of proper names,(3) resolving the ambiguous segmenta- tions.

The classical model, described in (Chen and Liu, 1992) and still adopted in many recent works, considers text segmentation as a tokenization. $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.

This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.

