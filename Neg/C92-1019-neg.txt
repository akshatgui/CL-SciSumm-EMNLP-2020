In general, the word segmentation program utilizes the word entries, part-of-speech (POS) information (Chen and Liu, 1992) in a monolingual dictionary, segmentation rules (Palmer, 1997), and some statistical information (Sproat, et al, 1994). $$$$$ However the basic unit for sentence parsing and understanding is word.

Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). $$$$$ Therefore the first step of processing Chinese sentences is to identify the words( i.e. segment the character strings of the sentences into word strings).
Early word-based segmentation work employed simple heuristics like dictionary-lookup maximum matching (Chen and Liu, 1992). $$$$$ The statistical data supports that the maximal match- ing algorithm is the most effective heuristics.

Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. $$$$$ Therefore the first step of processing Chinese sentences i to identify the words.
Here, the Forward Maximum Matching algorithm (Chen and Liu, 1992) based on a dictionary is adopted; c) Stop words filtering. $$$$$ Proper name is another type of words which cannot be listed exhaus- tively in the lexicon.

In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate asegmentation result. $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.
In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate asegmentation result. $$$$$ There is no agreement in what extend words are considered to be correctly iden- tified.
In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate asegmentation result. $$$$$ Also there are many word segmentation techniques been developed.

In our system, the well-known forward maximum matching algorithm (Chen and Liu, 1992) is implemented. $$$$$ In this paper, we propose the possible solutions for the above difficulties.

(Chen and Liu,1992) and many subsequent works), or uses the position of characters in a word as the basis for segmentation (Xue, 2003). In terms of processing model, this is a contradiction since segmentation should be the pre-requisite of dictionary lookup and should not presuppose lexical information. $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.

The classical model, described in (Chen and Liu, 1992) and still adopted in many recent works, considers text segmentation as a tokenization. $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.
The classical model, described in (Chen and Liu, 1992) and still adopted in many recent works, considers text segmentation as a tokenization. $$$$$ In this paper, we like to raise the ptx~blems and the difficulties in identifying words and suggest the possible solutions.

This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). $$$$$ We adopt a matching algo- rithm with 6 different heuristic rules to resolve the am- biguities and achieve an 99.77% of the success rate.
This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). $$$$$ For instance, compounds occur very often in Chi- nese text, but none of the existing systems except ours pay much attention to identify them.
This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006). $$$$$ The statistical data supports that the maximal match- ing algorithm is the most effective heuristics.

Take for example maximum matching, which was a popular algorithm at the early stage of research (Chen and Liu, 1992). $$$$$ Therefore the first step of processing Chinese sentences i to identify the words.
Take for example maximum matching, which was a popular algorithm at the early stage of research (Chen and Liu, 1992). $$$$$ Keh- J iann Chen Sh ing- l luan Liu Institute of lnfl~rmation Science Academia Sinica Chinese sentences are composed with string of characters without blanks to mark words.
Take for example maximum matching, which was a popular algorithm at the early stage of research (Chen and Liu, 1992). $$$$$ Therefore the first step of processing Chinese sentences i to identify the words.
