This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). $$$$$ For small values of Î± the net effect is the opposite of typical smoothing, since it tends to redistribute probably mass away from unlikely events onto more likely ones.
This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). $$$$$ However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession.
This is in line with earlier work on consistent estimation for similar models (Zollmann and Sima? an, 2006), and agrees with the most up-to-date work that employs Bayesian priors over the estimates (Zhang et al., 2008). $$$$$ However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession.

Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. $$$$$ Combining the two approaches, we have a staged training procedure going from the simplest unconstrained word based model to a constrained Bayesian word-level ITG model, and finally proceeding to a constrained Bayesian phrasal model.
Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. $$$$$ Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.
Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. $$$$$ The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large.
Also most up-to-date, (Zhang et al, 2008) report on a multi-stage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator and concentrating the efforts on pruning both the space of phrase pairs and the space of (ITG) analyses. $$$$$ A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure.

It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. $$$$$ Starting from the model from Section 2, we propose the following Bayesian extension, where A â€” Dir(B) means the random variable A is distributed according to a Dirichlet with parameter B: The parameters Î±X and Î±C control the sparsity of the two distributions in our model.
It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. $$$$$ Computational complexity arises from the exponentially large number of decompositions of a sentence pair into phrase pairs; overfitting is a problem because as EM attempts to maximize the likelihood of its training data, it prefers to directly explain a sentence pair with a single phrase pair.
It differs from (Zhang et al, 2008) in that it does postulate a latent segmentation variable and puts the prior directly over that variable rather than over the ITG synchronous rule estimates. $$$$$ Only spans that are within some threshold of the unrestricted Model 1 scores VF and VB are kept: Amongst those spans retained by this first threshold, we keep only those bitext cells satisfying both The tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O(n4) time.

As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. $$$$$ We can train this model using a two-dimensional extension of the inside-outside algorithm on bilingual data, assuming every phrase pair that can appear as a leaf in a parse tree of the grammar a valid candidate.
As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. $$$$$ The rewriting process continues until the third rule is invoked.
As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. $$$$$ We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases.
As well as smoothing, we find (in the same vein as (Zhang et al, 2008)) that setting effective priors/smoothing is crucial for EM to arrive at better estimates. $$$$$ (1996).

The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. $$$$$ We have presented an improved and more efficient method of estimating phrase pairs directly.
The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. $$$$$ A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure.
The samplers are initialised with trees created from GIZA++ Model 4 alignments, altered such that they are consistent with our ternary grammar. This is achieved by using the factorisation algorithm of Zhang et al (2008a) to first create initial trees. $$$$$ In this paper, we attempt to address these two issues in order to apply EM above the word level.

Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed. $$$$$ On top of these hard constraints, the sparse prior of VB helps make the model less prone to overfitting to infrequent phrase pairs, and thus improves the quality of the phrase pairs the model learns.
Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed. $$$$$ Unlike Johnson (2007), who found optimal performance when Î± was approximately 10âˆ’4, we observed monotonic increases in performance as Î± dropped.
Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed. $$$$$ Kurihara and Sato (2006) describe VB for PCFGs, showing the only need is to change the M step of the EM algorithm.
Zhang et al (2008) suggest tic-tac-toe pruning, which uses Model 1 posteriors to exclude ranges of cells from being computed. $$$$$ However, in experiments in unsupervised POS tag learning using HMM structured models, Johnson (2007) shows that VB is more effective than Gibbs sampling in approaching distributions that agree with the Zipfâ€™s law, which is prominent in natural languages.

 $$$$$ Now we transition into the second stage â€“ the phrasal training.
 $$$$$ The other is the distribution of the phrase pairs. Î±C is crucial, since the multinomial it is controlling has a high dimension.
 $$$$$ Pruning bitext spans and cells requires VF (i, j), the score of the best bitext cell within a given span, as well as all cells within a given threshold of that best score.
 $$$$$ Unfortunately these experiments are very slow.

 $$$$$ In this part, we use word-based ITG alignments as anchor points in the alignment space to pin down the potential phrases.
 $$$$$ The figure of merit considers the Model 1 scores of not only the words inside a given cell, but also all the words not included in the source and target spans, as in Moore (2003) and Vogel (2005).
 $$$$$ In reality, the set of useful chart elements is much smaller than the possible scriptO(n4), where n is the average sentence length.

We used a variant of the phrasal ITG described by Zhang et al (2008). $$$$$ We put a length limit of 35 on both sides, producing a training set of 141K sentence pairs.
We used a variant of the phrasal ITG described by Zhang et al (2008). $$$$$ The rewriting process continues until the third rule is invoked.
We used a variant of the phrasal ITG described by Zhang et al (2008). $$$$$ The decoder also used a trigram language model trained on the target side of the training data, as well as word count, phrase count, and distortion penalty features.
We used a variant of the phrasal ITG described by Zhang et al (2008). $$$$$ We have presented an improved and more efficient method of estimating phrase pairs directly.

Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. $$$$$ We have shown that VB is both practical and effective for use in MT models.
Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. $$$$$ The benefit is two-fold.
Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. $$$$$ The cost of generating an OUTSIDE at position a is O(a) = P(ta|NULL) + &0[i,j] P(ta|sb); likewise the cost of generating an INSIDE column Directly computing O and I would take time O(n2) for each source span, leading to an overall runtime of O(n4).
Since many widely used SCFGs meet these criteria, including hierarchical phrase-based translation grammars (Chiang, 2007), SAMT grammars (Zollmann and Venugopal, 2006), and phrasal ITGs (Zhang et al, 2008), a detailed analysis of  containing and higher rank grammars is left to future work. $$$$$ A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure.

We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). $$$$$ Only spans that are within some threshold of the unrestricted Model 1 scores VF and VB are kept: Amongst those spans retained by this first threshold, we keep only those bitext cells satisfying both The tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O(n4) time.
We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). $$$$$ The last author was supported by NSF IIS-0546554.
We opt for an alternative alignment technique, similar to the word-aligner described by Zhang et al (2008). $$$$$ We chose Variational Bayes, for its procedural similarity to EM and ease of implementation.

Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems.
Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ The last author was supported by NSF IIS-0546554.
Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ First, the Dirichlet is conjugate to the multinomial distribution, meaning that if we select a Dirichlet prior and a multinomial likelihood function, the posterior distribution will again be a Dirichlet.
Cherry and Lin (2007) and Zhang et al (2008) used synchronous ITG (Wu, 1997) and constraints to find non-compositional phrasal equivalences, but they suffered from intractable estimation problem. $$$$$ After several training iterations, we obtain the Viterbi alignments on the training data according to the final model.

The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). $$$$$ Over the pruned charts, we ran 10 iterations of word-based ITG using EM or VB.
The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). $$$$$ In this direction, Expectation Maximization at the phrase level was proposed by Marcu and Wong (2002), who, however, experienced two major difficulties: computational complexity and controlling overfitting.
The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). $$$$$ We attack computational complexity by adopting the polynomial-time Inversion Transduction Grammar framework, and by only learning small noncompositional phrases.
The block ITG family permits multiple links to be on (aij 6= off) for a particular word ei via terminal block productions, but ensures that every word is 32 in at most one such terminal production, and that the full set of terminal block productions is consistent with ITG reordering patterns (Zhang et al, 2008). $$$$$ We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework.

Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ The lowest AER using EM is achieved after the second iteration, which is .40.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ By both changing the objective function to include a bias toward sparser models and improving the pruning techniques and efficiency, we achieve significant gains on test data with practical speed.
Zhang et al (2008b) and Chang et al (2008) show that get ting the tokenization of one of the languages in the corpus close to a gold standard does not necessarily help with building better machine translation systems. $$$$$ We summarize the pipeline of our system, demonstrating the interactions between the three main contributions of this paper: Variational Bayes, tic-tactoe pruning, and word-to-phrase bootstrapping.

Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. $$$$$ There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.
Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. $$$$$ Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005).
Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. $$$$$ This section introduces a technique that bootstraps candidate phrase pairs for phrase-based ITG from word-based ITG Viterbi alignments.
Johnson (2007) and Zhang et al (2008a) show having small helps to control over fitting. $$$$$ A natural solution to several of these issues is unite the word-level and phrase-level models into one learning procedure.

Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). $$$$$ There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.
Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). $$$$$ To begin, let us restrict our attention to the forward direction for a fixed source span (i, j).
Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). $$$$$ We also trained a baseline model with GIZA++ (Och and Ney, 2003) following a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4.
Of these, some concentrate on evaluating word-alignment, directly such as (Zhang et al, 2008) or indirectly by evaluating a heuristically trained hierarchical translation system from sampled phrasal alignments (Blunsom et al, 2009). $$$$$ Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results.

Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. $$$$$ We computed Chinese-toEnglish and English-to-Chinese word translation tables using five iterations of Model 1.
Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. $$$$$ The scope of iterative phrasal ITG training, therefore, is limited to determining the boundaries of the phrases anchored on the given one-toone word alignments.
Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. $$$$$ Formally, given a word-based ITG alignment, the bootstrapping algorithm finds all the phrase pairs according to the definition of Och and Ney (2004) and Chiang (2005) with the additional constraint that each phrase pair contains at most one word link.
Similarly, the work in (Zhang et al, 2008) reports on a multistage model, without a latent segmentation variable, but with a strong prior preferring sparse estimates embedded in a Variational Bayes (VB) estimator. $$$$$ This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches.

In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. $$$$$ While this approach has been very successful, poor wordlevel alignments are nonetheless a common source of error in machine translation systems.
In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. $$$$$ We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases.
In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. $$$$$ Furthermore it would obviate the need for heuristic combination of word alignments.
In (Zhang et al, 2008), Bayesian learning was applied for estimating word-alignments within a synchronous grammar. $$$$$ The first stage of training is word-based ITG, using the standard iterative training procedure, except VB replaces EM to focus on a sparse prior.

Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). $$$$$ Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.
Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). $$$$$ Mathematically, let e(i, j) count the number of word links that are emitted from the substring ez...j, and f(l, m) count the number of word links emitted from the substring fl...ry,t.
Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). $$$$$ If we do not put any constraint on the distribution of phrases, EM overfits the data by memorizing every sentence pair.
Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst-case complexity of n2, compared with the n3 amortized time of the tic-tac-toe pruning algorithm described by (Zhang et al, 2008a). $$$$$ In the end, a Viterbi pass for the phrasal ITG is executed to produce the non-compositional phrasal alignments.

Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters. $$$$$ There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.
Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters. $$$$$ There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.
Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters. $$$$$ Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment.
Zhang et al (2008) and others propose dealing with this problem by putting a prior probability P (? x,? t) on the parameters. $$$$$ From this alignment, phrase pairs are extracted in the usual manner, and a phrase-based translation system is trained.
