This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. $$$$$ We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. $$$$$ Although they use different parsing algorithms, and differ on whether or not dependencies are labeled, they share the idea of greedily pursuing a single path, following parsing decisions made by a classifier.
This approach differs from that of Yamada and Matsumoto (2003) and Sagae and Lavie (2005), who parallelize according to the POS tag of one of the child items. $$$$$ Although the reasons for such a large disparity in results is currently the subject of further investigation, we speculate that a relatively small difference in initial classifier accuracy results in larger differences in parser performance, due to the deterministic nature of the parser (certain errors may lead to further errors).

Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. $$$$$ While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to those produced by statistical parsers with higher run-time complexity.
Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. $$$$$ MBLpar, on the other hand, performed poorly in terms of accuracy and speed.
Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. $$$$$ We conducted experiments with the parser described in section 2 using two different classifiers: TinySVM (a support vector machine implementation by Taku Kudo)2, and the memory-based learner TiMBL (Daelemans et al., 2004).
Like Yamada and Matsumoto (2003) and Sagae and Lavie (2005), our parser is driven by classifiers. $$$$$ We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.

Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Using SVMs for classification, the parser has labeled constituent precision and recall higher than 87% when using the correct part-of-speech tags, and slightly higher than 86% when using automatically assigned partof-speech tags.
Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Although its accuracy is not as high as those of state-of-the-art statistical parsers, our classifier-based parser is considerably faster than several well-known parsers that employ search or dynamic programming approaches.
Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ We present a classifier-based parser that produces constituent trees in linear time.
Although training time is still a concern in our setup, the situation is ameliorated by generating training examples in advance and inducing one-vs-all classifiers in parallel, a technique similar in spirit to the POS-tag parallelization in Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Every experiment reported here was performed on a Pentium IV 1.8GHz with 1GB of RAM.

We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002).
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ Specifically, what new non-terminal is introduced in unary or binary reduce actions, or which of the left or right children are chosen as the source of the lexical head of the new subtree produced by binary reduce actions.
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.

A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). $$$$$ In this work we focus only on the processing that occurs once POS tagging is completed.
A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). $$$$$ The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar.
A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). $$$$$ This may be accomplished with SVMs using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004).
A Shift-Reduce Algorithm for Deterministic Constituent Parsing In its deterministic form, our parsing algorithm is the same single-pass shift-reduce algorithm as the one used in the classifer-based parser of Sagae and Lavie (2005). $$$$$ We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.

Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. $$$$$ We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.
Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. $$$$$ Instead of using the classifier to determine the parser’s actions, we simply determine the correct action by consulting the correct parse trees.
Sagae and Lavie (2005) built two deterministic parsers this way, one using support vector machines, and one using k-nearest neighbors. $$$$$ We have presented a simple shift-reduce parser that uses a classifier to determine its parsing actions and runs in linear time.

Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). $$$$$ We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). $$$$$ Despite their greedy nature, these parsers achieve high accuracy in determining dependencies.
Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). $$$$$ The total time required to parse the entire test set was 11 minutes.
Features used for classification, with features 1 to 13 taken from Sagae and Lavie (2005). $$$$$ We present a classifier-based parser that produces constituent trees in linear time.

Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Because the parser greedily builds trees bottom-up in one pass, considering only one path at any point in the analysis, the task of assigning POS tags to words is done before other syntactic analysis.
Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ Using SVMs for classification, the parser has labeled constituent precision and recall higher than 87% when using the correct part-of-speech tags, and slightly higher than 86% when using automatically assigned partof-speech tags.
Training the maximum entropy classifier with such a large number (1.9 million) of training instances and features required more memory than was available (the maximum training set size we were able to train with 2GB of RAM was about 200,000 instances), so we employed the training set splitting idea used by Yamada and Matsumoto (2003) and Sagae and Lavie (2005). $$$$$ In particular, the use of tree features seems appealing.

For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. $$$$$ For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002).
For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. $$$$$ While Yamada and Matsumoto use a quadratic run-time algorithm with multiple passes over the input string, Nivre and Scholz use a simplified version of the algorithm described here, which handles only (labeled or unlabeled) dependency structures.
For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. $$$$$ In addition, Ratnaparkhi’s parser uses a more involved algorithm that allows it to work with arbitrary branching trees without the need of the binarization transform employed here.
For comparison, Sagae and Lavie (2005) report that training support vector machines for one-against-all multi-class classification on the same set of features for their deterministic parser took 62 hours, and training a k-nearest neighbors classifier took 11 minutes. $$$$$ While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to those produced by statistical parsers with higher run-time complexity.

More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). $$$$$ While this may seem obvious, the differences observed here are much greater than what would be expected from looking, for example, at results from chunking/shallow parsing (Zhang et al., 2001; Kudo and Matsumoto, 2001; Veenstra and van den Bosch, 2000).
More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). $$$$$ However, dependency analyses lack important information contained in constituent structures.
More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). $$$$$ We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
More interestingly, it parses all 2,416 sentences (more than 50,000 words) in only 46 seconds, 10 times faster than the deterministic SVM parser of Sagae and Lavie (2005). $$$$$ Although its accuracy is not as high as those of state-of-the-art statistical parsers, our classifier-based parser is considerably faster than several well-known parsers that employ search or dynamic programming approaches.

Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ Our parser employs a basic bottom-up shift-reduce parsing algorithm, requiring only a single pass over the input string.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ Using decision trees and fewer features, Kalt’s parser has significantly faster training and parsing times, but its accuracy is much lower than that of our parser.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ Future work includes the investigation of the effects of individual features, the use of additional classification features, and the use of different classifiers.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ The classifier in the SVM-based parser (denoted by SVMpar) uses the polynomial kernel with degree 2, following the work of Yamada and Matsumoto (2003) on SVM-based deterministic dependency parsing, and a one-against-all scheme for multi-class classification.

One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005). $$$$$ While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to those produced by statistical parsers with higher run-time complexity.
One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005). $$$$$ Although state-of-the-art statistical parsers (Collins, 1997; Charniak, 2000) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data.
One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005). $$$$$ Out of more than 2,400 sentences, only 26 were rejected by the parser (about 1.1%).
One extrapolation is to a very fast stochastic parser by Sagae and Lavie (2005). $$$$$ We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.

 $$$$$ This includes, crucially, the two topmost items in the stack S, and the item in front of the queue W. Additionally, a set of context features is derived from a (fixed) limited number of items below the two topmost items of S, and following the item in front of W. The specific features are shown in figure 2.
 $$$$$ This involves the removal of non-terminals introduced in the transformation process, producing trees with arbitrary branching.

In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). $$$$$ This may be the number of non-terminal types seen in the training data, or the length of the longest chain of unary productions seen in the training data.
In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). $$$$$ In a unary reduction, the item on top of S is popped, and a new item is pushed onto S. The new item consists of a tree formed by a non-terminal node with the popped item as its single child.
In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). $$$$$ While in many natural language processing tasks different classifiers perform at similar levels of accuracy, we have observed a dramatic difference between using support vector machines and a memory-based learner.
In the deterministic setting there is only one correct path, so example generation is identical to that of Sagae and Lavie (2005). $$$$$ These instances can be obtained by running the algorithm on a corpus of sentences for which the correct parse trees are known.

Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ The parser of Wong and Wu (1999) uses a separate NP-chunking step and, like Ratnaparkhi’s parser, does not require a binary transracy, and time required to parse the test set.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar.
Like our work, Ratnaparkhi (1999) and Sagae and Lavie (2005) generate examples off-line, but their parsing strategies are essentially shift-reduce so each sentence generates only O (n) training examples. $$$$$ Additionally, our parser is in some ways similar to the maximum-entropy parser of Ratnaparkhi (1997).

This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. $$$$$ We conducted experiments with the parser described in section 2 using two different classifiers: TinySVM (a support vector machine implementation by Taku Kudo)2, and the memory-based learner TiMBL (Daelemans et al., 2004).
This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. $$$$$ The parsing algorithm involves two main data structures: a stack S, and a queue W. Items in S may be terminal nodes (POS-tagged words), or (lexicalized) subtrees of the final parse tree for the input string.
This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. $$$$$ Using decision trees and fewer features, Kalt’s parser has significantly faster training and parsing times, but its accuracy is much lower than that of our parser.
This model is inspired by Sagae and Lavie (2005), in which a stack-based representation of monolingual parsing trees is used. $$$$$ This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing.

Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. $$$$$ Out of more than 2,400 sentences, only 26 were rejected by the parser (about 1.1%).
Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. $$$$$ Additionally, we plan to investigate the use of the beam strategy of Ratnaparkhi (1997) to pursue multiple parses while keeping the run-time linear.
Sagae and Lavie (2005) propose a constituency based parsing method to determine sentence dependency structures. $$$$$ Training the parser is accomplished by training its classifier.

In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. $$$$$ Although state-of-the-art statistical parsers (Collins, 1997; Charniak, 2000) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data.
In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. $$$$$ At the same time, it is significantly more accurate than previously proposed deterministic parsers for constituent structures.
In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. $$$$$ Additionally, our parser is in some ways similar to the maximum-entropy parser of Ratnaparkhi (1997).
In our approach, which is based on the shift-reduce parser for English reported in (Sagae and Lavie,2005), the parsing task is transformed into a succession of classification tasks. $$$$$ We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers.

A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. $$$$$ Like other deterministic parsers (and unlike many statistical parsers), our parser considers the problem of syntactic analysis separately from partof-speech (POS) tagging.
A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. $$$$$ We present a classifier-based parser that produces constituent trees in linear time.
A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. $$$$$ Like other deterministic parsers (and unlike many statistical parsers), our parser considers the problem of syntactic analysis separately from partof-speech (POS) tagging.
A simple transformation process as described in (Sagae and Lavie, 2005) is employed to convert between arbitrary branching trees and binary trees. $$$$$ The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar.

Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time. $$$$$ We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.
Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time. $$$$$ We have also shown that much of the success of a classifier-based parser depends on what classifier is used.
Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time. $$$$$ We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.
Sagae and Lavie (2005) have shown that this algorithm has linear time complexity, assuming that classification takes constant time. $$$$$ This involves the removal of non-terminals introduced in the transformation process, producing trees with arbitrary branching.
