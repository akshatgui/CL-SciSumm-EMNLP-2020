Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation.
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Thus, it penalizes or rewards a pair of links for being simultaneously present.
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ More complex models would widen BP’s advantage.
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation.

The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ Table 1 shows our asymptotic runtimes for all factors in §§3.3–3.4.
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ Handling the subfactors in parallel, (3)–(4), would need O(n) iterations.
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ Fig.
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ The difficult step in computing the gradient of our objective is finding Vθ log Z, where Z in equation (1) is the normalizing constant (partition function) that sums over all assignments A.

We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate $$$$$ BP computes local beliefs, e.g. the conditional probability that a link Lij is present.
We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate $$$$$ (In constraint satisfaction, joining constraints similarly makes arc consistency slower but better at eliminating impossible values.) accuracy under a given first-order, non-projective model.
We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate $$$$$ For PTREE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996).

In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ (In constraint satisfaction, joining constraints similarly makes arc consistency slower but better at eliminating impossible values.) accuracy under a given first-order, non-projective model.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ To determine BP’s search error at finding the MBR parse, we measured its dependency accuracy not against the gold standard, but against the optimal MBR parse under the model, which DP is able to find.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ Computational linguists worry constantly about runtime.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ 21There are no NOCROSSje factors with f = j.

The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ Computational linguists worry constantly about runtime.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ They marginalize over other variables such as tags and link roles.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ Computational linguists worry constantly about runtime.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ To determine BP’s search error at finding the MBR parse, we measured its dependency accuracy not against the gold standard, but against the optimal MBR parse under the model, which DP is able to find.

Our contributions are not limited to dependency parsing $$$$$ In our experiments, we actually take the edge weights to be not the messages �qLij from the links, def �bLij = log bLij(true)/bLij(false)).
Our contributions are not limited to dependency parsing $$$$$ PTREE still propagates in O(n3) time: simply change the first-order parser’s semiring (Goodman, 1999) to use max instead of sum.
Our contributions are not limited to dependency parsing $$$$$ We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
Our contributions are not limited to dependency parsing $$$$$ In the case of TREE, dynamic matrix inverse allows us to update any row or column (i.e., messages from all parents or children of a given word) and find the new inverse in O(n2) time (Sherman and Morrison, 1950).

Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ Shorthand for the family of O(n3) binary factors {PAIRij,ik}, which judge whether two children of the same parent are compatible.
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ This iterated negotiation among the factors is handled by message passing along the edges of the factor graph.
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ BP keeps runtime down to O(n3)—although with a higher constant factor, since it takes several rounds to converge, and since it computes more than just the best parse.29 Figures 2–3 compare the empirical runtimes for various input sentence lengths.
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ If max-product BP converges, we may simply output each variable’s favorite value (according to its belief), if unique.

Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ A first-order (or “edge-factored”) parsing model (McDonald et al., 2005) contains only LINK factors, along with a global TREE or PTREE factor.
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ (2) How helpful are such higherorder factors—particularly for non-projective parsing, where BP is needed to make them tractable?
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ F and k are clear from context below, so we simplify the notation so that (7)–(8) become TRIGRAM must sum over assignments to the tag sequence T. The belief (6) in a given assignment is a product of trigram scores (which play the role of transition weights) and incoming messages qTj (playing the role of emission weights).

Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ Thus first-order parsing, part-of-speech tagging, and other tasks on a common input could mutually influence one another.
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links.
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ Notice that the above decoding approaches do not enforce any hard constraints other than TREE in the final output.
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ Still, BP often leads to good, fast approximations.

Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ NOCROSS.
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Alternatively, we write down a better but intractable model and then use approximations.
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Belief propagation improves non-projective dependency parsing with features that would make exact inference intractable.
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.

 $$$$$ Sometimes we oversimplify our models, trading linguistic nuance for fast dynamic programming.
 $$$$$ Alternatively, we write down a better but intractable model and then use approximations.
 $$$$$ A first-order (or “edge-factored”) parsing model (McDonald et al., 2005) contains only LINK factors, along with a global TREE or PTREE factor.
 $$$$$ (If j and ` do not each have exactly one parent, NOCROSSjt fires with value 0; i.e., it incorporates EXACTLY1j and EXACTLY1t.

 $$$$$ BP’s behavior in our setup can be understood intuitively as follows.
 $$$$$ (Sleator and Temperley, 1993; Buch-Kromann, 2006).
 $$$$$ In general, one can get better BP approximations by replacing a group of factors F,,t(A) with their product.30 The above experiments concern gold-standard 30In the limit, one could replace the product (1) with a single all-purpose factor; then BP would be exact—but slow.

However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ Steps 3–4 are just as in EXACTLY1j.
However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ (Sleator and Temperley, 1993; Buch-Kromann, 2006).
However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ A slower but potentially more stable alternative is deterministic annealing.

Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ CHILDSEQi , like TRIGRAM, is propagated by a forward-backward algorithm.
Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ Handling the subfactors in parallel, (3)–(4), would need O(n) iterations.
Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ Incorporating additional features would increase the runtime additively rather than multiplicatively.
Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ Finally, we can take advantage of improvements to BP proposed in the context of other applications.

To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ We show how to apply loopy belief propagation (BP), a simple and tool for and inference.
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ TREE.
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ Loopy BP works not with random samples but their expectations.
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e' to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e'.

Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Fortunately, BP provides an estimate of that marginal distribution, namely, its belief about the factor Fm, given W and 0 (§4.2).25 Note that the hard constraints do not depend on 0 at all; so their summands in equation (10) will be 0.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Notice that the above decoding approaches do not enforce any hard constraints other than TREE in the final output.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ 23Ignoring the treatment of boundary symbols “#” (see §3.4).

For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ In this paper, we show that BP can be used to train and decode complex parsing models.
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ In all three languages, the first-order non-projective parser greatly overpredicts the number of crossing links.
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ Each outgoing message from a variable can be computed in time proportional to its size, which may be amortized against the cost of generating the corresponding incoming message.

In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ (The method is approximate because a first-order parser must equally penalize all parses containing e', even those that do not in fact contain e.) This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser.
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ Hence we seek approximations.
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ Other parsing-like problems that could be attacked with BP appear in syntax-based machine translation.
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ (Recall that Z, like each Fm, depends implicitly on W and 0.)

This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ However, our application to parsing requires an innovation to BP that we explain in §5—a global constraint to enforce that the parse is a tree.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ However, once we add the CHILDSEQ factor, BP is always faster— dramatically so for longer sentences (Fig.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ Computational linguists worry constantly about runtime.

One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ It is convenient to visualize an undirected factor graph (Fig.
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ Our method and its numerical details emerge naturally as an instance of the well-studied loopy BP algorithm, suggesting several potential future improvements to accuracy (Yedidia et al., 2004; Braunstein et al., 2005) and efficiency (Sutton and McCallum, 2007).
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ We built a first-order projective parser—one that uses only factors PTREE and LINK—and then compared the cost of incorporating second-order factors, GRAND and CHILDSEQ, by BP versus DP.28 Under DP, the first-order runtime of O(n3) is increased to O(n4) with GRAND, and to O(n5) when we add CHILDSEQ as well.
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ Hence we seek approximations.
