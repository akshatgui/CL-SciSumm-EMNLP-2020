Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ If all variables are observed in training, this objective function is convex (as for any log-linear model).
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Variable L34 maintains a distribution over values true and false—a “belief”—that is periodically recalculated based on the current distributions at other variables.8 Readers familiar with Gibbs sampling can regard this as a kind of deterministic approximation.
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
Note that finding taxonomy trees is a structurally identical problem to directed spanning trees (and thereby non-projective dependency parsing), for which belief propagation has previously been worked out in depth (Smith and Eisner, 2008). $$$$$ Our training method also uses beliefs computed by BP, but at the factors.

The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ Alternatively, we write down a better but intractable model and then use approximations.
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ We thus added NOCROSS factors, as well as GRAND and CHILDSEQ as before.
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ Alternatively, given any of the MAP decoding procedures from §6, we could use an error-driven learning method such as the perceptron or MIRA.26
The MST that is found using these edge scores is actually the minimum Bayes risk tree (Goodman, 1996) for an edge accuracy loss function (Smith and Eisner, 2008). $$$$$ That is, it prohibits Lij and LH from both being true if i —* j crosses k —* E. (These links are said to cross if one of k, E is strictly between i and j while the other is strictly outside that range.)

We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ In step 2, the marginal belief b(Lij = true) is now 7r times the total weight of all trees having edge i → j.
We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ TRIGRAM is a global factor that evaluates the tag sequence T according to a trigram model.
We present a unified view of two state-of-the-art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ Our final belief about the TREE factor is a distribution over such assignments, in which a tree’s probability is proportional to the probability of its edge weights gLij (incoming messages).

In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). $$$$$ Or would it suffice for more local hard constraints to negotiate locally via BP?
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). $$$$$ The O(n2) boolean variables {Lij : 0 < i < n,1 < j < n, i =� j} correspond to the possible links in the dependency parse.3 Lij = true is interpreted as meaning that there exists a dependency link from parent i —* child j.4 Link roles, etc.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al,2009). $$$$$ We’ll consider various higher-order soft factors: PAIR.

The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ In this paper, we show that BP can be used to train and decode complex parsing models.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ CHILDSEQi , like TRIGRAM, is propagated by a forward-backward algorithm.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ SIB.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ In step 2, the marginal belief b(Lij = true) is now 7r times the total weight of all trees having edge i → j.

Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). $$$$$ L34 and T3 from mutually reinforcing each other’s existing beliefs. an HMM, for example, BP reduces to the forwardbackward algorithm.
Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). $$$$$ Notice that the above decoding approaches do not enforce any hard constraints other than TREE in the final output.
Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). $$$$$ 22We still treat CHILDSEQi as a global factor and compute all its correct outgoing messages on a single BP iteration, via serial forward and backward sweeps through the subfactors.
Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints, which extends some combinatorial factors considered by Smith and Eisner (2008). $$$$$ Replace each factor Fm(A) with Fm(A)1/T , where T > 0 is a temperature.

Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mid (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc.
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ BP computes local beliefs, e.g. the conditional probability that a link Lij is present.
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ In our method, a first-order parser derives such input features from its own previous full output (but probabilistic output rather than just 1best).
Smith and Eisner (2008) proposed a factor graph representation for dependency parsing (Fig. 1). $$$$$ We say that the factor has degree d if it depends on the values of d variables in A, and that it is unary, binary, ternary, or global if d is respectively 1, 2, 3, or unbounded (grows with n).

Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ On the other hand, it may be undesirable for variables whose values we desire to recover.24
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly.
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ Indeed, BP’s beliefs may not be the true marginals of any distribution p(A) over assignments, i.e., they may be globally inconsistent.
Fortunately, for all the hard constraint factors in rows 3-5 of Table 1, this computation can be done in linear time (and polynomial for the TREE factor) - this extends results presented in Smith and Eisner (2008). $$$$$ (3) Do our global constraints (e.g., TREE) contribute to the goodness of BP’s approximation?

Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ For example, instead of updating all messages in parallel at every iteration, it is empirically faster to serialize updates using a priority queue (Elidan et al., 2006; Sutton and McCallum, 2007).31 31These methods need alteration to handle our global propagators, which do update all their outgoing messages at once.
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ We built a first-order projective parser—one that uses only factors PTREE and LINK—and then compared the cost of incorporating second-order factors, GRAND and CHILDSEQ, by BP versus DP.28 Under DP, the first-order runtime of O(n3) is increased to O(n4) with GRAND, and to O(n5) when we add CHILDSEQ as well.
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ Our experiments use the same features as McDonald et al. (2005).
Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP. $$$$$ ATMOST1.

Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Decoding is very expensive with a synchronous grammar composed with an n-gram language model (Chiang, 2007)—but our footnote 10 suggests that BP might incorporate a language model rapidly.
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Hence we seek approximations.
Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. $$$$$ Finally, Table 2 compares loopy BP to a previously proposed “hill-climbing” method for approximate inference in non-projective parsing McDonald and Pereira (2006).

 $$$$$ If the factor graph is acyclic, then BP computes these marginal distributions exactly.
 $$$$$ (Recall that Z, like each Fm, depends implicitly on W and 0.)
 $$$$$ Sometimes we oversimplify our models, trading linguistic nuance for fast dynamic programming.
 $$$$$ We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP).

 $$$$$ Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.
 $$$$$ For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008).
 $$$$$ These are collectively equivalent to ATMOST1, but expressed via a larger number of simpler constraints, which can make the BP approximation less effective (footnote 30).
 $$$$$ Hence we seek approximations.

However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ Finally, we can take advantage of improvements to BP proposed in the context of other applications.
However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ We show how to apply loopy belief propagation (BP), a simple and tool for and inference.
However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ Loopy BP has occasionally been used before in NLP, with good results, to handle non-local features (Sutton and McCallum, 2004) or joint decoding (Sutton et al., 2004).
However, as observed in Smith and Eisner (2008), we can encapsulating common dynamic programming algorithms within special-purpose factors to efficiently globally constrain variable configurations. $$$$$ We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP).

Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1
Let DEP-TREE be a global combinatorial factor, as presented in Smith and Eisner (2008), which attaches to all Link (i, j) variables and similarly contributes a factor of 1 iff the configuration of Link variables forms a valid projective dependency graph. $$$$$ E.g., a given verb may not like to have two noun children both to its left.6 The children do not need to be adjacent.

To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ As a parsing algorithm, BP is both asymptotically and empirically efficient.
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ We could also introduce new variables, e.g., nonterminal refinements (Matsuzaki et al., 2005), or secondary links Mid (not constrained by TREE/PTREE) that augment the parse with representations of control, binding, etc.
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ In general, one can get better BP approximations by replacing a group of factors F,,t(A) with their product.30 The above experiments concern gold-standard 30In the limit, one could replace the product (1) with a single all-purpose factor; then BP would be exact—but slow.
To experiment with this combined model we use loopy belief propagation (LBP; Pearl et al, 1985), previously applied to dependency parsing by Smith and Eisner (2008). $$$$$ Given the BP architecture, do we even need the hard TREE constraint?

Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Also, hill-climbing only produces an (approximate) 1-best parse, but BP also obtains (approximate) marginals of the distribution over all parses.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ String alignment with synchronous grammars is quite expensive even for simple synchronous formalisms like ITG (Wu, 1997)—but Duchi et al. (2007) show how to incorporate bipartite matching into max-product BP.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ It would be straightforward to add other variables, such as a binary variable Lij that is true iff there is a link i � j labeled with role r (e.g., AGENT, PATIENT, TEMPORAL ADJUNCT).
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ This iterated negotiation among the factors is handled by message passing along the edges of the factor graph.

For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ ATMOST1j requires j to have one or zero parents.
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ )7 TAGi is a unary factor that evaluates whether Ti’s value is consistent with W (especially Wi).
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ At step 2, the marginal belief b(Lij = true) sums over the n nonzero assignments containing i → j.
For example, the cubic grandparent edges in second-order dependency parsing slow down dynamic programs (McDonald and Pereira, 2006), belief propagation (Smith and Eisner, 2008) and LP solvers (Martins et al2009), since there are more value functions to evaluate, more messages to pass, or more variables to consider. $$$$$ Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1

In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ These are passed into a fast algorithm for maximum spanning tree (Tarjan, 1977) or maximum projective spanning tree (Eisner, 1996).
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ We’ll consider various higher-order soft factors: PAIR.
In this work we follow Smith and Eisner (2008) and train the models with stochastic gradient descent on the conditional log-likelihood of the training data, using belief propagation in order to calculate approximate gradients. $$$$$ Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard.

This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ Whenever we wish, we may compute the beliefs at V and F: These beliefs do not truly characterize the expected behavior of Gibbs sampling (§4.1), since the products in (5)–(6) make conditional independence assumptions that are valid only if the factor graph is acyclic.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ This procedure is equivalent to minimum Bayes risk (MBR) parsing (Goodman, 1996) with a dependency accuracy loss function.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ Inasmuch as the first-order parser finds that edge e is probable, the higher-order features will kick in and discourage other edges e' to the extent that they prefer not to coexist with e.2 Thus, the next call to the first-order parser assigns lower probabilities to parses that contain these e'.
This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). $$$$$ A more robust technique for extracting an assignment is to mimic Viterbi decoding, and “follow backpointers” of the max-product computation along some spanning subtree of the factor graph.

One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ Finally, we can take advantage of improvements to BP proposed in the context of other applications.
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ We built a first-order projective parser—one that uses only factors PTREE and LINK—and then compared the cost of incorporating second-order factors, GRAND and CHILDSEQ, by BP versus DP.28 Under DP, the first-order runtime of O(n3) is increased to O(n4) with GRAND, and to O(n5) when we add CHILDSEQ as well.
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ Compared to the MBR parse under that model, the search errors from decoding with weaker hard constraints were 2.2% for NOT2, 2.1% for EXACTLY1, 1.7% for EXACTLY1 + NO2CYCLE, and 0.0% for PTREE.
One behavior we observe in the graph is that the DD results tend to incrementally improve in accuracy while the BP results quickly stabilize, mirroring the result of Smith and Eisner (2008). $$$$$ (Note that fh is permitted to consult the observed input W. It also sees which factor Fm it is scoring, to support reuse of a single feature function fh and its weight θh by unboundedly many factors in a model.)
