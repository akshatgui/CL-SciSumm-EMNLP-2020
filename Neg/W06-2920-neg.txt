The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). $$$$$ All the sentences are in one text file and they are separated by a blank line after each sentence.

For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). $$$$$ The results show that experiments on just one or two languages certainly give an indication of the usefulness of a parsing approach but should not be taken as proof that one algorithm is better for “parsing” (in general) than another that performs slightly worse.
For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). $$$$$ While the training data contained all 10 columns (although sometimes only with dummy values, i.e. underscores), the test data given to participants contained only the first 6.
For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). $$$$$ Table 3 shows which column values have been used by participants.
For this paper since we are primarily concerned with the merging of tree structures we only evaluate UAS (Buchholz and Marsi, 2006). $$$$$ Fields are separated from each other by a TAB.5 The 10 fields are: resulting from the PHEAD column is guaranteed to be projective (but is not available for all data sets), whereas the structure resulting from the HEAD column will be non-projective for some sentences of some languages (but is always available).

Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). $$$$$ Finally, Section 8 describes possible directions for future research.
Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). $$$$$ It is surprising, however, how similar their total scores are, given that their approaches are quite different (see Table 2).
Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). $$$$$ “rer.”: used in the reranker only.
Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al, 2005) and information extraction (Culotta and Sorensen, 2004). $$$$$ The training data derived from the original treebanks (see Section 4) and given to the shared task participants was in a simple column-based format that is an extension of Joakim Nivre’s Malt-TAB format4 for the shared task and was chosen for its processing simplicity.

19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). $$$$$ Fields are separated from each other by a TAB.5 The 10 fields are: resulting from the PHEAD column is guaranteed to be projective (but is not available for all data sets), whereas the structure resulting from the HEAD column will be non-projective for some sentences of some languages (but is always available).
19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). $$$$$ Treebanks had to be actually available, large enough, have a license that allowed free use for research or kind treebank providers who temporarily waived the fee for the shared task, and be suitable for conversion into the common format within the limited time.
19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). $$$$$ Other theories related to dependency grammar are word grammar (Hudson, 1984) and link grammar (Sleator and Temperley, 1993).
19.6% of the sentences in the corpus contain non-projective edges and 1.8% of the edges are non-projective, which is almost 5 times more frequent than in English and is the same as the Czech non-projectivity level (Buchholz and Marsi, 2006). $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. $$$$$ Participants took the following approaches to non-projectivity: (Bick, 2006) or if the classifier chooses a special action (Attardi, 2006) or the parser predicts a trace (Schiehlen and Spranger, 2006). training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al., 2006).
The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. $$$$$ The character encoding of all data files is Unicode (specifically UTF-8), which is the only encoding to cover all languages and therefore ideally suited for multilingual parsing.
The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. $$$$$ Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.
The standard procedure for this purpose would be cross-validation. However, the popular data sets used for bench marking parsers, such as those that emerged 1176 from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), are typically based on monolingual text. $$$$$ (2006).

The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). $$$$$ Kudo and Matsumoto (2000) describe a dependency parser for Japanese and Yamada and Matsumoto (2003) an extension for English.
The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). $$$$$ During the last few years, however, treebanks for other languages have become available and some parsers have been applied to several different languages.
The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). $$$$$ We also give an overview of the parsing approaches that participants took and the results that they achieved.
The treebank data used to train the German parser is the Tiger Treebank (Brants et al, 2002), in the version released with the CoNLL-X shared task (Buchholz and Marsi, 2006). $$$$$ The results show that experiments on just one or two languages certainly give an indication of the usefulness of a parsing approach but should not be taken as proof that one algorithm is better for “parsing” (in general) than another that performs slightly worse.

We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. $$$$$ Tesni`ere (1959) introduced the idea of a dependency tree (a “stemma” in his terminology), in which words stand in direct head-dependent relations, for representing the syntactic structure of a sentence.
We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. $$$$$ It is quite small and has by far the longest parsing units.

Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks. $$$$$ ‘+’: used in at least some features.
Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks. $$$$$ As already mentioned in Section 3, the underscore has the punctuation character property, therefore non-last IGs (whose HEAD and DEPREL were introduced by us) are not scoring tokens.
Currently there are about a dozen input/output conversion filters available, covering various existing data formats including the TigerXML format, the for mats of the Penn Treebank (Marcus et al, 1994), the CoNLL-X shared task format (Buchholz and Marsi, 2006), and the formats of the Latin Dependency (Bamman and Crane, 2006), Sinica (Chu Ren et al, 2000), Slovene Dependency (D? zero ski et al, 2006) (SDT), and Alpino (van der Beek et al., 2002) tree banks. $$$$$ Each IG consists of either a stem or a derivational suffix plus all the inflectional suffixes belonging to that stem/derivational suffix.

 $$$$$ Treebanks had to be actually available, large enough, have a license that allowed free use for research or kind treebank providers who temporarily waived the fee for the shared task, and be suitable for conversion into the common format within the limited time.
 $$$$$ Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser?
 $$$$$ Nivre’s parser has been tested for Swedish (Nivre et al., 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al. (2005), while McDonald’s parser has been applied to English (McDonald et al., 2005a), Czech (McDonald et al., 2005b) and, very recently, Danish (McDonald and Pereira, 2006).
 $$$$$ Canisius et al. (2006) are six and Schiehlen and Spranger (2006) even eight ranks higher for Dutch than overall, while Riedel et al.

We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). $$$$$ It is also possible that parsers get confused by the high proportion (one third!)
We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). $$$$$ The character encoding of all data files is Unicode (specifically UTF-8), which is the only encoding to cover all languages and therefore ideally suited for multilingual parsing.
We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). $$$$$ There are many directions for interesting research building on the work done in this shared task.
We evaluate all constraints and measures described in the previous section on 12 languages, whose treebanks were made available in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi,2006). $$$$$ The training data derived from the original treebanks (see Section 4) and given to the shared task participants was in a simple column-based format that is an extension of Joakim Nivre’s Malt-TAB format4 for the shared task and was chosen for its processing simplicity.

Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. $$$$$ In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.
Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. $$$$$ Other easy PoS are articles, with accuracies in the nineties for German, Dutch, Swedish, Portuguese and Spanish.
Additionally, we converted the annotation about scope of negation into a token-per-token representation, following the standard format of the 2006 CoNLL Shared Task (Buchholz and Marsi, 2006), where sentences are separated by a blank line and fields are separated by a single tab character. $$$$$ Section 7 discusses the implications of the results and analyzes the remaining problems.

Datasets and Evaluation Our experiments are run on five different languages $$$$$ Of these, all but one parsed all 12 required languages and 13 also parsed the optional Bulgarian data.
Datasets and Evaluation Our experiments are run on five different languages $$$$$ One is the question which factors make data sets “easy” or difficult.

Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. $$$$$ This approach resulted in many different POSTAG values for the training set and even in unseen values in the test set.
Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. $$$$$ By the time we discovered this, the test data release date was very close and we decided not to release new bug-fixed training material that late. tion)”.29
Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. $$$$$ One is the question which factors make data sets “easy” or difficult.
Multilingual parsers of participants in the CoNLL 2006 shared task (Buchholz and Marsi, 2006) can handle Japanese sentences. $$$$$ Systems were scored by computing the labeled attachment score (LAS), i.e. the percentage of “scoring” tokens for which the system had predicted the correct head and dependency label.

We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). $$$$$ Finally, we checked that, had we also scored on punctuation tokens, total scores as well as rankings would only have shown very minor differences.
We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). $$$$$ It remains to be tested whether our approach resulted in data sets better suited for parsing than the original.
We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.
We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the aver age UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). $$$$$ For example, Johansson and Nugues (2006) and Yuret (2006) are seven ranks higher for Turkish than overall, while Riedel et al. (2006) are five ranks lower.

Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). $$$$$ Within the first approach, each dependency can be labeled independently (Corston-Oliver and Aue, 2006) or a (news, dialogue, novel); type of annotation (d=dependency, c=constituents, dc=discontinuous constituents, +f=with functions, +t=with types).
Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). $$$$$ All data sets except the Chinese one contain some non-projective dependency arcs, although their proportion varies from 0.1% to 5.4%.
Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). $$$$$ Converting a phrase structure treebank with only a few functions to a dependency format usually requires linguistic competence in the treebank’s language in order to create the head table and missing function labels.
Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). $$$$$ Finally, the set of DEPREL values is very small and consequently the ratio between (C)POSTAG and DEPREL values is extremely favorable.

We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. $$$$$ Finally, we checked that, had we also scored on punctuation tokens, total scores as well as rankings would only have shown very minor differences.
We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. $$$$$ We also give an overview of the parsing approaches that participants took and the results that they achieved.
We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. $$$$$ One factor that seems to be irrelevant is the head-final versus head-initial distinction, as both the “easiest” and the most difficult data sets are for head-final languages.
We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. $$$$$ As both groups had much prior experience in multilingual dependency parsing (see Section 2), it is not too surprising that they both achieved good results.

Penn Treebank (Marcus et al, 1993) the HPSG LinGo Redwoods Treebank (Oepen et al, 2002), and a smaller dependency treebank (Buchholz and Marsi, 2006). $$$$$ As both groups had much prior experience in multilingual dependency parsing (see Section 2), it is not too surprising that they both achieved good results.

The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. $$$$$ Cycles do not occur in the shared task data but are scored as normal if predicted by parsers.
The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. $$$$$ Mel’ˇcuk (1988) describes a multistratal dependency grammar, i.e. one that distinguishes between several types of dependency relations (morphological, syntactic and semantic).
The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. $$$$$ For dependency parsing, there seem to be two different interpretations of the term “bottom-up”.
The CoNLL-X (Buchholz and Marsi, 2006) and CoNLL 2007 (Nivre et al, 2007) shared tasks focused on multilingual dependency parsing. $$$$$ Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head.

Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). $$$$$ Each year the Conference on Computational Natural Language Learning features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.
Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). $$$$$ Another is finding out how much of parsing performance depends on annotations such as the lemma and morphological features, which are not yet routinely part of treebanking efforts.
Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). $$$$$ The evaluation script defines a nonscoring token as a token where all characters of the FORM value have the Unicode category property “Punctuation”.7 We tried to take a test set that was representative of the genres in a treebank and did not cut through text samples.
Parsing accuracy comparison and error analysis under the CoNLL-X dependency shared task data (Buchholz and Marsi, 2006) have been performed by McDonald and Nivre (2011). $$$$$ The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.

The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). $$$$$ In this respect, it would be interesting to repeat experiments with the recently released new version of the TIGER treebank which now contains this information.
The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). $$$$$ As 31Unfortunately, urgent other obligations prevented two participants (John O’Neil and Kenji Sagae) from submitting a paper about their shared task work.
The Spanish corpus was parsed using the MST dependency parser (McDonald et al, 2005) trained using dependency trees generated from the the English Penn Treebank (Marcus et al, 1993) and Spanish CoNLL-X data (Buchholz and Marsi, 2006). So that we could directly compare against statistical translation models, our Spanish and English monolingual corpora were drawn from the Europarl parallel corpus (Koehn, 2005). $$$$$ While the training data contained all 10 columns (although sometimes only with dummy values, i.e. underscores), the test data given to participants contained only the first 6.
