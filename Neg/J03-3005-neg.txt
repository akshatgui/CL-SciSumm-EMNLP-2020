Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). $$$$$ Banko and Brill (2001a, 2001b) experiment with context-sensitive spelling correction, a task for which large amounts of data can be obtained straightforwardly, as no manual annotation is required.
Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). $$$$$ In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP algorithms are typically optimized, tested, and compared on fairly small data sets (corpora with millions of words), even though data sets several orders of magnitude larger are available, at least for some NLP tasks.
Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences: low (5), medium-low (12), medium-high (22), and high (38). $$$$$ The present article is an extended and revised version of Keller, Lapata, and Ourioupina (2002).

Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ We are also grateful to four anonymous reviewers for Computational Linguistics; their feedback helped to substantially improve the present article.
Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.
Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ The work was inspired by a talk that Gregory Grefenstette gave in Saarbr¨ucken in 2001 about his research on using the Web as a corpus.

We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ The present article is an extended and revised version of Keller, Lapata, and Ourioupina (2002).
We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ We observe that the Web counts are substantially less sparse than the NANTC counts: In the worst case, there are nine bigrams for which our Web queries returned no matches (10% of the data), whereas up to 82 bigrams were unseen in the NANTC (91% of the data).
We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.

The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ Stephen Clark and Stefan Riezler provided valuable comments on this research.
The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ All nouns modifying one of the 30 nouns were extracted from the BNC using a heuristic from Lauer (1995) that looks for consecutive pairs of nouns that are neither preceded nor succeeded by another noun.
The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ This corpus differs in several important respects from the BNC.
The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ The joint probability model performs consistently worse than the conditional probability model: It achieves an overall accuracy of 72.7%, which is significantly lower than the accuracy of the Rooth et al. (1999) model (x2(1) = 19.50, p < .01).

Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ The lemmatized version of the corpus was obtained using Karp et al.’s (1992) morphological analyzer.
Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ This holds both for unseen bigrams and for seen bigrams that are treated as unseen by omitting them from the training corpus.
Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.
Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ There is a small body of existing research that tries to harness the potential of the Web for NLP.

More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ Particle verbs were retained only if the particle was adjacent to the verb (e.g., come off heroin).
More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ Materials.

See Keller and Lapata (2003) for more issues. $$$$$ The work was inspired by a talk that Gregory Grefenstette gave in Saarbr¨ucken in 2001 about his research on using the Web as a corpus.
See Keller and Lapata (2003) for more issues. $$$$$ The present article is an extended and revised version of Keller, Lapata, and Ourioupina (2002).
See Keller and Lapata (2003) for more issues. $$$$$ The queries were carried out in January 2003 (and thus the counts are higher than those reported in Keller, Lapata, and Ourioupina [2002], which were generated about a year earlier).
See Keller and Lapata (2003) for more issues. $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.

The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.

The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ This work was conducted while both authors were at the Department of Computational Linguistics, Saarland University, Saarbr¨ucken.
The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ It seems that the large amount of data available outweighs the problems associated with using the Web as a corpus (such as the fact that it is noisy and unbalanced).
The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts.

It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ Verbs followed by the preposition by and a head noun were considered instances of verb-subject relations.
It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.

We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ We found that the counts obtained from the Web are highly correlated with the counts obtained from the BNC.
We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ The work was inspired by a talk that Gregory Grefenstette gave in Saarbr¨ucken in 2001 about his research on using the Web as a corpus.
We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ Grefenstette (1998) therefore effectively uses the Web as a way of obtaining counts for compounds that are sparse in a given corpus.

Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ Finally, it will be crucial to test the usefulness of Web-based frequencies for realistic NLP tasks.
Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ We are also grateful to four anonymous reviewers for Computational Linguistics; their feedback helped to substantially improve the present article.
Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ Table 4 shows the NANTC counts for the set of seen bigrams from Table 2.
Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ This procedure is well-justified in the context of Rooth et al.’s (1999) and Prescher, Riezler, and Rooth’s (2000) work, which aimed at building models of lexical semantics, not of pseudodisambiguation.

 $$$$$ (3) How do the results reported in this article carry over to languages other than English (for which a much smaller amount of Web data are available)?
 $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
 $$$$$ An obvious limitation of our method is that it relies on the page counts returned by the search engines; we do not download the pages themselves for further processing.
 $$$$$ Here, the probability estimates f (v, n) and f (v, n') were used for the joint probability model, and f (v, n)/f (n) and f (v, n')/f (n') for the conditional probability model.

Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ An example is the French compound groupe de travail.
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ Zhu and Rosenfeld’s (2001) results demonstrate that the Web can be a source of data for language modeling.
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ The experiments reported in this article were carried out using the WebExp software package (Keller et al. 1998).
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.

 $$$$$ The work was inspired by a talk that Gregory Grefenstette gave in Saarbr¨ucken in 2001 about his research on using the Web as a corpus.
 $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
 $$$$$ The approach proposed here does not re-create the missing counts but instead retrieves them from a corpus that is much larger (but also much more noisy) than any existing corpus: it launches queries to a search engine in order to determine how often the bigram occurs on the Web.

The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ (4) What is the effect of the noise introduced by our heuristic approach?
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ We thus explore whether this approach generalizes to different predicateargument combinations.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Nissim, and Modjeska (2003) use the Web for the resolution of nominal anaphora, and Zhu and Rosenfeld (2001) use Web-based n-gram counts to improve language modeling.

It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ To summarize, Web counts are certainly less sparse than the counts in a corpus of a fixed size (see Section 2.3).
It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ This article explored a novel approach to overcoming data sparseness.
It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ For the adjective-noun bigrams, the AltaVista coefficient was significantly higher than the BNC coefficient (t(87) = 1.76, p < .05), whereas the difference between the Google coefficient and the BNC coefficient failed to reach significance.

However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ The results show that Web counts outperform counts re-created using a number of classbased smoothing techniques.
However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ Stephen Clark and Stefan Riezler provided valuable comments on this research.

 $$$$$ We applied this process to all the bigrams in our data set, covering seen and unseen adjective-noun, noun-noun, and verb-object bigrams (i.e., a set of 540 bigrams in total).
 $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
 $$$$$ This technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree in judging semantic similarity (Resnik 1999, 2000).

NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ Examples of the syntactic patterns the parser identified are given in Table 1.
NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ Then one bigram was chosen at random from each band.
NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ An obvious limitation of our method is that it relies on the page counts returned by the search engines; we do not download the pages themselves for further processing.
