Following the methodology in Keller and Lapata (2003), we divide the verbs into four frequency bands, frequency being absolute number of annotated sentences $$$$$ Stephen Clark and Stefan Riezler provided valuable comments on this research.

Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ This work was conducted while both authors were at the Department of Computational Linguistics, Saarland University, Saarbr¨ucken.
Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. $$$$$ As already mentioned in Section 2.1, it was assumed that the noun is the predicate in adjective-noun bigrams; for noun-noun bigrams, we treated the right noun as the predicate, and for verb-object bigrams, we treated the verb as the predicate.

We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ We applied our method to three data sets from the literature.
We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ Prescher, Riezler, and Rooth (2000) evaluated Rooth et al.’s (1999) EM-based clustering model again using pseudodisambiguation, but on a separate data set using a slightly different method for constructing the pseudobigrams.
We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). $$$$$ We systematically investigated the validity of this approach by using it to obtain frequencies for predicate-argument bigrams (adjective-noun, noun-noun, and verbobject bigrams).

The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ The fact that more zero frequencies were observed for noun-noun bigrams than for the other two types is perhaps not surprising considering the ease with which novel compounds are created (Levi 1978).
The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ Finally, it will be crucial to test the usefulness of Web-based frequencies for realistic NLP tasks.
The Web has already been used successfully for a series of NLP tasks such as Mt (Grefenstette, 1999), word sense disambiguation (Agirre and Martinez, 2000), synonym recognition (Turney, 2001), anaphora resolution (Modjeska et al, 2003) and determining frequencies for unseen bi-grams (Keller and Lapata, 2003). $$$$$ For comparison, this table also provides descriptive statistics for the BNC and NANTC counts (for seen bigrams only) and for the counts re-created using class-based smoothing (see Section 3.3 for details on the re-created frequencies).

Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ This procedure ensures that the whole range of frequencies is represented in our sample.
Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
Thus, the frequency of appearance of n-grams on the Web (via the Google search engine) appears as a good indicator of the n-gram popularity/soundness (Keller and Lapata, 2003). $$$$$ In the case of adjectives modifying compound nouns, only sequences of two nouns were included, and the rightmost-occurring noun was considered the head.

More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ This indicates that Web queries can generate frequencies that are comparable to the ones obtained from a balanced, carefully edited corpus such as the BNC, but also from a large news text corpus such as NANTC.
More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ This work was conducted while both authors were at the Department of Computational Linguistics, Saarland University, Saarbr¨ucken.
More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ We conclude that simple heuristics (see Section 2.3) are sufficient to obtain useful frequencies from the Web; it seems that the large amount of data available for Web counts outweighs the associated problems (noisy, unbalanced, etc.).
More recently, Keller and Lapata (2003) evaluate the utility of using Web search engines for obtaining frequencies for unseen bigrams. $$$$$ Web counts for bigrams were obtained using a simple heuristic based on queries to the search engines AltaVista and Google.

See Keller and Lapata (2003) for more issues. $$$$$ More precisely, zero frequencies were returned for 23 adjective-noun, 16 verb-noun, and 37 noun-noun bigrams.
See Keller and Lapata (2003) for more issues. $$$$$ Table 5 provides the number of zero counts for both Web search engines and compares them to the number of bigrams that yielded no matches in the NANTC.
See Keller and Lapata (2003) for more issues. $$$$$ Banko and Brill (2001a, 2001b) experiment with context-sensitive spelling correction, a task for which large amounts of data can be obtained straightforwardly, as no manual annotation is required.
See Keller and Lapata (2003) for more issues. $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.

The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ However, counts re-created using an EM-based smoothing approach yielded better pseudodisambiguation performance than Web counts.
The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ We found that Web frequencies and re-created frequencies are reliably correlated, and that Web frequencies are better at predicting plausibility judgments than smoothed frequencies.
The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.
The findings described in (Keller and Lapata, 2003) seem to suggest that count estimations we need in the present study over Subject-Verb bigrams are highly correlated to corpus counts. $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.

The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ This procedure is well-justified in the context of Rooth et al.’s (1999) and Prescher, Riezler, and Rooth’s (2000) work, which aimed at building models of lexical semantics, not of pseudodisambiguation.
The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ We are also grateful to four anonymous reviewers for Computational Linguistics; their feedback helped to substantially improve the present article.
The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ This was necessary because the bigrams were used in experiments involving native speakers (see Section 3.2), and we wanted to reduce the risk of including words unfamiliar to the experimental subjects.
The performance is similar to other published results like those by Keller and Lapata (2003), who adopted a similar feature set and reported around 75% success rates on the ACE data set. $$$$$ (4) What is the effect of the noise introduced by our heuristic approach?

It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ Stephen Clark and Stefan Riezler provided valuable comments on this research.
It has been shown that web documents (as Wikipedia) are reliable samples of language (Keller and Lapata, 2003). $$$$$ The experiment proper consisted of three phases: (1) a calibration phase, designed to familiarize subjects with the task, in which they had to estimate the length of five horizontal lines; (2) a practice phase, in which subjects judged the plausibility of eight bigrams (similar to the ones in the stimulus set); (3) the main experiment, in which each subject judged one of the six stimulus sets (90 bigrams).

We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ Table 2 lists examples for the seen and unseen noun-noun and verb-object bigrams generated by this procedure.
We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
We also use Keller and Lapata (2003)'s approach to obtaining web-counts. $$$$$ Although this is an important initial result, it raises the question of the generality of the proposed approach to overcoming data sparseness.

Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ The BNC is a large, synchronic corpus, consisting of 90 million words of text and 10 million words of speech.
Also, the Keller and Lapata (2003) approach will be undefined if the pair is unobserved on the web. $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.

 $$$$$ We are also grateful to four anonymous reviewers for Computational Linguistics; their feedback helped to substantially improve the present article.
 $$$$$ To calculate intersubject agreement we used leaveone-out resampling.
 $$$$$ Examples of the syntactic patterns the parser identified are given in Table 1.
 $$$$$ The method we used to retrieve Web counts is based on very simple heuristics; it is thus inevitable that the counts generated will contain a certain amount of noise.

Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ In two recent papers, Banko and Brill (2001a, 2001b) criticize the fact that current NLP algorithms are typically optimized, tested, and compared on fairly small data sets (corpora with millions of words), even though data sets several orders of magnitude larger are available, at least for some NLP tasks.
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ They obtain a standard language model from a 103-million-word corpus and employ Web-based counts to interpolate unreliable trigram estimates.
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ The joint probability model achieved 85.3% on this data set, also significantly outperforming Clark and Weir (2002) (x2(1) = 119.35, p < .01).
Recall that even the Keller and Lapata (2003) system, built on the world's largest corpus, achieves only 34% recall (Table 1) (with only 48% of positives and 27% of all pairs previously observed, but see Footnote 5). $$$$$ It is beyond the scope of the present study to undertake a full comparison between Web counts and frequencies re-created using all available smoothing techniques (and all available taxonomies that might be used for class-based smoothing).

 $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
 $$$$$ We are also grateful to four anonymous reviewers for Computational Linguistics; their feedback helped to substantially improve the present article.
 $$$$$ The work was inspired by a talk that Gregory Grefenstette gave in Saarbr¨ucken in 2001 about his research on using the Web as a corpus.
 $$$$$ Grammar rules are implemented as constraints associated with the nodes and edges.

The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ In a nutshell, this measure replaces Resnik’s (1993) information-theoretic approach with a simpler measure that makes no assumptions with respect to the contribution of a semantic class to the total quantity of information provided by the predicate about the semantic classes of its argument.

It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ The present article is an extended and revised version of Keller, Lapata, and Ourioupina (2002).
It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ Arguably, the largest data set that is available for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill’s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax.
It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ This article shows that the Web can be employed to obtain frequencies for bigrams that are unseen in a given corpus.
It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). $$$$$ Preliminary results are reported by Lapata and Keller (2003), who use Web counts successfully for a range of NLP tasks, including candidate selection for machine translation, context-sensitive spelling correction, bracketing and interpretation of compounds, adjective ordering, and PP attachment.

However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ The combined performance of 68.5% is significantly lower than the performance of both the VA model (x2(1) = 7.78, p < .01) and the VO model (x2(1) = 33.28, p < .01) reported by Prescher, Riezler, and Rooth (2000).
However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems. $$$$$ For unseen bigrams, the AltaVista coefficients and the coefficients obtained using smoothing were not significantly different for adjective-noun bigrams, but the difference reached significance for noun-noun and verb-object bigrams (t(87) = 2.08, p < .05; t(87) = 2.53, p < .01).

 $$$$$ They obtain a standard language model from a 103-million-word corpus and employ Web-based counts to interpolate unreliable trigram estimates.
 $$$$$ Special thanks are due to Stephen Clark and Detlef Prescher for making their pseudodisambiguation data sets available.
 $$$$$ Stephen Clark and Stefan Riezler provided valuable comments on this research.
 $$$$$ We describe a method for retrieving counts for adjective-noun, noun-noun, and verb-object bigrams from the Web by querying a search engine.

NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ We evaluate this method by demonstrating: (a) a high correlation between Web frequencies and corpus frequencies; (b) a reliable correlation between Web frequencies and plausibility judgments; (c) a reliable correlation between Web frequencies and frequencies recreated using class-based smoothing; (d) a good performance of Web frequencies in a pseudodisambiguation task.
NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ A particularly interesting application is proposed by Grefenstette (1998), who uses the Web for example-based machine translation.
NC analysis has benefited from the recent trend of using web-derived features rather than corpus based counts (Keller and Lapata, 2003). $$$$$ We obtained the data sets of Rooth et al. (1999), Prescher, Riezler, and Rooth (2000), and Clark and Weir (2002) described above.
