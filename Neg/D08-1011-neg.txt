Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ 1 (c), all other hypotheses are aligned to the backbone.
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence.
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ The best alignment is the one that gives the minimum number of translation edits.

Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). $$$$$ Since both words are in the same language, the similarity model can be derived based on both semantic similarity and surface similarity, and the overall similarity model is a linear interpolation of the two: where and reflect the semantic and surface similarity between and e; , respectively, and α is the interpolation factor.
Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). $$$$$ Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.
Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). $$$$$ The best alignment is the one that gives the minimum number of translation edits.

The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ We investigate the effect of the distance-based distortion model by varying the controlling factor K in (6).
The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ In contrast to previous methods, the similarity model explicitly incorporates both semantic and surface word similarity, which is critical to monolingual word alignment, and a smoothed distance-based distortion model is used to model the first-order dependency of word ordering, which is shown to be better than simpler approaches.
The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ High quality hypothesis alignment is crucial to the performance of the resulting system combination.
The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ It uses a similarity model for synonym matching and a distortion model for word ordering.

The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008). $$$$$ First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.
The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008). $$$$$ Synonym matching and word ordering are two central issues for hypothesis alignment in confusion-network-based MT system combination.

 $$$$$ All seven systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.
 $$$$$ The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets.
 $$$$$ The HMM provides a way to model both synonym matching and word ordering.
 $$$$$ Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the

To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. $$$$$ On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model.
To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. $$$$$ Our combined SMT system using the proposed method gave the best result on the Chinese-to-English test in the constrained training track of the 2008 NIST Open MT Evaluation (MT08).
To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. $$$$$ Moreover, the surface similarity information is explicitly incorporated in our model, while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006).

The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores.
The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ As shown in Table 5, the optimal result can be achieved using a properly smoothed distancebased distortion model.
The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities Following Och and Ney (2003), we use a fixed value p0 for the probability of jumping to a null state, which can be optimized on held-out data, and the overall distortion model becomes As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets.
The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ 1 (c) also illustrates the handling of synonym alignment (e.g., aligning “car” to “sedan”), and word re-ordering of the hypothesis.

He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. $$$$$ Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007).
He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. $$$$$ This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems.
He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. $$$$$ Therefore, it is not able to handle synonym matching well.
He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. $$$$$ 1 (d), a confusion network is constructed based on the aligned hypotheses, which consists of a sequence of sets in which each word is aligned to a list of alternative words (including null) in the same set.

We compute the association score from a linear combination of two clues $$$$$ (elj I , would take the value 1.0 if e’= e, and 0 otherwise 2 .
We compute the association score from a linear combination of two clues $$$$$ High quality hypothesis alignment is crucial to the performance of the resulting system combination.
We compute the association score from a linear combination of two clues $$$$$ Our experimental results show that the IHMMbased hypothesis alignment method gave superior results on the NIST MT08 C2E test set compared to the TER-based method.
We compute the association score from a linear combination of two clues $$$$$ It outperformed the best single system by 4.7 BLEU points and the TER-based system combination by 1.0 BLEU points.

IHMM-based $$$$$ It uses a similarity model for synonym matching and a distortion model for word ordering.
IHMM-based $$$$$ An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.
IHMM-based $$$$$ 1 (c), all other hypotheses are aligned to the backbone.

We compute the distortion model by following (He et al, 2008) for IHMM and CLA-based methods. $$$$$ In this paper, an IHMM-based method is proposed for hypothesis alignment.
We compute the distortion model by following (He et al, 2008) for IHMM and CLA-based methods. $$$$$ High quality hypothesis alignment is crucial to the performance of the resulting system combination.
We compute the distortion model by following (He et al, 2008) for IHMM and CLA-based methods. $$$$$ The last step in (3) assumes that first ei generates all source words including null.

We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Therefore, for an M-way system combination that uses N LMs, a total of M+N+1 decoding parameters, including M-1 system weights, one rank smoothing factor, N language model weights, and one weight for the word count feature, are optimized using Powell’s method (Brent, 1973) to maximize BLEU score on a development set4 .
We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Beside the word posteriors, we use language model scores and a word count as features for confusion network decoding.
We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Sys-13 is a phrasal system proposed by Koehn et al. (2003), Sys-14 is a hierarchical system proposed by Chiang (2007), and Sys-15 is a syntax-based system proposed by Galley et al.
We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Compared to the TER-based method, the IHMM-based method is about 1.5 BLEU points better.

Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first. $$$$$ Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the
Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first. $$$$$ 1 (c) also illustrates the handling of synonym alignment (e.g., aligning “car” to “sedan”), and word re-ordering of the hypothesis.

 $$$$$ Matusov et al. (2006) proposed using GIZA++ to align words between different MT hypotheses, where all hypotheses of the test corpus are collected to create hypothesis pairs for GIZA++ training.
 $$$$$ Note that in Fig.
 $$$$$ It uses a similarity model for synonym matching and a distortion model for word ordering.

On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ All seven systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.
On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples.
On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ Then in Fig.

Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ We compare to the TER-based method used by Rosti et al. (2007).
Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ It shows that the IHMMbased method is still about 1 BLEU point better than the TER-based method.
Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ Note that in Fig.
Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores.

 $$$$$ In our experiments, we apply length adaptation to the system combination output at the level of the whole test corpus.
 $$$$$ It uses a similarity model for synonym matching and a distortion model for word ordering.
 $$$$$ In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008).
 $$$$$ In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities Following Och and Ney (2003), we use a fixed value p0 for the probability of jumping to a null state, which can be optimized on held-out data, and the overall distortion model becomes As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets.

Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). $$$$$ Our combined SMT system using the proposed method gave the best result on the Chinese-to-English test in the constrained training track of the 2008 NIST Open MT Evaluation (MT08).
Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). $$$$$ Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007).

He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. $$$$$ The accuracy is significantly improved to 43.62% on the dev set and 30.89% on test set when α = 0.3.
He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. $$$$$ The major steps are illustrated in Figure 1.
He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. $$$$$ Therefore, for an M-way system combination that uses N LMs, a total of M+N+1 decoding parameters, including M-1 system weights, one rank smoothing factor, N language model weights, and one weight for the word count feature, are optimized using Powell’s method (Brent, 1973) to maximize BLEU score on a development set4 .
