Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ Therefore, we need to normalize the alignment produced by Viterbi search.
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ System combination based on this approach gives an improvement over the best single system.
Translation Edit Rate (TER, Snover et al (2006)) based alignment proposed in Sim et al (2007) is often taken as the baseline, and a couple of other approaches, such as the Indirect Hidden Markov Model (IHMM, He et al (2008)) and the ITG-based alignment (Karakos et al. (2008)), were recently proposed with better results reported. $$$$$ In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities Following Och and Ney (2003), we use a fixed value p0 for the probability of jumping to a null state, which can be optimized on held-out data, and the overall distortion model becomes As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets.

Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). $$$$$ 1 (c) the symbol ε denotes a null word, which is inserted by the alignment normalization algorithm described in section 3.4.
Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). $$$$$ Then in Fig.
Our incremental alignment approaches adopt the same heuristics for alignment normalization stated in He et al (2008). $$$$$ First, whenever more than one hypothesis words are aligned to one backbone word, we keep the link which gives the highest occupation probability computed via the forward-backward algorithm.

The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.
The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.
The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ The last step in (3) assumes that first ei generates all source words including null.
The various parameters in the IHMM model are set as the optimal values found in He et al (2008). $$$$$ The development (dev) set used for system combination parameter training contains 1002 sentences sampled from the previous NIST MT Chinese-to-English test sets: 35% from MT04, 55% from MT05, and 10% from MT06-newswire.

The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008). $$$$$ First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.
The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008). $$$$$ This is usually done by a sentence-level minimum Bayes risk (MBR) method which selects a hypothesis that has the minimum average distance compared to all hypotheses.
The comparison between the two pair-wise alignment methods shows that IHMM gives a 0.7 BLEU point gain over TER, which is a bit smaller than the difference reported in He et al (2008). $$$$$ Therefore, GIZA++ training on such a data set may be unreliable.

 $$$$$ In this paper, we propose an indirect hidden Markov model (IHMM) for MT hypothesis alignment.
 $$$$$ 1 (c) also illustrates the handling of synonym alignment (e.g., aligning “car” to “sedan”), and word re-ordering of the hypothesis.
 $$$$$ In the common SMT scenario where a large amount of bilingual parallel data is available, we can estimate the translation probabilities from a source word to a target word and vice versa via conventional bilingual word alignment.

To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. $$$$$ In HMM-based hypothesis alignment, emission probabilities model the similarity between a backbone word and a hypothesis word, and will be referred to as the similarity model.
To compute scores for word pairs, we perform pair-wise hypothesis alignment using the indirect HMM (He et al 2008) for every pair of input hypotheses. $$$$$ Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.

The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ In contrast to previous methods, the similarity model explicitly incorporates both semantic and surface word similarity, which is critical to monolingual word alignment, and a smoothed distance-based distortion model is used to model the first-order dependency of word ordering, which is shown to be better than simpler approaches.
The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment.
The baselines include a pair-wise hypothesis alignment approach using the indirect HMM (IHMM) proposed by He et al (2008), and an incremental hypothesis alignment approach using the incremental HMM (IncHMM) proposed by Li et al (2009). $$$$$ 1 (b), one of the hypotheses is selected as the backbone for hypothesis alignment.

He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. $$$$$ In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities Following Och and Ney (2003), we use a fixed value p0 for the probability of jumping to a null state, which can be optimized on held-out data, and the overall distortion model becomes As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets.
He et al (2008) proposed an IHMM-based word alignment method which the parameters are estimated indirectly from a variety of sources. $$$$$ The backbone determines the word order of the combined output.

We compute the association score from a linear combination of two clues: surface similarity computed as Equation (2) and position difference based distortion score by following (He et al, 2008). $$$$$ In this paper, an IHMM-based method is proposed for hypothesis alignment.

IHMM-based: He et al (2008) propose an indirect hidden Markov model (IHMM) for hypothesis alignment. $$$$$ Then both p(fk I e;) and in (3) can be derived: where is the translation model from the source-to-target word alignment model, and p(fk I e;) , which enforces the sum-to-1 constraint over all words in the source sentence, takes the following form, where A2s (fk I e;) is the translation model from the target-to-source word alignment model.
IHMM-based: He et al (2008) propose an indirect hidden Markov model (IHMM) for hypothesis alignment. $$$$$ Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.
IHMM-based: He et al (2008) propose an indirect hidden Markov model (IHMM) for hypothesis alignment. $$$$$ These settings are optimized on the dev set.

We compute the distortion model by following (He et al, 2008) for IHMM and CLA-based methods. $$$$$ Moreover, we show that our system combination method can scale up to combining more systems and produce a better output that has a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission of MT08.

We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Therefore, it is not able to handle synonym matching well.
We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Moreover, we show that our system combination method can scale up to combining more systems and produce a better output that has a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission of MT08.
We compared our approach with the state-of-the-art confusion-network-based system (He et al, 2008) and achieved a significant absolute improvement of 1.23 BLEU points on the NIST 2005 Chinese-to-English test set and 0.93 BLEU point on the NIST 2008 Chinese-to-English test set. $$$$$ Second, if hypothesis words are aligned to a null in the backbone or vice versa, we need to insert actual nulls into the right places in the hypothesis and the backbone, respectively.

Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first. $$$$$ 3 (a), if a hypothesis word e2’ is aligned to the backbone word e2, a null is inserted in front of the backbone word e2 linked to the hypothesis word e1’ that comes before e2’.
Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first. $$$$$ 1 (d), a confusion network is constructed based on the aligned hypotheses, which consists of a sequence of sets in which each word is aligned to a list of alternative words (including null) in the same set.
Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first. $$$$$ As shown in Fig.
Since the candidate hypotheses are aligned using Indirect-HMM-based (IHMM-based) alignment method (He et al, 2008) in both direction, we briefly review the IHMM-based alignment method first. $$$$$ Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the

 $$$$$ 1 (b), one of the hypotheses is selected as the backbone for hypothesis alignment.
 $$$$$ Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples.

On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.
On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ The similarity model, which specifies the emission probabilities of the HMM, models the similarity between a backbone word and a hypothesis word.
On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ There are two reasons for this.
On NIST MT05 test set, the lattice-based system gave better results with an absolute improvement of 1.23 BLEU points over the confusion network-based system (He et al, 2008) and 3.73 BLEU points over the best single system. $$$$$ Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty.

Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007).
Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ Synonym matching and word ordering are two central issues for hypothesis alignment in confusion-network-based MT system combination.
Aligning translation hypotheses can be challenging and has a substantial effect on combination performance (He et al, 2008). $$$$$ Moreover, the surface similarity information is explicitly incorporated in our model, while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006).

 $$$$$ Our combined SMT system using the proposed method gave the best result on the Chinese-to-English test in the constrained training track of the 2008 NIST Open MT Evaluation (MT08).
 $$$$$ Our experimental results show that the IHMMbased hypothesis alignment method gave superior results on the NIST MT08 C2E test set compared to the TER-based method.

Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). $$$$$ The backbone determines the word order of the combined output.
Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). $$$$$ Synonym matching and word ordering are two central issues for hypothesis alignment in confusion-network-based MT system combination.
Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al, 2008). $$$$$ Sys-13 is a phrasal system proposed by Koehn et al. (2003), Sys-14 is a hierarchical system proposed by Chiang (2007), and Sys-15 is a syntax-based system proposed by Galley et al.

He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. $$$$$ The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by using posterior probability estimates, or by using a combination of these measures and other features.
He et al (2008) proposed using an indirect hidden Markov model (IHMM) for pairwise alignment of system outputs. $$$$$ Second, correct translations may have different word orderings in different hypotheses and these words need to be properly reordered in hypothesis alignment.
