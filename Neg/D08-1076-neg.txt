As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ For this purpose, we first analyze the complexity of Algorithm 1 and derive from it the running time of Algorithm 2.
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ Table 3 shows the effect of optimizing the feature function weights along some randomly chosen directions in addition to the coordinate axes.
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ Once the upper envelope has been determined, we can project its constituent line segments onto the error counts of the corresponding candidate translations (cf.
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ Hence, as γ increases from~8to8, we will see that the most probable translation hypothesis will change whenever γ passes an intersection point.

Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ A downside of this approach is, however, that N-best lists can only capture a very small fraction of the search space.
Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ This problem can be alleviated by exploring a large number of random directions which update many feature weights simultaneously.
Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.

While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ If we add the slope and y-intercept of gpEto each line in the setf1, ..., fNu, then the upper envelope will be constituted by segments of fi + gp(E), ..., fN + gp(E).
While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.
While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ As a consequence, the optimization procedure may stop in a poor local optimum.

A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too.
A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ Table 3 shows the effect of optimizing the feature function weights along some randomly chosen directions in addition to the coordinate axes.
A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder.

Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.
Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ Once the upper envelope has been determined, we can project its constituent line segments onto the error counts of the corresponding candidate translations (cf.

Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT.
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ This huge number of alternative candidate translations makes updating the weights under lattice MERT more reliable and robust and, compared to N-best MERT, it becomes less likely that the same feature weight needs to be picked again and adjusted in subsequent iterations.
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ For any particular choice of γ, the decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment.

Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ In this paper, we present a novel algorithm that allows for efficiently constructing and representing the unsmoothed error surface for all translations that are encoded in a phrase lattice.
Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ One possible usecase is the computation of consensus translations from the outputs of multiple machine translation systems where this framework allows us to estimate the system prior weights directly on confusion networks (Rosti et al., 2007; Macherey and Och, 2007).
Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ This convex hull is again a convex polygon which consists of at most N1 + N2 edges, and therefore, the number of cost minimizing paths in G1' (and thus also in G) is upper bounded by NiN2.� ❑ Corollary: The upper envelope for a phrase lattice Gf~p = (Vf, Efq) consists of at most |IEf |I line segments.
Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ The random directions are chosen as the lines which connect some randomly distributed points on the surface of an M-dimensional hypersphere with the hypersphere’s center.

The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT.
The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ An initial-weights prior (Am~λm) can be used to confine changes in the parameter update with the consequence that the new parameter may be closer to the initial weights set.
The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.
The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT.

Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum.
Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.
Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ Whenever a line object is visited for the third time, it is irrevocably removed from the envelope.
Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on N-best re-ranking tasks.

We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ While this update scheme provides a ranking of the feature functions according to their discriminative power (each iteration picks the feature function for which changing the corresponding weight yields the highest gain), it does not take possible correlations between the feature functions into account.
We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ It remains to verify that the suggested algorithm is efficient in both running time and memory.
We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ One possible usecase is the computation of consensus translations from the outputs of multiple machine translation systems where this framework allows us to estimate the system prior weights directly on confusion networks (Rosti et al., 2007; Macherey and Och, 2007).

Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ For lattice MERT, we used a graph density of 40 arcs per phrase which corresponds to an N-best size of more than two octillionp2~1027qentries.
Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ (3)).
Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).

Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ An initial-weights prior (Am~λm) can be used to confine changes in the parameter update with the consequence that the new parameter may be closer to the initial weights set.
Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ Initial weights priors are useful in cases where the starting weights already yield a decent baseline.
Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.

In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ Initial weights priors are useful in cases where the starting weights already yield a decent baseline.
In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.
In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process.

In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ Lattice MERT is shown to yield significantly faster convergence rates while it explores a much larger space of candidate translations which is exponential in the lattice size.
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too.
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ For lattice MERT, we used a graph density of 40 arcs per phrase which corresponds to an N-best size of more than two octillionp2~1027qentries.
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on N-best re-ranking tasks.

To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ While this update scheme provides a ranking of the feature functions according to their discriminative power (each iteration picks the feature function for which changing the corresponding weight yields the highest gain), it does not take possible correlations between the feature functions into account.
To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ The criterion can be derived from Bayes’ decision rule as follows: Let ff1, ..., fi denote a source sentence (’French’) which is to be translated into a target sentence (’English’) ee1, ..., eI.
To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007).

To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ Section 4 presents an upper bound on the complexity of the unsmoothed error surface for the translation hypotheses represented in a phrase lattice.
To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ Hence, the objective of finding the best translation hypotheses in a phrase lattice becomes the problem of finding all cost-minimizing paths in a graph with affine cost functions.
To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ Both sequences together provide an exhaustive representation of the unsmoothed error surface for the sentence fs along the line λM + ry • dM.

Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ We further notice that either envelope is a convex hull whose constituent line segments inscribe a convex polygon, in the following denoted by P1 and P2.
Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ In contrast to lattice MERT, N-best MERT optimizes all dimensions in each iteration and, in addition, it also explores a large number of random starting points before it re-decodes and expands the hypothesis set.
Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder.

But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ To alleviate this problem, the following section lists some best practices that we found to be useful in the context of MERT.
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ As a consequence, the line optimization algorithm needs to repeatedly translate the development corpus and enlarge the candidate repositories with newly found hypotheses in order to avoid overfitting on Cs and preventing the optimization procedure from stopping in a poor local optimum.
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ Now, we combine both subgraphs into a single graph G1by merging the sink node t1 in G1 with the source node s2 in G2.

Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ Translation results were evaluated using the mixedcase BLEU score metric in the implementation as suggested by (Papineni et al., 2001).
Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ The contraction essentially amounts to a removal of arcs and is required in order to ensure that the sum of edges in both subgraphs does not exceed the number of edges in G. All nodes in G1 with out-degree zero are then combined into a single sink node t1.
Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ A class of training criteria that provides a tighter connection between the decision rule and the final error metric is known as Minimum Error Rate Training (MERT) and has been suggested for SMT in (Och, 2003).

If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ A different approach to minimize the expected BLEU score is suggested in (Smith and Eisner, 2006) who use deterministic annealing to gradually turn the objective function from a convex entropy surface into the more complex risk surface.
If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ The contraction essentially amounts to a removal of arcs and is required in order to ensure that the sum of edges in both subgraphs does not exceed the number of edges in G. All nodes in G1 with out-degree zero are then combined into a single sink node t1.
If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ Despite this vast search space, we show that the suggested algorithm is always efficient in both running time and memory.
If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ The remainder of this paper is organized as follows.
