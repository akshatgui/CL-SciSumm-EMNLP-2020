As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ Many statistical methods in natural language processing aim at minimizing the probability of sentence errors.
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ Experiments were conducted on the NIST 2008 translation tasks under the conditions of the constrained data track for the language pairs Arabicto-English (aren), English-to-Chinese (enzh), and Chinese-to-English (zhen).
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.
As already observed in previous literature (Macherey et al, 2008), first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. $$$$$ Hence, the objective of finding the best translation hypotheses in a phrase lattice becomes the problem of finding all cost-minimizing paths in a graph with affine cost functions.

Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ This is illustrated in Figure 3, where lattice MERT and N-best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.
Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ The decline of less than 0.5% in terms of BLEU is, however, almost negligible compared to the performance drop of more than 27% in case of N-best MERT.
Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007).
Recent efforts extended MERT to work on lattices (Macherey et al, 2008) and hypergraphs (Kumar et al, 2009). $$$$$ Section 5 lists some best practices for MERT.

While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.
While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ This problem can be alleviated by exploring a large number of random directions which update many feature weights simultaneously.
While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ Section 6 discusses related work.
While this approach is similar in spirit to lattice-based MERT (Macherey et al, 2008), there is a crucial difference. $$$$$ We now assume that the upper envelope for these partial translation hypotheses is known.

A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ By construction, NiN2E|.
A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007).
A key property of the line optimisation is that it can consider a large set of hypotheses encoded as a weighted directed acyclic graph (Macherey et al, 2008), which is called a lattice. $$$$$ While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too.

Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.
Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ Section 4 presents an upper bound on the complexity of the unsmoothed error surface for the translation hypotheses represented in a phrase lattice.
Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ The proposed algorithm was used to train the feature function weights of a log-linear model for a statistical machine translation system under the Minimum Error Rate Training (MERT) criterion.
Note that the upper envelope is completely defined by hypotheses e4 ,e3, and e1, together with the intersection points ?1 and ?2 (after Macherey et al (2008), Fig. 1). $$$$$ To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.

Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ We further notice that either envelope is a convex hull whose constituent line segments inscribe a convex polygon, in the following denoted by P1 and P2.
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ Starting with the source node, we combine for each node v the upper envelopes that are associated with vâ€™s incoming arcs by merging their respective line arrays and reducing the merged array into a combined upper envelope using Algorithm 1.
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements moderate BLEU score gains over MERT.
Macherey et al (2008) use methods from computational geometry to compute the upper envelope. $$$$$ The correctness of this procedure is based on the following two observations: again.

Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ For this purpose, we first analyze the complexity of Algorithm 1 and derive from it the running time of Algorithm 2.
Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ This can impair the ability of N-best MERT to generalize to unseen data if the initial weights are already capable of producing a decent baseline.
Macherey et al (2008) describe a procedure for conducting line optimisation directly over a word lattice encoding the hypotheses in Cs. $$$$$ In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT.

The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ Lattice MERT was shown analytically and experimentally to be superior over N-best MERT, resulting in significantly faster convergence speed and a reduced number of decoding steps.
The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ An effective means to compute the upper envelope is a sweep line algorithm which is often used in computational geometry to determine the intersection points of a sequence of lines or line segments (Bentley and Ottmann, 1979).
The SweepLine algorithm (Bentley and Ottmann, 1979) is applied to the union to discard redundant linear functions and their associated hypotheses (Macherey et al, 2008). $$$$$ As a consequence, the optimization procedure may stop in a poor local optimum.

Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.
Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ It could therefore be worthwhile to investigate whether a more elaborated version of an initial-weights prior allows for alleviating this effect in case of Nbest MERT.
Macherey's theorem (Macherey et al, 2008) states that an upper bound for the number of linear functions in the upper envelope at the final state is equal to the number of edges in the lattice. $$$$$ In a first experiment, we investigated the convergence speed of lattice MERT and N-best MERT.

We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ Figure 4 shows the evolution of the BLEU score on the zhen-dev1 corpus using lattice MERT with 5 weights updates per iteration.
We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ Once the upper envelope has been determined, we can project its constituent line segments onto the error counts of the corresponding candidate translations (cf.
We compare feature weight optimisation using k best MERT (Och, 2003), lattice MERT (Macherey et al, 2008), and tropical geometry MERT. $$$$$ Many statistical methods in natural language processing aim at minimizing the probability of sentence errors.

Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ On the other hand, it is difficult to compute a direction that decorrelates two or more correlated feature functions.
Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ The bound is not only meaningful for proving the space efficiency of lattice MERT, but it also provides deeper insight into the structure and complexity of the unsmoothed error surface induced by log-linear models.
Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ As suggested in (Och, 2003), an alternative method for the optimization of the unsmoothed error count is Powellâ€™s algorithm combined with a grid-based line optimization (Press et al., 2007, p. 509).
Both TGMERT and LMERT converge to a small gain over MERT in fewer iterations, consistent with previous reports (Macherey et al, 2008). $$$$$ This is illustrated in Figure 3, where lattice MERT and N-best MERT find different optima for the weight of the phrase penalty feature function after the first iteration.

Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.
Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ While the approach was used to optimize the model parameters of a single machine translation system, there are many other applications in which this framework can be useful, too.
Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ On the other hand, it is difficult to compute a direction that decorrelates two or more correlated feature functions.
Weights on feature functions are found by lattice MERT (Macherey et al, 2008). $$$$$ To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.

In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ The combined envelope is then propagated over the outgoing arcs by associating each e c outp(vq) with a copy of the combined envelope.
In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ Translation hypotheses produced on the blind test data were reranked using the MinimumBayes Risk (MBR) decision rule (Kumar and Byrne, 2004; Tromble et al., 2008).
In the future, we plan to optimize feature weights for max-translation decoding directly on the entire packed translation hypergraph rather than on n-best derivations, following the lattice based MERT (Macherey et al, 2008). $$$$$ After sorting, each line object in Algorithm 1 is visited at most three times.

In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ In contrast to lattice MERT, N-best MERT optimizes all dimensions in each iteration and, in addition, it also explores a large number of random starting points before it re-decodes and expands the hypothesis set.
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ Note that the component E.x does not change and therefore requires no update.
In all cases, the final log-linear models were optimized on the dev set using lattice-based Minimum Error Rate Training (Macherey et al, 2008). $$$$$ As a consequence, the optimization procedure may stop in a poor local optimum.

To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ In this paper, we present a novel algorithm that allows for efficiently constructing and reprethe exact error surface of translations that are encoded in a phrase lattice. to MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.
To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ This operation neither changes the number of line segments nor their relative order in the envelope, and therefore it preserves the structure of the convex hull.
To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al, 2008). $$$$$ One possible usecase is the computation of consensus translations from the outputs of multiple machine translation systems where this framework allows us to estimate the system prior weights directly on confusion networks (Rosti et al., 2007; Macherey and Och, 2007).

To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al, 2008). $$$$$ Our future work will therefore focus on how much system combination and syntax-augmented machine translation can benefit from lattice MERT and to what extent feature function weights can robustly be estimated using the suggested method.

Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ Both bounds are tight.
Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ The remainder of this paper is organized as follows.
Recognizing this shortcoming, Macherey et al (2008) extended the MERT algorithm so as to use the whole set of candidate translations compactly represented in the search lattice produced by the decoder, instead of only a N-best list of candidates extracted from it. $$$$$ Each arc is labeled with a phrase Ï•ij = ei1, ..., eij and the (local) feature function values hM(Ï•ij, fq).

But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ Typically, in MERT are represented as lists which contain the probable translation hypotheses produced by a decoder.
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ An initial-weights prior (Am~Î»m) can be used to confine changes in the parameter update with the consequence that the new parameter may be closer to the initial weights set.
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.
But the Down hill Simplex Algorithm loses its robustness as the dimension goes up by more than 10 (Machereyet al, 2008). $$$$$ This section addresses some techniques that we found to be beneficial in order to improve the performance of MERT. function weights by defining weights restrictions.

Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process.
Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ This section addresses some techniques that we found to be beneficial in order to improve the performance of MERT. function weights by defining weights restrictions.
Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ The vast number of alternative translation hypotheses represented in a lattice also increases the number of phase transitions in the error surface, and thus prevents MERT from selecting a low performing feature weights set at early stages in the optimization procedure.
Macherey et al (2008) propose a new variation of MERT where the algorithm is tuned to work on the whole phrase lattice instead of N-best list only. $$$$$ The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum.

If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ This copy is modified by adding the parameters (slope and y-intercept) of the line gp(e) to the envelopeâ€™s constituent line segments.
If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ It is also straightforward to extend the suggested method to hypergraphs and forests as they are used, e.g., in hierarchical and syntax-augmented systems (Chiang, 2005; Zollmann and Venugopal, 2006).
If we use either N-best lists or random samples to form the translation pool, and M is the size of the translation pool, then computing the envelope can be done in time O (M log M) using the SweepLine algorithm reproduced as Algorithm 1 in (Macherey et al, 2008). $$$$$ The performance drop in iteration 1 is also attributed to overfitting the candidate repository.
