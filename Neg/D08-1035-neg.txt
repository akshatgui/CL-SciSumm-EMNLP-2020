Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ They assume a set of documents that is characterized by some number of hidden topics that are shared across multiple documents.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ We can write the proposal distribution q(z'  |z) a c(z —* z')d(z —* z')A, where A < 0 sets the rate of exponential decay and c is an indicator function enforcing the constraint that the moves do not reach or cross existing segmentation points.6 We can also incorporate declarative linguistic knowledge by biasing the proposal distribution in favor of moves that place boundaries near known cue phrase markers.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ Our model also has a connection to entropy, and situates entropy-based segmentation within a Bayesian framework.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ The Bayesian framework provides a principled way to incorporate additional features such as cue phrases, a powerful indicator of discourse structure that has not been previously used in unsupervised segmentation systems.

Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ Having obtained an estimate for the language model ˆθj, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known.
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available.
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ More formally, we treat the words in each sentence as draws from a language model associated with the topic segment.
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.

Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.
Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ Finally, we have shown that our model provides a theoretical framework with connections to information theory, while also generalizing and justifying prior work.
Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994).

Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ This may help to explain why cue phrases improve performance for meeting transcripts, but not for the textbook.
Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ This property enables a dynamic programming solution to find the exact maximum-likelihood segmentation.
Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ We use a list of cue phrases identified by Hirschberg and Litman (1993).

This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ Using the DCM distribution, we can compute the data likelihood for each segment from the lexical counts over the entire segment.
This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.
This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ Thus far, we have described a segmentation in terms of two parameters: the segment indices z, and the set of language models Θ.

Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.
Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ A new segmentation z0 is drawn from the previous hypothesized segmentation z based on a proposal distribution q(z0|z).4 The probability of accepting a proposed transformation depends on the ratio of the joint probabilities and a correction term for asymmetries in the proposal distribution: The Metropolis-Hastings algorithm guarantees that by accepting samples at this ratio, our sampling procedure will converge to the stationary distribution for the hidden variables z.
Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ The expected value of this distribution is the multinomial distribution ˆθj, where, In this equation, W indicates the number of words in the vocabulary.
Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ This paper presents a novel Bayesian approach to unsupervised topic segmentation.

Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ Thanks to Aaron Adler, S. R. K. Branavan, Harr Chen, Michael Collins, Randall Davis, Dan Roy, David Sontag and the anonymous reviewers for helpful comments and suggestions.
Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ More recently, cue phrases have been applied to topic segmentation in the supervised setting.
Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ To investigate the quality of the cue phrases that our model extracts, we list its top ten cue phrases for each dataset in Table 2.

If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ For this reason, we apply the more general Metropolis-Hastings algorithm, which permits sampling arbitrary transformations of the latent variables.
If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.
If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ Baselines We compare against three competitive alternative systems from the literature: U&I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006).

The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ Any objective function that can be decomposed into a product across segments can be maximized using dynamic programming.
The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ Baselines We compare against three competitive alternative systems from the literature: U&I (Utiyama and Isahara, 2001); LCSEG (Galley et al., 2003); MCS (Malioutov and Barzilay, 2006).

The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ Without cue phrases, we use the dynamic programming inference described in section 3.3.
The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ Galley et al. (2003) performed a similar chi-squared analysis, but used the true segment boundaries in the labeled data; this can be thought of as a sort of ground truth.

This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ Finally, we have shown that our model provides a theoretical framework with connections to information theory, while also generalizing and justifying prior work.
This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available.
This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ Finally, we have shown that our model provides a theoretical framework with connections to information theory, while also generalizing and justifying prior work.

In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ We assume that p(z) is a uniform distribution over valid segmentations, and assigns no probability mass to invalid segmentations.
In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ U&I requires no parameter tuning, and is used “out of the box.” In all experiments, we assume that the number of desired segments is provided.
In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ In contrast to our model’s success at extracting cue phrases from the meeting dataset, only very common words are selected for the textbook dataset.

We compare APS with two recent systems $$$$$ Equation 1 defines the objective function as a product across sentences; using equations 3-5 we can decompose this across segments instead.
We compare APS with two recent systems $$$$$ Such approaches build a stationary Markov chain by repeatedly sampling among the hidden variables in the model.
We compare APS with two recent systems $$$$$ The use of annealing to obtain a maximum a posteriori (MAP) configuration from sampling-based inference is common (e.g., Finkel 2005; Goldwater 2007).

Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ Parameter settings For LCSEG, we use the parameter values specified in the paper (Galley et al., 2003).
Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ This is similar in spirit to hidden topic models such as latent Dirichlet allocation (Blei et al., 2003), but rather than assigning a hidden topic to each word, we constrain the topics to yield a linear segmentation of the document.
Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ Preprocessing Standard preprocessing techniques are applied to the text for all comparisons.

We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ We also show that both an entropy-based analysis and a well-known previous technique can be de
We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ The results on the meeting corpus also compare favorably with the topic-modeling method of Purver et al. (2006), who report a Pk of .289 and a WindowDiff of .329.
We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ The effectiveness of cue phrases as a feature depends on whether the writer or speaker uses them consistently.
We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ This paper presents a novel Bayesian approach to unsupervised topic segmentation.

 $$$$$ In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function.
 $$$$$ We show that in a special case of our model, the segmentation objective is equal to a weighted sum of the negative entropies for each topic segment.
 $$$$$ This dataset includes transcripts of 75 multi-party meetings, of which 25 are annotated for segment boundaries.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the NSF.

Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ Suppose that each language model is drawn from a symmetric Dirichlet prior: θj — Dir(θ0).
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006).
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168) and the Microsoft Research Faculty Fellowship.
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ In their unsupervised model, inference is performed by selecting segmentation points at the local maxima of the cohesion function.
