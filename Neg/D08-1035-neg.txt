Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ As shown in the left column of the table, our model has identified several strong cue phrases from the meeting dataset which appear to be linguistically plausible.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment.
Discourse cues as predictive features of topic boundaries have also been considered in Eisenstein and Barzilay (2008). $$$$$ More formally, we treat the words in each sentence as draws from a language model associated with the topic segment.

Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ These values can be stored in a table of size T (equal to the number of sentences); this admits a dynamic program that performs inference in polynomial time.3 If the number of segments is specified in advance, the dynamic program is slightly more complex, with a table of size TK.
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ The proposal is constructed so that no probability mass is allocated to moves that change the order of segment boundaries, or merge two segments; one consequence of this restriction is that moves cannot add or remove segments.5 We set the proposal distribution to decrease exponentially with the move distance, thus favoring incremental transformations to the segmentation.
Eisenstein and Barzilay (2008) extend this work by marginalizing the language models using the Dirichlet compound multinomial distribution; this permits efficient inference to be performed directly in the space of segmentations. $$$$$ BAYESSEG-CUE-PROP adds the linguisticallymotivated proposal distribution.

Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ This may help to explain why cue phrases improve performance for meeting transcripts, but not for the textbook.
Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168) and the Microsoft Research Faculty Fellowship.
Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ This paper presents a novel Bayesian approach to unsupervised topic segmentation.
Eisenstein and Barzilay (2008) describe a dynamic program for linear segmentation with a space complexity of O (T) and time complexities of O (T 2) to compute the A matrix and O (TW) to fill the B matrix. $$$$$ Thanks to Aaron Adler, S. R. K. Branavan, Harr Chen, Michael Collins, Randall Davis, Dan Roy, David Sontag and the anonymous reviewers for helpful comments and suggestions.

Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ As shown in the table, the Bayesian models achieve the best results on both metrics for both corpora.
Eisenstein and Barzilay (2008) use the same dataset to evaluate linear topic segmentation, though they evaluated only at the level of sections, given gold standard chapter boundaries. $$$$$ Each chapter contains an average of 140 sentences, giving an average of 28 sentences per segment.

This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ This form is equivalent to Laplacian smoothing (Manning and Sch¨utze, 1999), and is a special case of our equation 2, with B0 = 1.
This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ We model cue phrases as generated from a separate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases.
This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ This result suggests intriguing connections with prior work on the relationship between entropy and discourse structure (e.g., Genzel and Charniak, 2002; Sporleder and Lapata, 2006).
This is equivalent to BAYESSEG, which achieved the best reported performance on the linear segmentation of this same dataset (Eisenstein and Barzilay, 2008). $$$$$ We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available.

Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ The DCM distribution expresses the expectation over all multinomial language models, when conditioning on the Dirichlet prior θ0.
Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ This procedure is iterated until convergence or a maximum of twenty iterations.
Our work extends the Bayesian segmentation model (Eisenstein and Barzilay, 2008) for isolated texts, to the problem of segmenting parallel parts of documents. $$$$$ Other researchers have observed relationships between discourse structure and entropy (e.g., Genzel and Charniak, 2002).

Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ Galley et al. (2003) characterize cohesion in terms of lexical chains – repetitions of a given lexical item over some fixed-length window of sentences.
Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ We also show that both an entropy-based analysis and a well-known previous technique can be de
Unlike (Eisenstein and Barzilay, 2008), we cannot make an assumption that the number of segments is known a-priori, as the effective number of part-specific segments can vary significantly from document to document, depending on their size and structure. $$$$$ Unsupervised systems for this task are driven the tendency of wellformed segments to induce a compact and consistent lexical distribution.

If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ The symbol F refers to the Gamma function, an extension of the factorial function to real numbers.
If the actual number of segments is known and only a linear discourse structure is acceptable, then a single move, shift of the segment border (Fig. 2(a)), is sufficient (Eisenstein and Barzilay, 2008). $$$$$ At the same time, the addition of cue phrases prevents the use of exact inference techniques, which may explain the decline in results for the meetings dataset.

The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.
The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ Cue phrases are ranked by their chi-squared value, which is computed based on the number of occurrences for each word at the beginning of a hypothesized segment, as compared to the expectation.
The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ Thanks to Aaron Adler, S. R. K. Branavan, Harr Chen, Michael Collins, Randall Davis, Dan Roy, David Sontag and the anonymous reviewers for helpful comments and suggestions.
The second baseline is a pipeline approach (Pipeline), where we first segment the lecture transcript with BayesSeg (Eisenstein and Barzilay, 2008) and then use the pairwise alignment to find their best alignment to the segments of the story. $$$$$ Our model yields consistent improvements over an array of state-of-the-art systems on both text and speech datasets.

The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ This system is referred to as BAYESSEG in Table 1.
The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ We will assume that topic breaks occur at sentence boundaries, and write zt to indicate the topic assignment for sentence t. The observation likelihood is, where X is the set of all T sentences, z is the vector of segment assignments for each sentence, and Θ is the set of all K language models.2 A linear segmentation is ensured by the additional constraint that zt must be equal to either zt−1 (the previous sentence’s segment) or zt−1 + 1 (the next segment).
The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ This is consistent with the principle of lexical cohesion.
The Bayesian framework explored by Eisenstein and Barzilay (2008) is a potential route to a richer model, and they found their richer model beneficial for a meetings corpus but not for a textbook. $$$$$ Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model.

This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ The authors acknowledge the support of the National Science Foundation (CAREER grant IIS0448168) and the Microsoft Research Faculty Fellowship.
This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ The counts for φ are summed across every segment in the entire dataset, so shifting a boundary will affect the probability of every segment, not only the adjacent segments as before.
This insight has been used to tune supervised methods (Hsueh et al, 2006) and inspire unsupervised models of lexical cohesion using bags of words (Purver et al, 2006) and language models (Eisenstein and Barzilay, 2008). $$$$$ This contrasts with previous approaches, which relied on hand-crafted cohesion metrics.

In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ Lexical cohesion has provided the inspiration for several successful systems (e.g., Utiyama and Isahara, 2001; Galley et al.2003; Malioutov and Barzilay, 2006), and is currently the dominant approach to unsupervised topic segmentation.
In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ Thus, lexical cohesion arises naturally through the generative process, and other sources of information – such as cue words – can easily be incorporated as emissions from the segment boundaries.
In order to illustrate why using a single gold standard reference segmentation can be problematic, we evaluate three publicly available segmenters, MinCutSeg (Malioutov and Barzilay, 2006), BayesSeg (Eisenstein and Barzilay, 2008) and APS (Kazantseva and Szpakowicz, 2011), using several different gold standards and then using all available annotations. $$$$$ An alternative Bayesian approach to segmentation was proposed by Purver et al. (2006).

We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ We can re-estimate this prior based on the observed data by interleaving gradient-based search in a Viterbi expectation-maximization framework (Gauvain and Lee, 1994).
We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ WindowDiff is stricter in that it requires that the number of intervening segments between the two sentences be identical in the hypothesized and the reference segmentations, while Pk only asks whether the two sentences are in the same segment or not.
We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ Without supervision, it is not possible to combine such metrics with additional sources of information.

Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ We also thank Michel Galley, Igor Malioutov, and Masao Utiyama for making their topic segmentation code publically available.
Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ Setting B0 = 0 and rearranging equation 2, we obtain nj,i = Nj�Bj,i, with Nj = PW i nj,i, the total number of words in segment j.
Eisenstein and Barzilay (2008) treat words in a sentence as draws from a multinomial language model. $$$$$ On the medical textbook corpus, the Bayesian systems achieve a raw performance gain of 2-3% with respect to all baselines on both metrics.

We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ These parameters affect the rate of convergence but are unrelated to the underlying probability model.
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ However, Gibbs sampling is slow to converge to a stationary distribution when the hidden variables are tightly coupled.
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ We model cue phrases as generated from a separate multinomial that is shared across all topics and documents in the dataset; a high-likelihood model will obtain a compact set of cue phrases.

 $$$$$ Our approach generalizes U&I and provides a Bayesian justification for the language models that they apply.
 $$$$$ For clarity, our exposition will focus on the single-document case. bution does not affect the underlying probabilistic model – Metropolis-Hastings will converge to the same underlying distribution for any non-degenerate proposal.
 $$$$$ On the ICSI meeting corpus, the Bayesian systems perform 4-5% better than the best baseline on the Pk metric, and achieve smaller improvement on the WindowDiff metric.

Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision.
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ Unlike these approaches, we identify candidate cue phrases automatically from unlabeled data and incorporate them in the topic segmentation task without supervision.
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ In the future, we hope to explore the use of similar Bayesian techniques for hierarchical segmentation, and to incorporate additional features such as prosody and speaker change information.
Three automatic segmenters were trained - or had their parameters estimated upon - The Moonstone data set, including MinCut; (Malioutov and Barzilay, 2006), BayesSeg; (Eisenstein and Barzilay, 2008), and APS (Kazantseva and Szpakowicz, 2011). $$$$$ Our algorithm is capable of incorporating both lexical cohesion and cue phrase features in a principled manner, and outperforms state-of-the-art baselines on text and transcribed speech corpora.
