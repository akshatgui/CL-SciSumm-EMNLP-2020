Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ This work was supported in part by the Advanced Research Projects Agency (Order 8073, Contract MDA904-91-C-5224) and by the Department of Defense.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ In contrast, the two-level model provides for the automatic collection and implicit representation of collocational constraints between adjacent words.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ Two strategies have been used in lexical choice when knowledge gaps exist: selection of a default,4 and random choice among alternatives.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ But the default choices frequently are not the optimal ones; the hybrid model we describe provides more satisfactory solutions.

Over the years, several proposals of generic NLG system shave been made $$$$$ Twenty or thirty choice points typically multiply into millions or billions of potential sentences, and it is infeasible to generate them all independently.
Over the years, several proposals of generic NLG system shave been made $$$$$ These two categories of gaps include: The generation system we use, PENMAN (Penman, 1989), is robust because it supplies appropriate defaults when knowledge is missing.
Over the years, several proposals of generic NLG system shave been made $$$$$ Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual.
Over the years, several proposals of generic NLG system shave been made $$$$$ We would like to thank Yolanda Gil, Eduard Hovy, Kathleen McKeown, Jacques Robin, Bill Swartout, and the ACL reviewers for helpful comments on earlier versions of this paper.

The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ In our machine translation experiences, we traced generation disfluencies to two sources:1 (1) incomplete or inaccurate conceptual (interlingua) structures, caused by knowledge gaps in the source language analyzer, and (2) knowledge gaps in the generator itself.
The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ A bigram model will happily select a sentence like I only hires men who is good pilots.
The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ Although robustness has been heavily studied in natural language understanding (Weischedel and Black, 1980; Hayes, 1981; Lavie, 1994), it has received much less attention in NLG (Robin, 1995).

Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ Here are three randomly selected translations; note that the object of the &quot;establishing&quot; action is unspecified in the Japanese input, but PENMAN supplies a placeholder it when necessary, to ensure grammaticality: 'Available from the ACL Data Collection Initiative, as CD ROM 1. for bigrams A new company will have in mind that it is establishing it on February.
Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ This work was supported in part by the Advanced Research Projects Agency (Order 8073, Contract MDA904-91-C-5224) and by the Department of Defense.

(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ Choosing the is a good tactic, because the works with mass, count, singular, plural, and occasionally even proper nouns, while a does not.
(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ For example, does A contain frequent three-word sequences?
(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual.

And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. $$$$$ There are many such inflectional and derivational patterns.
And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. $$$$$ So our lattices included paths like Him saw I as well as He saw me.

A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ We would like to thank Yolanda Gil, Eduard Hovy, Kathleen McKeown, Jacques Robin, Bill Swartout, and the ACL reviewers for helpful comments on earlier versions of this paper.
A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ At the same time, we take advantage of the strength of the knowledge-based approach which guarantees grammatical inputs to the statistical component, and reduces the amount of language structure that is to be retrieved from statistics.
A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ This work was supported in part by the Advanced Research Projects Agency (Order 8073, Contract MDA904-91-C-5224) and by the Department of Defense.
A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.

Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995) $$$$$ For a generator to be able to produce sufficiently varied text, multiple renditions of the same concept must be accessible.
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995) $$$$$ In other words, how often have we seen A and B in the past?
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995) $$$$$ Given that perfect KBs do not yet exist, an important question arises: can we build high-quality NLG systems that are robust against incomplete KBs and inputs?
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995) $$$$$ Unfortunately, most of these constraints must be identified manually, and even when automatic methods for the acquisition of some types of this lexical knowledge exist (Smadja and McKeown, 1991), the extracted constraints must still be transformed to the generator's representation language by hand.
