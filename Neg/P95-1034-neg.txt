Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ We describe algorithms and show experimental results.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ Choices between alternative lexical islands for the same concept also become states in the lattice, with arcs leading to the sub-lattices corresponding to each island.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ Space limitations prevent us from tracing the generation of many long sentences—we show instead a few short ones.
Some NLG techniques use sampling strategies (Knight and Hatzivassiloglou, 1995) where a set of sentences is sampled from a data structure created from an underlying grammar and ranked according to how well they meet the communicative goal. $$$$$ This generates sentences like They plan the statement of the filing for bankruptcy, avoiding disasters like They plan that it is said to file for bankruptcy.

Over the years, several proposals of generic NLG system shave been made: Penman (Matthiessen and Bate man, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al, 2002), etc. $$$$$ Choosing the is a good tactic, because the works with mass, count, singular, plural, and occasionally even proper nouns, while a does not.
Over the years, several proposals of generic NLG system shave been made: Penman (Matthiessen and Bate man, 1991), FUF (Elhadad, 1991), Nitrogen (Knight and Hatzivassiloglou, 1995), Fergus (Bangalore and Rambow, 2000), HALogen (Langkilde-Geary, 2002), Amalgam (Corston-Oliver et al, 2002), etc. $$$$$ We would like to thank Yolanda Gil, Eduard Hovy, Kathleen McKeown, Jacques Robin, Bill Swartout, and the ACL reviewers for helpful comments on earlier versions of this paper.

The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ Consequently, the selection of the preposition can be postponed until the very end.
The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ But the default choices frequently are not the optimal ones; the hybrid model we describe provides more satisfactory solutions.
The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ Choosing the is a good tactic, because the works with mass, count, singular, plural, and occasionally even proper nouns, while a does not.
The extraction algorithm is presented in (Knight and Hatzivassiloglou, 1995). $$$$$ By retraining the statistical component on a different domain, we can automatically pick up the peculiarities of the sublanguage such as preferences for particular words and collocations.

Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ Here is an abridged list of outputs, loglikelihood scores heuristically corrected for length, and rankings: 401 The new companies will have in mind to establish it at February.
Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ Given that perfect KBs do not yet exist, an important question arises: can we build high-quality NLG systems that are robust against incomplete KBs and inputs?
Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ Also, the n-gram length is critical.
Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimotoetal., 2000), (Bangalore and Rambow, 2000) concentrate on the specifics of individual stochastic methods, ignoring other issues such as integrability, portability, and efficiency. $$$$$ These features were relevant to the semantic representation but their values were not extractable from the Japanese sentence, and thus each of their combinations corresponded to a particular interpretation among the many possible in the presence of incompleteness in the semantic input.

(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ We describe a hybrid model for natural language generation which offers improved performance in the presence of knowledge gaps in the generator (the grammar and the lexicon), and of errors in the semantic input.
(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ This work was supported in part by the Advanced Research Projects Agency (Order 8073, Contract MDA904-91-C-5224) and by the Department of Defense.
(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ Default choices have the advantage that they can be carefully chosen to mask knowledge gaps to some extent.
(Belz, 2005) To incorporate the statistical component, which allows for robust generalization, per (Knight and Hatzivassiloglou, 1995), the NLG on the target side is filtered through a language model (described above). $$$$$ At this point, we can make another approximation— modeling fluency as likelihood.

And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. $$$$$ For example, the selection of a word from a pair of frequently co-occurring adjacent words will automatically create a strong bias for the selection of the other member of the pair, if the latter is compatible with the semantic concept being lexicalized.
And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. $$$$$ This essentially assumes that our generator produces valid mappings from I, but may be unsure as to which is the correct rendition.
And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. $$$$$ To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.
And Knight and Hatzivassiloglou (1995) use a language model for selecting a fluent sentence among the vast number of surface realizations corresponding to a single semantic representation. $$$$$ Turning conceptual expressions into English requires the integration of large knowledge bases (KBs), including grammar, ontology, lexicon, collocations, and mappings between them.

A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ For example, does A contain frequent three-word sequences?
A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ But the default choices frequently are not the optimal ones; the hybrid model we describe provides more satisfactory solutions.
A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ This essentially assumes that our generator produces valid mappings from I, but may be unsure as to which is the correct rendition.
A language model is then used to select the most probable realization (Knight and Hatzivassiloglou, 1995). $$$$$ We have extended our notation to allow such constructions, but the full solution is to move to a unification-based framework, in which e-structures are replaced by arbitrary feature structures with syn, sem, and lat fields.

Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model. $$$$$ These two categories of gaps include: The generation system we use, PENMAN (Penman, 1989), is robust because it supplies appropriate defaults when knowledge is missing.
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model. $$$$$ Given that perfect KBs do not yet exist, an important question arises: can we build high-quality NLG systems that are robust against incomplete KBs and inputs?
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model. $$$$$ But the default choices frequently are not the optimal ones; the hybrid model we describe provides more satisfactory solutions.
Some current methods have a generate and filter approach (Knight and Hatzivassiloglou, 1995): all constructions are generated and then filtered based on a statistical model. $$$$$ But the default choices frequently are not the optimal ones; the hybrid model we describe provides more satisfactory solutions.
