A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). $$$$$ The SELECTIONAL ASSOCIATION between a predicate and a word is defined based on the contribution of the word to the conditional entropy of the predicate.
A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). $$$$$ This work has received valuable input from people too numerous to mention.
A concise review of this research area can be found in, for instance, Lauer (1995), which dates back to Finin (1980). $$$$$ I have argued that without it a broad coverage system would be impossible.

As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. $$$$$ To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus.
As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. $$$$$ This work has received valuable input from people too numerous to mention.
As Lauer (1995) pointed out, using (partial) parsing of the text is too costly. $$$$$ Conceptual association outperforms lexical association, presumably because of its ability to generalise.

Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. $$$$$ The normaliser ensures that all parameters for a head noun sum to unity.
Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. $$$$$ Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus).
Lauer (1995) compared a dependency model with adjacency models, and found that the dependency model is better. $$$$$ The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.

This is an extension of left branch preference in Lauer (1995). $$$$$ The most general of these is that even quite crude corpus statistics can provide information about the syntax of compound nouns.
This is an extension of left branch preference in Lauer (1995). $$$$$ In the experiments below the accuracy of such a system is measured.
This is an extension of left branch preference in Lauer (1995). $$$$$ Using a corpus to acquire associations, they bracket sequences of Kanji with lengths four to six (roughly equivalent to two or three words).
This is an extension of left branch preference in Lauer (1995). $$$$$ The proportion of cases in which the procedure was forced to guess, either because no data supported either analysis or because both were equally supported, is quite low.

 $$$$$ Also, the work reported in Lauer and Dras (1994) uses simplistic estimates of the probability of a word given its thesaurus category.
 $$$$$ This work has received valuable input from people too numerous to mention.
 $$$$$ It has the effect of dividing the evidence from a training instance across all possible categories for the words.

Lauer (1995) $$$$$ The proportion of cases in which the procedure was forced to guess, either because no data supported either analysis or because both were equally supported, is quite low.
Lauer (1995) $$$$$ However, for the pattern training scheme an improvement was made to the dependency model, producing the highest overall accuracy of 81%.
Lauer (1995) $$$$$ The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.
Lauer (1995) $$$$$ The significance of the use of conceptual association deserves some mention.

Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. $$$$$ The normaliser ensures that all parameters for a head noun sum to unity.
Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. $$$$$ I have argued that without it a broad coverage system would be impossible.
Lauer (1995) proposes an unsupervised method for estimating the frequencies of the competing bracketings based on a taxonomy or a thesaurus. $$$$$ Tagged Dependency -.— Tagged Adjacency -e— L. Pattern 3 5 Training scheme (integers denote window widths) Figure 5: Accuracy using a tagged corpus for various training schemes data would make a difference, the freely available Brill tagger (Brill, 1993) was applied to the corpus.

Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. $$$$$ In such cases, we observe that the test instance itself provides the information that the event 12 t3 can occur and we recalculate the ratio using Pr(12 19 = Jr for all possible categories 12,13 where k is any non-zero constant.
Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. $$$$$ The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.
Lauer (1995) tested both the adjacency and dependency models on 244 compounds extracted from Grolier's encyclopedia, a corpus of 8 million words. $$$$$ At the very least, this information can be applied in broad coverage parsing to assist in the control of search.

Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. $$$$$ Instead, the correct parse depends on whether calcium characterises the ions or mediates the exchange.
Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. $$$$$ This work has received valuable input from people too numerous to mention.
Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. $$$$$ Each test compound presents a set of possible analyses and the goal is to choose which analysis is most likely.
Lauer (1995) is the first to propose and evaluate an unsupervised probabilistic model of compound noun interpretation for domain independent text. $$$$$ The model also has the further commendation that it predicts correctly the observed proportion of left-branching compounds found in two independently extracted test sets.

Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. $$$$$ If anything, admitting additional training data based on the tagger introduces more noise, reducing the accuracy.
Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. $$$$$ However, no correction is made to the probability estimates for Pr(ti t2) and Pr(ti 13) for unseen cases, thus putting the dependency model on an equal footing with the adjacency model above.
Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. $$$$$ This work has received valuable input from people too numerous to mention.
Lauer (1995) tested the model in (7) on 282 compounds that he selected randomly from Grolier's encyclopedia and annotated with their paraphrasing prepositions. $$$$$ For example, when backup compiler disk is encountered, the analysis will be: The proposal of Liberman and Sproat (1992) is more sophisticated and allows for the frequency of the words in the compound.

The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). $$$$$ This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results.
The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). $$$$$ The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.
The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). $$$$$ Yarowsky (1992) uses a fixed 100 word window to collect information used for sense disambiguation.
The computational problem is thus deciding whether the three-word NC has a left or right-bracketing structure (Lauer, 1995). $$$$$ While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method.

Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. $$$$$ For three word compounds it suffices to compute the ratio of two probabilities, that of a left-branching analysis and that of a right-branching one.
Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. $$$$$ This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results.
Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. $$$$$ This results in rather poor tagging accuracy, so it is quite possible that a manually tagged corpus would produce better results.
Mark Lauer (1995) only considered English noun compounds and applied a different disambiguation strategy based on word association scores. $$$$$ For reasons made clear below, only sequences consisting entirely of words from Roget's thesaurus were retained, giving a total of 308 test triples.3 These triples were manually analysed using as context the entire article in which they appeared.

Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. $$$$$ Conceptual association outperforms lexical association, presumably because of its ability to generalise.
Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. $$$$$ The significance of the use of conceptual association deserves some mention.
Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. $$$$$ Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing.
Several researchers have tackled the syntactic analysis (Lauer, 1995), (Pustejovsky et al, 1993), (Liber man and Church, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. $$$$$ This work has received valuable input from people too numerous to mention.

We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets $$$$$ Not only does this require a vast amount of memory space, it creates a severe data sparseness problem since we require at least some data about each parameter.
We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets $$$$$ Their proposal involves comparing the mutual information between the two pairs of adjacent words and bracketing together whichever pair exhibits the highest.
We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets $$$$$ At the very least, this information can be applied in broad coverage parsing to assist in the control of search.
We also present empirical observations on the distribution of the syntax and meaning of noun phrases on two different corpora based on two state-of-the-art classification tag sets $$$$$ 5 Acknowledgements This work has received valuable input from people too numerous to mention.

On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). $$$$$ While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method.
On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). $$$$$ Eight different training schemes have been used to estimate the parameters and each set of estimates used to analyse the test set under both the adjacency and the dependency model.
On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). $$$$$ The experiments above demonstrate a number of important points.
On the other hand, the majority of corpus statistics approaches to noun compound interpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model (Lauer, 1995). $$$$$ To determine the difference made by conceptual association, the pattern training scheme has been retrained using lexical counts for both the dependency and adjacency model, but only for the words in the test set.

They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). $$$$$ Whichever is found is then chosen as the more closely bracketed pair.
They can vary from a few prepositions (Lauer, 1995) to hundreds or thousands specific semantic relations (Finin, 1980). $$$$$ The most significant contributions have been made by Richard Buckland, nowledged from the Microsoft Institute and the Australian Government.

(Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps $$$$$ In this study, conceptual association is used with groups consisting of all categories from the 1911 version of Roget's thesaurus.4 Given two thesaurus categories t1 and t2, there is a parameter which represents the degree of acceptability of the structure [n1n2j where ni is a noun appearing in t1 and n2 appears in t2.
(Lauer, 1995) points out that the existing approaches to resolving the ambiguity of noun phrases fall roughly into two camps $$$$$ Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus).

(Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. $$$$$ The proportion of cases in which the procedure was forced to guess, either because no data supported either analysis or because both were equally supported, is quite low.
(Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. $$$$$ In all experiments the dependency model provides a substantial advantage over the adjacency model, even though the latter is more prevalent in proposals within the literature.
(Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. $$$$$ Yet, far from being an obscure, endangered species, the noun compound is flourishing in modern language.
(Lauer and Dras, 1994) and (Lauer, 1995) address the issue of structural ambiguity by developing a dependency model where instead of computing the acceptability of A [YZ] one would compute the acceptability of A [XZ]. $$$$$ While using windowed co-occurrence did not help here, it is possible that under more data sparse conditions better performance could be achieved by this method.

(Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. $$$$$ Given the high frequency of occurrence of noun compounds in many texts, this suggests that the use of these techniques in probabilistic parsers will result in higher performance in broad coverage natural language processing.
(Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. $$$$$ To ensure that the test set is disjoint from the training data, all occurrences of the test noun compounds have been removed from the training corpus.
(Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. $$$$$ This is in contrast to previous work on conceptual association where it resulted in little improvement on a task which could already be performed.
(Lauer, 1995) argues that the dependency model is not only more intuitive than the adjacency model, but also yields better results. $$$$$ Levi, 1978; Ryder, 1994) and computational linguistics (Finin, 1980; McDonald, 1982; Isabelle, 1984), techniques suitable for broad coverage parsing remain unavailable.

This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. $$$$$ This work has received valuable input from people too numerous to mention.
This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. $$$$$ The schemes used are: The accuracy on the test set for all these experiments is shown in figure 2.
This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. $$$$$ In this study, not only has the technique proved its worth by supporting generality, but through generalisation of training information it outperforms the equivalent lexical association approach given the same information.
This method is tested using a set of general English nominal compounds developed by (Lauer, 1995) as well as a set of nominal compounds extracted from MEDLINE abstracts. $$$$$ Since no manually tagged training data is available for our corpus, the tagger's default rules were used (these rules were produced by Brill by training on the Brown corpus).
