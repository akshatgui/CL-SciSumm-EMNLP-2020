McDonald (McDonald, 2006) independently proposed a new machine learning approach. $$$$$ The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.
McDonald (McDonald, 2006) independently proposed a new machine learning approach. $$$$$ The final weight vector is the average of all weight vectors throughout training.
McDonald (McDonald, 2006) independently proposed a new machine learning approach. $$$$$ Finally, the author thanks the four reviewers for evaluating the compressed sentences.
McDonald (McDonald, 2006) independently proposed a new machine learning approach. $$$$$ For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system.

However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. $$$$$ We define the function I(yi) E {1, ... , n} that maps word yi in the compression to the index of the word in the original sentence.
However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. $$$$$ Finally we include the constraint I(yi) < I(yi+1), which forces each word in x to occur at most once in the compression y. Compressions are evaluated on three criteria, Typically grammaticality and importance are traded off with compression rate.
However, while the former problem can be solved efficiently using the dynamic programming approach of McDonald (2006), there are no efficient algorithms to recover maximum weighted non projective subtrees in a general directed graph. $$$$$ As a result, the model will sometimes return very short and ungrammatical compressions.

McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. $$$$$ This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.
McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. $$$$$ Of greater interest is that the compressions of our system are typically more grammatical than the decision tree model of Knight and Marcu.
McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. $$$$$ The most likely reason for this is that our model returns longer sentences and is thus less likely to prune away important information.
McDonald (2006) provides a Viterbi-like dynamic programming algorithm to recover the highest scoring sequence of order-preserving bigrams from a lattice, either in unconstrained form or with a specific length constraint. $$$$$ Of course, like most compression and translation tasks, this is not true, consider, TapeWare , which supports DOS and NetWare 286 , is a value-added process that lets you directly connect the QA150-EXAT to a file server and issue a command from any workstation to back up the server The human annotated compression is, TapeWare supports DOS and NetWare 286.

For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. $$$$$ These parses are provided from a parsing model trained on out-of-domain data (the WSJ), which can result in parse trees with many mistakes for both the original and compressed versions.
For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. $$$$$ The model uses a shift-reduce-drop parsing algorithm that starts with the sequence of words in x and the corresponding tree.
For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. $$$$$ Thanks also to Hal Daum´e and Fernando Pereira for useful discussions.
For consistent comparisons with the other systems, our re-implementation does not include the k-best inference strategy presented in McDonald (2006) for learning with MIRA. $$$$$ A fundamental flaw with all sentence compression systems is that model parameters are set with the assumption that there is a single correct answer for each sentence.

The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. $$$$$ We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.
The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. $$$$$ A quick scan of the evaluations shows that the few ungrammatical human compressions were for sentences that were not really grammatical in the first place.
The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. $$$$$ We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs.
The third one is the bigram model proposed by McDonald (McDonald, 2006) which adopts dynamic programming for efficient inference. $$$$$ As a result, the model will sometimes return very short and ungrammatical compressions.

 $$$$$ Our system performs well, however it leaves out the complementizer of the relative clause.
 $$$$$ The parsers are trained out-of-domain and contain a significant amount of noise.
 $$$$$ ATF Protype is a line of digital postscript typefaces that will be sold in packages of up to six fonts .
 $$$$$ It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.

Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006). $$$$$ Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted.
Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006). $$$$$ This is similar in purpose to the source model from the noisy-channel system.
Some utilize a parser to identify and later keep certain important relations but do not require a complete parse (Clarke & Lapata, 2008), or use a syntactic representation to extract features (McDonald, 2006). $$$$$ Formally, sentence compression aims to shorten a sentence x = x1 ... xn into a substring y = y1 ... ym, where yi E {x1, ... , xn}.

 $$$$$ That is, they represent just another layer of evidence to be considered during training when setting parameters.
 $$$$$ We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.
 $$$$$ These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise.

One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. $$$$$ In Section 3.3 we describe an online large-margin method for learning w. Here we present the feature representation f(x, I(yj−1), I(yj)) for a pair of adjacent words in the compression.
One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. $$$$$ Thanks also to Hal Daum´e and Fernando Pereira for useful discussions.
One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. $$$$$ Knight and Marcu (2000) first tackled this problem by presenting a generative noisy-channel model and a discriminative tree-to-tree decision tree model.
One successful recent approach (McDonald, 2006) combines a discriminative framework with a set of features that capture information similar to the K&M model. $$$$$ It is not unique to use soft syntactic features in this way, as it has been done for many problems in language processing.

This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006). $$$$$ Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression.
This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006). $$$$$ The noisy-channel model typically returned longer compressions.
This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006). $$$$$ Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate.
This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daume III and Marcu, 2004) and also in sentence compression (McDonald, 2006). $$$$$ The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.

 $$$$$ We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
 $$$$$ Formally, sentence compression aims to shorten a sentence x = x1 ... xn into a substring y = y1 ... ym, where yi E {x1, ... , xn}.

Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). $$$$$ We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). $$$$$ | t�1 of original sentences xt and their compressions yt.
Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). $$$$$ However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed.
Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). $$$$$ The author would like to thank Daniel Marcu for providing the data as well as the output of his and Kevin Knight’s systems.

This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ We present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers.
This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ Thanks also to Hal Daum´e and Fernando Pereira for useful discussions.
This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ Given a tree for a long sentence x and compressed sentence y, the channel probability is the product of the probability for each transformation required if the tree for y is to expand to the tree for x.
This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al, 2005). $$$$$ On the other hand, the decision tree model does not rely on the trees to align and instead simply learns a tree-totree transformation model to compress sentences.

McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. $$$$$ The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.
McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. $$$$$ This is most likely due to the fact that dropping the main verb phrase of a sentence is much less likely in the training data than dropping a relative clause.
McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. $$$$$ Results are shown in Table 1.
McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. $$$$$ The paper is organized as follows: Section 2 discusses previous approaches to sentence compression.

 $$$$$ Section 4 presents an experimental evaluation of our model compared to the models of Knight and Marcu (2000) and finally Section 5 discusses some areas of future work.
 $$$$$ For the rest of the paper we use x = x1 ... xn to indicate an uncompressed sentence and y = y1 ... ym a compressed version of x, i.e., each yj indicates the position in x of the jth word in the compression.
 $$$$$ However, since these systems require human evaluation we did not have the time or the resources to conduct these experiments.

Note that their model is a strong baseline $$$$$ | t�1 of original sentences xt and their compressions yt.
Note that their model is a strong baseline $$$$$ Our model includes all features, including those that are unsupported.
Note that their model is a strong baseline $$$$$ The longer our compressions, the less likely we are to remove important words or phrases crucial to maintaining grammaticality and the intended meaning.
Note that their model is a strong baseline $$$$$ In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning.

Both these systems reported results outperforming previous systems such as McDonald (2006). $$$$$ Thanks also to Hal Daum´e and Fernando Pereira for useful discussions.
Both these systems reported results outperforming previous systems such as McDonald (2006). $$$$$ We define a dynamic programming table C[i] which represents the highest score for any compression that ends at word xi for sentence x.
Both these systems reported results outperforming previous systems such as McDonald (2006). $$$$$ This differs from current state-of-the-art models (Knight and Marcu, 2000) that treat noisy parse trees, for both compressed and uncompressed sentences, as gold standard when calculating model parameters.
Both these systems reported results outperforming previous systems such as McDonald (2006). $$$$$ In Section 3.3 we describe an online large-margin method for learning w. Here we present the feature representation f(x, I(yj−1), I(yj)) for a pair of adjacent words in the compression.

For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). $$$$$ This decoding algorithm is dynamic with respect to compression rate.
For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). $$$$$ Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).
For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). $$$$$ Thus if a sentence has multiple valid compressions we can learn to score each valid one higher than all invalid compressions during training to avoid this problem.
For example, such solvers are unable to take advantage of efficient dynamic programming routines for sentence compression (McDonald, 2006). $$$$$ We found during experiments on the development data that lexical information was too sparse and led to overfitting, so we rarely include such features.

The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. $$$$$ This work was supported by NSF ITR grants 0205448 and 0428193.
The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. $$$$$ However, since these systems require human evaluation we did not have the time or the resources to conduct these experiments.
The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. $$$$$ Examples from this data set are given in Figure 1.
The same framework can be readily adapted to other compression models that are efficiently decodable, such as the semi-Markov model of McDonald (2006), which would allow incorporating a language model for the compression. $$$$$ In this section we described a discriminative online learning approach to sentence compression, the core of which is a decoding algorithm that searches the entire space of compressions.

McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm. $$$$$ We argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly.
McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm. $$$$$ This can easily be calculated by extending the decoding algorithm with standard Viterbi k-best techniques.
McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm. $$$$$ | t�1 of original sentences xt and their compressions yt.
McDonald (2006) also presents a sentence compression model that uses a discriminative large margin algorithm. $$$$$ Both models rely heavily on the output of a noisy parser to calculate probability estimates for the compression.
