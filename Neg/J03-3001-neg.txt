Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). $$$$$ The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.
Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). $$$$$ Noting the singular needs of lexicography for big data, in the 1970s Sinclair and Atkins inaugurated the COBUILD project, which raised the threshold of viable corpus size from one million to, by the early 1980s, eight million words (Sinclair 1987).
Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). $$$$$ The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.
Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). $$$$$ The authors use the Web again at a final stage to rerank possible translations by verifying which subsequences among the possible translations are most attested.

In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). $$$$$ In addition to Keller and Lapata (this issue) and references therein, Volk (2001) gathers lexical statistics for resolving prepositional phrase attachments, and Villasenor-Pineda et al. (2003) “balance” their corpus using Web documents.
In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). $$$$$ However, many of the resources that developers may wish to use are general-language resources, such as, for English, WordNet, ANLT, XTag, COMLEX, and the BNC.
In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). $$$$$ Unseen words, or word sequences—that is, words or sequences not occurring in training data—are a problem for language models.
In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). $$$$$ A language can be seen as a modest core of lexis, grammar, and constructions, plus a wide array of different sublanguages, as used in each of a myriad of human activities.

The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). $$$$$ In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work.
The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). $$$$$ We first consider whether the Web is indeed a corpus, then present a history of the theme in which we view the Web as a development of the empiricist turn that has brought corpora center stage in the course of the 1990s.

In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ This special issue of Computational Linguistics explores ways in which this dream is being explored.
In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ However, many of the resources that developers may wish to use are general-language resources, such as, for English, WordNet, ANLT, XTag, COMLEX, and the BNC.
In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ We wish to avoid a smuggling of values into the criterion for corpus-hood.
In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ They generate pairs of common words, constructing combinations that are and are not attested in the BNC.

It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). $$$$$ Also, the focus is usually on content words and topics or domains, with other differences of genre or sublanguage remaining unexamined.
It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). $$$$$ So that the semantic question may be set aside, the definition of corpus should be broad.
It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). $$$$$ In the text domain, organizations such as Reuters produce news feeds that are typically adapted to the style of a particular newspaper and then republished: Is each republication a new writing event?

 $$$$$ Nobody knows.
 $$$$$ Both original phrases and translations are chunked.

The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ Language scientists and technologists are increasingly turning to the Web as a source of language data, because it is so big, because it is the only available source for the type of language in which they are interested, or simply because it is free and instantly available.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ Santamar´ia et al. present an algorithm for attaching WordNet word senses to nodes in this same taxonomy, thus providing automatically created links between word senses and Web pages.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ The EU MEANING project (Rigau et al. 2002) takes forward the exploration of the Web as a data source for word sense disambiguation, working from the premise that within a domain, words often have just one meaning, and that domains can be identified on the Web.

For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ The answer to the question “Is the web a corpus?” is yes.
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ They then compare the frequency of these combinations in a larger 325-million-word corpus and on the Web.
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ This special issue of Computational Linguistics explores ways in which this dream is being explored.
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ The three possible positions are The problem with the first position is that, with all sublanguages removed, the residual core gives an impoverished view of language (quite apart from demarcation issues and the problem of determining what is left).

For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). $$$$$ Using the frequency of one word gives a first approximation.
For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). $$$$$ We can derive a more precise estimate of the number of words available through a search engine by using the counts of function words as predictors of corpus size.

It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ However, many of the resources that developers may wish to use are general-language resources, such as, for English, WordNet, ANLT, XTag, COMLEX, and the BNC.
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ Although the extensive literature on text classification (Manning and Sch¨utze 1999, pages 575–608) is certainly relevant, it most often starts from a given set of categories and cannot readily be applied to the situation in which the categories are not known in advance.
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.

The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ Web texts are produced by a wide variety of authors.
The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ Can they be used?
The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ The authors use the Web again at a final stage to rerank possible translations by verifying which subsequences among the possible translations are most attested.
The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ The Web is immense, free, and available by mouse click.

So we have the Internet: it is immense, free, easily accessible and can be used for all manner of language research (Kilgarriff and Grefenstette, 2003). $$$$$ So that the semantic question may be set aside, the definition of corpus should be broad.
So we have the Internet: it is immense, free, easily accessible and can be used for all manner of language research (Kilgarriff and Grefenstette, 2003). $$$$$ In the 90-million-word written-English component of the BNC, the appears 5,776,487 times, around seven times for every 100 words.

Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). $$$$$ To date, corpus developers have been obliged to make pragmatic decisions about the sorts of text to go into a corpus.
