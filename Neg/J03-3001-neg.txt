Early web-based models used search engines to collect N-gram counts, and thus could not use capitalization, punctuation, and annotations such as part-of-speech (Kilgarriff and Grefenstette, 2003). $$$$$ For linguistics, the object of study itself (in one of its two primary forms, the other being acoustic) is found on computers.

In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 $$$$$ The special issue explores a “homecoming” of Web technologies, with the Web now feeding one of the hands that fostered it.
In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 $$$$$ Can they be used?
In corpus linguistics building such mega corpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998 $$$$$ The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.

The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). $$$$$ This introduction to the issue aims to survey the activities and explore recurring themes.
The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). $$$$$ They also show how this method can be used for automatic acquisition of sense-tagged corpora, from which one could, among other things, produce language models tied to certain senses of words, or for a certain domain.
The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). $$$$$ It opens up a pressing yet almost untouched practical and theoretical issue for computational linguistics and language technology.

In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ Language identification algorithms (Beesley 1988; Grefenstette 1995), now widely used in Web search engines, were developed as NLP technology.
In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ The Web is immense, free, and available by mouse click.
In recent years, the Web has been increasingly used as a source of linguistic data (Kilgarriff and Grefenstette, 2003). $$$$$ It seems plausible that such automatic construction of translation dictionaries can palliate the lack of translation resources for many language pairs.

It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). $$$$$ First, “representativeness” begs the question “representative of what?” Outside very narrow, specialized domains, we do not know with any precision what existing corpora might be representative of.
It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). $$$$$ This special issue of Computational Linguistics explores ways in which this dream is being explored.
It is now easy to gather machine-readable sentences in various domains because of the ease of publication and access via the Web (Kilgarriff and Grefenstette, 2003). $$$$$ The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.

 $$$$$ The one-million-word Brown corpus opened the chapter on computer-based language study in the early 1960s.
 $$$$$ They also show how this method can be used for automatic acquisition of sense-tagged corpora, from which one could, among other things, produce language models tied to certain senses of words, or for a certain domain.
 $$$$$ They generate pairs of common words, constructing combinations that are and are not attested in the BNC.

The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ In this volume, Celina Santamar´ia, Julio Gonzalo, and Felisa Verdejo describe how to build sense-tagged corpora from the Web by associating word meanings with Web page directory nodes.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ It opens up a pressing yet almost untouched practical and theoretical issue for computational linguistics and language technology.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ It opens up a pressing yet almost untouched practical and theoretical issue for computational linguistics and language technology.
The web as a corpus has been successfully used for many areas in NLP (Kilgarriff and Grefenstette 2003) such as WSD (Mihalcea and Moldovan 1999), obtaining frequencies for bigrams (Keller and Lapata 2003) and noun compound bracketing (Nakov and Hearst 2005). $$$$$ Many of the collections of texts that people use and refer to as their corpus, in a given linguistic, literary, or language-technology study, do not fit.

For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ We then introduce the articles in the special issue and conclude with some thoughts on how the Web could be put at the linguist’s disposal rather more usefully than current search engines allow.
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ Exceptions focusing on genre include Kessler, Nunberg, and Sch¨utze (1997) and Karlgren and Cutting (1994).
For Hungarian, the highest quality (4 % threshold) stratum of the corpus contains 1.22m unique pages for a total of 699m tokens, already exceeding the 500m predicted in (Kilgarriff and Grefenstette, 2003). $$$$$ Rada Mihalcea and Dan Moldovan (1999) used hit counts for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine.

For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). $$$$$ We have measured the counts of some English phrases according to various search engines over time and compared them with counts in the BNC, which we know has 100 million words.
For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). $$$$$ Unseen words, or word sequences—that is, words or sequences not occurring in training data—are a problem for language models.
For collecting bilingual text data for the two sets S1, S2, the Web is an ideal source as it is large, free and available (Kilgarriff and Grefenstette, 2003). $$$$$ The Web, teeming as it is with language data, of all manner of varieties and languages, in vast quantity and freely available, is a fabulous linguists’ playground.

It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ Both original phrases and translations are chunked.
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ The Web is immense, free, and available by mouse click.
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ Philip Resnik was the first to recognize that it is possible to build large parallel bilingual corpora from the Web.
It is natural to question the appropriateness of web data for research purposes, because web data is inevitably noisy and search engines themselves can introduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003). $$$$$ Where researchers use established corpora, such as Brown, the BNC, or the Penn Treebank, researchers and readers are willing to accept the corpus name as a label for the type of text occurring in it without asking critical questions.

The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ This special issue of Computational Linguistics explores ways in which this dream is being explored.
The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ They find that Web frequency counts are consistent with those for other large corpora.
The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ They generate pairs of common words, constructing combinations that are and are not attested in the BNC.
The second corpus is a subset of the German part of the Wacky project (Kilgarriff and Grefenstette, 2003). $$$$$ It was only with the highly successful 1993 special issue of this journal, “Using Large Corpora” (Church and Mercer 1993), that the relation between computational linguistics and corpora was consummated.

So we have the Internet $$$$$ They generate pairs of common words, constructing combinations that are and are not attested in the BNC.
So we have the Internet $$$$$ The phrase work group is 15 times more frequent than any other and is also the best translation among the tested possibilities.
So we have the Internet $$$$$ The Web is clearly a multilingual corpus.

Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). $$$$$ In such cases, having more training data is normally more useful than any concerns of balance, and one should simply use all the text that is available.
Many NLP tasks have successfully utilized very large corpora, most of which were acquired from the Web (Kilgarriff and Grefenstette, 2003). $$$$$ We define a corpus simply as “a collection of texts.” If that seems too broad, the one qualification we allow relates to the domains and contexts in which the word is used rather than its denotation: A corpus is a collection of texts when considered as an object of language or literary study.
