For more information on the TE system described in this section, please see (Hickl et al, 2006b) and (Harabagiu and Hickl, 2006). $$$$$ Finally, Section 5 provides a discussion of our findings, and Section 6 summarizes our conclusions.
For more information on the TE system described in this section, please see (Hickl et al, 2006b) and (Harabagiu and Hickl, 2006). $$$$$ Much recent work on automatic paraphrasing (Barzilay and Lee, 2003) has used relatively simple statistical techniques to identify text passages that contain the same information from parallel corpora.

Following (Harabagiu and Hickl, 2006), we used TE information in order to filter answers identified by the Q/A system that were not entailed by the user's original question. $$$$$ We believe that these results suggest that current supervised machine learning approaches to the recognition of textual entailment may provide open-domain Q/A systems with the inferential information needed to develop viable answer validation systems.

While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. $$$$$ After an initial relevance ranking was determined from the PR engine, the top 50 passages were paired with the original question and were submitted to the TE system.
While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. $$$$$ Table 4 presents results from TE’s linearand Maximum Entropy-based Alignment Classifiers on a sample of 1000 alignment pairs selected at random from the 2006 PASCAL Test Set.
While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. $$$$$ Section 2 describes the three methods of using textual entailment in open-domain question answering that we have identified, while Section 3 presents the textual entailment system we have used.
While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al, 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. $$$$$ (Harabagiu et al., 2001; Moldovan et al., 2002)); (2) a combination of language processes that transform questions and candidate answers in logic representations such that reasoning systems can select the correct answer based on their proofs (cf.

In order to improve QA systems' performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. $$$$$ Each pair of chunks (p E Ct x Ch) is then submitted to a Maximum Entropy-based classifier which determines whether or not the pair of chunks represents a case of lexical entailment.
In order to improve QA systems' performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. $$$$$ The experiments reported in this paper suggest that current TE systems may be able to provide open-domain Q/A systems with the forms of semantic inference needed to perform accurate answer validation.
In order to improve QA systems' performance many research focus on different structures such as question processing (Huang et al., 2008), information retrieval (Clarke et al., 2006), information extraction (Saggion and Gaizauskas, 2006), textual entailment (TE) (Harabagiu and Hickl, 2006) for ranking, answer extraction, etc. $$$$$ Once a ranking was established, answers that were not judged to be entailed by the question were also removed from final ranking.

Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). $$$$$ This module assumes that since sets of entailing texts necessarily predicate about the same set of individuals or events, systems should be able to identify elements from each text that convey similar types of presuppositions.
Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). $$$$$ In contrast, when entailment information was used to rank passages and candidate answers, performance increased by 22% and 10% respectively.
Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). $$$$$ Results from this hybrid method are provided in Table 9.
Implementation of different TE models has previously shown to improve the QA task using supervised learning methods (Harabagiu and Hickl, 2006). $$$$$ In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.

Instead of matching headline and first sentence of the document as in (Harabagiu and Hickl, 2006), we followed a different approach. $$$$$ We have shown that when textual entailment information is used to either filter or rank candidate answers returned by a Q/A system, Q/A accuracy can be improved from 32% to 52% (when an answer type can be detected) and from 30% to 40% (when no answer type can be detected).
Instead of matching headline and first sentence of the document as in (Harabagiu and Hickl, 2006), we followed a different approach. $$$$$ Four types of sentence pairs were evaluated in the 2006 RTE Challenge, including: pairs derived from the output of In previous work (Hickl et al., 2006), we have found that the type and amount of training data available to our TE system significantly (p < 0.05) impacted its performance on the 2006 RTE Test Set.
Instead of matching headline and first sentence of the document as in (Harabagiu and Hickl, 2006), we followed a different approach. $$$$$ Work on the semantics of questions (Groenendijk, 1999; Lewis, 1988) has argued that the formal answerhood relation found between a question and a set of (correct) answers can be cast in terms of logical entailment.

In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. $$$$$ In this paper, we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems.
In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. $$$$$ In the first experiment, the ranked lists of answers produced by the Q/A system were submitted to the TE system for validation.
In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. $$$$$ Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.
In cases where simple question formulation is not satisfactory, many advanced QA systems implement more sophisticated syntactic, semantic and contextual processing such as named-entity recognition (Molla et al, 2006), coreference resolution (Vicedo and Ferrandez, 2000), logical inferences (abduction or entailment) (Harabagiu and Hickl, 2006) translation (Ma and McKeowon, 2009), etc., to improve answer ranking. $$$$$ Question-answer pairs deemed to be positive instances of entailment were then stored in a database and used as additional training data for the AutoQUAB module.

Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al, 2009), when used for filtering and ranking answers. $$$$$ (Prager et al., 2004)).
Recent work on textual entailment has shown improvements on QA results (Harabagiu and Hickl, 2006), (Celikyilmaz et al, 2009), when used for filtering and ranking answers. $$$$$ Table 4 presents results from TE’s linearand Maximum Entropy-based Alignment Classifiers on a sample of 1000 alignment pairs selected at random from the 2006 PASCAL Test Set.

For the task of Question Answering, (Harabagiu and Hickl, 2006) applied a TE component to rerank candidate answers returned by a retrieval step. $$$$$ Results for both of these conditions are presented in Table 9.
For the task of Question Answering, (Harabagiu and Hickl, 2006) applied a TE component to rerank candidate answers returned by a retrieval step. $$$$$ After training our TE system on this corpus, we performed the following four experiments: Method 1.
For the task of Question Answering, (Harabagiu and Hickl, 2006) applied a TE component to rerank candidate answers returned by a retrieval step. $$$$$ Our work suggests that considerable gains in performance can be obtained by incorporating TE during both answer processing and passage retrieval.

Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al, 2009), (Mirkin et al, 2009). $$$$$ Once ranking is complete, answer extraction takes place only on the set of entailed passages that the system considers likely to contain a correct answer to the user’s question.
Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al, 2009), (Mirkin et al, 2009). $$$$$ We also extracted 21,000 pairs of sentences linked by connectives such as even though, in contrast and but.
Techniques developed for RTE have now been successfully applied in the domains of Question Answering (Harabagiu and Hickl, 2006) and Machine Translation (Pado et al, 2009), (Mirkin et al, 2009). $$$$$ We have shown that when textual entailment information is used to either filter or rank candidate answers returned by a Q/A system, Q/A accuracy can be improved from 32% to 52% (when an answer type can be detected) and from 30% to 40% (when no answer type can be detected).

This includes finding question answer pairs (Cong et al, 2008) from online forums, auto-answering queries on a technical forum (Feng et al, 2006), ranking answers (Harabagiu and Hickl, 2006) etc. $$$$$ In previous work (Harabagiu et al., 2005b), we have described techniques that can be used to automatically generate well-formed natural language questions from the text of paragraphs retrieved by a PR module.
This includes finding question answer pairs (Cong et al, 2008) from online forums, auto-answering queries on a technical forum (Feng et al, 2006), ranking answers (Harabagiu and Hickl, 2006) etc. $$$$$ Examples of predicates and arguments aligned by this module are presented in Figure 3.
This includes finding question answer pairs (Cong et al, 2008) from online forums, auto-answering queries on a technical forum (Feng et al, 2006), ranking answers (Harabagiu and Hickl, 2006) etc. $$$$$ As described in (Hickl et al., 2006), the Preprocessing module is used to syntactically parse texts, identify the semantic dependencies of predicates, label named entities, normalize temporal and spatial expressions, resolve instances of coreference, and annotate predicates with polarity, tense, and modality information.


The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al, 2006), information extraction (Romano et al, 2006), and document summarization (Lloret et al, 2008). $$$$$ In the third experiment, TE was used to select AGQs that were entailed by the question submitted to the Q/A system.
The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al, 2006), information extraction (Romano et al, 2006), and document summarization (Lloret et al, 2008). $$$$$ Finally, the Classification Module employs a decision tree classifier in order to determine whether an entailment relationship exists for each pair of texts.
The great potential of integrating (monolingual) TE recognition components into NLP architectures has been reported in several works, such as question answering (Harabagiu and Hickl, 2006), information retrieval (Clinchant et al, 2006), information extraction (Romano et al, 2006), and document summarization (Lloret et al, 2008). $$$$$ Processing textual entailment, or recognizing whether the information expressed in a text can be inferred from the information expressed in another text, can be performed in four ways.

TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006). $$$$$ In our experiments, we show that when textual entailment information is used to either filter or rank answers returned by a Q/A system, accuracy can be increased by as much as 20% overall.
TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006). $$$$$ Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.
TE has been successfully applied to a variety of natural language processing applications, including information extraction (Romano et al, 2006) and question answering (Harabagiu and Hickl, 2006). $$$$$ In this paper, we demonstrate how computational systems to recognize entailment can be used to enhance the accuracy of current open-domain automatic question answering (Q/A) systems.

Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). $$$$$ In our system, the two highest-confidence entity alignments returned by the Lexical Alignment module were used to construct a query which was used to retrieve the top 500 documents from Google, as well as all matching instances from our training corpora described in Section 3.3.
Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). $$$$$ In our system, the two highest-confidence entity alignments returned by the Lexical Alignment module were used to construct a query which was used to retrieve the top 500 documents from Google, as well as all matching instances from our training corpora described in Section 3.3.
Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). $$$$$ Q2: “How hot does the inside of an active volcano get?'
Knowledge about entailment is beneficial for NLP tasks such as Question Answering (Harabagiu and Hickl, 2006). $$$$$ In this paper, we discussed three different ways that a state-of-the-art textual entailment system could be used to enhance the performance of an open-domain Q/A system.

Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ Work on the semantics of questions has argued that the relation between a question and its answer(s) can be cast in terms of logical entailment.
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ Once a ranking was established, answers that were not judged to be entailed by the question were also removed from final ranking.
Algorithms for computing semantic textual similarity (STS) are relevant for a variety of applications, including information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al, 2009). $$$$$ The experiments reported in this paper suggest that current TE systems may be able to provide open-domain Q/A systems with the forms of semantic inference needed to perform accurate answer validation.
