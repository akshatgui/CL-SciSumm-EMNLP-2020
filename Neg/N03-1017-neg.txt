A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003). $$$$$ Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance.
A word link extension algorithm similar to the one presented in this paper is given in (Koehn et al, 2003). $$$$$ Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.

Following phrase-based methods in statistical machine translation (Koehn et al., 2003). $$$$$ With larger beams sizes, only few sentences are translated differently.
Following phrase-based methods in statistical machine translation (Koehn et al., 2003). $$$$$ Note that also constructions such as “with regard to” and “note that” have fairly complex syntactic representations, but often simple one word translations.
Following phrase-based methods in statistical machine translation (Koehn et al., 2003). $$$$$ The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002].
Following phrase-based methods in statistical machine translation (Koehn et al., 2003). $$$$$ But what is the best method to extract phrase translation pairs?

Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases. $$$$$ Surprisingly, limiting the length to a maximum of only three words show that length 3 is enough per phrase already achieves top performance.
Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases. $$$$$ We explore the space between intersection and union with expansion heuristics that start with the intersection and add additional alignment points.
Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases. $$$$$ The restriction to only syntactic phrases (Syn) is harmful.
Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases. $$$$$ This is a challenge for syntactic translation models.

We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system. $$$$$ It is important to know if this is a helpful or harmful restriction.
We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system. $$$$$ Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.
We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al, 2003), and our tree let system. $$$$$ Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models.

Consider the lexical model pw (ry $$$$$ For more information on these models, please refer to Brown et al. [1993].
Consider the lexical model pw (ry $$$$$ In order to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table.
Consider the lexical model pw (ry $$$$$ Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models.
Consider the lexical model pw (ry $$$$$ Lexical weighting ofphrase translation helps.

All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003). $$$$$ We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside [Och et al., 1999].
All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003). $$$$$ Hence, the syntactically motivated phrase pairs learned are a subset of the phrase pairs learned without knowledge of syntax (Section 3.1).
All conditions use word alignments produced by sequential iterations of IBM model 1, HMM, and IBM model 4 in GIZA++ , followed by 'diag-and' symmetrization (Koehn et al., 2003). $$$$$ We carried out experiments to compare the performance of three different methods to build phrase translation probability tables.

We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule "grow-diag-final-and" (Koehn et al., 2003). $$$$$ We recombine search hypotheses as done by Och et al. [2001].
We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule "grow-diag-final-and" (Koehn et al., 2003). $$$$$ Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.
We obtained word alignments of training data by first running GIZA++ (Och and Ney, 2003) and then applying the refinement rule "grow-diag-final-and" (Koehn et al., 2003). $$$$$ We created a framework (translation model and decoder) that enables us to evaluate and compare various phrase translation methods.

Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. $$$$$ Och et al. [1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems' are augmented with phrase translation capability.
Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. $$$$$ We created a framework (translation model and decoder) that enables us to evaluate and compare various phrase translation methods.
Koehn et al (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. $$$$$ When augmenting such models with phrase translations, typically only translation of phrases that span entire syntactic subtrees is possible.

we achieved results similar to Koehn et al (2003a). $$$$$ As a consequence, straight-forward syntax-based mappings do not lead to better translations than unmotivated phrase mappings.
we achieved results similar to Koehn et al (2003a). $$$$$ We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.
we achieved results similar to Koehn et al (2003a). $$$$$ Learning only syntactically motivated phrases degrades the performance of our systems.
we achieved results similar to Koehn et al (2003a). $$$$$ It is important to know if this is a helpful or harmful restriction.

Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003). $$$$$ In order to investigate this question, we created a uniform evaluation framework that enables the comparison of different ways to build a phrase translation table.
Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003). $$$$$ We use Bayes rule to reformulate the translation probability for translating a foreign sentence into English as This allows for a language model and a separate translation model . argmax argmax During decoding, the foreign input sentence is segmented into a sequence of phrases .
Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003). $$$$$ We only add new alignment points that exist in the union of two word alignments.
Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003). $$$$$ We start with an initial empty hypothesis.

We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003). $$$$$ The results suggest that choosing the right alignment heuristic is more important than which model is used to create the initial word alignments.
We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003). $$$$$ The number of translation options is linear with the sentence length.
We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003). $$$$$ All phrases consistent with the word alignment (AP) are used.
We apply STIR as a pre-ordering step in a state of-the-art phrase-based translation system from English to Japanese (Koehn et al, 2003). $$$$$ We created a framework (translation model and decoder) that enables us to evaluate and compare various phrase translation methods.

This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003). $$$$$ Lexical weighting ofphrase translation helps.
This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003). $$$$$ Our results show that phrase translation gives better performance than traditional word-based methods.
This paper proposes a method for building a bilingual lexicon through a pivot language by using phrase-based statistical machine translation (SMT) (Koehn et al, 2003). $$$$$ Using different expansion heuristics during symmetrizing the word alignments has a bigger effect.

This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration. $$$$$ Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.
This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration. $$$$$ However, what constitutes the best heuristic differs from language pair to language pair and varies with the size of the training corpus.
This system is based on phrase-based statistical machine transliteration (SMT) (Finch and Sumita, 2008), an approach initially developed for machine translation (Koehn et al, 2003), where the SMT system's log-linear model is augmented with a set of features specifically suited to the task of transliteration. $$$$$ As a consequence, straight-forward syntax-based mappings do not lead to better translations than unmotivated phrase mappings.

In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012). $$$$$ To use this model in the context of our framework, we simply marginalize to conditional probabilities the joint probabilities estimated by Marcu and Wong [2002].
In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012). $$$$$ Even penalizing the use of syntactic phrase pairs does not harm performance significantly.
In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012). $$$$$ With larger beams sizes, only few sentences are translated differently.
In addition to the most popular techniques such as Phrase-Based Machine Transliteration (Koehnet al, 2003), CRF, re-ranking, DirecTL-pde coder, Non-Parametric Bayesian Co-segmentation (Finch et al, 2011), and Multi-to-Multi Joint Source Channel Model (Chen et al, 2011) in the News 2011, we are delighted to see that several new techniques have been proposed and explored with promising results reported, including RNN-based LM (Finch et al, 2012), English Segmentation algorithm (Zhang et al, 2012), JLIS reranking method (Wu et al, 2012) ,improved m2m-aligner (Okuno, 2012), multiple reference optimized CRF (Ammar et al, 2012), language dependent adaptation (Kondrak et al, 2012) and two-stage CRF (Kuo et al, 2012). $$$$$ We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.

 $$$$$ The foreign words are marked as translated and the probability cost of the hypothesis is updated.
 $$$$$ Performance differs widely depending on the methods used to build the phrase translation table.
 $$$$$ It matters how phrases are extracted.
 $$$$$ We also included in the figure the performance of an IBM Model 4 wordbased translation system (M4), which uses a greedy decoder [Germann et al., 2001].

Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003). $$$$$ More sophisticated approaches that make use of syntax do not lead to better performance.
Many research groups use a decoder based on a log-linear approach incorporating phrases as main paradigm (Koehn et al, 2003). $$$$$ The extraction heuristic is similar to the one used in the alignment template work by Och et al. [1999].

Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003). $$$$$ For this, we need a lexical translation probability distribution .
Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003). $$$$$ As a consequence, straight-forward syntax-based mappings do not lead to better translations than unmotivated phrase mappings.
Traditionally, these models are run in both directions and combined using heuristics to create many-to-many alignments (Koehn et al, 2003). $$$$$ In these models, reordering of words is restricted to reordering of constituents in well-formed syntactic parse trees.

The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010). $$$$$ During translation, future costs for uncovered foreign words can be quickly computed by consulting this table.
The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010). $$$$$ Usually, this factor is larger than 1, biasing longer output.
The translation quality of statistical phrase-based systems (Koehn et al, 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al, 2010). $$$$$ Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.

As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002) $$$$$ Our results show that phrase translation gives better performance than traditional word-based methods.
As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002) $$$$$ Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance.
As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002) $$$$$ We start with an initial empty hypothesis.
As for the SMT system, we use a standard log-linear PB-SMT model (Och and Ney, 2002) $$$$$ We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.

 $$$$$ It is important to know if this is a helpful or harmful restriction.
 $$$$$ As a consequence, straight-forward syntax-based mappings do not lead to better translations than unmotivated phrase mappings.
 $$$$$ But what is the best method to extract phrase translation pairs?
 $$$$$ For each stack, we only keep a beam of the best hypotheses.
