Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). $$$$$ The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate.
Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). $$$$$ Note that we are using concept to refer to a lexicalized concept or sense and not to a set of senses; we use class to refer to a set of senses.
Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). $$$$$ One response to this problem is to apply some kind of thresholding and either ignore counts below the threshold, or apply the test only to tables that do not contain low counts.

Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses.
Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ Estimates of p(v  |ci, r), for each child ci of c', can be compared to see whether p(v  |c', r) has significantly changed.
Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ One possibility would be to use all the classes dominated by the hypernyms of a concept, rather than just one, to estimate the probability of the concept.

Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. $$$$$ The approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the α parameter, and ultimately does not improve on the best performance.
Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. $$$$$ Note that the α values corresponding to the lowest scores lead to a significant amount of generalization, which provides additional evidence that MDL and Assoc are overgeneralizing for this task.
Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. $$$$$ In this section we show how to test whether p(v  |c', r) changes significantly when considering a node higher in the hierarchy.

Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ We would like to thank Diana McCarthy for suggesting the pseudo-disambiguation task and providing the MDL software, John Carroll for supplying the data, and Ted Briscoe, Geoff Sampson, Gerald Gazdar, Bill Keller, Ted Pedersen, and the anonymous reviewers for their helpful comments.
Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ Li and Abe refer to such a partition as a “cut” and the cut together with the probabilities as a “tree cut model.” The probabilities of the classes in a cut, P, satisfy the following constraint: (urban area), (geographical area), (region), (location), (object), (entity).
Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.

Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ MDL looks ideally suited to the task of model selection, since it is designed to deal with precisely this trade-off.
Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ Reading across a row shows how the generalization varies with sample size, and reading down a column shows how it varies with α.
Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ Abney and Light (1999) have tried a more motivated approach, using the expectation maximization algorithm, but with little success.

Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ We would also like to thank Ted Briscoe for presenting an earlier version of this article on our behalf at NAACL 2001.
Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.
Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ We would also like to thank Ted Briscoe for presenting an earlier version of this article on our behalf at NAACL 2001.
Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ We would also like to thank Ted Briscoe for presenting an earlier version of this article on our behalf at NAACL 2001.

Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ The data were again extracted from a subset of the BNC using the system of Briscoe and Carroll (1997), and the G2 statistic was used in the chi-square test.
Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ (By a conservative test we mean one in which the null hypothesis is not easily rejected.)
Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ There are around 66,000 different concepts in the noun hierarchy of WordNet version 1.6.
Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ For now, we continue with the discussion of how the chi-square test is used in the generalization procedure.

Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ (The fourth column gives a standard deviation figure.)
Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ The results using these test and training data are given in Table 6.
Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ One possible approach would be simply to substitute c' for the individual concept c. This is a poor solution, however, since p(c'  |v, r) is the conditional probability that some noun denoting a concept in c' appears in position r of verb v. For example, p((animal)  |run,subj) is the probability that some noun denoting a kind of animal appears in the subject position of the verb run.
Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ The probability p(c  |v, r) is to be interpreted as follows: This is the probability that some noun n in syn(c), when denoting concept c, appears in position r of verb v (given v and r).

There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. $$$$$ Each method uses a different generalization procedure; the first alternative uses the minimum description length principle, and the second uses Resnik’s measure of selectional preference.
There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. $$$$$ An estimate would be obtained for each hypernym, and the estimates combined in a linear interpolation.

Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ The number of generalized levels for a concept c (relative to a verb v and argument position r) is the difference in depth between c and top(c, v, r), as explained in Section 5.
Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ Thus, assuming this choice of class, p((dog)  |run,subj) would be approximated as follows: The following derivation shows that if p(v  |c'i, r) = k for each child c'i of c', and p(v  |c', r) = k, then p(v  |c', r) is also equal to k: Note that the proof applies only to a tree, since the proof assumes that c' is partitioned by c' and the sets of concepts dominated by each of the daughters of c', which is not necessarily true for a directed acyclic graph (DAG).
Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.

Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ In particular, we are not addressing the problem of acquiring selectional restrictions in the way this is usually construed (Resnik 1993; Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000).
Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.
Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ A related issue is how the structure of WordNet affects the accuracy of the probability estimates.

In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). $$$$$ A second column presents estimates of counts arising from concepts in ci appearing in the subject position of a verb other than run.
In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003): Resnik (1993) and Clark and Weir (2002). $$$$$ MDL looks ideally suited to the task of model selection, since it is designed to deal with precisely this trade-off.

Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ One solution is to treat α as a parameter and set it empirically by taking a held-out test set and choosing the value of α that maximizes performance on the relevant task.
Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ One of the features of the generalization procedure is the way that α, the level of significance in the chi-square test, is treated as a parameter.
Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ (We refer to c' as the “similarity class” of c with respect to v and r and the hypernym c' as top(c,v,r), since the chosen hypernym sits at the “top” of the similarity class.)

Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ The work on which it is based was carried out while the first author was a D.Phil. student at the University of Sussex and was supported by an EPSRC studentship.
Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ This did create a problem with overgeneralization: Many of the cuts returned by MDL were overgeneralizing at the (entity) node.
Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ This allows some control over the extent of generalization, which can be tailored to particular tasks.
Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.

We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic.
We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ For the purposes of this work we add a common root dominating the nine subhierarchies, which we denote (root).

Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ Here, top((soup), stir, obj) is being determined.
Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ The main problem with this test is that it is computationally expensive, especially for large contingency tables.
Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ We would like to thank Diana McCarthy for suggesting the pseudo-disambiguation task and providing the MDL software, John Carroll for supplying the data, and Ted Briscoe, Geoff Sampson, Gerald Gazdar, Bill Keller, Ted Pedersen, and the anonymous reviewers for their helpful comments.

It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ The work on which it is based was carried out while the first author was a D.Phil. student at the University of Sussex and was supported by an EPSRC studentship.
It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ This procedure was also used for the MDL alternative, but using the MDL method to estimate the probabilities.
It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ The test and training data were obtained as follows.
It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ In this article, a very simple approach is taken, which is to split the count for a noun evenly among the noun’s senses.

We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ This method has been shown to provide superior performance on a pseudo-disambiguation task, compared with two alternative approaches.
We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ The assumption underlying this approach is that the probability of a particular noun sense can be approximated by a probability based on a suitably chosen class.
We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ One column contains estimates of counts arising from concepts in ci appearing in the subject position of the verb run: fˆ (ci, run, subj).
We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ If the test indicates that the probabilities are sufficiently unlikely to be the same, then the null hypothesis is rejected, and the conclusion is that p(run  |(canine), subj) is not a good approximation of p(run  |(dog),subj).

In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. $$$$$ The procedure for finding a suitable class, c', to generalize concept c in position r of verb v works as follows.
In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. $$$$$ Since the class in the cut containing (pizza) is (food), the probability p((pizza)  |eat,obj) would be estimated as p((food)  |eat, obj)/|(food)|.
In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. $$$$$ One possibility would be to use all the classes dominated by the hypernyms of a concept, rather than just one, to estimate the probability of the concept.

SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ Note that, in practice, no distinction is made between the different senses of a verb (although the techniques do allow such a distinction) and that each use of a noun is assumed to correspond to exactly one concept.4
SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ These have been chosen because they directly address the question of how to find a suitable level of generalization in WordNet.
SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ Using our approach, the disambiguation decision for each (v, n, v') triple was made according to the following procedure: if max psc(c I v, obj) > max psc(c I v', obj) If n has more than one sense, the sense is chosen that maximizes the relevant probability estimate; this explains the maximization over cn(n).
SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ A procedure is developed that uses a chi-square test to determine a suitable level of generalization.
