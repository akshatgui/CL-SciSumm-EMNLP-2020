Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). $$$$$ Resnik’s solution to this problem (which he neatly refers to as the “vertical-ambiguity” problem) is to choose the class that maximizes the association score.
Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). $$$$$ The results also show that the extent of generalization increases with a decrease in sample size.
Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). $$$$$ Thus the generalization procedure can be thought of as one that finds “homogeneous” areas of the hierarchy, that is, areas consisting of classes that are associated to a similar degree with the verb (Clark and Weir 1999).

Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ The work on which it is based was carried out while the first author was a D.Phil. student at the University of Sussex and was supported by an EPSRC studentship.
Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ The preferences and class-based probabilities are then used to estimate probabilities of the form p(n  |v, r), where n is a noun, v is a verb, and r is an argument slot.
Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ But even with this variation, similarity class is still outperforming MDL and Assoc across the whole range of α values.
Clark and Weir (2002) investigate the task of generalizing a single relation concept pair. $$$$$ Let syn(c) be the synset for concept c, and let cn(n) = { c  |n E syn(c) } be the set of concepts that can be denoted by noun n. The hierarchy has the structure of a directed acyclic graph (although only around 1% of the nodes have more than one parent), where the edges of the graph constitute what we call the “direct–isa” relation.

Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. $$$$$ For example, it seems reasonable to suppose that the probability of (the food sense of) chicken appearing as an object of the verb eat can be approximated in some way by a probability based on a class such as FOOD.
Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. $$$$$ The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000).
Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent. $$$$$ To see why, consider the problem of deciding how well the concept (location) satisfies the preferences of the verb eat for its object.

Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ An estimate would be obtained for each hypernym, and the estimates combined in a linear interpolation.
Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ One possible approach would be simply to substitute c' for the individual concept c. This is a poor solution, however, since p(c'  |v, r) is the conditional probability that some noun denoting a concept in c' appears in position r of verb v. For example, p((animal)  |run,subj) is the probability that some noun denoting a kind of animal appears in the subject position of the verb run.
Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ Using the association score for each test triple, the decision was made according to the following procedure: then choose (v', n) else choose at random We use h(c) to denote the set consisting of the hypernyms of c. The inner maximization is over h(c), assuming c is the chosen sense of n, which corresponds to Resnik’s method of choosing a set to represent c. The outer maximization is over the senses of n, cn(n), which determines the sense of n by choosing the sense that maximizes the association score.
Other models also relying on the WordNet resource include Abe and Li (1996) and Clark and Weir (2002). $$$$$ In particular, we are not addressing the problem of acquiring selectional restrictions in the way this is usually construed (Resnik 1993; Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000).

Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic.
Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ There are a number of ways in which this work could be extended.
Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic.
Calling the generalization problem a case of engineering in the face of sparse data, Clark and Weir (2002) looked at a number of previous methods, one conclusion being that the approach of Li and Abe appears to over-generalize. $$$$$ The set top(c, v, r) is the similarity class of c for verb v and position r. Figure 1 gives an algorithm for determining top(c,v,r).

Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.
Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ There are two elements involved in the problem of using a class to estimate the probability of a noun sense.
Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or class based approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.

Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ The next section describes the noun hierarchy from WordNet and gives a more precise description of the probabilities to be estimated.
Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ We also feel that the pseudo-disambiguation task is useful for evaluating the different estimation methods, since it directly addresses the question of how likely a particular predicate is to take a given noun as an argument.
Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ For example, this approach does not always generalize appropriately for arguments that are negatively associated with some verb.
Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). $$$$$ This article concerns the estimation of a particular kind of probability, namely, the probability of a noun sense appearing as a particular argument of a predicate.

Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ We would like to thank Diana McCarthy for suggesting the pseudo-disambiguation task and providing the MDL software, John Carroll for supplying the data, and Ted Briscoe, Geoff Sampson, Gerald Gazdar, Bill Keller, Ted Pedersen, and the anonymous reviewers for their helpful comments.
Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ Li and Abe’s application of MDL requires the hierarchy to be in the form of a thesaurus, in which each leaf node represents a noun and internal nodes represent the class of nouns that the node dominates.
Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ In addition, the performance of our method is investigated using both the standard Pearson chisquare statistic and the log-likelihood chi-square statistic.
Methods for the induction of semantically inspired word clusters have been widely used in language modeling and lexical acquisition tasks (e.g. (Clark and Weir, 2002. $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.

There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. $$$$$ The 6, 000 triples were taken from the first experimental test set described in Section 7, and the training data from this experiment were used to provide the counts.
There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. $$$$$ Initially, top is set to (soup), and the probabilities corresponding to the children of (dish) are compared: p(stir  |(soup),obj), p(stir  |(lasagne),obj), p(stir | (haggis),obj), and so on for the rest of the children.
There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
There is scope for experimenting with other approaches such as (Clark and Weir, 2002), however, we feel a type-based approach is worthwhile to avoid the noise introduced from frequent but polysemous arguments and bias from highly frequent arguments which might be part of a multiword rather than a prototypical argument of the predicate in question, for example eat hat. $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.

Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ The usual statistic encountered in textbooks is the Pearson chi-square statistic, denoted X2: where oij is the observed value for the cell in row i and column j, and eij is the corresponding expected value.
Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ The random choice was made according to the verb’s frequency in the original data set, subject to the condition that the pair (v', n) did not occur in the training data.
Further comparison of WNPROTOs and DSPROTOs to other WordNet models are warranted to contrast the effect of our proposal for disambiguation using word types with iterative approaches, particularly those of Clark and Weir (2002). $$$$$ An analysis of the results has shown that the other approaches appear to be overgeneralizing, at least for this task.

Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ Let isa be the transitive, reflexive closure of direct–isa; then c' isa c implies c' is a kind of c. If c' isa c, then c is a hypernym of c' and c' is a hyponym of c. In fact, the hierarchy is not a single hierarchy but instead consists of nine separate subhierarchies, each headed by the most general kind of concept, such as (entity), (abstraction), (event), and (psychological feature).
Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ One of the features of the generalization procedure is the way that α, the level of significance in the chi-square test, is treated as a parameter.
Clark and Weir (2002) present a model that, while not explicitly described as cut-based, likewise seeks to find the right level of generalisation for an observation. $$$$$ This did create a problem with overgeneralization: Many of the cuts returned by MDL were overgeneralizing at the (entity) node.

In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003) $$$$$ The 6, 000 triples were taken from the first experimental test set described in Section 7, and the training data from this experiment were used to provide the counts.
In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003) $$$$$ The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate.
In order to compare against previously proposed selectional preference approaches based on WordNet we also reimplemented the methods that performed best in the evaluation of Brockmann and Lapata (2003) $$$$$ Li and Abe use MDL to select a set of classes from a hierarchy, together with their associated probabilities, to represent the selectional preferences of a particular verb.

Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ This is to be expected, since, given a contingency table chosen at random, a higher value of α is more likely to lead to a significant result than a lower value of α.
Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ An approach similar to this is taken by Bikel (2000), in the context of statistical parsing.
Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ To make this distinction clear, we use c = { c'  |c' isa c } to denote the set of concepts dominated by concept c, including c itself.
Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001). $$$$$ This article concerns the problem of how to estimate the probabilities of noun senses appearing as particular arguments of predicates.

Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ We have presented a class-based estimation method that incorporates a procedure for finding a suitable level of generalization in WordNet.
Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ A procedure is developed that uses a chi-square test to determine a suitable level of generalization.
Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ However, Agresti (1996, page 34) makes the opposite claim: “The sampling distributions of X2 and G2 get closer to chi-squared as the sample size n increases....
Pseudo-disambiguation was introduced by Clark and Weir (2002) to evaluate models of selectional preference. $$$$$ The task we used to compare the class-based estimation techniques is a decision task previously used by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999).

We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ We would also like to thank Ted Briscoe for presenting an earlier version of this article on our behalf at NAACL 2001.
We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ For X2, the null hypothesis is rejected for α values greater than 0.005.
We follow the approach by Clark and Weir (2002) to create the test data. $$$$$ In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses.

Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ This method has been shown to provide superior performance on a pseudo-disambiguation task, compared with two alternative approaches.
Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ For example, (animal) is the set consisting of those concepts corresponding to kinds of animals (including (animal) itself).
Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ In related work on acquiring selectional preferences (Ribas 1995; McCarthy An algorithm for determining top(c, v, r).
Briefly, Clark and Weir (2002) populate the WordNet hierarchy based on corpus frequencies (of all nouns for a verb/slot pair), and then determine the appropriate probability estimate at each node in the hierarchy by using chi square to determine whether to generalize an estimate to a parent node in the hierarchy. $$$$$ The low-class method scores highly for this data set also, but given that the task is one that apparently favors a low level of generalization, the high score is not too surprising.

It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ Note that the α values corresponding to the lowest scores lead to a significant amount of generalization, which provides additional evidence that MDL and Assoc are overgeneralizing for this task.
It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ ((canine) is the parent of (dog) in WordNet.)
It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ There is a particular focus on the problem of how to determine a suitable class for a given sense, or, alternatively, how to determine a suitable level of generalization in the hierarchy.
It is worth noting that the method of Clark and Weir (2002) does not yield a tree cut, but instead generally populates the WordNet hierarchy with non-zero probabilities. $$$$$ In cases where a concept has more than one parent, the parent is chosen that results in the lowest value of the chi-square statistic, as this indicates the probabilities are the most similar.

We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well.
We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001.
We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ (The particular choice of classes for the cut in this example is not too important; the example is designed to show how probabilities of senses are estimated from class probabilities.)
We evaluate the SPD method on sense profiles created using the method of Clark and Weir (2002), with comparison to the other distance measures (skew and cos) as explained above. $$$$$ In cases where a concept has more than one parent, the parent is chosen that results in the lowest value of the chi-square statistic, as this indicates the probabilities are the most similar.

In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. $$$$$ Our method outperforms these alternatives on the pseudo-disambiguation task, and an analysis of the results shows that the generalization methods of Resnik and Li and Abe appear to be overgeneralizing, at least for this task.
In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. $$$$$ (We refer to c' as the “similarity class” of c with respect to v and r and the hypernym c' as top(c,v,r), since the chosen hypernym sits at the “top” of the similarity class.)
In an approach inspired by the works of Li and Abe (1998) and Clark and Weir (2002), McCarthy and Carroll use grammatically connected words from a corpus to induce a distribution of senses over subtrees in the WordNet hierarchy. $$$$$ Continuing with the eat (location) example, our generalization procedure is unlikely to get as high as (entity) (assuming a reasonable number of examples of eat in the training data), since the probabilities corresponding to the daughters of (entity) are likely to be very different with respect to the object position of eat.

SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ A note of clarification is required before presenting the results.
SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ In order to overcome the accompanying sparse-data problem, the proposal here is to define the probabilities in terms of senses from a semantic hierarchy and exploit the fact that the senses can be grouped into classes consisting of semantically similar senses.
SPs can help resolve syntactic, word sense, and reference ambiguity (Clark and Weir, 2002), and so gathering them has received a lot of attention in the NLP community. $$$$$ We would like to thank Diana McCarthy for suggesting the pseudo-disambiguation task and providing the MDL software, John Carroll for supplying the data, and Ted Briscoe, Geoff Sampson, Gerald Gazdar, Bill Keller, Ted Pedersen, and the anonymous reviewers for their helpful comments.
