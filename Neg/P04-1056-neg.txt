We also provide the results from Bunescu and Mooney (2004) for comparison. $$$$$ One particular feature is e.label which is set to 1 if e is considered a valid extraction, and 0 otherwise.
We also provide the results from Bunescu and Mooney (2004) for comparison. $$$$$ However, in many cases, considering influences between extractions can be very useful.
We also provide the results from Bunescu and Mooney (2004) for comparison. $$$$$ Since IE systems are difficult and time-consuming to construct, most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora (Cardie, 1997; Califf, 1999).

In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. $$$$$ Each extracted protein name in the test data was compared to the human-tagged data, with the positions taken into account.
In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. $$$$$ Like most entity names, almost all proteins in our data are base noun phrases or parts of them.
In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task. $$$$$ The second dataset is Aimed2 which has been previously used for training the protein interaction extraction systems in (Bunescu et al., 2004).

The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. $$$$$ We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions.
The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. $$$$$ It contains a total of 3713 protein references.
The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al, 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes. $$$$$ Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999).

Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. $$$$$ The number of factor graphs for which the sum-product algorithm did not converge was non-negligible, and our approach stopped after a fix number of iterations.
Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. $$$$$ However, in many cases, considering influences potential extractions could improve overall accuracy.
Bunescu and Mooney (2004) define a Relational Markov Network (RMN) which explicitly models long-distance dependencies, and use it to represent relations between entities. $$$$$ Currently, LT-RMN's accuracy is still significantly less than CRF's, which limits the performance of the full system.

The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005). $$$$$ The set 5, may contain the same feature from different entities.
The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al, 2005). $$$$$ To circumvent this complexity, we use Collins' voted perceptron approach (Collins, 2002), which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01.

 $$$$$ The number of factor graphs for which the sum-product algorithm did not converge was non-negligible, and our approach stopped after a fix number of iterations.
 $$$$$ Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999).
 $$$$$ This allows for &quot;collective information extraction&quot; that exploits the mutual influence between possible extractions.
 $$$$$ This allows for &quot;collective information extraction&quot; that exploits the mutual influence between possible extractions.

We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. $$$$$ The RMN approach handles these and potentially other mutual influences between entities in a more complete, probabilistically sound manner.
We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. $$$$$ Most information extraction (IE) systems treat separate potential extractions as independent.
We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al, 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. $$$$$ It can be shown that this gradient is equal with the difference between the empirical counts of fc and their expectation under the current set of parameters w. This expectation is expensive to compute, since it requires summing over all possible configurations of candidate entity labels from a given document.

Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. $$$$$ But a candidate entity may be inside more than one &quot;including&quot; entity, and the number of including entities may vary from one candidate extraction to another.
Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. $$$$$ For example, the word TGF-1 would be mapped to type A-0.
Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. $$$$$ Definition 1: A base noun phrase is a maximal contiguous sequence of tokens whose POS tags are from { &quot;JJ&quot;, &quot;VBN&quot;, &quot;VBG&quot;, &quot;POS&quot;, &quot;NN&quot;, &quot;NNS&quot;, &quot;NNP&quot;, &quot;NNPS&quot;, &quot;CD&quot;, &quot;-&quot;}, and whose last word (the head) is tagged either as a noun, or a number.
Recent work looking to directly model non-local dependencies and do approximate inference are that of Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly model long-distance dependencies, Sutton and McCallum (2004), who introduce skip-chain CRFs, which add additional non-local edges to the underlying CRF sequence model (which Bunescu and Mooney (2004) lack) and Finkel et al (2005) who hand-set penalties for inconsistency in labels based on the training data and then use Gibbs Sampling for doing approximate inference where the goal is to obtain the label sequence that maximizes the product of the CRF objective function and their penalty. $$$$$ We have tested the RMN approach on two datasets that have been hand-tagged for human protein names.

In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. $$$$$ As typically applied, CRF's, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation.
In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. $$$$$ It can be shown that this gradient is equal with the difference between the empirical counts of fc and their expectation under the current set of parameters w. This expectation is expensive to compute, since it requires summing over all possible configurations of candidate entity labels from a given document.
In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. $$$$$ Of these, 147 have been randomly selected by posing a query containing the (Mesh) terms protein binding, interaction, and molecular to Medline, while the rest of 53 have been extracted randomly from the GENIA corpus (Collier et al., 1999).
In the following experiments, we used AImed (Bunescu and Mooney, 2004), which is a popular corpus for the evaluation of PPI extraction systems. The corpus consists of 225 biomedical paper abstracts (1970 sentences), which are sentence-split, tokenized, and annotated with proteins and PPIs.We use gold protein annotations given in the corpus. $$$$$ Soft correlations between repetitions and acronyms and their long form in the same document have been captured by global clique templates, allowing for local extraction decisions to propagate and mutually influence each other.

Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). $$$$$ As typically applied, CRF's, like almost all IE methods, assume separate extractions are independent and treat each potential extraction in isolation.
Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). $$$$$ For example, in our protein-tagging task, repeated references to the same protein are common.
Fortunately, research in machine learning has produced methods for global inference and joint classification that can help to address this deficiency (e.g. Bunescu and Mooney (2004), Roth and Yih (2004)). $$$$$ A clique template is a procedure that finds all subsets of entities satisfying a given constraint, after which, for each entity subset, it connects a selected set of feature nodes so that they form a clique.

In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). $$$$$ CRF's are a restricted class of undirected graphical models (Jordan, 1999) designed for sequence segmentation tasks such as IE, part-of-speech (POS) tagging (Lafferty et al., 2001), and shallow parsing (Sha and Pereira, 2003).
In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). $$$$$ Regarding future work, a richer set of features for the local templates would likely improve performance.
In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). $$$$$ We could specify the potential for the repeat template in a similar 2-by-2 table, this time leaving the table entries to be learned, given that it is not a hard constraint.
In the first approach, heuristic rules are used to find the dependencies (Bunescu and Mooney, 2004) or penalties for label inconsistency are required to handset ad-hoc (Finkel et al, 2005). $$$$$ Regarding future work, a richer set of features for the local templates would likely improve performance.

However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. $$$$$ The full set of features types is listed in Table 1, where we consider a generic elabel φHD=enzyme elabel φPF=A0_a ... φSF=A0_a ... φSF=a Note that the factor graph above has an equivalent RMN graph consisting of a one-node clique only, on which it is hard to visualize the various potentials involved.
However, Table 1 shows that the word distance is long between interacting protein names annotated on the AImed corpus (Bunescu and Mooney, 2004), and we have to treat long-distance relations for information like protein-protein interactions. $$$$$ For example, in our protein-tagging task, repeated references to the same protein are common.

See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. $$$$$ Continuing with the previous example, because 'gintathione S' and 'S - transferase' are two overlapping entities, the factor graph model will contain an overlap potential node connected to the label nodes of these two entities.
See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. $$$$$ Later we discuss situations in which repetitions of the same protein name are not tagged as proteins, and design an approach to handle this.
See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. $$$$$ Many of these features use the concept of word type, which allows a different form of token generalization than POS tags.
See Bunescu and Mooney (2004) and Loeliger (2004) for a detailed introduction to factor graphs. $$$$$ A clique template is a procedure that finds all subsets of entities satisfying a given constraint, after which, for each entity subset, it connects a selected set of feature nodes so that they form a clique.

 $$$$$ All these will be illustrated with examples in Sections 4 and 5 where the clique templates used in our model are described in detail.
 $$$$$ Acronym Template (AT): It is common convention that a protein is first introduced by its long name, immediately followed by its short-form (acronym) in parentheses.

Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). $$$$$ Repeat Template (RT): If multiple entities in the same document are repetitions of the same name, their labels tend to have the same value (i.e. most of them are protein names, or most of them are not protein names).
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). $$$$$ In this document model, labels are the only hidden features, and the inference procedure will try to find a most probable assignment of values to labels, given the current model parameters.
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). $$$$$ To circumvent this complexity, we use Collins' voted perceptron approach (Collins, 2002), which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01.
Skip-chain CRFs and collective inference have been applied to problems in IE, and RMNs to named entity recognition (NER) (Bunescu and Mooney, 2004). $$$$$ Statistical methods on models, such as random fields have been shown to be an effective approach to learning accurate IE systems.

Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. $$$$$ Following a maximum likelihood estimation, we shall use the log-linear representation of potentials: where A is a vector of binary features, one for each configuration of values for X, and K. Let w be the concatenated vector of all potential parameters wc.
Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. $$$$$ To circumvent this complexity, we use Collins' voted perceptron approach (Collins, 2002), which approximates the full expectation of fc with the fc counts for the most likely labeling under the current parameters, w. In all our experiments, the perceptron was run for 50 epochs, with a learning rate set at 0.01.
Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. $$$$$ Like most entity names, almost all proteins in our data are base noun phrases or parts of them.
Supervised approaches (McCallum and Li 2003, Bunescu and Mooney 2004) rely on large sets of labeled examples, perform targeted extraction and employ a variety of sentence and corpus-level features. $$$$$ When this template matches a subset of entities el, e2, ... , en, it will create an auxiliary OR entity e,, with a single feature e„ .1 abel .

Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004). $$$$$ Currently, LT-RMN's accuracy is still significantly less than CRF's, which limits the performance of the full system.
Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004). $$$$$ All these will be illustrated with examples in Sections 4 and 5 where the clique templates used in our model are described in detail.
Examples of classifier-based IE systems are SRV (Freitag, 1998), HMM approaches (Freitag and McCallum, 2000), ALICE (Chieu et al, 2003), and Relational Markov Networks (Bunescu and Mooney, 2004). $$$$$ Many of these features use the concept of word type, which allows a different form of token generalization than POS tags.
