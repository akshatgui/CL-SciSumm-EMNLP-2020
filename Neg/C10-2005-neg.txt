Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ is 1 for thisparticular data.
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ represents that this message is areply to the user ?twUser?; ?#obama?
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it.

 $$$$$ For test data, we manually labeled 1,000 tweets as positive, negative and neu tral.
 $$$$$ There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005).
 $$$$$ These noisy labels were pro vided by a few sentiment detection websites over twitter data.

The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ For instance, Pang and Lee (Pang and Lee, 2004) explores the fact that sentences close in a text might share the same subjectivity to create a better subjectivity detector and, similar to (Pang etal., 2002), uses unigrams as features for the polar ity detection.
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ The main limitation of our approach is the cases of sentences that contain antagonistic sentiments.
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ Most of theseapproaches use the raw word representation (n grams) as features to build a model for sentiment detection and perform this task over large pieces of texts.
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ Detailed explanation about them are as follows: ? ReviewSA: this is the approach proposed by Pang and Lee (Pang and Lee, 2004)for sentiment analysis in regular online reviews.

The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it.
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ Thus, it is not correct to as sume that prior polarity of ?spot?
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ Moreover, we leverage sources of noisy labels as our training data.

In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ In this section, we give some context about Twitter messages and the sources used for our data-driven approach.
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ is positive accord ing to this list.
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it.

We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ The Web sources used in this paper and some other websites provide sentiment detection for tweets.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ To better utilize these sources, we verify the potential value of using and combining them, providing an analysis of the provided labels, examine different strategies of combining these sources in order to obtain the best outcome; and, propose a more robust feature set that captures a more abstract representation of tweets, composedby meta-information associated to words and spe cific characteristics of how tweets are written.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ For the polarity detection task, we built a few classifiers to compare theirperformances: TwitterSA(single) and Twit terSA(weights) are two classifiers we trained using combined data from the 3 sources.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ For instance, for a training size with 2,000 tweets, the error rate for Unigrams was 46% versus 23.8% for our approach.

From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ For both tasks, subjectivity and po larity detection, we compared our approach with previous ones reported in the literature.
From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.
From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.

One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ These noisy labels were provided by a few sentiment detectionwebsites over twitter data.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ Figure 3shows the error rate of both approaches3 in function of the training size.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ We showed in Section 4 that our approach works better than theirs for this problem, obtaining lower error rates.

One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ is a tag provided by the user for this message, so-called hash tag; and ?http://bit.ly/9K4n9p?
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ An other common characteristic of some of them isthe use of n-grams as features to create their mod els.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ Most of these ap proaches try to perform this task on large texts, ase.g., newspaper articles and movie reviews.

The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ The best performance was ob tained by TwitterSA(maxconf), which combines results of the 3 classifiers, respectively trained from each source, by taking the output by themost confident classifier, as the final prediction.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ In this paper, we propose a 2-step sentiment analysis classification method for Twitter, whichfirst classifies messages as subjective and ob jective, and further distinguishes the subjectivetweets as positive or negative.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ We collected data from 3 different websites that provide almost real-time sentiment detection for tweets: Twendz, Twitter Sen timent and TweetFeel.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ Table 1 shows more details aboutthese sources.

Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ Table 1 shows more details aboutthese sources.
Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ We showed in Section 4 that our approach works better than theirs for this problem, obtaining lower error rates.
