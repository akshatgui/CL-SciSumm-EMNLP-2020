Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ These noisy labels were provided by a few sentiment detectionwebsites over twitter data.
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ 4.3 Polarity Detection Evaluation.
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ By using it, we aim to handle better: the problem of lack of information on tweets, helping on thegeneralization process of the classification algo rithms; and the noisy and biased labels provided by those websites.The remainder of this paper is organized as fol lows.
Recent examples of this approach are Barbosa and Feng (2010) and Pak and Paroubek (2010). $$$$$ To collect data, we issued a query containing a common stopword ?of?, as we are interested in collecting generic data, and retrieved tweets from these sites for three weeks,archiving the returned tweets along with their sen timent labels.

 $$$$$ As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it.
 $$$$$ Moreover, we leverage sources of noisy labels as our training data.
 $$$$$ Moreover, we leverage sources of noisy labels as our training data.
 $$$$$ An other common characteristic of some of them isthe use of n-grams as features to create their mod els.

The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ is a link to someexternal source.
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ For instance, labelers might be interested in only providing labels that they are more confident about;?
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ We introduce the features used in the sentiment detection and also provide a deep analysis of the labels generated by those sources in Section 3.
The size of our hand-labeled data allows us to perform cross validation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). $$$$$ Different labeler bias: if labelers make simi lar mistakes, the combination of them might not bring much improvement.

The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ For instance, a sitemay only present the tweets it has more confi dence about their sentiment.
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ There is a rich literature in the area of sentiment detection (see e.g., (Pang et al, 2002; Pang and Lee, 2004; Wiebe and Riloff, 2005; Go et al,2009; Glance et al, 2005).
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ In our experi ments, we show that since our features areable to capture a more abstract representation of tweets, our solution is more ef fective than previous ones and also more robust regarding biased and noisy data, which is the kind of data provided by these sources.
The state-of-the-art approaches for solving this problem, such as (Go et al, 20095; Barbosa and Feng, 2010), basically follow (Pang et al, 2002), who utilize machine learning based classifiers for the sentiment classification of texts. $$$$$ Approach Error rate TwitterSA(cleaning) 18.1 TwitterSA(no-cleaning) 19.9 Unigrams 27.6 ReviewSA 32 Table 4: Results for subjectivity detection.

In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ To reduce the la beling effort in creating these classifiers, instead of using manually annotated data to compose thetraining data, as regular supervised learning ap proaches, we leverage sources of noisy labels asour training data.
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ For instance, Pang and Lee (Pang and Lee, 2004) explores the fact that sentences close in a text might share the same subjectivity to create a better subjectivity detector and, similar to (Pang etal., 2002), uses unigrams as features for the polar ity detection.
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ In this paper, we propose a 2-step sentiment analysis classification method for Twitter, whichfirst classifies messages as subjective and ob jective, and further distinguishes the subjectivetweets as positive or negative.
In contrast, (Barbosa and Feng, 2010) propose a two-step approach to classify the sentiments of tweets using SVM classifiers with abstract features. $$$$$ Finally, since the data qual ity provided by TwitterSentiment is better than the3For this experiment, we used the TwitterSA(single) con figuration.

We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ As future work, we want to perform a more fine grained analysis of sentences in order to identifyits main focus and then based the sentiment clas sification on it.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ First, we measurethe polarity detection quality of a source by calcu lating the probability p of a label from this source being correct.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ Due to this lack of information in terms of words present in a tweet, we explore some of the tweet features listed above to boost the sentiment detection, as we will show in detail in Section 3.Data Sources.
We also re-implemented the method proposed in (Barbosa and Feng, 2010) for comparison. $$$$$ However, these approaches do not obtain a good performance on detecting sentimenton tweets, as we showed in Section 4, mainly be cause tweets are very short messages.

From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ Similar to subjectivity detection, the training size does not have much influ ence in the error rate for TwitterSA.
From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.
From Table 1, we can see that all our systems perform better than (Barbosa and Feng, 2010) on our data set. $$$$$ The main limitation of our approach is the cases of sentences that contain antagonistic sentiments.

One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ We have presented an effective and robust sen timent detection approach for Twitter messages, which uses biased and noisy labels as input to build its models.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ For instance, Pang and Lee (Pang and Lee, 2004) explores the fact that sentences close in a text might share the same subjectivity to create a better subjectivity detector and, similar to (Pang etal., 2002), uses unigrams as features for the polar ity detection.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ 39 Data sources Kappa Twendz/TwitterSentiment 0.58 TwitterSentiment/TweetFeel 0.58 Twendz/TweetFeel 0.44 Table 3: Kappa coefficient between pairs of sources.From this analysis we can conclude that com bining the labels provided by the 3 sources canimprove the performance of the polarity detection instead of using one of them in isolation be cause they provide diverse labels (moderate kappa agreement) of reasonable quality, although thereis some issues related to bias of the labels pro vided by them.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ To better utilize these sources, we verify the potential value of using and combining them, providing an analysis of the provided labels, examine different strategies of combining these sources in order to obtain the best outcome; and, propose a more robust feature set that captures a more abstract representation of tweets, composedby meta-information associated to words and spe cific characteristics of how tweets are written.

One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ To better utilize these sources, we verify the potential value of using and combining them, providing an analysis of the provided labels, examine different strategies of combining these sources in order to obtain the best outcome; and, propose a more robust feature set that captures a more abstract representation of tweets, composedby meta-information associated to words and spe cific characteristics of how tweets are written.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ Top opinion words: to clean the objective.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ We showed in Section 4 that our approach works better than theirs for this problem, obtaining lower error rates.
One possible reason is that (Barbosa and Feng, 2010) use only abstract features while our systems use more lexical features. $$$$$ The number of Twitter users reached an estimated75 million by the end of 2009, up from approx imately 5 million in the previous year.

The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ is a link to someexternal source.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ As a result, these numbers confirm that features inferred from meta information of words and specific syntax featuresfrom tweets are better indicators of the subjectivity than unigrams.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ To illustrate which of the proposed features are more effective for this task, the top-5 features in terms of information gain, based on our trainingdata, are: positive polarity, link, strong subjec tive, upper case and verbs.
The results show that our system using both content features and sentiment lexicon features performs slightly better than (Barbosa and Feng, 2010). $$$$$ 3.3.1 Analysis of the Data Sources The 3 data sources used in this work provide some kind of polarity labels (see Table 1).

Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ The quality of the data and its individual bias have certainly impact in the combination of labels.
Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ http://oohja.com/x9UbC?.
Finally, multiple models can be blended into a single classifier (Barbosa and Feng, 2010). $$$$$ Sentiment detection of tweets is one of the basicanalysis utility functions needed by various applications over twitter data.
