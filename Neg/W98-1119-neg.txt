Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al 1998). $$$$$ The results are shown in Table 1 (all obtained from cross-validation).
Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al 1998). $$$$$ This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm.
Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al 1998). $$$$$ We do not use the &quot;mention&quot; probabilities P(alma), as they are not given in the unmarked text.

Ge et al (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm. $$$$$ However, any syntax-only pronoun resolution strategy will be wrong some of the time - these methods know nothing about discourse boundaries, intentions, or real-world knowledge.
Ge et al (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs's (1978) seminal syntax-based pronoun resolution algorithm. $$$$$ We combine into a single probability that enables to identify the referent.

Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. $$$$$ The first piece of useful information we consider is the distance between the pronoun and the candidate antecedent.
Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. $$$$$ This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text (Marcus et al., 1993) that has been marked with co-reference information.
Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. $$$$$ In resolving inter-sentential pronouns, the algorithm searches the previous sentence, again in left-to-right, breadth-first order.
Ge et al [1998] also present a statistical algorithm based on the study of statistical data in a large corpus and the application of a naive Bayes model. $$$$$ The impact of this probability can be seen more clearly from another experiment in which we tested the program (using just Hobbs distance and gender information) on the training data.

Ge et al (1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank. $$$$$ The system is not entirely &quot;statistical&quot; in that it consists of various types of rule-based knowledge — syntactic, semantic, domain, discourse, and heuristic.
Ge et al (1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank. $$$$$ The system is not entirely &quot;statistical&quot; in that it consists of various types of rule-based knowledge — syntactic, semantic, domain, discourse, and heuristic.
Ge et al (1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank. $$$$$ That is, the probability of a referent ref being in gender class gc, is P(ref E gc,) (9) I refs to refwith p E gci I El refs to ref with p E gci In this work we have considered only three gender classes, masculine, feminine and inanimate, which are indicated by their typical pronouns, HE, SHE, and IT.
Ge et al (1998) uses a non-parametrized statistical model to find the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank. $$$$$ We have presented a statistical method for pronominal anaphora that achieves an accuracy of 84.2%.

Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al, 1998) assumes that correct parse trees are given. $$$$$ The system employs various constraints for NPpronoun non-coreference within a sentence.
Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al, 1998) assumes that correct parse trees are given. $$$$$ Instead of computing the probability for each one of them we group them into &quot;buckets&quot;, so that ma is the bucket for the number of times that a is mentioned.
Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al, 1998) assumes that correct parse trees are given. $$$$$ We use a small portion of the Penn Wall Street Journal Tree-bank as our training corpus.
Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al, 1998) assumes that correct parse trees are given. $$$$$ The cases where &quot;it&quot; is merely a dummy subject in a cleft sentence (example 1) or has conventional unspecified referents (example 2) are excluded from computing the precision: We performed a ten-way cross-validation where we reserved 10% of the corpus for testing and used the remaining 90% for training.

In other cases, these modules are integrated by means of statistical (Ge et al, 1998) or uncertainty reasoning techniques (Mitkov, 1997). $$$$$ There are several things to note about these results.
In other cases, these modules are integrated by means of statistical (Ge et al, 1998) or uncertainty reasoning techniques (Mitkov, 1997). $$$$$ This program differs from earlier work in its almost complete lack of hand-crafting, relying instead on a very small corpus of Penn Wall Street Journal Tree-bank text (Marcus et al., 1993) that has been marked with co-reference information.
In other cases, these modules are integrated by means of statistical (Ge et al, 1998) or uncertainty reasoning techniques (Mitkov, 1997). $$$$$ Lappin and Leass (1994) report on a (essentially non-statistical) approach that relies on salience measures derived from syntactic structure and a dynamic model of attentional state.
In other cases, these modules are integrated by means of statistical (Ge et al, 1998) or uncertainty reasoning techniques (Mitkov, 1997). $$$$$ In order to gather statistics on the gender of referents in a corpus, there must be some way of identifying the referents.

(Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. $$$$$ The second half of the paper describes a method for using (portions of) the aforementioned program to learn automatically the typical gender of English words, information that is itself used in the pronoun resolution program.
(Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. $$$$$ However, we chose to use only the Hobbs distance portion thereof.
(Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. $$$$$ An examination of these show that all but three are correct.
(Ge et al 1998) incorporate gender, number, and animaticity information into a statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs. $$$$$ Their Machine Learning-based Resolver (MLR) is trained using decision trees with 1971 anaphoras (excluding those referring to multiple discontinuous antecedents) and they report an average success rate of 74.8%.

Thus the size of the annotated data (3,115 personal pronouns, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)). $$$$$ This strategy is certainly simple-minded but, as noted earlier, it achieves an accuracy of 43%.
Thus the size of the annotated data (3,115 personal pronouns, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)). $$$$$ (Mitkov (1997) does a detailed study on factors in anaphora resolution.)
Thus the size of the annotated data (3,115 personal pronouns, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)). $$$$$ One classical approach to resolving pronouns in text that takes some syntactic factors into consideration is that of Hobbs (1976).
Thus the size of the annotated data (3,115 personal pronouns, 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al, 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)). $$$$$ We also observe that the position of a pronoun in a story influences the mention count of its referent.

Exceptions are existant but few (2.5%) $$$$$ Thus we compute precision as follows: precision = r attrib. as HE A Mr. E r I + r attrib. as SHE A Mrs. or Ms. E r I Mr., Mrs., or Ms. E r Here r varies over referent types, not tokens.
Exceptions are existant but few (2.5%) $$$$$ We present some experiments illustrating the accuracy of the method and note that with this information added, our pronoun resolution method achieves 84.2% accuracy.
Exceptions are existant but few (2.5%) $$$$$ The Hobbs algorithm makes a few assumptions about the syntactic trees upon which it operates that are not satisfied by the tree-bank trees that form the substrate for our algorithm.

Ge et al (1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already. $$$$$ The main advantage of the method is its essential simplicity.
Ge et al (1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already. $$$$$ The results are shown in Table 1 (all obtained from cross-validation).
Ge et al (1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already. $$$$$ To that end, we devised a more objective test, useful only for scoring the subset of referents that are names of people.
Ge et al (1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already. $$$$$ There are many factors, both syntactic and semantic, upon which a pronoun resolution system relies.

Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data. $$$$$ It is quite possible that a word appears in the test data that the program never saw in the training data and fow which it hence has no P(plwo) probability.
Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data. $$$$$ Their Machine Learning-based Resolver (MLR) is trained using decision trees with 1971 anaphoras (excluding those referring to multiple discontinuous antecedents) and they report an average success rate of 74.8%.
Like (Ge et al, 1998), Strube (1998) evaluates on ideal hand annotated data. $$$$$ This too is a topic for future research.

 $$$$$ This equation is decomposed into pieces that correspond to all the above factors but are more statistically manageable.
 $$$$$ We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.
 $$$$$ That is, we suspect that &quot;husband&quot;, for example, was decided incorrectly because the topic of the article was the woman, there was a mention of her &quot;husband,&quot; but the article kept on talking about the woman and used the pronoun &quot;she.&quot; While our simple program got confused, a program using better statistics might not have.

Ge et al (1998)'s probabilistic approach combines three factors (aside from the agreement filter) $$$$$ To this end, we ran the program &quot;incrementally&quot;, each time incorporating one more probability.
Ge et al (1998)'s probabilistic approach combines three factors (aside from the agreement filter) $$$$$ References by pronouns are closely related to the topic or the center of the discourse.
Ge et al (1998)'s probabilistic approach combines three factors (aside from the agreement filter) $$$$$ As a measure of the utility of these results, we also ran our pronoun-anaphora program with these statistics added.

The choice of entities may reasonably be considered to be independent given the mixing weights, but how we realize an entity is strongly dependent on context (Ge et al, 1998). $$$$$ It is quite possible that a word appears in the test data that the program never saw in the training data and fow which it hence has no P(plwo) probability.
The choice of entities may reasonably be considered to be independent given the mixing weights, but how we realize an entity is strongly dependent on context (Ge et al, 1998). $$$$$ It might have been hoped that the statistics would make things considerably more accurate.
The choice of entities may reasonably be considered to be independent given the mixing weights, but how we realize an entity is strongly dependent on context (Ge et al, 1998). $$$$$ To that end, we devised a more objective test, useful only for scoring the subset of referents that are names of people.

Ge et al (1998) exploit a similar idea to assign gender to proper mentions. $$$$$ Our first experiment shows the relative contribution of each source of information and demonstrates a success rate 82.9% for all sources combined. experiment investigates a method for unsupervised learning of gender/number/animaticity information.
Ge et al (1998) exploit a similar idea to assign gender to proper mentions. $$$$$ (Mitkov (1997) does a detailed study on factors in anaphora resolution.)
Ge et al (1998) exploit a similar idea to assign gender to proper mentions. $$$$$ Thus our second method of finding the pronoun/noun co-occurrences is simply to parse the text and then assume that the noun-phrase at Hobbs distance one is the antecedent.
Ge et al (1998) exploit a similar idea to assign gender to proper mentions. $$$$$ In order to gather statistics on the gender of referents in a corpus, there must be some way of identifying the referents.

Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference annotated corpus (Ge et al, 1998). $$$$$ We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.
Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference annotated corpus (Ge et al, 1998). $$$$$ In order to gather statistics on the gender of referents in a corpus, there must be some way of identifying the referents.
Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference annotated corpus (Ge et al, 1998). $$$$$ The first sections of this paper describe this program: the probabilistic model behind it, its implementation, and its performance.
Implementation of constraints and preferences can be based on empirical insight (Lappin and Leass, 1994), or machine learning from a reference annotated corpus (Ge et al, 1998). $$$$$ The ith candidate is regarded as occurring at &quot;Hobbs distance&quot; dH = i.

There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al, 1998), or contextual role-knowledge (Bean and Riloff, 2004). $$$$$ We believe, however, that there are ways to improve the accuracy of the learning method and thus increase its influence on pronoun anaphora resolution.
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al, 1998), or contextual role-knowledge (Bean and Riloff, 2004). $$$$$ Also, some head verbs are too general to restrict the selection of any NP.
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al, 1998), or contextual role-knowledge (Bean and Riloff, 2004). $$$$$ Since the research described herein we have thought of other influences on anaphora resolution and their statistical correlates.
There are also approaches to anaphora resolution using unsupervised methods to extract useful information, such as gender and number (Ge et al, 1998), or contextual role-knowledge (Bean and Riloff, 2004). $$$$$ The method described here is based on simply counting co-occurrences of pronouns and noun phrases, and thus can employ any method of analysis of the text stream that results in referent/pronoun pairs (cf.

Incorporating context only through the governing constituent was also done in (Ge et al, 1998). $$$$$ We believe, however, that there are ways to improve the accuracy of the learning method and thus increase its influence on pronoun anaphora resolution.
Incorporating context only through the governing constituent was also done in (Ge et al, 1998). $$$$$ We incorporate multiple anaphora resolution factors into a statistical framework — specifically the distance between the pronoun and the proposed antecedent, gender/number/animaticity of the proposed antecedent, governing head information and noun phrase repetition.
Incorporating context only through the governing constituent was also done in (Ge et al, 1998). $$$$$ This paper presents an algorithm for identifying pronominal anaphora and two experiments based upon this algorithm.

Ge et al (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. $$$$$ This implements the observed preference for subject position antecedents.
Ge et al (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. $$$$$ We have presented a statistical method for pronominal anaphora that achieves an accuracy of 84.2%.
Ge et al (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. $$$$$ It might have been hoped that the statistics would make things considerably more accurate.
Ge et al (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. $$$$$ We present a statistical method for determining pronoun anaphora.

Their factors are taken from Ge et al (1998), with two exceptions. $$$$$ Except for implementing the Hobbs referent-ordering algorithm, all other system knowledge is imbedded in tables giving the various component probabilities used in the probability model.
Their factors are taken from Ge et al (1998), with two exceptions. $$$$$ We will say that the gender class for which this relative frequency is the highest is the gender class to which the referent most probably belongs.
Their factors are taken from Ge et al (1998), with two exceptions. $$$$$ In equation (7) we make the following independence assumptions: P(sp, data, = P (s p, data) Then we combine so and dc, into one variable dH , Hobbs distance, since the Hobbs algorithm takes both the syntax and distance into account.
