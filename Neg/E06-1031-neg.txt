Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ Like the Levenshtein distance, the long jump distance can be depicted using an alignment grid as shown in Figure 1: Here, each grid point corresponds to a pair of inter-word positions in candidate and reference sentence, respectively. dLJ is the minimum cost of a path between the lower left (first) and the upper right (last) alignment grid point which covers all reference and candidate words.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ Our measure can be exactly calculated in quadratic time.

 $$$$$ Furthermore, we will show how some evaluation measures can be improved
 $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation
 $$$$$ In many cases though such movements still result in correct or almost correct sentences.
 $$$$$ When extending the metric by block movements, we drop this constraint for the candidate translation.

Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ CDER allows for reordering blocks of words at constant cost.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ The results in Section 5.2 will show that the measure using the originally proposed direction has a significantly higher correlation with human evaluation than the other directions.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ Deletions and insertions correspond to horizontal and vertical edges, respectively.

The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ Consequently, the proposed search algorithm will not necessarily find an optimal solution for the sum of dCD and lpmisc.
The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.
The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ As it could be arguable whether Pearson’s r is meaningful for categorical data like human MT evaluation, we have also calculated Kendall’s correlation coefficient T. Because of the high number of samples (= sentences, 4460) versus the low number of categories (= outcomes of adequacy+fluency, 9), we calculated T separately for each source sentence.
The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ They are thus not well-suited for sentence-level evaluation.

For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ Future work will aim at finding a suitable length penalty for CDER.
For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ Edges between arbitrary grid points from the same row correspond to long jump operations.
For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ Thus, the block movements can equivalently be expressed as long jump operations that jump over the gaps between two blocks.

In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ We also studied the reverse direction of the described measure; that is, we dropped the coverage constraints for the reference sentence instead of the candidate sentence.
In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ The highest correlation with human assessment was achieved through linear interpolation of the new CDER with PER.
In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.
In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.

For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks.
For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ Deletions and insertions correspond to horizontal and vertical edges, respectively.
For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ In most applications of MT, understandability for humans in terms of readability as well as semantical correctness should be the evaluation criterion.

Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ An interesting topic in MT evaluation research is the question whether a linear combination of two MT evaluation measures can improve the correlation between automatic and human evaluation.
Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ The correlation with human assessment will be measured for several different statistical MT systems.
Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ It is based on edit distance – such as the well-known word error rate (WER) – but allows for reordering of blocks.
Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.

Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ In MT – as opposed to other natural language processing tasks like speech recognition – there is usually more than one correct outcome of a task.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ 2.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation

For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ For each block of over-covered candidate words, replace the aligned substitution and/or identity edges by insertion edges; move the long jump at the beginning of the block accordingly.
For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ Therefore, the long jump distance between the sentences is five.
For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ Instead, a modification of the Hungarian algorithm (Knuth, 1993) can be used.
For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ State-of-the-art evaluation measures for MT penalize movement of blocks rather severely: ngram based scores such as BLEU or NIST still yield a high unigram precision if blocks are reordered.

Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ On the other hand, the common prefix length is sensitive to critical prefixes such as “mis-” for the same reason.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ The long jump distance as well as the Levenshtein distance require both reference and candidate translation to be covered completely and disjointly.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ We will present an algorithm which computes the new error measure in quadratic time.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ Consequently, an evaluation measure for MT should be able to detect and allow for block reordering.

 $$$$$ Experimental evaluation on two different translation tasks shows a significantly improved correlation with human judgment in comparison with state-of-the-art measures such as BLEU.
 $$$$$ We presented CDER, a new automatic evaluation measure for MT, which is based on edit distance extended by block movements.
 $$$$$ As a consequence, this affects the overall score significantly.
 $$$$$ The highest correlation with human assessment was achieved through linear interpolation of the new CDER with PER.

Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ As a consequence, this affects the overall score significantly.
Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.
Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation
