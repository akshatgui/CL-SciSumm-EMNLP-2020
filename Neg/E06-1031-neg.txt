Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ This paper is organized as follows: In Section 2, we will present the state of the art in MT evaluation and discuss the problem of block reordering.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy; and a linear regression model developed by (Russo-Lassner et al., 2005), which makes use of stemming, Word Net synonymy, verb class synonymy, matching noun phrase heads, and proper name matching. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.

 $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
 $$$$$ For the PER, it is no longer possible to use a linear time algorithm in the general case.
 $$$$$ More sophisticated methods could be considered for word-dependent substitution costs as well.

Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ Not a single one of the 7 candidate four-grams occurs in the reference sentence.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ CDER allows for reordering blocks of words at constant cost.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ Furthermore, we will show how some evaluation measures can be improved

The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ As a further improvement, we will introduce word dependent substitution costs.
The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ For the two corpora studied here, the results of the combination can be found in Table 6: On the Chinese–English task, there is an additional gain of more than 1% absolute in correlation over CDER alone.
The CDER measure (Leusch et al, 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks. $$$$$ We adopted this notion for our measures. this path will always start at a node with the lowest D value in its row3.

For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ Nevertheless, a higher “amount” of reordering between a candidate translation and a reference translation should still be reflected in a worse evaluation score.
For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ Nevertheless, a higher “amount” of reordering between a candidate translation and a reference translation should still be reflected in a worse evaluation score.
For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ This auxiliary quantity can then be calculated recursively from DLev(i − 1, l), DLev(i, l − 1), and DLev(i − 1, l − 1).
For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nieen et al, 2000) because of their support on fast and inexpensive evaluation. $$$$$ Additionally, we showed how word-dependent substitution costs can be applied to enhance the new error measure as well as existing approaches.

In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ In other words, the more blocks there are to be reordered between reference and candidate sentence, the higher we want the measure to evaluate the distance between these sentences.
In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.
In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ Thus, the calculation takes three steps for each l: i0 There is always an optimal dCD alignment path that does not contain any deletion edges, because each deletion can be replaced by a long jump, at the same costs.
In addition to the widely used BLEU (Papineni et al, 2002) and NIST (Doddington, 2002) scores, we also evaluate translation quality with the recently proposed Meteor (Banerjee and Lavie, 2005) and four edit-distance style metrics, Word Error Rate (WER), Position independent word Error Rate (PER) (Tillmann et al., 1997), CDER, which allows block reordering (Leusch et al, 2006), and Translation Edit Rate (TER) (Snover et al, 2006). $$$$$ Furthermore, it will be interesting to see how this new error measure affects system development: We expect it to allow for a better sentence-wise error analysis.

For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ The number of blocks in a sentence is equal to the number of gaps among the blocks plus one.
For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation
For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences.
For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006). $$$$$ We presented CDER, a new automatic evaluation measure for MT, which is based on edit distance extended by block movements.

Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ The introduction of smoothing (Lin and Och, 2004) solves this problem only partially.
Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ In previous work, researchers have tried to reduce the complexity, for example by restricting the possible permutations on the block-level, or by approximation or heuristics during the calculation.
Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexical choices as the reference translation is not justified semantically. $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation

Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ But only the latter allows for an efficient calculation of confidence intervals.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ CDER allows for reordering blocks of words at constant cost.
Meteor, as well as several other proposed metrics such as GTM (Melamed et al, 2003), TER (Snover et al, 2006) and CDER (Leusch et al, 2006) aim to address some of these weaknesses. $$$$$ For example, the only necessary modification in the CDER algorithm in Equation 1 is to replace 1 − δ(e, ˜e) by cSUB(e, ˜e).

For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ HR001106-C-0023, and was partly funded by the European Union under the integrated project TC-STAR – Technology and Corpora for Speech to Speech Translation
For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al, 1997), CDER (Leusch et al, 2006) and WER (Nie?en et al, 2000). $$$$$ Additionally, we showed how word-dependent substitution costs can be applied to enhance the new error measure as well as existing approaches.

Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ For system optimization, preliminary experiments have shown the need for a suitable length penalty.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ PER, on the other hand, ignores the ordering of the words in the sentences completely.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ These experiments showed that Kendall’s T reflects the same tendencies as Pearson’s r regarding the ranking of the evaluation measures.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al, 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy. $$$$$ In contrast, the best Levenshtein path contains one deletion edge, four identity and five consecutive substitution edges; the Levenshtein distance between the two sentences is six.

 $$$$$ Most state-of-the-art evaluation measures for machine translation assign high costs to movements of word blocks.
 $$$$$ In contrast, the best Levenshtein path contains one deletion edge, four identity and five consecutive substitution edges; the Levenshtein distance between the two sentences is six.
 $$$$$ For CDER, there is hardly any difference between the two methods.
 $$$$$ The resulting long jump distance dLJ gives the minimum number of operations which are necessary to transform the candidate sentence into the reference sentence.

Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ Consequently, we use the following modification of the Levenshtein recursion: where δ is the Kronecker delta.
Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ In this paper, we will present a new evaluation measure which explicitly models block reordering as an edit operation.
Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nie?en et al, 2000), and TER (Snover et al, 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. $$$$$ The calculation of D(i, l) requires all values of D(i', l) to be known, even for i' > i.
