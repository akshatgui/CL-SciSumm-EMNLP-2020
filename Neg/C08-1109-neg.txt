The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ is the PN and ?line?
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ While the work presented here has focused on prepositions, the arguments against using only one rater, and for using a sampling approach generalize to other error types, such as determiners and collocations.
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance.

(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ If it came from the ?OK? sub corpus, then the case is a Miss (an error that the system failed to detect).
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ The obvious benefit of this analysis is that it can focus development of the system.
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.

This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ While their perfor mance results are quite high, 88% precision and 868 78% recall, it should be noted that their evaluation was on a small corpus with a highly constraineddomain, and focused on a limited number of prepo sitions, thus making direct comparison with our approach difficult.Although our recall figures may seem low, es pecially when compared to other NLP tasks such as parsing and anaphora resolution, this is really a reflection of how difficult the task is. For example, in the problem of preposition selection in native text, a baseline using the most frequent preposition(of) results in precision and recall of 26%.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ for efficiently evaluat ing a system that does not require the amount ofeffort needed in the standard approach to annota tion.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ corpora, one consisting of the system?s ?OK? prepositions and the other of the system?s ?Error?

Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ We found that extrane ous use errors usually constituted up to 18% of all preposition errors, and our extraneous use filters handle a quarter of that 18%.Thresholding: The final step for the preposi tion error detection system is a set of thresholds that allows the system to skip cases that are likely to result in false positives.
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ So for the example, the Combo:tag N-p-N feature would be ?NN-NN?, and the Combo:word+tag N-p-N feature would beplace NN+line NN (see the fourth column of Ta ble 1).
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ It should be noted that with the inclusion of the extraneous use filter, performance of the +Combo:tag rose to 84% precision and close to 19% recall.

(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner.
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ the sample from the ?Error?
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ Some rights reserved.

For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ Our results show that relyingon one rater for system evaluation can be problem atic, and we provide a sampling approach which can facilitate using multiple raters for this task.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ Rater?s Prep.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ Extraneous Use Filter: To cover extraneous use errors, we developed two rule-based filters: 1) Plural Quantifier Constructions, to handle casessuch as ?some of people?

There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance.
There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ It also poses special chal lenges for developing and evaluating an NLP error detection system.
There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance.
There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ First, we discussed a system that detects preposition errors with high precison (up to 84%) and is competitive 871 Writer?s Prep.

In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.

Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ While developing an error detection system forprepositions is certainly challenging, given the re sults from our work and others, evaluation also poses a major challenge.
Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ This paper has two contributions to the field of error detection in non-native writing.

The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ The top ten prepositions accounted for 93.8% of all preposition errors in our learner corpus.Next, we ranked the common preposition ?con fusions?, the common mistakes made for each preposition.
The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition).

Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ resulted in a removal of nearly 200,000 training events.
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ The training was very extensive: both raters were trained on 2000 preposition contexts and the annotation manual was it eratively refined as necessary.
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ In this section, we show how relying on only onerater can be problematic for difficult error detec tion tasks, and in section 4, we propose a method(?the sampling approach?)

The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ One aspect of automatic error detection that usu ally is under-reported is an analysis of the errors that learners typically make.
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.

Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ 3.1 Annotation.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ The most common of fenses were actually extraneous errors (see Table5): using to and of when no preposition was li censed accounted for 16.8% of all errors.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ It also poses special chal lenges for developing and evaluating an NLP error detection system.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ This was also meant to avoid flagging cases where the system?s preposition has a score only slightly higher than the writer?s preposition score, such as: ?My sister usually gets home around 3:00?

Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ Second, we showed that the standard ap proach to evaluating NLP error detection systems (comparing a system?s output with a gold-standard annotation) can greatly skew system results when the annotation is done by only one rater.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ To date there have been relatively few attempts to address preposition error detection,though the sister task of detecting determiner errors has been the focus of more research.

 $$$$$ Second, we showed that the standard ap proach to evaluating NLP error detection systems (comparing a system?s output with a gold-standard annotation) can greatly skew system results when the annotation is done by only one rater.
 $$$$$ This paper has two contributions to the field of error detection in non-native writing.
 $$$$$ Model Precision Recall Baseline 79.8% 11.7% +Combo:word 79.8% 12.8% +Combo:tag (with purge) 82.1% 14.1%Table 2: Best System Results on Incorrect Selec tion Task 2.5 Related Work.
 $$$$$ Results The baseline system (described in(Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall.

For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ be an error, check to see which sub-corpus itcame from.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ First, we discussed a system that detects preposition errors with high precison (up to 84%) and is competitive 871 Writer?s Prep.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ for efficiently evaluat ing a system that does not require the amount ofeffort needed in the standard approach to annota tion.

For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ To address this efficiency issue, we presented a sampling approach that produces results comparable to exhaustive annotation.
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition).
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Acknowledgements We would first like to thank our two annotators Sarah Ohls and Waverly VanWinkle for their hours of hard work.
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.

We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ Their system performs at 79% precision (which is on par with our system),however recall figures are not presented thus making comparison difficult.
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ It also poses special chal lenges for developing and evaluating an NLP error detection system.
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ 2.3 Combination Features.
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition).

We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ We extend our previous work (Chodorow etal., 2007) by experimenting with combination fea tures, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994).Second, we discuss drawbacks in current meth ods of annotating ESL data and evaluating errordetection systems, which are not limited to prepo sition errors.
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ One aspect of automatic error detection that usu ally is under-reported is an analysis of the errors that learners typically make.

The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ We used an ME approach augmented with combination features and a series of thresholds.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ The long-term goal of our work is to develop asystem which detects errors in grammar and us age so that appropriate feedback can be given to non-native English writers, a large and growing segment of the world?s population.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ The top ten most common confusions are listed in Table 5, where null refers to cases where no preposition is licensed (the writer usedan extraneous preposition).
