The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ 3.1 Annotation.
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ Finally, the selection of a preposition for a given context also depends upon the intended meaning of the writer (?we sat at the beach?, ?on the beach?, ?near the beach?, ?by the beach?).
The accuracy of the choice classifier, the part of the system to which the work at hand is most similar, is 62.32% when tested on text from Encarta and Reuters news. Tetreault and Chodorow (2008a) present a system for detecting preposition errors in learner text. $$$$$ Some rights reserved.

(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ Next we tested the differ ent combination models: word, tag, word+tag, andall three.
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ While the work presented here has focused on prepositions, the arguments against using only one rater, and for using a sampling approach generalize to other error types, such as determiners and collocations.
(Tetreault and Chodorow, 2008b) challenged the view that using one rater is adequate by showing that preposition usage errors actually do not have high inter-annotator reliability. $$$$$ This paper has two contributions to the field of error detection in non-native writing.

This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ While developing an error detection system forprepositions is certainly challenging, given the re sults from our work and others, evaluation also poses a major challenge.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner.
This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. $$$$$ This system is currently incorporated in the Criterion writing evaluationservice.

Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Examples include the Cambridge Learners Corpus2 used in (Felice and Pullman, 2009), and TOEFL data, used in (Tetreault and Chodorow, 2008a). $$$$$ 3.1 Annotation.

(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ When a preposition marks the argument of a predicate, such as a verb, an adjective, or a noun, preposition selection is con strained by the argument role that it marks, thenoun which fills that role, and the particular predi cate.
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ In this section, we show how relying on only onerater can be problematic for difficult error detec tion tasks, and in section 4, we propose a method(?the sampling approach?)
(Tetreault and Chodorow, 2008b) showed that trained human raters can achieve very high agreement (78%) on this task. $$$$$ every time the system is changed, a rater is needed to re-check the output, and 2.

For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ To summarize the procedure, the two raters were shown sentences randomly selected from student essays with each preposition highlighted in the sentence.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ The obvious benefit of this analysis is that it can focus development of the system.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ Our results show that relyingon one rater for system evaluation can be problem atic, and we provide a sampling approach which can facilitate using multiple raters for this task.
For example, (Tetreault and Chodorow, 2008b) found kappa between two raters averaged 0.630. Because there is no gold standard for the error detection task, kappa was used to compare Turker responses to those of three trained annotators. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.

There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ For example, if we assume that500 prepositions can be annotated in 4 hours us ing our annotation scheme, and that the base rate for preposition errors is 10%, then it would take atleast 80 hours for a rater to find and mark 1000 er rors.
There is already existing work that addresses specific problems in this area (see, for ex ample, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. $$$$$ 3.2 Reliability.

In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ judgments translates to variability of system evaluation.
In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ While the work presented here has focused on prepositions, the arguments against using only one rater, and for using a sampling approach generalize to other error types, such as determiners and collocations.
In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). $$$$$ If it came from the ?OK? sub corpus, then the case is a Miss (an error that the system failed to detect).

Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ Inthe context of a diagnostic feedback and assess ment tool for writers, a spell checker would first highlight the spelling errors and ask the writer tocorrect them before the system analyzed the prepo sitions.Post-Processing Filter: After the ME clas sifier has output a probability for each of the 34prepositions but before the system has made its fi nal decision, a series of rule-based post-processingfilters block what would otherwise be false posi tives that occur in specific contexts.
Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ 3.2 Reliability.
Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.

The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ This suggests that our effort to handle the 34 most frequently occurring prepositions maybe overextended and that a system that is specifically trained and refined on the top ten preposi tions may provide better diagnostic feedback to a learner.
The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ Conversely, of the prepositions Rater 2 judged to be erroneous, Rater 1 found 38.1% acceptable.
The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
The latter is complicated by the fact that native speakers differ widely with respect to what constitutes acceptable usage (Tetreault and Chodorow, 2008a). To date, a common approach to annotating non native text has been to use one rater (Gamon et al, Source Errors Errors Mistakes by error type language total per 100 Repl. $$$$$ Our results showed only about75% agreement between the two raters, and be tween each of our raters and Encarta.The presence of so much variability in prepo sition function and usage makes the task of thelearner a daunting one.

Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ In addition, we address the problem of annotation and evaluation inthis domain by showing how current ap proaches of using only one rater can skew system evaluation.
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ 4.2 Application.

The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ It is interesting to note that the most common usage errors by learners overwhelmingly involved the ten most frequently occurring prepositions in native text.
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ If it came from the ?Error?
The usage feature detects errors related to articles (Han et al, 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al, 2008). $$$$$ These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per formance.

Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
Some researchers (Tetreault and Chodorow, 2008) exploited syntactic information and n-gram features to represent verb usage context. $$$$$ corpora, one consisting of the system?s ?OK? prepositions and the other of the system?s ?Error?

Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ The kappa of 0.630 shows the difficulty of this task and also shows how two highly trained raters can produce very different judgments.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ This is a tactic also used for determiner selection in (Nagata et al, 2006) and (Han et al, 2006).
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ We wouldalso like to acknowledge the three anonymous reviewers and Derrick Higgins for their helpful com ments and feedback.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ (Lee and Seneff, 2006) used a language model to tackle the novel problem of prepositionselection in a dialogue corpus.

 $$$$$ The top ten prepositions accounted for 93.8% of all preposition errors in our learner corpus.Next, we ranked the common preposition ?con fusions?, the common mistakes made for each preposition.
 $$$$$ and 2) Repeated Prepo sitions, where the writer accidentally repeated the same preposition two or more times, such as ?canfind friends with with?.
 $$$$$ sub-corpus contains the remaining 10%.
 $$$$$ Our system performs at 84% precision andclose to 19% recall on a large set of stu dent essays.

For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ sub-corpus is sampled while only 16% of the ?OK? sub-corpus is sampled.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ In this paper we describe a methodologyfor detecting preposition errors in the writ ing of non-native English speakers.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ In addi tion, the cloze tests presented earlier indicate thateven in well-formed text, agreement between na tive speakers on preposition selection is only 75%.In texts written by non-native speakers, rater dis agreement increases, as will be shown in the next section.
For example Tetreault and Chodorow (2008) use a maximum entropy classifier to build a model of correct preposition usage, with 7 million instances in their training set, and Lee and Knutsson (2008) use memory-based learning ,with10 million sentences in their training set. $$$$$ We have used a Maximum Entropy (ME) classi fier (Ratnaparkhi, 1998) to build a model of correctpreposition usage for 34 common English prepo sitions.

For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Overall rates for FPs and Misses are calculated in a similar manner.
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Even using one rater to produce a sizeable evaluation corpus of preposition errors is extremely costly.
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ From our annotated set of preposition errors, we found that the most common prepositions that learners used incorrectly were in (21.4%), to (20.8%) and of (16.6%).
For further perspective on these results, note Chodorow et al (2007) achieved 69% with 7M training examples, while Tetreault and Chodorow (2008) found the human performance was around 75%. $$$$$ Figure 1: Sampling Approach ExampleThe sampling procedure outlined here is inspired by the one described in (Chodorow and Lea cock, 2000) for the task of evaluating the usage of nouns, verbs and adjectives.

We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ This paper has two contributions to the field of error detection in non-native writing.
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ Our results showed only about75% agreement between the two raters, and be tween each of our raters and Encarta.The presence of so much variability in prepo sition function and usage makes the task of thelearner a daunting one.
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ The intuition with the Combo:tag features is that the Combo:word features have the potentialto be sparse, and these capture more general pat terns of usage.
We adopt the features from previous work by Han et al (2006), Tetreault and Chodorow (2008), and Rozovskaya et al (2011) for our system. $$$$$ While this improvement may seemsmall, it is in part due to the difficulty of the prob lem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007).

We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ Another problem for the classifier involves differentiating between certain adjuncts and arguments.
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ is theFH.
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ What is responsiblefor making preposition usage so difficult for non native speakers?
We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T& amp; C08) originally trained with lexical features and augment it with parser output features. $$$$$ sub-corpus because we want to ?enrich?

The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ The central idea is to skew the annotation corpus so that it contains a greater proportion of errors.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ Figure 1: Sampling Approach ExampleThe sampling procedure outlined here is inspired by the one described in (Chodorow and Lea cock, 2000) for the task of evaluating the usage of nouns, verbs and adjectives.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ To address this efficiency issue, we presented a sampling approach that produces results comparable to exhaustive annotation.
The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). $$$$$ We present a sampling approach to circumvent some of the issuesthat complicate evaluation of error detec tion systems.
