This problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (Lin et al, 2003) and extraction patterns (Yangarber, 2003), which require the user to provide only a small set of lexical items of the target classes or extraction patterns for the target domain. $$$$$ We observe that the counter-training termination point is near the mono-trained curve, and has a good recall-precision trade-off.
This problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (Lin et al, 2003) and extraction patterns (Yangarber, 2003), which require the user to provide only a small set of lexical items of the target classes or extraction patterns for the target domain. $$$$$ Semantic patterns are useful for a variety of text understanding tasks, in particular for locating events in text for information extraction.
This problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (Lin et al, 2003) and extraction patterns (Yangarber, 2003), which require the user to provide only a small set of lexical items of the target classes or extraction patterns for the target domain. $$$$$ The main features of counter-training are: Training several simple learners in parallel; Competition among learners; Convergence of the overall learning process; Termination with good recall-precision tradeoff, compared to the single-trained learner.

This is termed constraint-driven learning in (Chang et al., 2007), coupled learning in (Carlson et al, 2010) and counter-training in (Yangarber, 2003). $$$$$ The curve labeled Mono shows the performance of the baseline algorithm up to 150 iterations.
This is termed constraint-driven learning in (Chang et al., 2007), coupled learning in (Carlson et al, 2010) and counter-training in (Yangarber, 2003). $$$$$ Some of the chosen scenarios will be better represented in the corpus than others, which may block learning of the under-represented scenarios.
This is termed constraint-driven learning in (Chang et al., 2007), coupled learning in (Carlson et al, 2010) and counter-training in (Yangarber, 2003). $$$$$ This likelihood is proportional to the amount of anticipated ambiguity or overlap among the counter-trained scenarios.
This is termed constraint-driven learning in (Chang et al., 2007), coupled learning in (Carlson et al, 2010) and counter-training in (Yangarber, 2003). $$$$$ It is a simple way to combine unsupervised learners for a kind of “mutual supervision”, where they prevent each other from degradation of accuracy.

Once extraction relations were obtained for a particular set of documents, the resulting set of relations were ranked according to a method proposed in (Yangarber, 2003). $$$$$ This effect is seen in Figure 2; the Lawsuit learner is inhibited by the other, stronger scenarios.
Once extraction relations were obtained for a particular set of documents, the resulting set of relations were ranked according to a method proposed in (Yangarber, 2003). $$$$$ The name classifier also factors out other out-ofvocabulary (OOV) classes of items: dates, times, numeric and monetary expressions.

We begin by outlining the general process of learning extraction patterns using a semi-supervised algorithm, similar to one presented by Yangarber (2003). $$$$$ A data-point can be thought of having one view: the patterns that match on the data-point.
We begin by outlining the general process of learning extraction patterns using a semi-supervised algorithm, similar to one presented by Yangarber (2003). $$$$$ We are given an IE scenario , e.g., “Management Succession” (as in MUC-6).
We begin by outlining the general process of learning extraction patterns using a semi-supervised algorithm, similar to one presented by Yangarber (2003). $$$$$ One obvious contributing factor is the choice of seed patterns, since seeds may cause the learner to explore different parts of the document space first, which may affect the subsequent outcome.

 $$$$$ Many current systems achieve this by pattern matching.
 $$$$$ Equations 6 and 5 imply that the learner will disfavor a pattern if it has too much opposition from other scenarios.
 $$$$$ However, in the first 103 iterations, over 90% of the patterns are good Management Succession patterns.

 $$$$$ In this paper we have presented counter-training, a method for strengthening unsupervised strategies for knowledge acquisition.
 $$$$$ We review the main features of the underlying unsupervised pattern learner and related work in Section 2.
 $$$$$ It is a simple way to combine unsupervised learners for a kind of “mutual supervision”, where they prevent each other from degradation of accuracy.
 $$$$$ This likelihood is proportional to the amount of anticipated ambiguity or overlap among the counter-trained scenarios.

 $$$$$ We are still in the early stages of exploring the space of possibilities provided by this methodology, though it is clear that it is affected by several factors.

We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003). $$$$$ The KBs cover different levels, viz. a lexicon, a semantic conceptual hierarchy, a set of patterns, a set of inference rules, a set of logical representations for objects in the domain.
We begin by outlining the general process of learning extraction patterns, similar to one presented by (Yangarber, 2003). $$$$$ As with a variety of related iterative, unsupervised methods, the output of the system is a stream of patterns, in which the quality is high initially, but then gradually degrades.

 $$$$$ The problem is to find a good set of patterns in , which cover events relevant to .
 $$$$$ As a by-product of pattern acquisition, the algorithm acquires a set of relevant documents (more precisely, a distribution of document relevance weights).
 $$$$$ In (Riloff, 1996) the system AutoSlog-TS learns patterns for filling an individual slot in an event template, while simultaneously acquiring a set of lexical elements/concepts eligible to fill the slot.

 $$$$$ The formula used for scoring candidate patterns in step 3 is similar to that in (Riloff, 1996): where are documents where matched, and the support is computed as the sum of their relevance: Document relevance is computed as in (Yangarber et al., 2000) where is the set of accepted patterns that match ; this is a rough estimate of the likelihood of relevance of , based on the pattern accuracy measure.
 $$$$$ Syntactic Normalization: To reduce variation in the corpus further, we apply a tree-transforming program to the parse trees.

For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al (2003) allow any subpart of the tree to act as an extraction pattern. $$$$$ Syntactic Normalization: To reduce variation in the corpus further, we apply a tree-transforming program to the parse trees.
For example, Yangarber (2003) uses just subject-verb-object tuples while Sudo et al (2003) allow any subpart of the tree to act as an extraction pattern. $$$$$ The initial part of the curve is difficult to see because it overlaps largely with the Counter curve.

Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb object tuples from the dependency parse as extraction patterns. $$$$$ The main features of counter-training are: Training several simple learners in parallel; Competition among learners; Convergence of the overall learning process; Termination with good recall-precision tradeoff, compared to the single-trained learner.
Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb object tuples from the dependency parse as extraction patterns. $$$$$ This degradation is inherent in the trade-off, or tension, in the scoring metrics: between trying to achieve higher recall vs. higher precision.
Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb object tuples from the dependency parse as extraction patterns. $$$$$ This converts for passive, relative, subordinate clauses, etc. into active clauses.
Predicate-Argument Model (SVO): A simple approach, used by Yangarber (2003) and Stevenson and Greenwood (2005), is to use subject-verb object tuples from the dependency parse as extraction patterns. $$$$$ Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously.

 $$$$$ An important issue is the extent of overlap among scenarios: Management Succession and Mergers and Acquisitions are likely to have more documents in common than either has with Natural Disasters.
 $$$$$ We checked the quality of the discovered patterns by hand.
 $$$$$ The parser recognizes the name tags generated in the preceding step, and treats them as atomic.

 $$$$$ (Yangarber et al., 2000) attempts to find extraction patterns, without a pre-classified corpus, starting from a set of seed patterns.
 $$$$$ We are still in the early stages of exploring the space of possibilities provided by this methodology, though it is clear that it is affected by several factors.
 $$$$$ The Lawsuit learner ran against the same scenarios as in Table 1, but some of the other learners were “weakened”: they were given smaller seeds, and therefore picked up fewer documents initially.5 This enabled them to provide sufficient guidance to the Lawsuit learner to maintain high precision, without inhibiting high recall.

Yangarber (2003) and Etzioni et al (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. $$$$$ It stops learning good patterns after 60 iterations, at 73% recall, from which point precision drops.
Yangarber (2003) and Etzioni et al (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. $$$$$ The main features of counter-training are: Training several simple learners in parallel; Competition among learners; Convergence of the overall learning process; Termination with good recall-precision tradeoff, compared to the single-trained learner.
Yangarber (2003) and Etzioni et al (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. $$$$$ Each KB can be expected to be domain-specific, to a greater or lesser degree.
Yangarber (2003) and Etzioni et al (2005) utilize the so-called Counter-Training for detecting negative rules for a specific domain or a specific class by learning from multiple domains or classes at the same time. $$$$$ The algorithm/learner achieves this bootstrapping by utilizing the duality between the space of documents and the space of patterns: good extraction patterns select documents relevant to the chosen scenario; conversely, relevant documents typically contain more than one good pattern.

 $$$$$ For every (non-auxiliary) verb heading its own clause, the transformer produces a corresponding active tree, where possible.

The predicate-argument (SVO) model allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates (Yangarber,2003). $$$$$ The last column shows the number of iterations before learning stopped.
The predicate-argument (SVO) model allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates (Yangarber,2003). $$$$$ We first present the basic algorithm for pattern acquisition, similar to that presented in (Yangarber et al., 2000).
The predicate-argument (SVO) model allows subtrees containing only a verb and its direct subject and object as extraction pattern candidates (Yangarber,2003). $$$$$ The Baseline 54% is the precision we would expect to get by randomly marking the documents as relevant to the scenario.

Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. $$$$$ In co-training, they do so by providing reliable positive examples to each other.
Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. $$$$$ Counter-training is applicable in settings where a set of data points has to be categorized as belonging to one or more target categories.
Yangarber (2003) proposed a counter-training approach to provide natural stopping criteria for unsupervised learning. $$$$$ This likelihood is proportional to the amount of anticipated ambiguity or overlap among the counter-trained scenarios.

Yangarber et al (2000) and Yangarber (2003) present an algorithm that can find patterns automatically, but it requires an initial seed of manually designed patterns for each semantic relation. $$$$$ This paper presents a method for unsupervised discovery of semantic patterns.
Yangarber et al (2000) and Yangarber (2003) present an algorithm that can find patterns automatically, but it requires an initial seed of manually designed patterns for each semantic relation. $$$$$ Counter-training is applicable in settings where a set of data points has to be categorized as belonging to one or more target categories.
Yangarber et al (2000) and Yangarber (2003) present an algorithm that can find patterns automatically, but it requires an initial seed of manually designed patterns for each semantic relation. $$$$$ One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns, with gradually degrading precision.
Yangarber et al (2000) and Yangarber (2003) present an algorithm that can find patterns automatically, but it requires an initial seed of manually designed patterns for each semantic relation. $$$$$ This closeness will need to be qualified at a future time.
