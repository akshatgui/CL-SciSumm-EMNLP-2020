The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank $$$$$ In general, the top levels of the tree represent the morphological information (the three suffix letter features and the prefix letter), while the deeper levels contribute contextual disambiguation.
The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank $$$$$ In our memory-based approach, feature weighting (rather than feature selection) for determining the relevance of features is integrated more smoothly with the similarity metric, and our results are based on experiments with a larger corpus (3 million cases).
The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank $$$$$ 'We are not convinced that variation in the results of the experiments in a 10-fold-cv set-up is statistically meaningful (the 10 experiments are not independent), but follow common practice here.
The first four lines show the token-level accuracy for standard POS tagging tools trained and evaluated on the BulTreeBank $$$$$ Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar.

We use the Memory Based Tagger (Daelemans et al, 1996) System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. $$$$$ In this distance metric, all features describing an example are interpreted as being equally important in solving the classification problem, but this is not necessarily the case.
We use the Memory Based Tagger (Daelemans et al, 1996) System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. $$$$$ The approach combines some of the best features of learned rule-based and statistical systems (small training corpora needed, incremental learning, understandable and explainable behavior of the system).
We use the Memory Based Tagger (Daelemans et al, 1996) System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. $$$$$ The idea is that it is not necessary to fully store a case as a path when only a few feature values of the case make its classification unique.
We use the Memory Based Tagger (Daelemans et al, 1996) System Headline Source Florida executes notorious serial killer PBMT Serial killer executed in Florida Word Sub. $$$$$ On the other hand, after retrieving the top k cases, the algorithm does prefer those cases that match the test word when making its final predictions.

To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al,1996), on the training portions of our 10-fold cross validation experiment for the maximal data and let it predict tags for the test material. $$$$$ The most important improvement is the use of IGTree to index and search the case base, solving the computational complexity problems a case-based approach would run into when using large case bases.
To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al,1996), on the training portions of our 10-fold cross validation experiment for the maximal data and let it predict tags for the test material. $$$$$ Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g.
To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al,1996), on the training portions of our 10-fold cross validation experiment for the maximal data and let it predict tags for the test material. $$$$$ A memorybased approach has features of both learning rule-based taggers (each case can be regarded as a very specific rule, the similarity based reasoning as a form of conflict resolution and rule selection mechanism) and of stochastic taggers: it is fundamentally a form of k-nearest neighbors (k-nn) modeling, a well-known non-parametric statistical pattern recognition technique.
To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al,1996), on the training portions of our 10-fold cross validation experiment for the maximal data and let it predict tags for the test material. $$$$$ Our goal is to adhere to the concept of memory-based learning with full memory while at the same time keeping memory and processing speed within attractive bounds.

In that table, TBL stands for Brill &apos; s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw. $$$$$ Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging.
In that table, TBL stands for Brill &apos; s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw. $$$$$ More specifically, memory-based tagging with IGTrees has the following advantages.
In that table, TBL stands for Brill &apos; s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw. $$$$$ In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive and time complexity properties when using tree-based formalism for indexing and searching huge case bases.
In that table, TBL stands for Brill &apos; s transformation-based error-driven tagget (Brill, 1995), ME stands for a tagger based on the maximum entropy modelling (Ratnaparkhi, 1996), SPATTER stands for a statistical parser based on decision trees (Magerman, 1996), IGTREE stands for the memory-based tagger by Daelemans et al (1996), and, finally, TComb stands for a tagger that works by combination of a statistical trigram-based tagger, 59 Tagger TBL ME SPATTER IGTREE TComb STT+ (CPD+ENS) Train Test 950 Kw 150 Kw. $$$$$ Distance between two values is measured using equation (2), an overlap metric, for symbolic features (we will have no numeric features in the tagging application).

The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al, 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). $$$$$ We use IGTrees (Daelemans et al. 1996) to compress the memory.
The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al, 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). $$$$$ Based on such a corpus, the tagger-generator automatically builds a tagger which is able to tag new text the same way, diminishing development time for the construction of a tagger considerably.
The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al, 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). $$$$$ Thanks to Antal van den Bosch, Ton Weijters, and Gert Durieux for discussions about tagging, IGTree, and machine learning of natural language.
The English data was automatically labeled with part-of-speech and chunk tags from the memory-based tagger and chunker (Daelemans et al, 1996), and the German data was labelled with the decision-tree-based TreeTagger (Schmidt, 1994). $$$$$ In general, the top levels of the tree represent the morphological information (the three suffix letter features and the prefix letter), while the deeper levels contribute contextual disambiguation.

Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al (1996). $$$$$ Cardie (p.c.) reports 89.1% correct tagging for unknown words.
Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al (1996). $$$$$ Compared to learning rule-based approaches such as the one by Brill (1992), a k-nn approach provides a uniform approach for all disambiguation tasks, more flexibility in the engineering of case representations, and a more elegant approach to handling of unknown words (see e.g.
Direct feedback loops that copy a predicted output label to the input representation of the next example have been used in symbolic machine-learning architectures such as the the maximum-entropy tagger described by Ratnaparkhi (1996) and the memory-based tagger (MBT) proposed by Daelemans et al (1996). $$$$$ The approach combines some of the best features of learned rule-based and statistical systems (small training corpora needed, incremental learning, understandable and explainable behavior of the system).

Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memory based learning for part-of-speech tagging as MBT (Daelemans et al, 1996). $$$$$ The most important advantages compared to current stochastic approaches are that (i) few training items (a small tagged corpus) are needed for relatively good performance, (ii) the approach is incremental: adding new cases does not require any recomputation of probabilities, and (iii) it provides explanation capabilities, and (iv) it requires no additional smoothing techniques to avoid zero-probabilities; the IGTree takes care of that.
Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memory based learning for part-of-speech tagging as MBT (Daelemans et al, 1996). $$$$$ The approach combines some of the best features of learned rule-based and statistical systems (small training corpora needed, incremental learning, understandable and explainable behavior of the system).
Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memory based learning for part-of-speech tagging as MBT (Daelemans et al, 1996). $$$$$ In general, the top levels of the tree represent the morphological information (the three suffix letter features and the prefix letter), while the deeper levels contribute contextual disambiguation.
Comparison of generalization performances in terms of F-score of MBL on the three test sets, with and without a feedback loop, and the error reduction attained by the feedback-loop method, the F-score of the trigram-class method, and the F-score of the combination of the two methods. approach was proposed in the context of memory based learning for part-of-speech tagging as MBT (Daelemans et al, 1996). $$$$$ Again, this compression does not affect IGTree's generalisation performance.

For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used. $$$$$ Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing & Skousen, 1989; Chandler, 1992; Scha, 1992).
For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used. $$$$$ In AT, the concept has appeared in several disciplines (from computer vision to robotics), using terminology such as similarity-based, example-based, memory-based, exemplarbased, case-based, analogical, lazy, nearest-neighbour, and instance-based (Stanfill and Waltz, 1986; Kolodner, 1993; Aha et al. 1991; Salzberg, 1990).
For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used. $$$$$ Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging.
For the part of speech tagging, the memory-based tagger MBT (Daelemans et al, 1996), trained on the Wall Street Journal corpus2, was used. $$$$$ Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar.

The CoNLL data differs slightly from the original Alpino tree bank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al, 1996). $$$$$ On the same training set, 76% of word tokens are ambiguous.
The CoNLL data differs slightly from the original Alpino tree bank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al, 1996). $$$$$ Weiss & Kulikowski, 1991): independent training and test sets were selected from the original corpus, the system was trained on the training set, and the generalization accuracy (percentage of correct category assignments) was computed on the independent test set.
The CoNLL data differs slightly from the original Alpino tree bank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al, 1996). $$$$$ The IGTree version turns out to be better or equally good in terms of generalization accuracy, but also is more than 100 times faster for tagging of new words4, and compresses the original case base to 4% of the size of the original case base.
The CoNLL data differs slightly from the original Alpino tree bank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al, 1996). $$$$$ A ten-fold cross-validation experiment on the first two million words of the WSJ corpus shows an average generalization performance of IGTree (on known words only) of 96.3%.

The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al, 1999) as well as possible. The data consists of words, entity tags and part of-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al, 1996). $$$$$ Table 3 lists the results in generalization accuracy, storage requirements and speed for the three algorithms using a ddf at pattern, a 100,000 word training set, and a 10,000 word test set.
The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al, 1999) as well as possible. The data consists of words, entity tags and part of-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al, 1996). $$$$$ Thanks to Antal van den Bosch, Ton Weijters, and Gert Durieux for discussions about tagging, IGTree, and machine learning of natural language.
The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al, 1999) as well as possible. The data consists of words, entity tags and part of-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al, 1996). $$$$$ We have barely begun to optimise the approach: a more intelligent similarity metric would also take into account the differences in similarity between different values of the same feature.
The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al, 1999) as well as possible. The data consists of words, entity tags and part of-speech tags which have been derived by a Dutch part-of-speech tagger (Daelemans et al, 1996). $$$$$ A windowing approach (Sejnowski & Rosenberg, 1987) was used to represent the tagging task as a classification problem.

We used the provided POS annotation in Dutch (Daelemans et al, 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g .is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). $$$$$ In the unknown words case base, the trie representation provides an automatic integration of information about the form and the context of a focus word not encountered before.
We used the provided POS annotation in Dutch (Daelemans et al, 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g .is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). $$$$$ Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar.
We used the provided POS annotation in Dutch (Daelemans et al, 1996) and a minimally supervised tagger (Yarowsky and Cucerzan, 2002) for Spanish to restrict the space of words accepted by the discriminators (e.g .is_B_candidate rejects prepositions, conjunctions, pronouns, adverbs, and those determiners that are the first word in the sentence). $$$$$ As explained earlier, both case bases are implemented as IGTrees.

We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1corpus2. $$$$$ These rules can either be hand-crafted (Garside et al., 1987; Klein & Simmons, 1963; Green 8.6 Rubin, 1971), or learned, as in Hindle (1989) or the transformation-based error-driven approach of Brill (1992).
We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1corpus2. $$$$$ The approach combines some of the best features of learned rule-based and statistical systems (small training corpora needed, incremental learning, understandable and explainable behavior of the system).
We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1corpus2. $$$$$ In this paper we show that a large-scale application of the memory-based approach is feasible: we obtain a tagging accuracy that is on a par with that of known statistical approaches, and with attractive and time complexity properties when using tree-based formalism for indexing and searching huge case bases.
We employ MBT, a memory-based tagger-generator and tagger (Daelemans et al, 1996) to produce apart-of-speech (PoS) tagger based on the ATB1corpus2. $$$$$ In IB1, search complexity is 0(N * F) (with N the number of stored cases).

A particular instantiation, MBT, was proposed in (Daelemans et al, 1996). $$$$$ During testing, a set of previously unseen feature-value patterns (the test set) is presented to the system.
A particular instantiation, MBT, was proposed in (Daelemans et al, 1996). $$$$$ IGTrees provide an elegant way of automatic determination of optimal context size.
A particular instantiation, MBT, was proposed in (Daelemans et al, 1996). $$$$$ The approach combines some of the best features of learned rule-based and statistical systems (small training corpora needed, incremental learning, understandable and explainable behavior of the system).
A particular instantiation, MBT, was proposed in (Daelemans et al, 1996). $$$$$ In practice, when using our tagger, this is of course not the case because the disambiguated tags in the left context of the current word to be tagged are the result of a previous decision of the tagger, which may be a mistake.

We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags. $$$$$ Stanfill & Waltz, 1986, Cost & Salzberg, 1994).
We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags. $$$$$ The information gain values are given as well.
We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags. $$$$$ Memory-based learning is an expensive algorithm: of each test item, all feature values must be compared to the corresponding feature values of all training items.
We use the Memory-Based Tagger (Daelemans et al, 1996) trained on the Brown corpus to compute the part-of speech tags. $$$$$ In this distance metric, all features describing an example are interpreted as being equally important in solving the classification problem, but this is not necessarily the case.

Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009). $$$$$ Memory-based learning is a form of supervised learning based on similarity-based reasoning.
Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009). $$$$$ We have shown that a memory-based approach to large-scale tagging is feasible both in terms of accuracy (comparable to other statistical approaches), and also in terms of computational efficiency (time and space requirements) when using IGTree to compress and index the case base.
Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009). $$$$$ The first feature (the expansion of the root node of the tree) is the focus word, then context features are added as further expansions of the tree until the context disambiguates the focus word completely.
Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al, 1996), dependency parsing (Nivre, 2003) and word sense disambiguation (Kubler and Zhekova, 2009). $$$$$ Ideas about this type of analogical reasoning can be found also in non-mainstream linguistics and pyscholinguistics (Skousen, 1989; Derwing & Skousen, 1989; Chandler, 1992; Scha, 1992).

This fact prohibits the feeding of the training algorithms with patterns that have the form $$$$$ Leaf nodes contain the unique class label corresponding to a path in the tree.
This fact prohibits the feeding of the training algorithms with patterns that have the form $$$$$ Thanks to Antal van den Bosch, Ton Weijters, and Gert Durieux for discussions about tagging, IGTree, and machine learning of natural language.
This fact prohibits the feeding of the training algorithms with patterns that have the form $$$$$ Thanks to Antal van den Bosch, Ton Weijters, and Gert Durieux for discussions about tagging, IGTree, and machine learning of natural language.
This fact prohibits the feeding of the training algorithms with patterns that have the form $$$$$ Church, 1988; DeRose, 1988; Cutting et al. 1992; Merialdo, 1994, etc.).

Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al, 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case. $$$$$ Average accuracy provides a reliable estimate of the generalization accuracy.
Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al, 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case. $$$$$ Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar.
Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al, 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case. $$$$$ A considerable compression is obtained as similar cases share partial paths.
Comparing our tree-induction algorithm and IGTREE, the algorithm used in MBT (Daelemans et al, 1996), their main difference is that IGTREE produces oblivious decision trees by supplying an a priori ordered list of best features instead of re-computing the best feature during each branching, which is our case. $$$$$ We will refer to this approach as IB1 (Aha et al., 1991).

The MBT POS tagger (Daelemans et al, 1996) is used to provide POS information. $$$$$ The time and speed advantage of IGTree grows with larger training sets.
The MBT POS tagger (Daelemans et al, 1996) is used to provide POS information. $$$$$ In this experiment, numbers were not stored in the known words case base; they are looked up in the unknown words case base.
The MBT POS tagger (Daelemans et al, 1996) is used to provide POS information. $$$$$ The first feature (the expansion of the root node of the tree) is the focus word, then context features are added as further expansions of the tree until the context disambiguates the focus word completely.
The MBT POS tagger (Daelemans et al, 1996) is used to provide POS information. $$$$$ Thanks to Antal van den Bosch, Ton Weijters, and Gert Durieux for discussions about tagging, IGTree, and machine learning of natural language.

This material was POS-tagged using MBT (Memory Based Tagger) (Daelemans et al,1996). $$$$$ The use of IGTree has as additional advantage that optimal context size for disambiguation is dynamically computed.
This material was POS-tagged using MBT (Memory Based Tagger) (Daelemans et al,1996). $$$$$ Additional advantages specific to a memory-based approach include (i) the relatively small tagged corpus size sufficient for training, (ii) incremental learning, (iii) explanation capabilities, (iv) flexible integration of information in case representations, (v) its non-parametric nature, (vi) reasonably good results on unknown words without morphological analysis, and (vii) fast learning and tagging.
This material was POS-tagged using MBT (Memory Based Tagger) (Daelemans et al,1996). $$$$$ We therefore weigh each feature with its information gain; a number expressing the average amount of reduction of training set information entropy when knowing the value of the feature (Daelemans & van de Bosch, 1992, Quinlan, 1993; Hunt et al. 1966) (Equation 3).
This material was POS-tagged using MBT (Memory Based Tagger) (Daelemans et al,1996). $$$$$ Where possible, we used a 10-fold cross-validation approach.

Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set. $$$$$ Apart from linguistic engineering refinements of the similarity metric, we are currently experimenting with statistical measures to compute such more fine-grained similarities (e.g.
Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set. $$$$$ Stanfill & Waltz, 1986, Cost & Salzberg, 1994).
Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set. $$$$$ Research of the first author was done while he was a visiting scholar at NIAS (Netherlands Institute for Advanced Studies) in Wassenaar.
Its original PoS tag set is very coarse and the PoS and the word stem information is not very reliable. We therefore decided to retag the tree bank automatically using the Memory-Based Tagger (MBT) (Daelemans et al, 1996) which uses a very fine-grained tag set. $$$$$ We will refer to this approach as IB1 (Aha et al., 1991).
