Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data.
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system.

In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ The well-formed dependency structures defined here are similar to the data structures in previous work on mono-lingual parsing (Eisner and Satta, 1999; McDonald et al., 2005).
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ This work was supported by DARPA/IPTO Contract No.
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ However, dependency structures allow the use of a dependency LM which gives rise to significant improvement.
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side.

The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ Suppose we use a tri-gram dependency LM, wh-as-head represents wh used as the head, and it is different from wh in the dependency language model.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ We first introduce three meta category operations.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ LF and RF represent the farthest two children on the left and right sides respectively.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ An ANDf aligned to Pm,n e , we mean all words in Pi,j f are either aligned to words in Pm,n e or unaligned, and vice versa.

Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ 8.
Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ X is the only non-terminal, which is similar to the Hiero system (Chiang, 2007).

The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ Rescoring We rescore 1000-best translations (Huang and Chiang, 2005) by replacing the 3-gram LM score with the 5-gram LM score computed offline.
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ For example, it cannot handle the above example.
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration.
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.

(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.
(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ We used part of the NIST 2006 ChineseEnglish large track data as well as some LDC corpora collected for the DARPA GALE program (LDC2005E83, LDC2006E34 and LDC2006G05) as our bilingual training data.
(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ Following (Och, 2003), the k-best results are accumulated as the input of the optimizer.

(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ Following (Chiang, 2005), we also use concatenation rules like X —* XX for backup.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ As such, dependency trees are a desirable prior model of the target sentence.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ The definition of the operations in the left direction is as follows.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ A string-to-dependency grammar G is a 4-tuple G =< R, X, Tf, Te >, where R is a set of transfer rules.

We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. $$$$$ Let (Pi,j f , Pm,n e , D1, A) be a valid rule template, and (Pp,q f , Ps,t e , D2, A) a valid phrase alignment, where [p, q] C [i, j], [s, t] C [m, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned.
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. $$$$$ However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation.
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons: It provides a strong baseline, which ensures the validity of the improvement we would obtain. $$$$$ Let wh be the head, and wL1wL2...wLn be the children on the left side from the nearest to the farthest.

In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing.
In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ For the dependency tree in Figure 1, we calculate the probability of the tree as follows ×PL(will|find-as-head) ×PL(boy|will, find-as-head) ×PL(the|boy-as-head) ×PR(it|find-as-head) ×PR(interesting|it, find-as-head) Here PT(x) is the probability that word x is the root of a dependency tree.
In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set.

Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations.
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ We take the replicated Hiero system as our baseline because it is the closest to our string-todependency model.

When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ Child and parent subtrees can be combined with adjoining which is similar to the traditional dependency formalism.
When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ With categories, we can easily track the types of dependency structures and constrain operations in decoding.

Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ Similarly, LN and RN represent the nearest two children on the left and right sides respectively.
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration.
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ The procedure is similar to (Chiang, 2007) except that we maintain tree structures on the target side, instead of strings.

Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ Graehl and Knight (2004) proposed the use of targettree-to-source-string transducers (xRS) to model translation.
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ For example, it cannot handle the above example.
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ The depLM is used in a bottom-up style, while SLM is employed in a left-to-right style.

Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ The category of the combined dependency structure is the result of the combinatory category operations.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ They have similar rule extraction and decoding algorithms.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ In the future, we will continue our research in this direction to carry out translation with deeper features, for example, propositional structures (Palmer et al., 2005).

It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.
It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ Following (Och, 2003), the k-best results are accumulated as the input of the optimizer.

Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ They have similar rule extraction and decoding algorithms.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ Discount on ill-formed dependency structures We have eight features in our system.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ The rule extraction procedure is as follows.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.

Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model.
Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ The depLM is used in a bottom-up style, while SLM is employed in a left-to-right style.
Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ In order to calculate the dependency language model score, or depLM score for short, on the fly for partial hypotheses in a bottom-up decoding, we need to save more information in categories and states.
Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system.

Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ The procedure is similar to (Chiang, 2007) except that we maintain tree structures on the target side, instead of strings.
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ Discount on ill-formed dependency structures We have eight features in our system.
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.

Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ The values of the first four features are accumulated on the rules used in a translation.
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be attached by specifying NTs in dependency trees.
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ Discount on ill-formed dependency structures We have eight features in our system.

To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ The 5th feature counts the number of concatenation rules used in a translation.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ In the next section, we will explain how to extend categories and states to exploit a dependency language model during decoding.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ We create a new valid rule template (P0 f, P0e, D0, A), where we obtain Pf0 by replacing Pp,q f with label X in Pi,j f , and obtain Among all valid rule templates, we collect those that contain at most two NTs and at most seven elements in the source as transfer rules in our system.
