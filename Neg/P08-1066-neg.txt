Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ In order to take advantage of dynamic programming, we fixed the positions onto which another another tree could be attached by specifying NTs in dependency trees.
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ Child and parent subtrees can be combined with adjoining which is similar to the traditional dependency formalism.
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system.
Both language-model and translation-model adaptation are implemented on top of a hierarchical Arabic-to-English translation system with string-to dependency rules as described in Shen et al (2008). $$$$$ All valid phrase templates are valid rules templates.

In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ The right side probability is similar.
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ However, we need to specify one of the three types of the dependency structure: fixed, floating on the left side, or floating on the right side.
In addition to the features described in Shen et al (2008), a new feature is added to the model for the bias rule weight, allowing the translation system to effectively tune the probability of the rules added by translation model adaptation in order to improve performance on the tuning set. $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.

The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005).
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side.

Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ PL and PR are left and right side generative probabilities respectively.
Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ The use of dependency structures is due to the flexibility of dependency trees as a representation method which does not rely on constituents (Fox, 2002; Ding and Palmer, 2005; Quirk et al., 2005).
Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.
Shen et al (2008) proposed a string-to-dependency model, which restricted the target-side of a rule by dependency structures. $$$$$ Similar operations as described in Section 2.2 are used to keep track of the head and boundary child nodes which are then used to compute depLM scores in decoding.

The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ Dependency structures provide a desirable platform to employ linguistic knowledge in MT.
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ Figure 1 shows an example of a dependency tree.
The MT system we used is based on a phrase-based hierarchical model similar to that of Shen et al (2008). $$$$$ In (Galley et al., 2004), rules contain NT slots and combination is only allowed at those slots.

(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ In our system, we allow substitutions of dependency structures with unmatched categories, but there is a discount for such substitutions.
(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.
(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ PL and PR are left and right side generative probabilities respectively.
(Shen et al, 2008) presents a string-to-dependency model, which restricts the target side of each hierarchical rule to be a well-formed dependency tree fragment, and employs a dependency language model to make the output more grammatically. $$$$$ HR0011-06-C-0022 under the GALE program.

(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ They have similar rule extraction and decoding algorithms.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ HR0011-06-C-0022 under the GALE program.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing.
(Shen et al, 2008) extends the hierarchical phrase-based model and present a string-to-dependency model, which employs string-to-dependency rules whose source side are string and the target as well-formed dependency structures. $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.

We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons $$$$$ Let (Pi,j f , Pm,n e , D1, A) be a valid rule template, and (Pp,q f , Ps,t e , D2, A) a valid phrase alignment, where [p, q] C [i, j], [s, t] C [m, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned.
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons $$$$$ Since our dependency LM models structures over target words directly based on dependency trees, we can build a single-step system.
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons $$$$$ We believe that the fixed and floating structures proposed in this paper can be extended to model predicates and arguments.
We take BBN's HierDec, a string-to-dependency decoder as described in (Shen et al, 2008), as our baseline for the following two reasons $$$$$ We use dependency structures instead of strings; thus, the comparison will show the contribution of using dependency information in decoding.

In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ We use a 5-tuple (LF, LN, h, RN, RF) to represent the category of a dependency structure. h represents the head.
In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ This work was supported by DARPA/IPTO Contract No.
In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ Let (Pi,j f , Pm,n e , D1, A) be a valid rule template, and (Pp,q f , Ps,t e , D2, A) a valid phrase alignment, where [p, q] C [i, j], [s, t] C [m, n], D2 is a sub-structure of D1, and at least one word in Pi,j f but not in Pp,q f is aligned.
In the original string-to-dependency model (Shen et al, 2008), a translation rule is composed of a string of words and non-terminals on the source side and a well-formed dependency structure on the target side. $$$$$ Our experiments show that the string-to-dependency decoder achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to a standard hierarchical string-tostring system on the NIST 04 Chinese-English evaluation set.

Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005).
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ Let wh be the head, and wL1wL2...wLn be the children on the left side from the nearest to the farthest.
Thus, we can compute the source dependency LM score in the same way we compute the target side score, using a procedure described in (Shen et al, 2008). $$$$$ HR0011-06-C-0022 under the GALE program.

When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ Supposing we use a traditional tri-gram language model in decoding, we need to specify the leftmost two words and the rightmost two words in a state.
When extracting rules with source dependency structures, we applied the same well-formedness constraint on the source side as we did on the target side, using a procedure described by (Shen et al, 2008). $$$$$ For the dependency tree in Figure 1, we calculate the probability of the tree as follows ×PL(will|find-as-head) ×PL(boy|will, find-as-head) ×PL(the|boy-as-head) ×PR(it|find-as-head) ×PR(interesting|it, find-as-head) Here PT(x) is the probability that word x is the root of a dependency tree.

Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation.
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ However, here we have fixed structures growing on both sides to exploit various translation fragments learned in the training data, while the operations in mono-lingual parsing were designed to avoid artificial ambiguity of derivation.
Following Shen et al (2008), we distinguish between fixed, floating, and ill-formed structures. $$$$$ HR0011-06-C-0022 under the GALE program.

Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005).
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ On decoding output, the string-todependency system achieved 1.48 point improvement in BLEU and 2.53 point improvement in TER compared to the baseline hierarchical stringto-string system.
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ In order to calculate the dependency language model score, or depLM score for short, on the fly for partial hypotheses in a bottom-up decoding, we need to save more information in categories and states.
Experiments show that our approach significantly outperforms both phrase-based (Koehn et al, 2007) and string-to dependency approaches (Shen et al, 2008) in terms of BLEU and TER. $$$$$ In this paper, we propose a novel string-todependency algorithm for statistical machine translation.

Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ A string-to-dependency transfer rule R E R is a 4-tuple R =< 5f, 5e, D, A >, where 5f E (Tf U {X})+ is a source string, 5e E (Te U {X})+ is a target string, D represents the dependency structure for 5e, and A is the alignment between 5f and 5e.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ Suppose we use a tri-gram dependency LM, wh-as-head represents wh used as the head, and it is different from wh in the dependency language model.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ The use of a dependency LM in MT is similar to the use of a structured LM in ASR (Xu et al., 2002), which was also designed to exploit long-distance relations.
Following Shen et al (2008), string-to dependency rules without non-terminals can be extracted from the training example. $$$$$ Dependency structures provide a desirable platform to employ linguistic knowledge in MT.

It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration.
It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005).
It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ Hypothesis dependency structures are organized in a shared forest, or AND-OR structures.
It is easy to verify that the reduce left and reduce right actions are equivalent to the left adjoining and right adjoining operations defined by Shen et al (2008). $$$$$ Following previous work on hierarchical MT (Chiang, 2005; Galley et al., 2006), we solve decoding as chart parsing.

Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ With this new framework, we employ a target dependency language model during decoding to exploit long distance word relations, which are unavailable with a traditional n-gram language model.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ We always build well-formed partial structures on the target side in decoding.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ For example, Chiang (2007) showed that the Hiero system achieved about 1 to 3 point improvement in BLEU on the NIST 03/04/05 Chinese-English evaluation sets compared to a start-of-the-art phrasal system.
Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al, 2008). $$$$$ The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side.

Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ This dependency LM can also be used in hierarchical MT systems using lexicalized CFG trees.
Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ Supposing we use a traditional tri-gram language model in decoding, we need to specify the leftmost two words and the rightmost two words in a state.
Shen et al (2008) use only phrases that meet certain restrictions. $$$$$ Table 2 shows the BLEU and TER scores on MT04.

Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ We are grateful to Roger Bock, Ivan Bulyko, Mike Kayser, John Makhoul, Spyros Matsoukas, AnttiVeikko Rosti, Rich Schwartz and Bing Zhang for their help in running the experiments and constructive comments to improve this paper.
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ For example, the following rule A number of techniques have been proposed to improve rule coverage.
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ LF and RF represent the farthest two children on the left and right sides respectively.
Our machine translation system is a string-to dependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). $$$$$ We create a new valid rule template (P0 f, P0e, D0, A), where we obtain Pf0 by replacing Pp,q f with label X in Pi,j f , and obtain Among all valid rule templates, we collect those that contain at most two NTs and at most seven elements in the source as transfer rules in our system.

Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ Powell’s method is used for optimization with 20 random starting points around the weight vector of the last iteration.
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ Similar operations as described in Section 2.2 are used to keep track of the head and boundary child nodes which are then used to compute depLM scores in decoding.
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ For comparison purposes, we replicated the Hiero system as described in (Chiang, 2005).
Furthermore, we used a state of the art string-to-tree decoder (Shen et al, 2008) to establish the strongest possible baseline. $$$$$ Furthermore, at least one word in Pi,j structure represents an application of a rule over component OR-structures, and an OR-structure represents a set of alternative AND-structures with the same state.

To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ Our string-to-dependency system generates 80% fewer rules, and achieves 1.48 point improvement in BLEU and 2.53 point improvement in TER on the decoding output on the NIST 04 Chinese-English evaluation set.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ This work was supported by DARPA/IPTO Contract No.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ Both systems use only one non-terminal label in rules.
To establish strong baselines, we used a string-to tree SMT system (Shen et al, 2008), one of the top performing systems in the NIST 2009 MT evaluation, and trained it with very large amounts of parallel and language model data. $$$$$ Hypothesis dependency structures are organized in a shared forest, or AND-OR structures.
