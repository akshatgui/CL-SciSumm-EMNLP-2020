More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. $$$$$ Our work targets errors involving prepositions, specifically those of incorrect preposition selection, such as arrive to the town, and those of extraneous prepositions, as in most ofpeople.
More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. $$$$$ We are particularly concerned about precision because the feedback that students receive from an automated writing analysis system should, above all, avoid false positives, i.e., marking correct usage as incorrect.
More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. $$$$$ To identify extraneous preposition errors we devised two rule-based filters which were based on analysis of the development set.
More recently, Han et al (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al (2003) and Chodorow et al (2007) present techniques of automatic preposition choice modeling. $$$$$ Because misspellings were not in the training, the model was unable to use the features associated with them (e.g., FHword#frinzy) in its decision making.

Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. $$$$$ To our knowledge, only three other groups have attempted to automatically detect errors in preposition usage.
Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. $$$$$ Our strategy of training the ME classifier on grammatical, edited text precluded detection of extraneous prepositions as these did not appear in the training corpus.
Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. $$$$$ Preposition errors account for a significant proportion of all ESL grammar errors.
Chodorow et al (2007) present numbers on an independently developed system for detection of preposition error in non-native English. $$$$$ The paper is structured as follows: in the next section, we describe the difficulty in learning English preposition usage; in Section 3, we discuss related work; in Sections 4-7 we discuss our methodology and evaluation.

For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. $$$$$ Our strategy of training the ME classifier on grammatical, edited text precluded detection of extraneous prepositions as these did not appear in the training corpus.
For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. $$$$$ While most NLP systems are a balancing act between precision and recall, the domain of designing grammatical error detection systems is distinguished in its emphasis on high precision over high recall.
For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. $$$$$ To avoid this situation, we re-ran the test allowing the classifier to skip any preposition if its top ranked and second ranked choices differed by less than a specified amount.
For a first human evaluation of our system prototype, we decided to Chodorow et al (2007) evaluate their system on. $$$$$ In a sentence that begins After the crowd was whipped up into a frenzy of anticipation, the preposition into is preceded by a VP.

Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. $$$$$ In the sentence They described a part for a kid, the preposition of has a higher probability.
Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. $$$$$ Currently, we are in the course of annotating additional ESL essays for preposition errors in order to obtain a larger-sized test set.
Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. $$$$$ Even though they reported a precision of 0.88 and recall of 0.78, their evaluation was on a very restricted domain with only a limited number of prepositions, nouns and verbs.
Chodorow et al (2007) employed a maximum entropy model to estimate the probability of 34 prepositions based on 25 local context features ranging from words to NP/VP chunks. $$$$$ To detect the first type of error, incorrect selection, we have employed a maximum entropy (ME) model to estimate the probability of each of 34 prepositions, based on the features in their local contexts.

Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). $$$$$ The paper is structured as follows: in the next section, we describe the difficulty in learning English preposition usage; in Section 3, we discuss related work; in Sections 4-7 we discuss our methodology and evaluation.
Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). $$$$$ They loaded the wagon with hay, which show that, depending on the verb, an argument may sometimes be marked by a preposition and sometimes not.
Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). $$$$$ In the training corpus, commas generally separated parenthetical expressions, such as from my point of view, from the rest of the sentence.
Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al, 2004) and prepositions (Chodorow et al., 2007). $$$$$ Our strategy of training the ME classifier on grammatical, edited text precluded detection of extraneous prepositions as these did not appear in the training corpus.

(Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. $$$$$ Izumi et al. (2003) and (2004) used errorannotated transcripts of Japanese speakers in an interview-based test of spoken English to train a maximum entropy classifier (Ratnaparkhi, 1998) to recognize 13 different types of grammatical and lexical errors, including errors involving prepositions.
(Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. $$$$$ This suggests the need for much more data. model in this study contains no semantic information.
(Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. $$$$$ The judged rate of occurrence of preposition errors was 0.109 for Rater 1 and 0.098 for Rater 2, i.e., about 1 out of every 10 prepositions was judged to be incorrect.
(Chodorow et al, 2007) present a system for detecting errors in English prepositions using machine learning. $$$$$ Why are prepositions so difficult to master?

Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. $$$$$ The overall proportion of agreement was 0.942, but kappa was only 0.365 due to the high level of agreement expected by chance, as the Classifier used the response category of “correct” more than 97% of the time.
Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. $$$$$ Even with these constraints, there are still variations in the ways in which arguments can be expressed.
Chodorow and Leacock (2000) and Chodorow et al (2007) argue that precision-oriented is better, but they do not give any concrete reason. $$$$$ Without the comma, the model selected of as the most probable preposition following because, instead of from.

Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. $$$$$ When the classifier marked the preposition as incorrect in an ungrammatical context, it was credited with correctly detecting a preposition error.
Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. $$$$$ Levin (1993) catalogs verb alternations such as They loaded hay on the wagon vs.
Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. $$$$$ This suggests that a considerable improvement in performance can be achieved by using a more conservative approach based on a higher confidence level for the classifier.
Chodorow et al (2007) instead treat it as a classification problem and employed a maximum entropy classifier. $$$$$ Of the 500-600 errors in the ESL test set, 142 were errors of this type.

The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. $$$$$ Preposition errors are not only prominent among error types, they are also quite frequent in ESL writing.
The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. $$$$$ The overall proportion of agreement between Rater1 and Rater 2 was 0.926, and kappa was 0.599.
The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. $$$$$ As noted earlier, the table does not include the cases that the classifier skipped due to misspelling, antonymous prepositions, and benefactives.
The work of Chodorow et al (2007) and T & C 08 treat the tasks of preposition selection and error detection as a classification problem. $$$$$ Even though they reported a precision of 0.88 and recall of 0.78, their evaluation was on a very restricted domain with only a limited number of prepositions, nouns and verbs.

A context is represented by 25 lexical features and 4 combination features $$$$$ English has hundreds of phrasal verbs, consisting of a verb and a particle (some of which are also prepositions).
A context is represented by 25 lexical features and 4 combination features $$$$$ Even though they reported a precision of 0.88 and recall of 0.78, their evaluation was on a very restricted domain with only a limited number of prepositions, nouns and verbs.
A context is represented by 25 lexical features and 4 combination features $$$$$ To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.
A context is represented by 25 lexical features and 4 combination features $$$$$ While the training for the ME classifier was done on a separate corpus, and it was this classifier that contributed the most to the high precision, it should be noted that some of the filters were tuned on the evaluation corpus.

Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). $$$$$ The paper is structured as follows: in the next section, we describe the difficulty in learning English preposition usage; in Section 3, we discuss related work; in Sections 4-7 we discuss our methodology and evaluation.
Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). $$$$$ This corresponded to a situation in which two or more prepositions were likely to be found in a given context.
Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). $$$$$ Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.
Research on automatic grammar correction has been conducted on a number of different parts-of speech, such as articles (Knight and Chander, 1994) and prepositions (Chodorow et al, 2007). $$$$$ The goal of the research described here is to provide software for detecting common grammar and usage errors in the English writing of non-native English speakers.

Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). $$$$$ Plural Quantifier Constructions This filter addresses the second most common extraneous preposition error where the writer has added a preposition in the middle of a plural quantifier construction, for example: some ofpeople.
Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). $$$$$ To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.
Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). $$$$$ Although our work is preliminary, we achieve a precision of 0.8 with a recall of 0.3.
Automatic error detection has been performed on other parts-of-speech, e.g., articles (Knight and Chander, 1994) and prepositions (Chodorow et al,2007). $$$$$ Plural Quantifier Constructions This filter addresses the second most common extraneous preposition error where the writer has added a preposition in the middle of a plural quantifier construction, for example: some ofpeople.

We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. $$$$$ Results of the comparison between the classifier and the test set showed that the overall proportion of agreement between the text and the classifier was 0.69.
We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. $$$$$ This corresponded to a situation in which two or more prepositions were likely to be found in a given context.
We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al, 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. $$$$$ For each context, the model predicted the probability of each preposition given the contextual representation.

The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. $$$$$ English has hundreds of phrasal verbs, consisting of a verb and a particle (some of which are also prepositions).
The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. $$$$$ In particular, preposition usage is one of the most difficult aspects of English grammar for non-native speakers to master.
The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. $$$$$ Plural Quantifier Constructions This filter addresses the second most common extraneous preposition error where the writer has added a preposition in the middle of a plural quantifier construction, for example: some ofpeople.
The baseline system (described in (Chodorow et al, 2007)) performed at 79.8% precision and 11.7% recall. $$$$$ Results of the comparison between the classifier and the test set showed that the overall proportion of agreement between the text and the classifier was 0.69.

While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). $$$$$ For each context, the model predicted the probability of each preposition given the contextual representation.
While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). $$$$$ The goal of the research described here is to provide software for detecting common grammar and usage errors in the English writing of non-native English speakers.
While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). $$$$$ To address the problem of low recall, we have targeted another type of ESL preposition error: extraneous prepositions.
While this improvement may seem small, it is in part due to the difficulty of the problem, but also the high baseline system score that was established in our prior work (Chodorow et al., 2007). $$$$$ Our strategy of training the ME classifier on grammatical, edited text precluded detection of extraneous prepositions as these did not appear in the training corpus.

For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. $$$$$ The National Clearinghouse for English Language Acquisition (2002) estimates that 9.6% of the students in the US public school population speak a language other than English and have limited English proficiency.
For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. $$$$$ This corresponded to a situation in which two or more prepositions were likely to be found in a given context.
For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. $$$$$ The rules, which were based on the kinds of errors that were found in a training set of text produced by non-native Swedish writers, targeted spelling errors involving prepositions and some particularly problematic Swedish verbs.
For instance, in our previous work (Chodorow et al, 2007), we found that when our system's output was compared to judgments of two different raters, there was a 10% difference in precision and a 5% difference in recall. $$$$$ Preposition errors account for a significant proportion of all ESL grammar errors.

Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ More recently, (Lee and Seneff, 2006) used a language model and stochastic grammar to replace prepositions removed from a dialogue corpus.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ The National Clearinghouse for English Language Acquisition (2002) estimates that 9.6% of the students in the US public school population speak a language other than English and have limited English proficiency.
Typically, data-driven approaches to learner errors use a classifier trained on contextual information such as tokens and part-of-speech tags within a window of the preposition/article (Gamon et al 2008, 2010, DeFelice and Pulman 2007, 2008, Han et al 2006, Chodorow et al 2007, Tetreault and Chodorow 2008). $$$$$ Preposition errors are not only prominent among error types, they are also quite frequent in ESL writing.

Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). $$$$$ As noted earlier, the table does not include the cases that the classifier skipped due to misspelling, antonymous prepositions, and benefactives.
Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). $$$$$ They loaded the wagon with hay, which show that, depending on the verb, an argument may sometimes be marked by a preposition and sometimes not.
Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). $$$$$ Both precision and recall are low in these comparisons to the human raters.
Similarly, web-based models built on Google Web1T 5-gram Corpus (Bergsma et al., 2009) achieve better results when compared to a maximum entropy model that uses a corpus 10,000 times smaller (Chodorow et al, 2007). $$$$$ To address this problem, we use a maximum entropy classifier combined with rule-based filters to detect preposition errors in a corpus of student essays.

Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. $$$$$ This approach has been shown to perform well in combining heterogeneous forms of evidence, as in word sense disambiguation (Ratnaparkhi, 1998).
Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. $$$$$ In other words, we permitted it to respond only when it was confident of its decision.
Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. $$$$$ These 7 million prepositions and their contexts were extracted from the MetaMetrics corpus of 1100 and 1200 Lexile text (11th and 12th grade) and newspaper text from the San Jose Mercury News.
Chodorow et al (2007) present an approach to preposition error detection which also uses a model based on a maximum entropy classifier trained on a set of contextual features, together with a rule-based filter. $$$$$ Since adjuncts are optional and tend to be flexible in their position in a sentence, the task facing the learner is quite complex.

Preposition errors are common among new English speakers (Chodorow et al, 2007). $$$$$ PHR pre is the “preceding phrase” feature that indicates whether the preposition was preceded by a noun phrase (NP) or a verb phrase (VP).
Preposition errors are common among new English speakers (Chodorow et al, 2007). $$$$$ Some of the features represented the words and tags found at specific locations adjacent to the preposition; others represented the head words and tags of phrases that preceded or followed the preposition.
Preposition errors are common among new English speakers (Chodorow et al, 2007). $$$$$ We found similar results when comparing the judgements of the Classifier to Rater 2: agreement was high and kappa was low.
