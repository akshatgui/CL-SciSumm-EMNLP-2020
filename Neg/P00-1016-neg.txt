(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. $$$$$ Collection of data is neither easy nor cheap.
(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. $$$$$ Although several systems have recently achieved slightly higher published results (Munoz et al. : 92.8, Tjong Kim Sang & Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to implement in an active learning framework.
(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. $$$$$ Sets of 50 sentences were selected because it takes approximately 15-30 minutes for humans to annotate them, a reasonable amount of work and time for the annotator to spend before taking a break while the machine selects the next set.
(Ngai and Yarowsky, 2000) and (Ngai, 2001) provide a thorough description of many experiments involving rule-based systems and statistical learners for NP bracketing. $$$$$ Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.

Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. $$$$$ In general, these methods calculate the usefulness of an example by first having the learner classify it, and then seeing how uncertain that classification was.
Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. $$$$$ • Annotation-based learning can more effectively combine the efforts of multiple individuals.
Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. $$$$$ This section will analyze and compare the performance of systems constructed with hand-built rules with systems that were trained from data selected during real-time active learning.
Ngai and Yarowsky (2000) investigated the effectiveness of rule-writing versus annotation (using active learning) for chunking, and found the latter to be far more effective. $$$$$ Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.

Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. $$$$$ A significant amount of work has been done in this domain and many different methods have been applied: Church's PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen's NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan & Krymolowski (1998), Daelemans, van den Bosch & Zavrel (1999) and Tjong Kim Sang & Veenstra (1999) used memory-based systems; Ramshaw & Marcus (1999) and Cardie & Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar.
Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. $$$$$ This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.
Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. $$$$$ In a way that is similar to Brill & Ngai's system, our rules were translated into Perl regular expressions and evaluated on the corpus.
Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. $$$$$ This paper will estimate and contrast these costs relative to performance.

One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. $$$$$ This paper has illustrated that there are potentially compelling practical and performance advantages to pursuing active-learning based annotation rather than rule-writing to develop base noun phrase chunkers.
One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. $$$$$ On average, the annotators took 17 minutes to annotate each set of 50 sentences, ranging from 8 to 30 minutes.
One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. $$$$$ On average, the annotators took 17 minutes to annotate each set of 50 sentences, ranging from 8 to 30 minutes.
One empirical study (Ngai and Yarowsky, 2000) found that it also required more annotation time than active learning. $$$$$ The relative balance depends ultimately on one's cost model, but given the goal of minimizing total human labor cost, it appears to be consistently more efficient and effective to invest these human resources in systemdevelopment via annotation rather than rule writing.

The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). $$$$$ The f-complement disagreement measure was used to select 50 sentences from the rest of Ramshaw & Marcus' training set and the annotator was instructed to annotate them.
The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). $$$$$ Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). $$$$$ The relative balance depends ultimately on one's cost model, but given the goal of minimizing total human labor cost, it appears to be consistently more efficient and effective to invest these human resources in systemdevelopment via annotation rather than rule writing.
The f-complement has been suggested for active learning in the context of NP chunking as a structural comparison between the different analyses of a committee (Ngai and Yarowsky, 2000). $$$$$ The time that the students spent on the task varied widely, from a minimum of 1.5 hours to a maximum of 9 hours, with an average of 5 hours.

One exception is (Ngai and Yarowsky, 2000) (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking. $$$$$ Although several systems have recently achieved slightly higher published results (Munoz et al. : 92.8, Tjong Kim Sang & Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to implement in an active learning framework.

In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. $$$$$ The relative balance depends ultimately on one's cost model, but given the goal of minimizing total human labor cost, it appears to be consistently more efficient and effective to invest these human resources in systemdevelopment via annotation rather than rule writing.
In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. $$$$$ To facilitate contrastive studies, we have evaluated our active learning and cost model comparisons using Ramshaw & Marcus' system as the reference algorithm in these experiments.
In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. $$$$$ One of the primary problems that NLP researchers who work in new languages or new domains encounter is a lack of available annotated data.
In contrast to the model presented by Ngai and Yarowsky (2000), which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. $$$$$ However, for a new language, a similar investment of effort in time and money is most likely prohibitive, if not impossible.

Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking. $$$$$ Although several systems have recently achieved slightly higher published results (Munoz et al. : 92.8, Tjong Kim Sang & Veenstra: 92.37, XTAG Research Group: 92.4), their algorithms are significantly more costly, or not feasible, to implement in an active learning framework.
Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking. $$$$$ To facilitate contrastive studies, we have evaluated our active learning and cost model comparisons using Ramshaw & Marcus' system as the reference algorithm in these experiments.
Ngai and Yarowsky (2000) used an ensemble based on bagging and partitioning for active learning for base NP chunking. $$$$$ This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.

In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). $$$$$ This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.
In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). $$$$$ The question we are trying to address then is: for a given cost assumption, which approach would be the most effective.

AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ A significant amount of work has been done in this domain and many different methods have been applied: Church's PARTS (1988) program used a Markov model; Bourigault (1992) used heuristics along with a grammar; Voutilainen's NPTool (1993) used a lexicon combined with a constraint grammar; Juteson and Katz (1995) used repeated phrases; Veenstra (1998), Argamon, Dagan & Krymolowski (1998), Daelemans, van den Bosch & Zavrel (1999) and Tjong Kim Sang & Veenstra (1999) used memory-based systems; Ramshaw & Marcus (1999) and Cardie & Pierce (1998) used rule-based systems, Munoz et al. (1999) used a Winnow-based system, and the XTAG Research Group(1998) used a tree-adjoining grammar.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ • Finally, the current performance of annotation-based training is only a lower bound based on the performance of current learning algorithms.
AL has been successfully applied already for a wide range of NLP tasks, including POS tagging (Engelson and Dagan, 1996), chunking (Ngai and Yarowsky, 2000), statistical parsing (Hwa, 2004), and named entity recognition (Tomanek et al, 2007). $$$$$ Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.

These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. $$$$$ The relative balance depends ultimately on one's cost model, but given the goal of minimizing total human labor cost, it appears to be consistently more efficient and effective to invest these human resources in systemdevelopment via annotation rather than rule writing.
These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. $$$$$ Of all the systems, Ramshaw & Marcus' transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain.
These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. $$$$$ The rules presented here may be considered less cumbersome and more intuitive.
These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al, 2008a; Settles et al, 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. $$$$$ Of all the systems, Ramshaw & Marcus' transformation rule-based system had the best published performance (f-measure 92.0) for several years, and is regarded as the de facto standard for the domain.

In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. $$$$$ This paper has illustrated that there are potentially compelling practical and performance advantages to pursuing active-learning based annotation rather than rule-writing to develop base noun phrase chunkers.
In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. $$$$$ New rules are appended onto the end of the list and each rule applied in order, in the paradigm of a transformation-based rule list.
In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. $$$$$ In bagging, we randomly sentences selected by active learning and annotated sentences selected sequentially shows that active learning reduces the amount of data needed to reach a given level of performance by approximately a factor of two.
In their study on AL for base noun phrase chunking, Ngai and Yarowsky (2000) compare the costs of rule-writing with (AL-driven) annotation to compile a base noun phrase chunker. $$$$$ This paper has illustrated that there are potentially compelling practical and performance advantages to pursuing active-learning based annotation rather than rule-writing to develop base noun phrase chunkers.
