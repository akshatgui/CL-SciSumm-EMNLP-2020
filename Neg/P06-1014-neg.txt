However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). $$$$$ These words had 2,600 senses in the ODE.
However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). $$$$$ Most of the approaches in the literature make use of the WordNet structure to cluster its senses.
However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). $$$$$ Also, most of the present research and standard data sets focus on WordNet.
However, in the coarse-grained task, the sense inventory was first clustered semi-automatically with each cluster representing an equivalence class over senses (Navigli, 2006). $$$$$ To this end, we aim to set up an Open Mind-like experiment for the validation of the entire mapping from WordNet to ODE, so that only a minimal error rate would affect the experiments to come.

Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarse grained sense distinctions of the Oxford Dictionary of English (ODE). $$$$$ The increase in recall is mostly due to the fact that different senses belonging to the same cluster now contribute together to the choice of that cluster (rather than individually to the choice of a fine-grained sense).
Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarse grained sense distinctions of the Oxford Dictionary of English (ODE). $$$$$ A semantic interconnection pattern is a relevant sequence of edges selected according to a manually-created context-free grammar, i.e. a path connecting a pair of word senses, possibly including a number of intermediate concepts.
Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarse grained sense distinctions of the Oxford Dictionary of English (ODE). $$$$$ One could argue that the adoption of the ODE as a sense inventory for WSD would be a better solution.
Navigli (2006) proposed an automatic approach for mapping WordNet senses to the coarse grained sense distinctions of the Oxford Dictionary of English (ODE). $$$$$ They study semantic regularities or generalizations obtained and analyze the effect of clustering on the compatibility of language-specific wordnets.

The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavaglia`, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). $$$$$ As we are experimenting on an automaticallyacquired clustering, all the figures are affected by the 22.06% error rate resulting from Table 2.
The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavaglia`, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). $$$$$ First, for each dictionary D E {WORDNET, ODE}, and for each sense S E SensesD(w), the sense description of S is disambiguated by applying SSI to dD(S).
The classifier also made use of resources such as topic signatures data (Agirre and de Lacalle, 2004), the WordNet domain dataset (Magnini and Cavaglia`, 2000), and the mappings of WordNet senses to ODE senses produced by Navigli (2006). $$$$$ The overall purity of a clustering is obtained as a weighted sum of the individual cluster purities: We calculated the entropy and purity of the clustering produced automatically with the lexical and the semantic method, when compared to the grouping induced by our manual mapping (ODE), and to the grouping manually produced for the English all-words task at Senseval-2 (3,499 senses of 403 nouns).

In addition, we show in Table 7 the F-score results provided by Snow et al (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE. $$$$$ To evaluate the performance of the five systems on our coarse clustering, we considered a fine-grained answer to be correct if it belongs to the same cluster as that of the correct answer.
In addition, we show in Table 7 the F-score results provided by Snow et al (2007) for their SVM-based system and for the mapping-based approach of Navigli (2006), denoted by ODE. $$$$$ Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation.

Navigli (2006) has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource. $$$$$ For each word w, and for each sense S of w in a given dictionary D E {WORDNET, ODE}, we construct a sense description dD(S) as a bag of words: Specifically, in the case of WordNet, we generate def WN(S) from the gloss of S, hyperWN(S) from the noun and verb taxonomy, and domainsWN(S) from the subject field codes, i.e. domain labels produced semi-automatically by Magnini and Cavagli`a (2000) for each WordNet synset (we exclude the general-purpose label, called FACTOTUM).
Navigli (2006) has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource. $$$$$ Following these observations, the main question that we tackle in this paper is: can we produce and evaluate coarse-grained sense distinctions and show that they help boost disambiguation on standard test sets?
Navigli (2006) has induced clusters by mapping WordNet senses to a more coarse-grained lexical resource. $$$$$ Unfortunately, the very same concept can be defined with entirely different words.

(Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. $$$$$ We also disabled the first sense baseline heuristic, that most of the systems use as a back-off when they have no information about the word at hand.
(Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. $$$$$ All the differences between Lesk and SSI are statistically significant (p < 0.01).
(Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. $$$$$ Mihalcea and Moldovan (2001) study the structure of In this paper, we presented a study on the construction of a coarse sense inventory for the WordNet lexicon and its effects on unrestricted WSD.
(Navigli, 2006) presents an automatic approach for mapping between sense inventories; here similarities in gloss definition and structured relations between the two sense inventories are exploited in order to map between WordNet senses and distinctions made within the coarser-grained Oxford English Dictionary. $$$$$ In a future work, we plan to investigate the contribution of coarse disambiguation to such real-world applications.

Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses. $$$$$ In the example, group#n, the hypernym of race#n#2, is also present in the definition of race#n#1.1.
Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses. $$$$$ The average Cohen’s κ agreement between the two annotators was 0.874.
Finally, we use as a feature the mappings produced in (Navigli, 2006) of WordNet senses to Oxford English Dictionary senses. $$$$$ We evaluated the accuracy of the mapping produced with the lexical and semantic methods described in Sections 2.3.1 and 2.3.2, respectively.

In order to evaluate the entire sense-clustered taxonomy, we have employed an evaluation method inspired by Word Sense Disambiguation (this is similar to an evaluation used in Navigli, 2006, however we do not remove monosemous clusters). $$$$$ Indeed, such an evaluation would be difficult and timeconsuming without a coarse sense inventory like that of ODE.
In order to evaluate the entire sense-clustered taxonomy, we have employed an evaluation method inspired by Word Sense Disambiguation (this is similar to an evaluation used in Navigli, 2006, however we do not remove monosemous clusters). $$$$$ We call this new setting SSI* (as opposed to SSI used in Table 4).
In order to evaluate the entire sense-clustered taxonomy, we have employed an evaluation method inspired by Word Sense Disambiguation (this is similar to an evaluation used in Navigli, 2006, however we do not remove monosemous clusters). $$$$$ We believe that this is a crucial research topic in the field of WSD, that could potentially benefit several application areas.
In order to evaluate the entire sense-clustered taxonomy, we have employed an evaluation method inspired by Word Sense Disambiguation (this is similar to an evaluation used in Navigli, 2006, however we do not remove monosemous clusters). $$$$$ Second, we evaluate the performance of WSD systems when using coarse-grained sense inventories (Section 4).

To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory based on the procedure described by Navigli (2006). $$$$$ Overall, 4,599 out of the 5,077 WordNet senses had a corresponding sense in ODE (i.e. the ODE covered 90.58% of the WordNet senses in the data set), while 2,053 out of the 2,600 ODE senses had an analogous entry in WordNet (i.e.
To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory based on the procedure described by Navigli (2006). $$$$$ Given a word w, and a sense group G E c(w), the entropy of G is defined as: i.e., the entropy4 of the distribution of senses of group G over the groups of the manual clustering ˆc(w).
To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory based on the procedure described by Navigli (2006). $$$$$ The baseline was computed averaging among 200 random clustering solutions for each word.
To tackle the granularity issue, we produced a coarser-grained version of the WordNet sense inventory based on the procedure described by Navigli (2006). $$$$$ We modified the sense inventory of the SSI lexical knowledge base by adopting the coarse inventory acquired automatically.

The data were annotated with coarse-grained senses which were obtained by clustering senses from the Word Net 2.1 sense inventory based on the procedure proposed by Navigli (2006). $$$$$ Furthermore, disregarding errors, the clustering would be well-founded, as the ODE sense groupings were manually crafted by expert lexicographers.
The data were annotated with coarse-grained senses which were obtained by clustering senses from the Word Net 2.1 sense inventory based on the procedure proposed by Navigli (2006). $$$$$ As a second experiment, we used two information-theoretic measures, namely entropy and purity (Zhao and Karypis, 2004), to compare an automatic clustering c(w) (i.e. the sense groups acquired for word w) with a manual clustering ˆc(w).
The data were annotated with coarse-grained senses which were obtained by clustering senses from the Word Net 2.1 sense inventory based on the procedure proposed by Navigli (2006). $$$$$ These figures exclude monosemous senses and derivatives in WordNet.
The data were annotated with coarse-grained senses which were obtained by clustering senses from the Word Net 2.1 sense inventory based on the procedure proposed by Navigli (2006). $$$$$ We show that this method is well-founded and accurate with respect to manually-made clusterings (Section 3).

 $$$$$ They study semantic regularities or generalizations obtained and analyze the effect of clustering on the compatibility of language-specific wordnets.
 $$$$$ These words had 2,600 senses in the ODE.
 $$$$$ Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer, 2004) and 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002).
 $$$$$ Another approach exploits the (dis)agreements of human annotators to derive coarse-grained sense clusters (Chklovski and Mihalcea, 2003), where sense similarity is computed from confusion matrices.

This clustering was created automatically with the aid of a methodology described in (Navigli, 2006). $$$$$ The clustering is evaluated against WordNet cousins and used for the study of inter-annotator disagreement.
This clustering was created automatically with the aid of a methodology described in (Navigli, 2006). $$$$$ We excluded monosemous clusters from the test set (i.e. words with all the senses mapped to the same ODE entry), so as to clarify the real impact of properly grouped clusters.
This clustering was created automatically with the aid of a methodology described in (Navigli, 2006). $$$$$ Finally, we define the clusteri In Sections 2.3.1 and 2.3.2 we describe two different choices for the match function, respectively based on the use of lexical and seman where match : SensesWNxSensesODE → [0, 1] is a function that measures the degree of matching between the sense descriptions of S and S'.
This clustering was created automatically with the aid of a methodology described in (Navigli, 2006). $$$$$ We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task.

 $$$$$ This task required WSD systems to provide a sense choice for 2,081 content words in a set of 301 sentences from the fiction, news story, and editorial domains.
 $$$$$ Overall, 4,599 out of the 5,077 WordNet senses had a corresponding sense in ODE (i.e. the ODE covered 90.58% of the WordNet senses in the data set), while 2,053 out of the 2,600 ODE senses had an analogous entry in WordNet (i.e.
 $$$$$ Vickrey et al. (2005) and Stokoe (2005)), the present performance of the best ranking WSD systems does not provide a sufficient degree of accuracy to enable real-world, language-aware applications.
 $$$$$ In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English.

Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006). $$$$$ As remarked in Section 2.2, the method can employ any dictionary with a sufficiently structured inventory of senses, and can thus be applied to reduce the granularity of, e.g., wordnets of other languages.
Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006). $$$$$ The data set was created by two annotators and included only polysemous words.
Automatically creating new alignments is difficult because of word ambiguities, different granularities of senses, or language specific conceptualizations (Navigli, 2006). $$$$$ Finally, McCarthy (2006) proposes the use of ranked lists, based on distributionally nearest neighbours, to relate word senses.
