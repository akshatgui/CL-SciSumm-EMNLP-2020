Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ We can partially address this by cross-validation.
Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ A pointwise sampler resamples a single state ti (labeling a single word wi) at each step, while a blocked sampler resamples the labels for all of the words in a sentence at a single step using a dynamic programming algorithm based on the Forward-Backward algorithm.
Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ Besag (2004) provides a tutorial on MCMC techniques for HMM inference.

We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ We insert endmarkers at the beginning and end of the corpus and between sentence boundaries, and constrain the estimators to associate endmarkers with a special HMM state that never appears elsewhere in the corpus (we ignore these endmarkers during evaluation).
We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank.
We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.

All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ Then it uses the new HMM parameters to resample the states t for the training corpus using the algorithm just mentioned.
All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ We leave it to future work.
All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech.

Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study.
Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ This can be done in parallel for each sentence in the training corpus.
Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.

The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ This means that a single iteration can be performed in O(nm2) time, just as for the EM algorithm.
The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ Variational Bayes converges faster than all of the other estimators we examined here.
The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ Recent papers have given contradictory results when comparing Bayesian estimators to Expectation Maximization (EM) for unsupervised HMM POS tagging, and we show that the difference in reported results is largely due to differences in the size of the training data and the number of states in the HMM.

[cross val] $$$$$ But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used.
[cross val] $$$$$ In this section we compare their performance for English part-ofspeech tagging.
[cross val] $$$$$ In more detail, the HMM is specified by a pair of multinomials θt and φt associated with each state t, where θt specifies the distribution over states t0 following t and φt specifies the distribution over words w given state t. The Bayesian model we consider here puts a fixed uniform Dirichlet prior on these multinomials.

 $$$$$ There is growing interest in applying Bayesian techniques to NLP problems.
 $$$$$ Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.
 $$$$$ Probabilistic models now play a central role in computational linguistics.

We selected the three evaluation criteria of Gao and Johnson (2008) $$$$$ We make a “mean-field” assumption that the posterior can be well approximated by a factorized model Q in which the state sequence t does not covary with the model parameters θ, φ: The calculus of variations is used to minimize the KL divergence between the desired posterior distribution and the factorized approximation.
We selected the three evaluation criteria of Gao and Johnson (2008) $$$$$ There is growing interest in applying Bayesian techniques to NLP problems.
We selected the three evaluation criteria of Gao and Johnson (2008) $$$$$ This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.
We selected the three evaluation criteria of Gao and Johnson (2008) $$$$$ In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.

M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.
M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.
M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ However, the Bayesian literature describes a number of methods for approximating the posterior P(0  |d).
M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ We found that the speed of convergence of our samplers depends to a large degree upon the values of the hyperparameters α and α', with larger values leading to much faster convergence.

To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used.
To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ The samplers that Goldwater and Griffiths (2007) and Johnson (2007) describe are pointwise collapsed Gibbs samplers.

 $$$$$ We call the accuracy of the resulting tagging the crossvalidation accuracy.
 $$$$$ If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state.
 $$$$$ These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t  |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods.
 $$$$$ We ran each estimator with the eight different combinations of values for the hyperparameters α and α' listed below, which include the optimal values for the hyperparameters found by Johnson (2007), and report results for the best combination for each estimator below 1.

We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.
We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ Following these authors, we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 Then we use the dynamic programming sampler deaccuracy.
We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ A pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm2) time per iteration, where m is the number of HMM states and n is the length of the training corpus.
We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ This means that a single iteration can be performed in O(nm2) time, just as for the EM algorithm.

They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ We leave it to future work.
They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ We can partially address this by cross-validation.
They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ But on larger data sets, which Goldwater et al did not study, the results are much less clear, and depend on which evaluation measure is used.
They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ However, as the size of the training data distribution increases the likelihood function and therefore the posterior distribution becomes increasingly peaked, so one would expect this variational approximation to become increasingly accurate.

We experimented with the following models $$$$$ There is growing interest in applying Bayesian techniques to NLP problems.
We experimented with the following models $$$$$ A Gibbs sampler is a simple kind of MCMC algorithm that is well-suited to sampling highdimensional spaces.
We experimented with the following models $$$$$ Figure 6 shows that pointwise-samplers initially converge faster, but are overtaken later by the blocked samplers.
We experimented with the following models $$$$$ The collapsed blocked Gibbs sampler is a straight-forward application of the Metropoliswithin-Gibbs approach proposed by Johnson et al. (2007) for PCFGs, so we only sketch it here.

For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ 1We found that on some data sets the results are sensitive to the values of the hyperparameters.
For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).

 $$$$$ There is growing interest in applying Bayesian techniques to NLP problems.
 $$$$$ In virtually all statistical approaches the parameters 0 are chosen or estimated on the basis of training data d. This paper studies unsupervised estimation, so d = w = (w1, ... , wn) consists of a sequence of words wi containing all of the words of training corpus appended into a single string, as explained below.
 $$$$$ In order to avoid this problem we focus here on evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMM’s states.

 $$$$$ This is perhaps not too surprising, as the Bayesian prior plays a comparatively stronger role with a smaller training corpus (which makes the likelihood term smaller) and the approximation used by Variational Bayes is likely to be less accurate on smaller data sets.
 $$$$$ We can partially address this by cross-validation.
 $$$$$ Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.
 $$$$$ A Maximum Likelihood estimator sets the parameters to the value 0� that makes the likelihood Ld of the data d as large as possible: In this paper we use the Inside-Outside algorithm, which is a specialized form of ExpectationMaximization, to find HMM parameters which (at least locally) maximize the likelihood function Ld.

Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well).
Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ These models define a probability distribution P(x) over structures or analyses x.
Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.

Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ Inspired by this, one can devise hybrid strategies that interleave blocked and pointwise sampling; these might perform better than both the blocked and pointwise samplers described here.
Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.
Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ The results of our experiments are summarized in Figures 2–5.

Each of these techniques provide significant improvements over the standard HMM model $$$$$ However, in our experiments the best results are obtained in most settings with small values for α and α', usually between 0.1 and 0.0001.
Each of these techniques provide significant improvements over the standard HMM model $$$$$ In order to avoid this problem we focus here on evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMM’s states.
Each of these techniques provide significant improvements over the standard HMM model $$$$$ Informally, α controls the sparsity of the state-to-state transition probabilities while α0 controls the sparsity of the state-toword emission probabilities.
Each of these techniques provide significant improvements over the standard HMM model $$$$$ In all but the simplest models there is no known closed form for the posterior distribution.
