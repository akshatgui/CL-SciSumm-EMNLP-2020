Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ On small data sets all of the Bayesian estimators strongly outperform EM (and, to a lesser extent, VB) with respect to all of our evaluation measures, confirming the results reported in Goldwater and Griffiths (2007).
Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ We found that the speed of convergence of our samplers depends to a large degree upon the values of the hyperparameters α and α', with larger values leading to much faster convergence.
Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. $$$$$ In practice, with all but the very smallest training corpora the acceptance rate is very high; the acceptance rate for all of our collapsed blocked Gibbs samplers was over 99%.

We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others.
We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ An iteration of the pointwise explicit Gibbs sampler consists of resampling 0 and 0 given the stateto-state transition counts n and state-to-word emission counts n0 using (5), and then resampling each state ti given the corresponding word wi and the neighboring states ti−1 and ti+1 using (6).
We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ At each iteration the explicit blocked Gibbs sampler resamples 0 and 0 using (5), just as the explicit pointwise sampler does.
We use a Gibbs sampler (Gao and Johnson, 2008) to learn the parameters of this and all other models under consideration. $$$$$ So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others.

All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ We make a “mean-field” assumption that the posterior can be well approximated by a factorized model Q in which the state sequence t does not covary with the model parameters θ, φ: The calculus of variations is used to minimize the KL divergence between the desired posterior distribution and the factorized approximation.
All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ A pointwise sampler resamples a single state ti (labeling a single word wi) at each step, while a blocked sampler resamples the labels for all of the words in a sentence at a single step using a dynamic programming algorithm based on the Forward-Backward algorithm.
All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). $$$$$ In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.

Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ Variational Bayes converges faster than all of the other estimators we examined here.
Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. $$$$$ In general, the E-step for VB inference for HMMs is the same as in EM, while the M-step is as follows: where m0 and m are the number of word types and states respectively, IF is the digamma function and the remaining quantities are as in (2).

The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ We invesigate a variety of samplers for HMMs, including some that these earlier papers did not study.
The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ Then it uses the new HMM parameters to resample the states t for the training corpus using the algorithm just mentioned.
The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ This paper compares a variety of different Bayesian estimators for Hidden Markov Model POS taggers with various numbers of hidden states on data sets of different sizes.
The approximation can be corrected using the Metropolis-Hastings algorithm, in which the sample drawn from the proposal lattice is accepted only with a certain probability a; but Gao and Johnson (2008) report that a $$$$$ We can partially address this by cross-validation.

[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. $$$$$ We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.
[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. $$$$$ (In principle it is possible to use block sizes other than the sentence, but we did not explore this here).
[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. $$$$$ However, the Bayesian literature describes a number of methods for approximating the posterior P(0  |d).
[cross val]: Cross-validation accuracy (Gao and Johnson, 2008) is intended to address the problem with many-to-one accuracy which is that assigning each word to its own class yields a perfect score. $$$$$ However, as the size of the training data distribution increases the likelihood function and therefore the posterior distribution becomes increasingly peaked, so one would expect this variational approximation to become increasingly accurate.

 $$$$$ Maximum Likelihood (ML) is the most common estimation method in computational linguistics.
 $$$$$ There is growing interest in applying Bayesian techniques to NLP problems.
 $$$$$ A Bayesian approach uses Bayes theorem to factorize the posterior distribution P(0  |d) into the Priors can be useful because they can express preferences for certain types of models.
 $$$$$ We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.

We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. $$$$$ For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank.
We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. $$$$$ As α0 approaches zero the prior strongly prefers models in which each state emits as few words as possible, capturing the intuition that most word types only belong to one POS mentioned earlier.
We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. $$$$$ This is done by first computing the parameters 0* and 0* of a proposal HMM using (7). scribed above to produce a proposal state sequence t* for the words in the sentence.

M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ Variational Bayes converges faster than all of the other estimators we examined here.
M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ Second, the sampler can either be explicit or collapsed.
M-to-1 and 1-to-1 are the tagging accuracies under the best many-to-one map and the greedy one-to-one map respectively; VI is a map-free information theoretic criterion - see Gao and Johnson (2008) for details. $$$$$ In virtually all statistical approaches the parameters 0 are chosen or estimated on the basis of training data d. This paper studies unsupervised estimation, so d = w = (w1, ... , wn) consists of a sequence of words wi containing all of the words of training corpus appended into a single string, as explained below.

To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ A Bayesian approach uses Bayes theorem to factorize the posterior distribution P(0  |d) into the Priors can be useful because they can express preferences for certain types of models.
To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ So, there is a bit uncertainty in our comparison results because it is possible that the values we tried were good for one estimator and bad for others.
To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ Maximum Likelihood (ML) is the most common estimation method in computational linguistics.
To further gain insight into how successful current models are at disambiguating when they have the power to do so, we examined a collection of HMM-VB runs (Gao and Johnson 2008) and asked how the accuracy scores would change if, after training was completed, the model were forced to assign the same label to all tokens of the same type. $$$$$ Further, we ran each setting of each estimator at least 10 times (from randomly jittered initial starting points) for at least 1,000 iterations, as Johnson (2007) showed that some estimators require many iterations to converge.

 $$$$$ Also, the studies differed in the size of the corpora used.
 $$$$$ A Bayesian approach uses Bayes theorem to factorize the posterior distribution P(0  |d) into the Priors can be useful because they can express preferences for certain types of models.
 $$$$$ There is growing interest in applying Bayesian techniques to NLP problems.
 $$$$$ We leave it to future work.

We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ Specifically, if the parameter estimate at iteration ` is (θ(`), φ(`)), then the re-estimated parameters at itwhere n0w,t is the number of times word w occurs with state t, nt,,t is the number of times state t0 follows t and nt is the number of occurences of state t; all expectations are taken with respect to the model (θ(`),φ(`)).
We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ This paper compares the performance of four different kinds of Gibbs samplers, Variational Bayes and Expectation Maximization on unsupervised POS tagging problems of various sizes.
We tuned the prior using the same set of 8 value pairs suggested by Gao and Johnson (2008), using a held out set of POS-tagged CDS to evaluate final performance. $$$$$ There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.

They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ Expectation-Maximization is a procedure that iteratively re-estimates the model parameters (θ, φ), converging on a local maximum of the likelihood.
They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ A pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm2) time per iteration, where m is the number of HMM states and n is the length of the training corpus.
They are also competitive with Bayesian estimators, on larger data sets, with cross-validation (Gao and Johnson, 2008). $$$$$ We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech.

We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models). $$$$$ We evaluate four different Gibbs samplers in this paper, which vary along two dimensions.
We experimented with the following models: ARR10 (Abend et al,2010), Clark03 (Clark, 2003), GG07 (Goldwater and Griffiths, 2007), GJ08 (Gao and Johnson, 2008), and GVG09 (Van Gael et al, 2009) (three models). $$$$$ Thus for HMMs with many states it is possible to perform one or two orders of magnitude more iterations of the point-wise Gibbs samplers in the same run-time as the other samplers, so it is plausible that they would yield better results.

For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ It turns out that if the likelihood and conjugate prior belong to exponential families then the optimal Q1 and Q2 do too, and there is an EM-like iterative procedure that finds locally-optimal model parameters (Bishop, 2006).
For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ The results of our experiments are summarized in Figures 2–5.
For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ Then it uses the new HMM parameters to resample the states t for the training corpus using the algorithm just mentioned.
For example, Gao and Johnson (2008) proposed to induce a many-to-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. $$$$$ To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well).

 $$$$$ There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.
 $$$$$ The upper bound (3) is called the Variational Free Energy.
 $$$$$ The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used.
 $$$$$ Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.

 $$$$$ These models define a probability distribution P(x) over structures or analyses x.
 $$$$$ Variational Bayes converges faster than all of the other estimators we examined here.
 $$$$$ Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag.
 $$$$$ We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set).

Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set.
Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ Maximum Likelihood (ML) is the most common estimation method in computational linguistics.
Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ One of the primary motivations for this paper was to understand and resolve the difference in these results.
Note that LDC significantly outperforms all HMMs (Gao and Johnson, 2008) in every case except PTB45 under the OTO mapping. $$$$$ There are a number of different estimators for Bayesian models, and it is useful to know what kinds of tasks each does well on.

Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used.
Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank.
Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ Expectation Maximization does surprisingly well on larger data sets and is competitive with the Bayesian estimators at least in terms of cross-validation accuracy, confirming the results reported by Johnson (2007).
Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. $$$$$ This means that we can formally treat the training corpus as one long string, yet each sentence can be processed independently by a firstorder HMM.

Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy. $$$$$ The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank.
Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy. $$$$$ This paper compares the performance of four different kinds of Gibbs samplers, Variational Bayes and Expectation Maximization on unsupervised POS tagging problems of various sizes.
Each of these techniques provide significant improvements over the standard HMM model: for example Gao and Johnson (2008) show that sparse priors can gain from 4% (.62 to .66 with a 1M word corpus) in cross-validated many to-one accuracy. $$$$$ We find that all of Gibbs samplers do well with small data sets and few states, and that Variational Bayes does well on large data sets and is competitive with the Gibbs samplers.
