Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores.
Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ Moreover, out-of-domain corpora introduce additional difficulties related to limited dictionary coverage.
Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus.

Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. $$$$$ The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations.
Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. $$$$$ Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.
Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. $$$$$ We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
Sentence-level filter: The word-overlap filtering (Munteanu and Marcu, 2005) has been implemented: for a sentence pair (S, T) to be considered parallel the ratio of the lengths of the two sentences has to be smaller than two. $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.

The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data.
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ The task can be easily parallelized for increased speed.
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ The evaluation methodologies of these previous approaches are less direct than ours.

Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ All the operations necessary for the classification of a sentence pair (filter, word alignment computation, and feature extraction) can be implemented efficiently and scaled up to very large amounts of data.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ For example, extracting data from 600k English documents and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ In such situations, instead of arbitrarily choosing the first instance or a random instance, we attempt to make a ”smarter” decision.

In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7).
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments.

Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ We also measured the MT performance impact of the extracted corpora described in Section 4.2.
Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ As mentioned in Section 1, our goal is to use the extracted data as additional MT training data and obtain better translation performance on a given in-domain MT test set.
Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ Both numbers are expressed as percentages.
Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ In contrast, our method is more robust.

Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus.
Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system.
Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ (and evaluated) an MT system on the initial data plus the data extracted in that iteration.

In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ Its impact on MT performance is comparable to that of human-translated data of similar size and domain.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ However, identifying good translations in comparable corpora is hard.

We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ We iterated until there were no further improvements in MT performance on our development data.
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles).
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus.
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ In an automatically learned dictionary, many words (especially the frequent, non-content ones) will have a lot of spurious translations.

One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ Figures 12 and 13 show the BLEU scores of these MT systems.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ The sentences in that example are tables of soccer team statistics.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents.

In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ 2 If such a resource is unavailable, other dictionaries can be used.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ Zhao and Vogel (2002) go one step further and show that the sentences extracted with their method improve the accuracy of automatically computed word alignments, to an F-score of 52.56% over a baseline of 46.46%.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ We are aware of only three previous efforts aimed at discovering parallel sentences.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ The filter verifies that the ratio of the lengths of the two sentences is no greater than two.

Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc).
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ Iteration 0 is the one that uses the dictionary learned from the initial corpus.
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ Thus, if two sentences share several content words, these incorrect links together with the correct links between the common content words will yield an alignment good enough to make the classifier judge the sentence pair as parallel.
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation.

Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ As explained in Section 2 (and shown in Figure 2), when extracting data from a comparable corpus, we only apply the classifier on the output of the word-overlap filter.
Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001).

Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ Pairs that do not fulfill these two conditions are discarded.
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ The comparable corpora that we use for parallel sentence extraction are collections of news stories published by the Agence France Presse and Xinhua News agencies.
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations.
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ The results are presented in the first two rows of Table 7.

The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances.
The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ The computational processes involved in our system are quite modest.
The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.

In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. $$$$$ The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. $$$$$ Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. $$$$$ We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005): pairing all source phrases with all target phrases, but filter out the parallel pairs and those that have high length difference or a low lexical overlap, and then randomly select a subset of phrase pairs as the negative training set. $$$$$ The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words.

Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again.
Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring.
Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.

This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ Figures 10 and 11 show the sizes of the data extracted at each iteration, for both initial corpus sizes.
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7–10 BLEU points of systems trained on parallel corpora of more than 100M English tokens.
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ In contrast, our method is more robust.
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6.

We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ We want to align a large number of sentences, with many out-of-vocabulary words, in reasonable time.
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ This step of the process emphasizes recall rather than precision.
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ In the experiments described in Section 3.4, we started out with five out-of-domain initial parallel corpora of various sizes and obtained five dictionaries and five out-ofdomain trained classifiers (per language pair).
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments.

In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2. $$$$$ We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing.
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2. $$$$$ Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system.
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used: the length ratio of source and target sentence has to be smaller than 2. $$$$$ Thus, if we start with very little parallel data, we do not make good use of the comparable corpora.
