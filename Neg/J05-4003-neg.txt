Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system.
Nevertheless, very little of that text seems to be genuinely parallel, although recent work (Munteanu and Marcu, 2005) indicates that true parallelism may not be required for some tasks, eg machine translation, in order to gain acceptable results. $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.

Sentence-level filter $$$$$ Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring.
Sentence-level filter $$$$$ Another problem is that the large majority of sentence pairs in the Cartesian product have low word overlap (i.e., few words that are translations of each other).
Sentence-level filter $$$$$ As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size.

The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ Each query is thus run against fewer documents, so it becomes faster and has a better chance of getting the right documents at the top.
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ The resources required by the system are minimal: a bilingual dictionary and a small amount of parallel data (used for training the ME classifier).
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations.
The baseline uses only the length-based filtering and the coverage filtering without caching the coverage decisions (Munteanu and Marcu, 2005). $$$$$ Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.

Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data.
Currently, we are working on a feature-rich approach (Munteanu and Marcu, 2005) to improve the sentence-pair selection accuracy. $$$$$ One such model is the IBM Model 1 (Brown et al. 1993).

In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ It is important to note that our work assumes that the comparable corpus does contain parallel sentences (which is the case for our data).
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ And, most importantly, we are able to reliably judge each sentence pair in isolation, without need for context.
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). $$$$$ The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7.

Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance.
Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system.
Munteanu and Marcu (2005) use publication date and vector-based similarity (after projecting words through a bilingual dictionary) to identify similar news articles. $$$$$ Any remaining errors are of course our own.

Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ Clearly, we have access to no UpperBound system in this case.
Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data.
Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based on a bilingual dictionary). $$$$$ We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.

In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus.
In both the binary classifier approach and the ranking approach, we use a Maximum Entropy classifier, following Munteanu and Marcu (2005). $$$$$ We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.

We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ In such pairs, one of the sentences contains a transSizes of the Chinese-English corpora extracted using bootstrapping, in millions of English tokens.
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ We create training instances for our classifier from a small parallel corpus.
We use a feature set inspired by (Munteanu and Marcu, 2005), who defined features primarily based on IBM Model 1 alignments (Brown et al, 1993). $$$$$ We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.

One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ We iterated until there were no further improvements in MT performance on our development data.
One approach that begins to address this problem is the use of self-training, as in (Munteanu and Marcu, 2005). $$$$$ Two sentences may share many content words and yet express different meanings (see Figure 14, example 1).

In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ (and evaluated) an MT system on the initial data plus the data extracted in that iteration.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ The second concerns pairs in which the two sentences convey different amounts of information.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms.
In addition, machine translation (MT) systems can be improved by training on sentences extracted from parallel or comparable documents mined from the Web (Munteanu and Marcu, 2005). $$$$$ We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.

Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data.
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ In a subsequent publication, Vogel (2003) evaluates these sentences in the context of an MT system and shows that they bring improvement under special circumstances (i.e., a language model constructed from reference translations) designed to reduce the noise introduced by the automatically extracted corpus.
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context.
Others extract parallel sentences from comparable or non-parallel corpora (Munteanu and Marcu 2005, 2006). $$$$$ This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276.

Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ In order to get as many articles as possible, we used the web site’s search engine to get lists of articles and their URLs, and then crawled those lists.
Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Alignments computed using this model and a noisy, automatically learned, dictionary will contain many incorrect links.
Cross-lingual information retrieval methods (Munteanu and Marcu, 2005) and other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc.

Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance.
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ We train a Baseline MT system on that initial corpus.
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ The performance of these approaches depends heavily on the ability to reliably find similar document pairs.
Such classifiers have been used in the past to detect parallel sentence pairs in large collections of comparable documents (Munteanu and Marcu, 2005). $$$$$ Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms.

The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ 2 If such a resource is unavailable, other dictionaries can be used.
The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.
The feature set we use is inspired by Munteanu and Marcu (2005) who define the features based on IBM Model-1 (Brown et al, 1993) alignments for source and target pairs. $$$$$ We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.

In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005) $$$$$ We also want a model with as few parameters as possible—preferably only wordfor-word translation probabilities.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005) $$$$$ The others are passed on to the parallel sentence selection stage.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005) $$$$$ In this article, we have shown how they can be efficiently mined for parallel sentences.
In selecting negative examples, we followed the same approach as in (Munteanu and Marcu, 2005) $$$$$ We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan 2001).

Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ Any remaining errors are of course our own.
Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ This is due to the low coverage of the dictionary learned from that corpus.
Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc).
Based on this observation, dynamic programming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ The candidates are presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the sentences in each pair are mutual translations of each other.

This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990).
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ For each language pair, we used the highest precision classifier from those presented in Section 3.4.
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ Clearly, we have access to no UpperBound system in this case.
This paper extends previous work on extracting parallel sentence pairs from comparable data (Munteanu and Marcu, 2005). $$$$$ We are aware of only three previous efforts aimed at discovering parallel sentences.

We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ The problem of aligning sentences in comparable corpora was also addressed for monolingual texts.
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990).
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ The method presented in this paper is a step towards the important goal of automatic acquisition of such corpora.
We select source-target sentence pairs (S, T) based on a ME classifier (Munteanu and Marcu, 2005). $$$$$ We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.

In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used $$$$$ The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6.
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used $$$$$ We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus.
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used $$$$$ We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.
In addition, the sentence length filter in (Munteanu and Marcu, 2005) is used $$$$$ However, the filter also accepts many wrong pairs, because the word-overlap condition is weak; for instance, stopwords almost always have a translation on the other side, so if a few of the content For each candidate sentence pair, we need a reliable way of deciding whether the two sentences in the pair are mutual translations.
