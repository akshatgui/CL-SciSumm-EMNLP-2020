Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ Hence, there is a mismatch between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task.
Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ Computing the most probable sentence out of a set of candidate translation (see Eq.

The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ These training criteria make use of recently proposed automatic evaluation metrics.
The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ In recent years, various methods have been proposed to automatically evaluate machine translation quality by comparing hypothesis translations with reference translations.
The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ The use of statistical techniques in natural language processing often starts out with the simplifying (often implicit) assumption that the final scoring is based on simply counting the number of wrong decisions, for instance, the number of sentences incorrectly translated in machine translation.

 $$$$$ Note that NIST and BLEU scores are not additive for different sentences, i.e. the score for a document cannot be obtained by simply summing over scores for individual sentences.
 $$$$$ Examples of such methods are word error rate, position-independent word error rate (Tillmann et al., 1997), generation string accuracy (Bangalore et al., 2000), multi-reference word error rate (Nießen et al., 2000), BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002).

We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ The loss function is either identical or closely related to the final evaluation criterion.
We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ This means that improved translation evaluation measures lead directly to improved machine translation quality.
We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation.
We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ Which error rate should be optimized during training?

 $$$$$ Hence, there is a mismatch between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task.
 $$$$$ As a result, error rate cannot increase on the training corpus.
 $$$$$ The new algorithm is much faster and more stable than the grid-based line optimization method.

Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ Between BLEU and NIST, the differences are more moderate, but by optimizing on NIST, we still obtain a large improvement when measured with NIST compared to optimizing on BLEU.
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ In this framework, we have a set of feature functions .

In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ As a result, error rate cannot increase on the training corpus.
In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ 7 converges to the unsmoothed criterion of Eq.
In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ In the extreme case, for , Eq.

The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ Many tasks in natural language processing have evaluation criteria that go beyond simply counting the number of wrong decisions the system makes.
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq.
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ In addition, even if we manage to solve the optimization problem, we might face the problem of overfitting the training data.
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ Third, using this extended-best list new model parameters are computed.

Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ Hence, this approach poses new challenges for developers of automatic evaluation criteria.
Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ In Section 7, we evaluate the different training criteria in the context of several MT experiments.
Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ The direct translation probability is given by: In this framework, the modeling problem amounts to developing suitable feature functions that capture the relevant properties of the translation task.

We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ These training criteria make use of recently proposed automatic evaluation metrics.
We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ 6) along a line with parameter results in an optimization problem of the following functional form: Here, and are constants with respect to .
We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ We describe a new algorithm for efficient training an unsmoothed error count.

Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ In the field of natural language processing this approach has been applied for example in parsing (Goodman, 1996) and word alignment (Kumar and Byrne, 2002).
Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ The development corpus was used to optimize the parameters of the log-linear model.
Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ Hence, there is a mismatch between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task.
Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ The use of this type of smoothed error count is a common approach in the speech community (Juang et al., 1995; Schl¨uter and Ney, 2001).

The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ We observe that if we choose a certain error criterion in training, we obtain in most cases the best results using the same criterion as the evaluation metric on the test data.
The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ Hence, there is a mismatch between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task.
The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ It might happen that by directly optimizing an error measure in the way described above, weaknesses in the measure might be exploited that could yield better scores without improved translation quality.
The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ We expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems.

All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ Yet, the ultimate goal is to obtain good translation quality on unseen test data.
All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.
All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.

 $$$$$ Paciorek and Rosenfeld (2000) use minimum classification error training for optimizing parameters of a whole-sentence maximum entropy language model.
 $$$$$ We expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems.
 $$$$$ The basic feature functions of our model are identical to the alignment template approach (Och and Ney, 2002).

We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ The above stated optimization criterion is not easy to handle:
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ Of course, the approach presented here places a high demand on the fidelity of the measure being optimized.
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ By merging all sequences and for all different sentences of our corpus, the complete set of interval boundaries and error count changes on the whole corpus are obtained.
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ 6) along a line with parameter results in an optimization problem of the following functional form: Here, and are constants with respect to .

The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ We present results on the 2002 TIDES Chinese– English small data track task.
The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ The goal is the translation of news text from Chinese to English.
The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ The smoothed error count is much more stable and has fewer local optima.
The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ Note that better results correspond to larger BLEU and NIST scores and to smaller error rates.

The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ In the extreme case, for , Eq.
The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.
The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ These training criteria make use of recently proposed automatic evaluation metrics.
The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ For all error rates, we show the maximal occurring 95% confidence interval in any of the experiments for that column.

In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ The development corpus was used to optimize the parameters of the log-linear model.
In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ 7 converges to the unsmoothed criterion of Eq.
In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ This is iterated until the resulting-best list does not change.

We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ The system we use does not include rule-based components to translate numbers, dates or names.
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ Altogether, the log-linear model includes different features.
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ Note that, due to the re-definition of the notion of reference translation by using minimum edit distance, the results of the MMI criterion are biased toward mWER.
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ The use of log-linear models for statistical machine translation was suggested by Papineni et al. (1997) and Och and Ney (2002).

The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ The development corpus was used to optimize the parameters of the log-linear model.
The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ The last column provides the number of words in the produced translations which can be compared with the average number of reference words occurring in the development and test corpora given in Table 1.
The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ However, the different evaluation criteria yield quite different results on our Chinese–English translation task and therefore we expect that not all of them correlate equally well to human translation quality.
