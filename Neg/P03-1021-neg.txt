Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ The use of minimum classification error training and using a smoothed error count is common in the pattern recognition and speech Table 3: Effect of different error criteria used in training on the test corpus.
Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ The use of statistical techniques in natural language processing often starts out with the simplifying (often implicit) assumption that the final scoring is based on simply counting the number of wrong decisions, for instance, the number of sentences incorrectly translated in machine translation.
Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ 1 minimizes the number of decision errors.
Alternatively, one can train them with respect to the final translation quality measured by some error criterion (Och, 2003). $$$$$ It might happen that by directly optimizing an error measure in the way described above, weaknesses in the measure might be exploited that could yield better scores without improved translation quality.

The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ 7 converges to the unsmoothed criterion of Eq.
The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ In contrast to the approach presented in this paper, the training criterion and the statistical models used remain unchanged in the minimum Bayes risk approach.
The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ As the true probability distribution Pr is unknown, we have to develop a model that approximates Pr .
The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). $$$$$ In Section 3, we review various automatic evaluation criteria used in statistical machine translation.

 $$$$$ Many tasks in natural language processing have evaluation criteria that go beyond simply counting the number of wrong decisions the system makes.
 $$$$$ 3.
 $$$$$ In our experiments, we compute in every iteration about 200 alternative translations.

We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ In Section 4, we present two different training criteria which try to directly optimize an error count.
We tune all feature weights automatically (Och, 2003) to maximize the BLEU (Papineni et al, 2002) score on the dev set. $$$$$ The use of statistical techniques in natural language processing often starts out with the simplifying (often implicit) assumption that the final scoring is based on simply counting the number of wrong decisions, for instance, the number of sentences incorrectly translated in machine translation.

 $$$$$ BLEU score: This criterion computes the geometric mean of the precision of-grams of various lengths between a hypothesis and a set of reference translations multiplied by a factor BP that penalizes short sentences: NIST score: This criterion computes a weighted precision of-grams between a hypothesis and a set of reference translations multiplied by a factor BPâ€™ that penalizes short sentences: Heredenotes the weighted precision ofgrams in the translation.
 $$$$$ Note that NIST and BLEU scores are not additive for different sentences, i.e. the score for a document cannot be obtained by simply summing over scores for individual sentences.
 $$$$$ 6).
 $$$$$ We showed that this approach obtains significantly better results than using the MMI training criterion (with our method to define pseudoreferences) and that optimizing error rate as part of the training criterion helps to obtain better error rate on unseen test data.

Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation.
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ A major problem with the standard approach is the fact that grid-based line optimization is hard to adjust such that both good performance and efficient search are guaranteed.
Feature function scaling factors m are optimized based on a maximum likelihood approach (Och and Ney, 2002) or on a direct error minimization approach (Och, 2003). $$$$$ The optimal can now be computed easily by traversing the sequence of interval boundaries while updating an error count.

In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
In our setup, we use minimum error-rate training (MERT, Och (2003)) to optimize weights of model components. $$$$$ 5) is Powells algorithm combined with a grid-based line optimization method (Press et al., 2002).

The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ 7 converges to the unsmoothed criterion of Eq.
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ A technically very different approach that has a similar goal is the minimum Bayes risk approach, in which an optimal decision rule with respect to an application specific risk/loss function is used, which will normally differ from Eq.
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked retrieval, and BLEU or multi-reference word error rate for statistical machine translation.
The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. $$$$$ 3) which is guaranteed to find the optimal solution.

Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ By merging all sequences and for all different sentences of our corpus, the complete set of interval boundaries and error count changes on the whole corpus are obtained.
Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ If a fine-grained grid is used then the algorithm is slow.
Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ A standard algorithm for the optimization of the unsmoothed error count (Eq.
Feature weights were tuned with MERT (Och, 2003) to maximize BLEU on the NIST MT06 corpus. $$$$$ We use .

We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ 6) along a line with parameter results in an optimization problem of the following functional form: Here, and are constants with respect to .
We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ A major problem with the standard approach is the fact that grid-based line optimization is hard to adjust such that both good performance and efficient search are guaranteed.
We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ In Section 7, we evaluate the different training criteria in the context of several MT experiments.
We adapt the Minimum Error Rate Training (MERT) (Och, 2003) algorithm to estimate parameters for each member model in co-decoding. $$$$$ Hence, it is possible that our new search produces translations with more errors on the training corpus.

Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ Paciorek and Rosenfeld (2000) use minimum classification error training for optimizing parameters of a whole-sentence maximum entropy language model.
Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ We expect that directly optimizing error rate for many more parameters would lead to serious overfitting problems.
Feature weights were set with minimum error rate training (Och, 2003) on a development set using BLEU (Papineniet al, 2002) as the objective function. $$$$$ The use of statistical techniques in natural language processing often starts out with the simplifying (often implicit) assumption that the final scoring is based on simply counting the number of wrong decisions, for instance, the number of sentences incorrectly translated in machine translation.

The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ Between BLEU and NIST, the differences are more moderate, but by optimizing on NIST, we still obtain a large improvement when measured with NIST compared to optimizing on BLEU.
The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ Many tasks in natural language processing have evaluation criteria that go beyond simply counting the number of wrong decisions the system makes.
The component features are weighted to minimize a translation error criterion on a development set (Och, 2003). $$$$$ We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.

All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ These training criteria make use of recently proposed automatic evaluation metrics.
All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ 3.
All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ For each feature function, there exists a model parameter .
All model weights were trained on development sets via minimum-error rate training (MERT) (Och, 2003) with 200 unique n-best lists and optimizing toward BLEU. $$$$$ We present results on the 2002 TIDES Chineseâ€“ English small data track task.

 $$$$$ The new algorithm is much faster and more stable than the grid-based line optimization method.
 $$$$$ In this framework, we have a set of feature functions .
 $$$$$ We present results on the 2002 TIDES Chineseâ€“ English small data track task.

We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ These training criteria make use of recently proposed automatic evaluation metrics.
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ In this paper, we analyze various training criteria which directly optimize translation quality.
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ Note, that this approach can be applied to any evaluation criterion.
We carried out all our experiments using a state-of the-art phrase-based statistical English-to-Japanese machine translation system (Och, 2003). $$$$$ In this paper, we analyze various training criteria which directly optimize translation quality.

The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ The goal is the translation of news text from Chinese to English.
The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al, 2008). $$$$$ In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as measured by automatic evaluation criteria such as word error rate and BLEU.

The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ Is it possible to optimize more parameters using the smoothed error rate criterion?
The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ These training criteria make use of recently proposed automatic evaluation metrics.
The model scaling factors are optimized with respect to the BLEU score similar to (Och, 2003). $$$$$ Computing the most probable sentence out of a set of candidate translation (see Eq.

In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ Our goal is to obtain a minimal error count on a representative corpus with given reference translations and a set of different candidate translations for each input sentence .
In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ 3.
In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as measured by automatic evaluation criteria such as word error rate and BLEU.
In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). $$$$$ Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.

We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ In this translation model, a sentence is translated by segmenting the input sentence into phrases, translating these phrases and reordering the translations in the target language.
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ Hence, there is a mismatch between the basic assumptions of the used statistical approach and the final evaluation criterion used to measure success in a task.
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ We present results on the 2002 TIDES Chineseâ€“ English small data track task.
We carried out all our experiments using a state of-the-art phrase-based statistical Japanese-to English machine translation system (Och, 2003) with pre-ordering. $$$$$ We observe that if we choose a certain error criterion in training, we obtain in most cases the best results using the same criterion as the evaluation metric on the test data.

The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ We directly model the posterior probability Pr by using a log-linear model.
The weights of the log-linear combination of feature functions were estimated by using MERT (Och, 2003) on the development set described in Table 6. $$$$$ In this paper, we investigate methods to efficiently optimize model parameters with respect to machine translation quality as measured by automatic evaluation criteria such as word error rate and BLEU.
