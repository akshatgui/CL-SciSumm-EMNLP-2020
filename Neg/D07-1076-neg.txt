In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ As an alternative to the feature-based methods, the kernel-based methods (Haussler, 1999) have been proposed to implicitly explore various features in a high dimensional space by employing a kernel to cal culate the similarity between two objects directly.
In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ This may be due to that, although our experimentation on the training data indicates that more than 80% (on average) of subtrees has a root node path longer than 3 (since most of the subtrees are deep from the root node and more than 90% of the parsed trees in the training data are deeper than 6 levels), including a root node path longer than 3 may be vulnerable to the full parsing errors and have negative impact.
In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ Extraction of semantic relations between entities can be very useful in many applica tions such as question answering, e.g. to answer the query ?Who is the president of the United States??, and information retrieval, e.g. to expand the query ?George W. Bush?

As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ One problem with Collins and Duffy?s convolution tree kernel is that the sub-trees involved in the tree kernel computa tion are context-free, that is, they do not consider the information outside the sub-trees.
As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ The training parameters are chosen using cross-validation on the ACE RDC 2003 training data.
As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ (organization name).
As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ In particular, the kernel-based methods could be very effective at reducing the burden of feature engineer ing for structured objects in NLP researches, e.g. the tree structure in relation extraction.

In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ However, there are two problems in Collins and Duffy?s convolution tree kernel for relation extraction.
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ They achieved the F-measures of 61.9 and 63.6 on the 5 relation types of the ACE RDC 2003 corpus and the 7 relation types of the ACE RDC 2004 corpus respectively without entity-related information while the F measure on the 5 relation types in the ACE RDC 2003 corpus reached 68.7 when entity-related infor mation was included in the parse tree.
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ As the state-of-the-art, Zhang et al(2006) applied the convo lution tree kernel (Collins and Duffy 2001) and achieved comparable performance with a state-of-the art linear kernel (Zhou et al2005) on the 5 relation types in the ACE RDC 2003 corpus.

We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ In Section 2, we review related work in more details.
We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ in the sen tence ?John and Mary got married?
We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ The ACE RDC corpora are gathered from various newspapers, newswire and broadcasts.
We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.

 $$$$$ This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.
 $$$$$ Culotta and Sorensen (2004) extended this work to estimate simi larity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus.
 $$$$$ This convolution tree kernel has been successfully applied by Zhang et al(2006) in relation extraction.
 $$$$$ The ACE RDC corpora are gathered from various newspapers, newswire and broadcasts.

To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ chain ing of co-reference (i.e. as annotated by LDC annotators ).
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ Moreover, we will explore more entity-related information in the parse tree.
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ We would also like to thank the critical and insightful comments from the four anonymous reviewers.
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ For compari son, we use the same setting as Zhang et al(2006) by applying a 5-fold cross-validation on a subset of the 2004 data, containing 348 documents and 4400 rela tion instances.

 $$$$$ That is, those entity pairs, which do not belong to the four well defined and easily detected categories (i.e. embedded, PP-liked, semi-structured and descriptive), are classified into the ?predicate-linked?
 $$$$$ To our knowledge, this is the first research to dem onstrate that, without extensive feature engineer ing, an individual tree kernel can achieve much better performance than the state-of-the-art linear kernel in re lation extraction.
 $$$$$ the tree kernel proposed in Culota and Sorensen (2004) which is context-sensitive, that is, it considers the path from the tree root node to the sub-tree root node.
 $$$$$ chain ing of co-reference (i.e. as annotated by LDC annotators ).

Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ It also shows that feature-based 8 There might be some typing errors for the performance.
Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.

Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). $$$$$ This is contrast to our intuition.
Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). $$$$$ Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel.

To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ The ACE RDC corpora are gathered from various newspapers, newswire and broadcasts.
To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ They argued that the information to model a relationship between two entities can be typically captured by the shortest path between them in the dependency graph.
To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ For example, the sentence ?Bill Gates is the chairman and chief software architect of Microsoft Corporation.?

 $$$$$ While the shortest path may not be able to well preserve structured de pendency tree information, another problem with their kernel is that the two paths should have same length.
 $$$$$ Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy?s convolution tree kernel.
 $$$$$ This paper proposes a contextsensitive convolution tree kernel to resolve two critical problems in previous tree kernels for relation ex traction by first automatically determining a dynamic context-sensitive tree span and then applying a con text-sensitive convolution tree kernel.

Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ reported in Zhao and Grishman(2005) since P, R and F do not match.
Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ D= m i NnNn ii C iiii nnTTK 1 ]2[]2[],1[]1[ 11 1111 ])2[],1[(])2[],1[( (3) Where ? ][1 jN i is the set of root node paths with length i in tree T[j] while the maximal length of a root node path is defined by m. ? ])[...(][ 211 jnnnjn ii = is a root node path with length i in tree T[j] , which takes into account the i-1 ancestral nodes in2 [j] of 1n [j] in T[j].

Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ Further more, a composite kernel is applied to combine our tree kernel and a state-of-the-art linear kernel for in tegrating both flat and structured features in relation extraction as well as validating their complementary nature.
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ This shows the great potential of structured parse tree information for relation extrac tion and our tree kernel takes a big stride towards the right direction.
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.

Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.
Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ Since then, many methods, such as feature-based (Kambhatla 2004; Zhou et al2005, 2006), tree ker nel-based (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a; Zhang et al2006) and composite kernel-based (Zhao and Gris hman 2005; Zhang et al2006), have been proposed in lit erature.
Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ and ?Mary?
Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.

Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ It resolves two critical problems in previous tree kernels for relation extraction in two ways.
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ 7 Here, we use the same set of flat features (i.e. word,.
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ One problem with the above two tree kernels is that matched nodes must be at the same height and have the same path to the root node.

Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ Further more, a composite kernel is applied to combine our tree kernel and a state-of-the-art linear kernel for in tegrating both flat and structured features in relation extraction as well as validating their complementary nature.
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ information.
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ the node of entity 1 and the 1st right sibling of the node of entity 2.
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ The first is that the sub-trees enumerated in the tree kernel computation are context-free.

Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ That is, all the reported performances in this paper on the ACE RDC 2004 corpus are evalu ated using 5-fold cross validation on the entire corpus . Both corpora are parsed using Charniak?s parser (Charniak, 2001) with the boundaries of all the entity mentions kept 4 . We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances5.
Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ To answer the above two questions, we randomly chose 100 positive instances from the ACE RDC 2003 training data and studied their necessary tree spans.
Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ Culotta and Sorensen (2004) extended this work to estimate simi larity between augmented dependency trees and achieved the F-measure of 45.8 on the 5 relation types in the ACE RDC 2003 corpus.
Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ This shows the great potential of structured parse tree information for relation extrac tion and our tree kernel takes a big stride towards the right direction.

(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ One problem with Collins and Duffy?s convolution tree kernel is that the sub-trees involved in the tree kernel computa tion are context-free, that is, they do not consider the information outside the sub-trees.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ Relation extraction is to find various predefined se mantic relations between pairs of entities in text.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ They achieved the F-measures of 61.9 and 63.6 on the 5 relation types of the ACE RDC 2003 corpus and the 7 relation types of the ACE RDC 2004 corpus respectively without entity-related information while the F measure on the 5 relation types in the ACE RDC 2003 corpus reached 68.7 when entity-related infor mation was included in the parse tree.

In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Our preliminary work of including the entity type information significantly improves the per formance.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.

Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ 732 3) Calculate ])2[],1[( 11 ii nnD recursively as: ? = D+= D ])1[(# 1 11 11 1 ))],2[(),],1[((1( ])2[],1[( inch k ii ii knchknch nn l (4) where ])],[( 1 kjnch i is the k th context-sensitive child of the context-sensitive sub-tree rooted at ][1 jn i with ])[(# 1 jnch i the number of the con text-sensitive children.
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ 6 Significance test shows that the dynamic tree span per-.
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ One problem with the context-sensitive tree span explored in Zhang et al(2006) is that it only considers the availability of entities?
