In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ It achieved the Fmeasure of 70.9/57.2 on the 5 relation types/24 rela tion subtypes in the ACE RDC 2003 corpus and the F-measure of 72.1/63.6 on the 7 relation types/23 relation subtypes in the ACE RDC 2004 corpus.
In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.
In the news domain, the best reported results on the ACE dataset have been achieved by a composite kernel which depends partially on a full parse, and partially on a collection of shallow syntactic features (Zhou et al, 2007). $$$$$ We would also like to thank the critical and insightful comments from the four anonymous reviewers.

As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.
As shown in Zhou et al (2007), the context path from root to the phrase node is an effective context information feature. $$$$$ This paper proposes a tree kernel with contextsensitive structured parse tree information for re lation extraction.

In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ Finally, let?s study the computational issue with our tree kernel.
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ Relation extraction is to find various predefined se mantic relations between pairs of entities in text.
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ Since then, many methods, such as feature-based (Kambhatla 2004; Zhou et al2005, 2006), tree ker nel-based (Zelenko et al2003; Culotta and Sorensen 2004; Bunescu and Mooney 2005a; Zhang et al2006) and composite kernel-based (Zhao and Gris hman 2005; Zhang et al2006), have been proposed in lit erature.
In this paper, we use the same settings in (Zhou et al, 2007), i.e., each phrase node is enriched with its context paths of length 1, 2, 3. $$$$$ In particular, the kernel-based methods could be very effective at reducing the burden of feature engineer ing for structured objects in NLP researches, e.g. the tree structure in relation extraction.

We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ This suggests that about 50% dis count is done as our tree kernel moves down one level in computing ])2[],1[( 11 ii nnD . 4.2 Experimental Results.
We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ The 2003 corpus defines 5 entity types, 5 major relation types and 24 relation subtypes.
We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al, 2007). $$$$$ This paper uses the ACE RDC 2003 and 2004 cor pora provided by LDC in all our experiments.

 $$$$$ It also shows that our tree kernel achieves much bet ter performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.
 $$$$$ Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.
 $$$$$ The idea behind the algorithm is that the necessary tree span for a relation should be determined dynamically according to its tree span category and context.
 $$$$$ We will study this issue in the near future.

To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ This suggests the usefulness of extending the tree span beyond SPT for the ?predicate-linked?
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ Although our tree kernel takes the context sensitive sub-trees into consideration, it only slightly increases the computational burden, compared with Collins and Duffy?s convolution tree kernel.
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ It achieved the Fmeasure of 70.9/57.2 on the 5 relation types/24 rela tion subtypes in the ACE RDC 2003 corpus and the F-measure of 72.1/63.6 on the 7 relation types/23 relation subtypes in the ACE RDC 2004 corpus.
To resolve this problem, Zhou et al (2007) took the ancestral information of sub-trees into consideration. $$$$$ Finally, we will study how to resolve the data imbalance and sparseness issues from the learn ing algorithm viewpoint.

 $$$$$ Composite Kernel In this paper, a composite kernel via polynomial interpolation, as described Zhang et al(2006), is ap plied to integrate the proposed context-sensitive convolution tree kernel with a state-of-the-art linear kernel (Zhou et al2005) 7: ),()1(),(),(1 ???-+???=??
 $$$$$ information.
 $$$$$ in the entity node 1 of Figure 1(b)) into both entity nodes.
 $$$$$ First, it automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the widely-used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.

Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ 9 All the state-of-the-art systems apply the entity-related.
Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ Zhang et al(2006) explored five tree spans in relation extraction and it was a bit sur prising to find that the Shortest Path-enclosed Tree (SPT, i.e. the sub-tree enclosed by the shortest path linking two involved entities in the parse tree) performed best.
Later, Zhang et al (2006) developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al (2007) experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans. $$$$$ This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high preci sion but very low recall.

Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). $$$$$ This paper proposes a contextsensitive convolution tree kernel to resolve two critical problems in previous tree kernels for relation ex traction by first automatically determining a dynamic context-sensitive tree span and then applying a con text-sensitive convolution tree kernel.
Zhou et al (2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005). $$$$$ tion extraction on ?true?

To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ Our preliminary work of including the entity type information significantly improves the per formance.
To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ as shown in Figure 1(e).
To the best of our knowledge, the most recent result was reported by (Zhou and Zhu, 2011), who extended their previous work in (Zhou et al, 2007). $$$$$ This is contrast to 2 That is, each node n encodes the identity of a sub-.

 $$$$$ as shown in Figure 1(e).
 $$$$$ (person name) and ?Microsoft Corporation?
 $$$$$ ])2[],1[( 11 ii nnD measures the common context sensitive sub-trees rooted at root node paths ]1[1in and ]2[1in 3.
 $$$$$ in the sentence ?John and Mary got married??

Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ It first automatically determines a dynamic context-sensitive tree span for relation ex traction by extending the Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT.
Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.
Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ as shown in Figure 1(a) . However, SPT is not enough in the coordinated cases, e.g. to determine the relationship between ?John?
Zhou et al (2007) tested their system on the ACE 2004 data. $$$$$ It is not supervising: our experiments show that using the entity-related information gives a large performance improvement.

Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ The relation extraction task was first introduced as part of the Template Element task in MUC6 and then formulated as the Template Relation task in MUC7.
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ System P(%) R(%) F Linear Kernel 78.2 (77.2) 63.4 (60.7) 70.1 (68.0) Context-Sensitive Con volution Tree Kernel 81.1 (80.1) 66.7 (63.8) 73.2 (71.0) Composite Kernel 82.2 (80.8) 70.2 (68.4) 75.8 (74.1) Table 3: Performance of the compos ite kernel via polynomial interpolation on the major relation types of the ACE RDC 2003 (inside the parentheses) and 2004 (outside the parentheses) corpora Comparison with Other Systems ACE RDC 2003 P(%) R(%) F Ours: composite kernel 80.8 (65.2) 68.4 (54.9) 74.1 (59.6) Zhang et al(2006): composite kernel 77.3 (64.9) 65.6 (51.2) 70.9 (57.2) Ours: context-sensitive convolution tree kernel 80.1 (63.4) 63.8 (51.9) 71.0 (57.1) Zhang et al(2006): convolution tree kernel 76.1 (62.4) 62.6 (48.5) 68.7 (54.6) Bunescu et al(2005): shortest path dependency kernel 65.5 (-) 43.8 (-) 52.5 (-) Culotta et al(2004): dependency kernel 67.1 (-) 35.0 (-) 45.8 (-) Zhou et al (2005): feature-based 77.2 (63.1) 60.7 (49.5) 68.0 (55.5) Kambhatla (2004): feature-based - (63.5) - (45.2) - (52.8) Table 4: Comparison of difference systems on the ACE RDC 2003 corpus over both 5 types (outside the parentheses) and 24 subtypes (inside the parentheses) ACE RDC 2004 P(%) R(%) F Ours: composite kernel 82.2 (70.3) 70.2 (62.2) 75.8 (66.0) Zhang et al(2006): composite kernel 76.1 (68.6) 68.4 (59.3) 72.1 (63.6) Zhao et al(2005):8 composite kernel 69.2 (-) 70.5 (-) 70.4 (-) Ours: context-sensitive convolution tree kernel 81.1 (68.8) 66.7 (60.3) 73.2 (64.3) Zhang et al(2006): convolution tree kernel 72.5 (-) 56.7 (-) 63.6 (-) Table 5: Comparison of difference systems on the ACE RDC 2004 corpus over both 7 types (outside the parentheses) and 23 subtypes (inside the parentheses) Finally, Tables 4 and 5 compare our system with other state-of-the-art systems9 on the ACE RDC 2003 and 2004 corpora, respectively.
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ with degree d=2, i.e. 2( , ) ( ( , ) 1)pK K?
Zhou et al (2007) proposed the so called context sensitive tree kernel approach based on PST, which expands PET to include necessary contextual in formation. $$$$$ 2) If both 1n and 2n are POS tags, 1 2( , ) 1n n lD = ? ; Otherwise go to 3.

Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ Zhou et al(2005) further systematically ex plored diverse features through a linear kernel and Support Vector Machines, and achieved the F measures of 68.0 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 cor pus respectively.
Zhou et al (2007) further extend it to Context-Sensitive Shortest Path-enclosed Tree (CS SPT), which dynamically includes necessary predicate-linked path information. $$$$$ Our random selection of 100 pos i tive training instances from the ACE RDC 2003 training corpus shows that ~25% of the cases need contextual information outside the shortest path.

Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ For ex ample, ?got married?
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ 3) Calculate 1 2( , )n nD recursively as: ? = D+=D )(# 1 2121 1 )),(),,((1(),( nch k knchknchnn l (2) where )(# nch is the number of children of node n , ),( knch is the k th child of node n andl (0< l <1) is the decay factor in order to make the kernel value less variable with respect to different sub-tree sizes.
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ (organization name).
Zhou et al (2007) point out that both SPT and the convolution tree kernel are context-free. $$$$$ Moreover, this paper evaluates the complementary nature between our tree kernel and a state-of-the-art linear kernel.

Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ tree span category.
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ One problem with Collins and Duffy?s convolution tree kernel is that the sub-trees involved in the tree kernel computa tion are context-free, that is, they do not consider the information outside the sub-trees.
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ Zhang et al(2006) also showed that the widely-used Shortest Path-enclosed Tree (SPT) performed best.
Zhou et al (2007) describe a composite kernel to integrate a context-sensitive CTK and a state-of-the-art linear kernel. $$$$$ Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than the widely -used Shortest Path-enclosed Tree and our tree kernel outperforms the state-of-the-art Collins and Duffy?s con volution tree kernel.

Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ information.
Zhou et al (2007) further propose Context-Sensitive SPT (CS-SPT), which can dynamically determine the tree span by extending the necessary predicate-linked path information outside SPT. $$$$$ That is, all the reported performances in this paper on the ACE RDC 2004 corpus are evalu ated using 5-fold cross validation on the entire corpus . Both corpora are parsed using Charniak?s parser (Charniak, 2001) with the boundaries of all the entity mentions kept 4 . We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances5.

(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ 18% of positive instances in the ACE RDC 2003 test data belong to the predicate-linked category.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ Second, it pro poses a context-sensitive convolution tree kernel, which enumerates both context-free and context sensitive sub-trees by considering their ancestor node paths as their contexts.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ Zhang et al(2006) explored five tree spans in relation extraction and it was a bit sur prising to find that the Shortest Path-enclosed Tree (SPT, i.e. the sub-tree enclosed by the shortest path linking two involved entities in the parse tree) performed best.
(2) CS-SPT only captures part of context sensitive information relating to predicate-linked structure (Zhou et al, 2007) and still loses much context-sensitive information. $$$$$ Zhao and Grishman (2005) defined several feature based composite kernels to integrate diverse features for relation extraction and achieved the F-measure of 70.4 on the 7 relation types of the ACE RDC 2004 corpus.

In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than the widely -used Shortest Path-enclosed Tree and our tree kernel outperforms the state-of-the-art Collins and Duffy?s con volution tree kernel.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Moreover, please note that the final performance of relation extraction may change much with different range of parsing errors.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ For better differentiation, the label of each ancestral node in in1 [j] is augmented with the POS tag of its head word.

Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ Acknowledgement This research is supported by Project 60673041 under the National Natural Science Foundation of China and Project 2006AA01Z147 under the ?863?
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ Zelenko et al (2003) proposed a kernel between two parse trees, which recursively matches nodes from roots to leaves in a top-down manner.
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ This makes it suffer from the similar behavior with that of Culotta and Sorensen (2004): high preci sion but very low recall.
Furthermore, when the UPST (FPT) kernel is combined with a linear state-of-the-state feature based kernel (Zhou et al, 2005) into a composite one via polynomial interpolation in a setting similar to Zhou et al (2007) (i.e. polynomial degree d=2 and coefficient? =0.3), we get the so far best performance of 77.1 in F-measure for 7relation types on the ACE RDC 2004 data set. $$$$$ 6 Significance test shows that the dynamic tree span per-.
