 $$$$$ The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice in most large-scale applications.
 $$$$$ In comparison, OWL-QN requires much more memory, due to the internals of the update routines, which require several histories of the parameter vector and of its gradient.
 $$$$$ The features used in Nettalk experiments take the form fy,,,, (unigram) and fy0,y,,,, (bigram), where w is a n-gram of letters.
 $$$$$ Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.

Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al (2010) to regularize CRFs. $$$$$ (2009), who use approximations to simplify the forward-backward recursions.
Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al (2010) to regularize CRFs. $$$$$ The performance it achieves are consistently slightly worst than the other optimizers, and only catch up when the parameters are fine-tuned (see above).
Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al (2010) to regularize CRFs. $$$$$ Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) constitute a widely-used and effective approach for supervised structure learning tasks involving the mapping between complex objects such as strings and trees.
Elastic nets interpolate between L1 and L2, having been proposed by Zou and Hastie (2005) and used by Lavergne et al (2010) to regularize CRFs. $$$$$ The gradient of l(θ) is where Epθ(y|x) denotes the conditional expectation given the observation sequence, i.e.

 $$$$$ This benchmark, which has been used in many studies, allows for direct comparisons with other published work.
 $$$$$ This is implemented by patching the update (7) as follows Based on a study of three NLP benchmarks, the authors of (Tsuruoka et al., 2009) claim this approach to be much faster than the orthant-wise approach and yet to yield very comparable performance, while selecting slightly larger feature sets.
 $$$$$ In this respect, BCD is the most efficient, as it only requires to store one K-dimensional vector for the parameter itself.
 $$$$$ Efficiency stems here from the sparinduced by the use of a term.

 $$$$$ SGD requires two such vectors, one for the parameter and one for storing the zk (see Eq.
 $$$$$ One advantage of the resulting algorithm, termed BCD in the following, is that the update of θk only involves carrying out the forward-backward recursions for the set of sequences that contain symbols x such that at least one {fk(y', y, x)}(y y1)EY 2 is non null, which can be much smaller than the whole training set.
 $$$$$ The n-grm feature sets (n = 11, 3, 5, 7}) includes all features testing embedded windows of k letters, for all 0 < k < n; the n-grm- setting is similar, but only includes the window of length n; in the n-grm+ setting, we add features for odd-size windows; in the ngrm++ setting, we add all sequences of letters up to size n occurring in current window.
 $$$$$ In this section, we present several optimizations devised to speed up training.

 $$$$$ Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al.
 $$$$$ Based on our own implementation, we compare three recent proposals for implementing this regularization strategy.
 $$$$$ In this setting, the number of parameters K is equal to |Y |2x|X|train, where |· |denotes the cardinal and |X|train refers to the number of configurations of xt observed during training.
 $$$$$ Table 5 gives the results obtained for the larger Nettalk+prosody task.

 $$$$$ From a performance point of view, it might also be interesting to combine the use of large-scale feature sets with other recent improvements such as the use of semi-supervised learning techniques (Suzuki and Isozaki, 2008) or variable-length dependencies (Qian et al., 2009).
 $$$$$ As an alternative way of avoiding numerical problems, our implementation, like crfSuite’s, resorts to “scaling”, a solution commonly used for HMMs.
 $$$$$ Also note that in our implementation, all the computations of exp(x) are vectorized, which provides an additional speed up of about 20%.

As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields (Lavergne et al, 2010) with the L-BFGS optimizer and elastic net regularization with default settings. $$$$$ Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning.
As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields (Lavergne et al, 2010) with the L-BFGS optimizer and elastic net regularization with default settings. $$$$$ An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) performing the update, which might cause some component of the parameter vector to change sign; and (iii) projecting back the parameter value onto the initial orthant, thereby zeroing out those components.
As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields (Lavergne et al, 2010) with the L-BFGS optimizer and elastic net regularization with default settings. $$$$$ As far as convergence is concerned, the two forms of regularization (E2 and E1) yield the same performance (see Table 3), and the three algorithms exhibit more or less the same behavior.
As our sequence labeling model we use the Wapiti implementation of Conditional Random Fields (Lavergne et al, 2010) with the L-BFGS optimizer and elastic net regularization with default settings. $$$$$ Most empirical studies on CRFs thus either consider tasks with a restricted output space (typically in the order of few dozens of output labels), heuristically reduce the use of features, especially of features that test pairs of adjacent labels1, and/or propose heuristics to simulate contextual dependencies, via extended tests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)).

 $$$$$ Based on our own implementation, we compare three recent proposals for implementing this regularization strategy.
 $$$$$ In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models.
 $$$$$ On the other hand, bigram features {fy/,y,x}(y,x)∈Y 2×X are helpful in modelling dependencies between successive labels.

 $$$$$ Our first benchmark is the word phonetization task, using the Nettalk dictionary (Sejnowski and Rosenberg, 1987).
 $$$$$ The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice in most large-scale applications.

We report results for the following hybridizations and CRF-based system using Wapiti (Lavergne et al., 2010). $$$$$ The normalization factor in (1) is defined by The most common choice of feature functions is to use binary tests.
We report results for the following hybridizations and CRF-based system using Wapiti (Lavergne et al., 2010). $$$$$ In this setting, the number of parameters K is equal to |Y |2x|X|train, where |· |denotes the cardinal and |X|train refers to the number of configurations of xt observed during training.
We report results for the following hybridizations and CRF-based system using Wapiti (Lavergne et al., 2010). $$$$$ Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.
We report results for the following hybridizations and CRF-based system using Wapiti (Lavergne et al., 2010). $$$$$ Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.

 $$$$$ Our analysis demonstrate that training large-scale sparse models can be done efficiently and allows to improve over the performance of smaller models.
 $$$$$ The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice in most large-scale applications.

 $$$$$ Here, we only report the results obtained with SGD and BCD.
 $$$$$ Based on an efficient implementation of these algorithms, we were able to train very large CRFs containing more than a hundred of output labels and up to several billion features, yielding results that are as good or better than the best reported results for two NLP benchmarks, text phonetization and part-of-speech tagging.

 $$$$$ Of the three algorithms, BCD is the most affected by increases in the number of features, or more precisely, in the number of features blocks, where one block correspond to a specific test of the observation.
 $$$$$ An OWL-QN update then simply consists in (i) computing the Newton update in a well-chosen orthant; (ii) performing the update, which might cause some component of the parameter vector to change sign; and (iii) projecting back the parameter value onto the initial orthant, thereby zeroing out those components.
 $$$$$ Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels.
 $$$$$ Given N independent sequences {x(i), y(i)}Ni=1, where x(i) and y(i) contain T(i) symbols, conditional maximum likelihood estimation is based on the minimization, with respect to θ, of the negated conditional log-likelihood of the observations This term is usually complemented with an additional regularization term so as to avoid overfitting (see Section 3.1 below).

 $$$$$ Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels.
 $$$$$ Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.
 $$$$$ Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets.
 $$$$$ Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning.

 $$$$$ Various strategies have been tried to further accelerate BCD, such as processing blocks that only visit one observation in parallel and updating simultaneously all the blocks that visit all the training instances, leading to a small speed-up on the POS-tagging task.
 $$$$$ This comparison is made fair and reliable thanks to the reimplementation of these techniques in the same software package.
 $$$$$ Even for the simple linearchain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set.
 $$$$$ Efficiency stems here from the sparinduced by the use of a term.

 $$$$$ This package is released as open-source software and is available at http://wapiti.limsi.fr.
 $$$$$ In the following, we will jointly use both penalty terms, yielding the socalled elastic net penalty (Zhou and Hastie, 2005) which corresponds to the objective function The use of both penalty terms makes it possible to control the number of non zero coefficients and to avoid the numerical problems that might occur in large dimensional parameter settings (see also (Chen, 2009)).
 $$$$$ Typically, our implementation necessitates in the order of a dozen K-dimensional vectors.
 $$$$$ CRFs are based on the following model where x = (x1, ...,xT) and y = (y1, ... , yT) are, respectively, the input and output sequences2, and Fk(x, y) is equal to PTt=1 fk(yt−1, yt, xt), where {fk}1≤k≤K is an arbitrary set of feature functions and {θk}1≤k≤K are the associated parameter values.

 $$$$$ Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers “generalized” feature functions; and by Jeong et al.
 $$$$$ The CRF package developed in the course of this study implements many algorithmic optimizations and allows to design innovative training strategies, such as the one presented in section 5.4.
 $$$$$ It is however very inefficient from an implementation point of view, due to the repeated calls to the exp() and log() functions.

 $$$$$ We study and compare three different ways to implement il penalty for CRFs that have been introduced recently: orthantwise Quasi Newton (Andrew and Gao, 2007), stochastic gradient descent (Tsuruoka et al., 2009) and coordinate descent (Sokolovska et al., 2010), concluding that these methods have complementary strengths and weaknesses.
 $$$$$ In the future, we intend to study how sparsity can be used to speed-up training in the face of more complex dependency patterns (such as higher-order CRFs or hierarchical dependency structures (Rozenknop, 2002; Finkel et al., 2008).
 $$$$$ Pal et al. (2006) propose to use a “sparse” version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning.
 $$$$$ Assume the set of bigram features {λy0,y,xt+1}(y0,y)∈Y 2 is sparse with only r(xt+1) « |Y |2 non null values and define al., 2010) explains how computational savings can be obtained using the fact that the vector/matrix products in the recursions above only involve the sparse matrix Mt+1(y0, y).

 $$$$$ SGD updates have the following form where η is the learning rate.
 $$$$$ This package is released as open-source software and is available at http://wapiti.limsi.fr.
 $$$$$ In comparison, OWL-QN requires much more memory, due to the internals of the update routines, which require several histories of the parameter vector and of its gradient.

We therefore treat the prediction of IS labels as a sequence labeling task. We train a CRF using wapiti (Lavergne et al, 2010), with the features outlined in Table 1. $$$$$ Full convergence, reflected by a stabilization of the objective function, is however not so easily achieved.
We therefore treat the prediction of IS labels as a sequence labeling task. We train a CRF using wapiti (Lavergne et al, 2010), with the features outlined in Table 1. $$$$$ This package is released as open-source software and is available at http://wapiti.limsi.fr.
