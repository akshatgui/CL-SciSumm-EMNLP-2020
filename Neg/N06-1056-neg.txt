We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. $$$$$ A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. $$$$$ We present a novel statistical approach to parsing, for constructing a complete, formal meaning representation of a sentence.
We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. $$$$$ CITY —* (nyuu yooku, cityid(’new york’, )) for Japanese).
We compared our system's performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser. $$$$$ A similar feature set is used by Zettlemoyer and Collins (2005).

We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). $$$$$ In the ROBOCUP Coach Competition, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CLANG (Chen et al., 2003).
We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). $$$$$ In the future, we would like to develop a word-based alignment model that is aware of the MRL syntax, so that better lexicons can be learned.
We use a maximum-entropy model similar to that of Zettlemoyer and Collins (2005) and Wong and Mooney (2006). $$$$$ Figure 1 shows a sample MR in CLANG.

Following Wong and Mooney (2006), only candidate predicates and composition rules that are used in the best semantic derivations for the training set are retained for testing. $$$$$ It requires no prior knowledge of the NL syntax, although it assumes that an unambiguous, context-free grammar (CFG) of the target MRL is available.

WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. $$$$$ In initial evaluation on several real-world data sets, we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring the same amount of supervision, and shows better robustness to variations in task complexity and word order.
WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. $$$$$ The second case arises when a predicate and its arguments are not realized close enough.
WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. $$$$$ Semantic parsing has found its way in practical applications such as natural-language (NL) interfaces to databases (Androutsopoulos et al., 1995) and advice taking (Kuhlmann et al., 2004).
WASP (Wong and Mooney, 2006) is a system motivated by statistical machine translation techniques. $$$$$ However, these systems make no use of the MRL grammar, thus allocating probability mass to MR translations that are not even syntactically well-formed.

To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). $$$$$ Since there is no lexicon to begin with, it is not possible to include correct derivations in the training data.
To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). $$$$$ We presented a simple greedy algorithm for removing links that destroy phrasal coherence.
To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). $$$$$ Finally, we report on experiments that show the robustness of WASP in Section 6, followed by the conclusion in Section 7.
To make our system directly comparable to previous systems, all our experiments were based on identical training and test data splits of both corpora as reported in the experiments of Wong and Mooney (2006). $$$$$ This process can be formalized as an instance of synchronous parsing (Aho and Ullman, 1972), originally developed as a theory of compilers in which syntax analysis and code generation are combined into a single phase.

Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). $$$$$ Judging from the pt token alone, the word alignment model would not be able to identify its exact meaning.
Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). $$$$$ Prior research in semantic parsing has mainly focused on relatively simple domains such as ATIS (Air Travel Information Service) (Miller et al., 1996; Papineni et al., 1997; Macherey et al., 2001), in which a typcial MR is only a single semantic frame.
Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). $$$$$ The first domain is ROBOCUP.
Executable system actions include access to databases such as the GEOQUERY database on U.S. geography (Wong and Mooney (2006), inter alia), the ATIS travel planning database (Zettlemoyer and Collins (2009), inter alia), robotic control in simulated navigation tasks (Chen and Mooney (2011), interalia), databases of simulated card games (Goldwasser and Roth (2013), interalia), or the user-generated contents of FREEBASE (Cai and Yates (2013), inter alia). $$$$$ The number of features is quite modest (less than 3,000 in our experiments).

 $$$$$ For example, for the bowner predicate, the extracted rule would be CONDITION —* (TEAM 1 player UNUM 2 has (1) ball, (bowner TEAM 1 {UNUM 2 })), where (1) denotes a word gap of size 1, due to the unaligned word the that comes between has and ball.
 $$$$$ The maximum conditional likelihood criterion is used for estimating the model parameters, Ai.
 $$$$$ Recent work on natural language understanding has mainly focused on shallow semantic analysis, such as semantic role labeling and word-sense disambiguation.
 $$$$$ 2004.

In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). $$$$$ Consider the task of translating the sentence in Figure 1 into its MR in CLANG.
In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL). $$$$$ In Proc. of ACL-04, pages 653–660, Barcelona, Spain.

For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). $$$$$ More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL.
For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). $$$$$ A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.
For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). $$$$$ Our method is like many phrasebased translation models, which require a simpler, word-based alignment model for the acquisition of a phrasal lexicon (Och and Ney, 2003).
For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)). $$$$$ The problem is that, by treating MRL productions as atomic units, current word-based alignment models have no knowledge about the tree structure hidden in a linearized MR parse.

Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. $$$$$ Our algorithm is called WASP, short for Word Alignment-based Semantic Parsing.
Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. $$$$$ Generation of unseen words is modeled using an extra feature whose value is the total number of words generated from word gaps.
Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. $$$$$ In the ROBOCUP Coach Competition, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CLANG (Chen et al., 2003).
Our work is based on the WASP semantic parsing algorithm (Wong and Mooney, 2006), which translates NL sentences into MRs using an SCFG. $$$$$ Once a lexicon is acquired, the next task is to learn a probabilistic model for the semantic parser.

While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic. $$$$$ To assume as little as possible, A is initialized to 0.
While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic. $$$$$ Since gold-standard derivations are not available in the training data, correct derivations must be treated as hidden variables.
While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic. $$$$$ Non-terminals are indexed to show their association between a pattern and a template.
While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic. $$$$$ Also for each word w there is a feature function that returns the number of times w is generated from word gaps.

We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence. $$$$$ First, not all MR tokens carry specific meanings.
We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence. $$$$$ For each rule r in the lexicon there is a feature function that returns the number of times r is used in a derivation.
We use the maximum-entropy model proposed in Wong and Mooney (2006), which defines a conditional probability distribution over derivations given an observed NL sentence. $$$$$ I. D. Melamed.

For details regarding non-isomorphic NL/MR parse trees, removal of bad links from alignments, and extraction of word gaps (e.g. the token (1) in the last rule of Figure 3), see Wong and Mooney (2006). $$$$$ Figure 1 shows a sample MR in CLANG.
For details regarding non-isomorphic NL/MR parse trees, removal of bad links from alignments, and extraction of word gaps (e.g. the token (1) in the last rule of Figure 3), see Wong and Mooney (2006). $$$$$ Statistical machine translation by parsing.
For details regarding non-isomorphic NL/MR parse trees, removal of bad links from alignments, and extraction of word gaps (e.g. the token (1) in the last rule of Figure 3), see Wong and Mooney (2006). $$$$$ For ROBOCUP, it took 47 minutes to learn a parser using IIS.

The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). $$$$$ We present a novel statistical approach to parsing, for constructing a complete, formal meaning representation of a sentence.
The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). $$$$$ Although it is shown to be quite effective in the current domains, it is preferable to have a more principled way of promoting phrasal coherence.
The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). $$$$$ 250 of the queries were also translated into Spanish, Japanese and Turkish, resulting in a smaller, multilingual data set.
The larger GEOQUERY corpus consists of 880 English questions gathered from various sources (Wong and Mooney, 2006). $$$$$ The maximum conditional likelihood criterion is used for estimating the model parameters, Ai.

We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL. $$$$$ According to this theory, a semantic parser defines a translation, a set of pairs of strings in which each pair is an NL sentence coupled with its MR. To finitely specify a potentially infinite translation, we use a synchronous context-free grammar (SCFG) for generating the pairs in a translation.
We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL. $$$$$ All derivations start with a pair of associated start symbols, (51 , 51 ).
We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL. $$$$$ Figure 2

Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. $$$$$ Details can be found in a longer version of this paper (Wong, 2005).
Like the hybrid tree semantic parser (Lu et al, 2008) and the synchronous grammar based WASP (Wong and Mooney, 2006), our model simultaneously generates the input MR tree and the output NL string. $$$$$ A word alignment model is used for lexical acquisition, and the parsing model itself can be seen as a syntax-based translation model. show that favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision, and shows better robustness to variations in task complexity and word order.

WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. $$$$$ Each step of a derivation involves the rewriting of a pair of associated non-terminals in both of the NL and MRL streams.
WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. $$$$$ We counted the number of sentences that were translated into an MR, and the number of translations that were correct.
WASP (Wong and Mooney, 2006) is an example of the former perspective, coupling the generation of the MR and NL with a synchronous grammar, a formalism closely related to tree transducers. $$$$$ To build a corpus for ROBOCUP, 300 pieces of coach advice were randomly selected from the log files of the 2003 ROBOCUP Coach Competition, which were manually translated into English (Kuhlmann et al., 2004).

We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. $$$$$ In the ROBOCUP Coach Competition, teams of agents compete on a simulated soccer field and receive coach advice written in a formal language called CLANG (Chen et al., 2003).
We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. $$$$$ Figure 1).
We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. $$$$$ The first domain is ROBOCUP.
We evaluate the system on GeoQuery (Wong and Mooney, 2006), a parallel corpus of 880 English questions and database queries about United States geography, 250 of which were translated into Spanish, Japanese, and Turkish. $$$$$ We present a novel statistical approach to parsing, for constructing a complete, formal meaning representation of a sentence.

WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ The first case arises when a component of an MR is not realized, e.g. assumed in context.

A recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. $$$$$ More specifically, a statistical word alignment model (Brown et al., 1993) is used to acquire a bilingual lexicon consisting of NL substrings coupled with their translations in the target MRL.
A recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. $$$$$ Finally, we report on experiments that show the robustness of WASP in Section 6, followed by the conclusion in Section 7.
A recent SMT-based semantic parser, WASP (Wong and Mooney, 2006), in order to produce a more effective generation system. $$$$$ Statistical machine translation by parsing.
