These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ In the chart in Figure 12, these signs are packed into an equivalence class.
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ The difference between these studies and our work is that we used feature forests to avoid the exponential increase in the number of structures that results from unpacked parse results.
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ One reason for the assumption is to enable dynamic programming algorithms, such as the Viterbi algorithm.

 $$$$$ The outside of a disjunctive node is equivalent to the outside of its daughter nodes.
 $$$$$ Y(x) is a set of y for given x; for example, in parsing, x is a given sentence and Y(x) is a parse forest for x.
 $$$$$ These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.

Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. $$$$$ Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classifiers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing.
Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. $$$$$ When training data is represented with feature forests, model parameters are estimated at a tractable cost without unpacking the forests.

As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ Future work includes not only the investigation of these features but also the abstraction of predicate–argument dependencies using semantic classes.
As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ Conjunctive nodes may represent any fragments of a complete structure, which are not necessarily linguistically meaningful.
As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ This notion yields a novel algorithm for parameter estimation for maximum entropy models, as described in the next section.
As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ The cost of the parameter estimation algorithms is bound by the computation of model expectation, µi, given as (Malouf 2002): As shown in this definition, the computation of model expectation requires the summation over Y(x) for every x in the training data.

Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ One may claim that restricting the domain of feature functions to (em, el, er) limits the flexibility of feature design.
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ With the algorithm, the computation becomes tractable.
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ The accuracies for the sentences with more than 10 words are not very different, although data points for sentences with more than 50 words are not reliable.
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ Locality: In each step of composition of structure, only a limited depth of the structures are referred to.

We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ The probability, p(t|w), of producing parse result t of a given sentence w is defined as where where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution) and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) represents the characteristics of t and w, and the corresponding model parameter λi is its weight.
We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ Following Definition 4, the first element of each set is the root node, c1, and the rest are elements of the product of {c2, c3}, {c4, c5}, and {c6, c7}.
We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ Actually, we successfully developed a probabili stic model including features on nonlocalpredicate–argument dependencies, as described subsequently.
We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ The tradeoff between parsing cost and accuracy will be examined experimentally in Section 5.4.

We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ Feature forest models are maximum entropy models defined over feature forests.
We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ The probability of a non-local dependency was conditioned on multiple words to preserve the consistency of the probability model; that is, probability p(Ilwant, dispute) in Section 4.3 was directly estimated.
We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.
We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ The corresponding conjunctive node (α' in Figure 18) has two inactives, for want and dispute1.

 $$$$$ The feature function method achieved lower accuracy in our experiments.
 $$$$$ We have several ways to integrate p¯ with the estimated model p(tjT(w)).
 $$$$$ Furthermore, it is applicable to complex data structures where an event is difficult to decompose into independent sub-events.
 $$$$$ The method provides a more flexible modeling scheme than previous methods of application of maximum entropy models to natural language processing.

This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ Hence, the outside α-product of a disjunctive node is propagated to its daughter conjunctive nodes (Figure 7).
This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.
This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ The remaining issue is how to estimate parameters.

Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ For an internal conjunctive node c E C, an unpacked tree is a combination of trees, each of which is selected from a disjunctive daughter.
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models.
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ Because inactive parts will not change during the rest of the parsing process, they can be placed in a conjunctive node.

Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ Because they did not use the probabilities of supertags in a parsing stage, their method corresponds to our “filtering only” method.
Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.
Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.
Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004).

We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ In our implementation, some of the atomic features are abstracted (i.e., ignored) for smoothing.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ This implies that experiments with only short sentences overestimate the performance of parsers.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.

For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ As mentioned, a feature forest is a packed representation of trees of features.
For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ Feature forest models are maximum entropy models defined over feature forests.
For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ Conjunctive nodes may represent any fragments of a complete structure, which are not necessarily linguistically meaningful.
For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ This notion yields a novel algorithm for parameter estimation for maximum entropy models, as described in the next section.

Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.
Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ Each feature structure expresses an equivalence class, and the arrows represent immediate-dominance relations.
Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005).
Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ Our algorithm avoids exponential explosion by representing probabilistic events with feature forests, which are packed representations of tree structures.

For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ Feature forest models are maximum entropy models defined over feature forests.
For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ Although the algorithm proposed in the present article is applicable to all of the above algorithms, we used L-BFGS for experiments.
For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.
For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ Hence, this method provides a principled solution for the estimation of consistent probabilistic distributions over feature structure grammars.

The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ Disjunctive nodes are for enumerating alternative choices.
The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ Although feature functions must be defined locally in conjunctive nodes, they are not necessarily equivalent.
The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.
The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ Because feature forests have a structure isomorphic to parse forests of PCFG, it might seem that they can represent only immediate dominance relations of CFG rules as in PCFG, resulting in only a slight, trivial extension of PCFG.

We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ Recently, the applicability of the HPSG parser to practical applications, such as information extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al.
We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ This step solves feature structure constraints given in the previous step, and fills unspecified constraints.
We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ This demonstrates that feature forest models are applicable to probabilistic models far beyond PCFGs.
