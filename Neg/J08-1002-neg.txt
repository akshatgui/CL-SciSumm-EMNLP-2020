These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ Table 16 summarizes the best performance of the HPSG parser described in this article.
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing.
These corpora are automatically parsed by Enju 2.3.1 (Miyao and Tsujii, 2008), and the features are extracted from the parsing results. $$$$$ Feature forest models are maximum entropy models defined over feature forests.

 $$$$$ The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.
 $$$$$ The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.
 $$$$$ Because a probability is defined over atomic structures, they should also be meaningful so as to be assigned a probability.
 $$$$$ The authors wish to thank the anonymous reviewers of Computational Linguistics for their helpful comments and discussions.

Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. $$$$$ Table 15 shows a manual classification of the causes of disambiguation errors in 100 sentences randomly chosen from Section 00.
Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. $$$$$ The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002).
Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. $$$$$ The remaining issue is how to estimate parameters.
Following our precious work (Wu et al, 2010), we use head-drive phrase structure grammar (HPSG) forests generated by Enju (Miyao and Tsujii, 2008), which is a state-of-the-art HPSG parser for English. $$$$$ We can expect that the same approach would be effective for maximum entropy models as well.

As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.
As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005).
As the syntactic parser, we used the Enju (Miyao and Tsujii, 2008) English HPSG parser. $$$$$ Finally, we obtain lexical entries from the HPSG parse trees.

Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ For a terminal node c E C, obviously Ω(c) = {{c}}.
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.
Models based on deep grammars such as CCG (Hockenmaier and Steed man, 2003) and HPSG (Miyao and Tsujii, 2008) could in principle use inflectional morphology, but they currently rely on functional information mainly. $$$$$ This causes an exponential explosion when estimating the parameters of maximum entropy models.

We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ Conjunctive nodes may represent any fragments of a complete structure, which are not necessarily linguistically meaningful.
We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.
We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3.
We also experimented with the ENJU parses (Miyao and Tsujii, 2008) provided by the shared task organizers. $$$$$ It could not produce a solution for complex structures as our model can.

We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.
We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models.
We used the C & C parser (Clark and Curran, 2007), ENJU (Miyao and Tsujii, 2008), and a variant of ENJU (Hara et al, 2007) adapted for the biomedical domain (i.e., ENJU-Genia); There were a number of practical issues to consider when using parsers for this task. $$$$$ A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.

 $$$$$ For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate–argument dependencies differently.
 $$$$$ All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods.
 $$$$$ However, the applicability of this algorithm relies on the constraint that features are defined locally in conjunctive nodes.
 $$$$$ Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.

This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ We first defined the notion of a feature forest, which is a packed representation of an exponential number of trees of features.
This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars.
This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.
This metric is broadly comparable to the predicate-argument dependencies of CCGBank (Hockenmaier and Steed man, 2007) or of the ENJU grammar (Miyao and Tsujii, 2008), and also somewhat similar to the grammatical relations (GR) of the Briscoe and Carroll (2006) version of DepBank. $$$$$ We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.

Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ In this definition, ˜p(x, y) is the relative frequency of (x, y) in the training data. fi is a feature function, which represents a characteristic of probabilistic events by mapping an event into a real value. λi is the model parameter of a corresponding feature function fi, and is determined so as to maximize the likelihood of the training data (i.e., the optimization in this definition).
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ With the algorithm, the computation becomes tractable.
Isozaki et al (2010b) proposed a simple method of Head Finalization, by using an HPSG-based deep parser for English (Miyao and Tsujii, 2008) to obtain phrase structures and head information. $$$$$ Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.

Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ This causes an exponential explosion when estimating the parameters of maximum entropy models.
Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.
Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ We would also like to thank Takashi Ninomiya and Kenji Sagae for their precious support.
Second, input English sentences are parsed by a high-quality parser, Enju (Miyao and Tsujii, 2008), which outputs syntactic heads. $$$$$ In such a case, the parser output nothing, and the recall was computed as zero.

We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ Clark and Curran (2004a) described a method for reducing the cost of parsing a training treebank without sacrificing accuracy in the context of CCG parsing.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.
We used Enju (Miyao and Tsujii, 2008) v2.4.2 for parsing the English side of the training data. $$$$$ However, the disambiguation model wrongly assigned a lexical entry for a transitive verb because of the sparseness of the training data (tempts occurred only once in the training data).

For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ Another reason is to estimate plausible probabilities.
For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997).
For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.
For this experiment, we choose the C & C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ The descriptive power of feature forests will be discussed again in Section 6.

Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.
Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ Section 4 describes the application of feature forest models to probabilistic HPSG parsing.
Figure 4 shows an example of text conversion and annotation alignment that are required when the Enju parser (Miyao and Tsujii, 2008) needs to be used for the annotation of protein names. $$$$$ The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.

For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ The geometry of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word, MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints of a specifier, a syntactic subject, and complements, respectively.
For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ However, if predicate–argument structures are constructed as in the manner described subsequently, they can be represented by feature forests of a tractable size.
For training, we use the feature forest model (Miyao and Tsujii, 2008), which was originally designed as an efficient algorithm for solving maximum entropy models for data with complex structures. $$$$$ We map the tuple (em, el, er), which corresponds to (m, l, r), into a conjunctive node.

The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ The outside of a disjunctive node is equivalent to the outside of its daughter nodes.
The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.
The other deep parser used was the HPSG parser Enju by Miyao and Tsujii (2008), also trained on GTB. $$$$$ From our extensive investigation of HPSG parsing, we observed that exploration of new types of features is indispensable to further improvement of parsing accuracy.

We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ Section 2 discusses a problem of conventional probabilistic models for lexicalized grammars.
We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al.
We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ However, a computational problem arises in these parameter estimation algorithms.
We employed the parser and the supertagger of (Miyao and Tsujii, 2008), specifically, its generalized modules for lexicalized grammars. $$$$$ The cost of the parameter estimation algorithms is bound by the computation of model expectation, µi, given as (Malouf 2002): As shown in this definition, the computation of model expectation requires the summation over Y(x) for every x in the training data.
