They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context.
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ (1).
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi, and let the surrounding words decide which concept associated to Wi has more relevance.

A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ Given a text sequence, SSI first identifies monosemous words and assigns the corresponding synset to them.
A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.

Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ We first insert the context words into the graph G as nodes, and link them with directed edges to their respective concepts.
Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ The Spr method lies in between, requiring 2 hours for completing the task, but its overall performance is well below the PageRank based Ppr w2w method.
Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ As before, the algorithm is applied over the MCR17 + Xwn LKB, and evaluated on the Senseval-2 all words dataset.
Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.

PPR2 (Agirre et al, 2010) $$$$$ The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi, and let the surrounding words decide which concept associated to Wi has more relevance.
PPR2 (Agirre et al, 2010) $$$$$ These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.
PPR2 (Agirre et al, 2010) $$$$$ Section 7 for further details).
PPR2 (Agirre et al, 2010) $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.

Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ The dataset consists on 2473 word instances appearing on 476 different sentences.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ However, as pointed out by (Haveliwala, 2002), the vector v can be non-uniform and assign stronger probabilities to certain kinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.

Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ Both for Spanish and English the algorithm attains performances close to the MFS.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ Here the differences are in many cases significant.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ In the traditional PageRank formulation the vector v is a stochastic normalized vector whose element values are all 1N, thus assigning equal probabilities to all nodes in the graph in case of random jumps.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ Both for Spanish and English the algorithm attains performances close to the MFS.

Tool $$$$$ We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS baseline.
Tool $$$$$ In this paper we will use two datasets for comparing graph-based WSD methods, namely, the Senseval-2 (S2AW) and Senseval-3 (S3AW) all words datasets (Snyder and Palmer, 2004; Palmer et al., 2001), which are both labeled with WordNet 1.7 tags.
Tool $$$$$ These results are surprising, as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results, compared to the automatically disambiguated gloss relations from the eXtended WordNet (linked to version 1.7).

 $$$$$ The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet.
 $$$$$ Table 1 shows the results as recall of the graph-based WSD system over these datasets on the different LKBs.
 $$$$$ The disambiguation is typically performed by applying a ranking algorithm over the graph, and then assigning the concepts with highest rank to the corresponding words.

In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ The second term represents, loosely speaking, the probability of a surfer randomly jumping to any node, e.g. without following any paths on the graph.
In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ Whereas (Navigli and Lapata, 2007) apply a depth-first search algorithm over the LKB graph —and restrict the depth of the subtree to a value of 3—, Spr relies on shortest paths between word synsets.

In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ The figure shows that, depending on the LKB complexity, the user can tune the algorithm and lower the number of iterations, thus considerably reducing the time required for disambiguation.
In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS baseline.
In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ At the end, we obtain a set of minimum length paths each of them having a different concept as a source.

In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.
In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.

The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ The damping factor, usually set in the [0.85..0.95] range, models the way in which these two terms are combined at each step.
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.

Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus disambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).
Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ The damping factor, usually set in the [0.85..0.95] range, models the way in which these two terms are combined at each step.
Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ Note that, in order to discard the effect of dangling nodes (i.e. nodes without outlinks) we slightly modified Eq.

In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ In our experiments we have tried our algorithms using three different LKBs: Given an input text, we extract the list Wi i = 1... m of content words (i.e. nouns, verbs, adjectives and adverbs) which have an entry in the dictionary, and thus can be related to LKB concepts.
In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.

In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. $$$$$ The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet.

The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. $$$$$ Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008).
The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. $$$$$ We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.

A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ In this case the MFS baseline was computed using previously availabel training data like SemCor.
A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ Note that the standard evaluation script provided in the Senseval competitions treats multiple senses as if one was chosen at random, i.e. for evaluation purposes our method is equivalent to breaking ties at random.
A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ They also compare different graph-based centrality algorithms to rank the vertices of the complete graph.
A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ (Sinha and Mihalcea, 2007) extends their previous work by using a collection of semantic similarity measures when assigning a weight to the links across synsets.

This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus disambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).
This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ Given an input piece of text (typically one sentence, or a small set of contiguous sentences), we want to disambiguate all open-class words in the input taken the rest as context.
This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ For example, if we concentrate all the probability mass on a unique node i, all random jumps on the walk will return to i and thus its rank will be high; moreover, the high rank of i will make all the nodes in its vicinity also receive a high rank.
This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ The dataset consists on 2473 word instances appearing on 476 different sentences.

The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ We have tested the graphalgorithms proposed in this paper on a Spanish dataset, using the Spanish WordNet as knowledge source (Atserias et al., 2004a).
The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus disambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).

In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus disambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ Graph-based WSD is performed over a graph composed by senses (nodes) and relations between pairs of senses (edges).
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context.
