They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ The dataset contains examples of the 150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish WordNet synsets.
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time.
They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), with two variants. $$$$$ (1) can also be seen as a smoothing factor that makes any graph fulfill the property of being aperiodic and irreducible, and thus guarantees that PageRank calculation converges to a unique stationary distribution.

A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ The Spr method lies in between, requiring 2 hours for completing the task, but its overall performance is well below the PageRank based Ppr w2w method.
A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ Regarding PageRank calculation, we have tried different numbers of iterations, and analyze the rate of convergence of the algorithm.
A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
A natural next step for this research would be to couple sense distribution estimation and the detection of unattested senses with evidence from the context, using topics or other information about the local context (e.g. Agirre and Soroa (2009)) to carry out unsupervised WSD of individual token occurrences of a given word. $$$$$ In case of ties we assign all the concepts with maximum rank.

Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ Note that for this dataset the supervised algorithm could barely improve over the MFS, suggesting that for this particular dataset MFS is particularly strong.
Moreover, jcn+wmfvec produces similar results to state of-the-art unsupervised systems on SE02, 61.92% F-mearure in (Guo and Diab, 2010) using WN1.7.1, and SE03, 57.4% in (Agirre and Soroa, 2009) using WN1.7. $$$$$ The differences between methods are not statistically significant, which is a common problem on this relatively small datasets (Snyder and Palmer, 2004; Palmer et al., 2001).

PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). $$$$$ This BFS computation is repeated for every concept of every word in the input context, storing mdpvi accordingly.
PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). $$$$$ Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.
PPR2 (Agirre et al, 2010): The system uses the UMLS meta thesaurus as a lexical knowledge graph and executes the Personalized PageRank, a state-of-the-art graph-based method, on the graph (Agirre and Soroa, 2009). $$$$$ One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words.

Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ Both for Spanish and English the algorithm attains performances close to the MFS.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) over a graph representing WordNet to disambiguate ambiguous words by taking their context into consideration. $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.

Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
Our approach uses the Personalized PageRank algorithm (Agirre and Soroa, 2009) with WordNet as the lexical knowledge base (LKB) to perform WSD. $$$$$ Both for Spanish and English the algorithm attains performances close to the MFS.

Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank. $$$$$ It is split into a train and test part, and has an “all words” shape i.e. input consists on sentences, each one having at least one occurrence of a target noun.
Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank. $$$$$ In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank. $$$$$ (Tsatsaronis et al., 2007) is another example of a two-stage process, the first one consisting on finding a relevant subgraph by performing a BFS dataset, including MFS and the best supervised system in the competition. search over the LKB.
Tool: We used UKB tool 3 (Agirre and Soroa,2009) which provides an implementation of personalized PageRank. $$$$$ The disambiguation is typically performed by applying a ranking algorithm over the graph, and then assigning the concepts with highest rank to the corresponding words.

 $$$$$ We did not try other damping factors.
 $$$$$ Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.
 $$$$$ The Spr method is very similar to (Navigli and Lapata, 2007), the main difference lying on the initial method for extracting the context subgraph.
 $$$$$ Regarding PageRank calculation, we have tried different numbers of iterations, and analyze the rate of convergence of the algorithm.

In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ In case of ties we assign all the concepts with maximum rank.
In (Agirre and Soroa, 2009), a comparative analysis of different graph-based models over two well known WSD benchmarks is reported. $$$$$ In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.

In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ The second term on Eq.
In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ Section 4 shows the experimental setting and the main results, and Section 5 compares our methods with related experiments on graph-based WSD systems.
In particular, a variant called Personalized PageRank (PPR) is proposed (Agirre and Soroa, 2009) that tries to trade-off between the amount of the employed lexical information and the overall efficiency. $$$$$ We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.

In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ (1) successively until convergence below a given threshold is achieved, or, more typically, until a fixed number of iterations are executed.
In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ It is split into a train and test part, and has an “all words” shape i.e. input consists on sentences, each one having at least one occurrence of a target noun.
In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ The main idea of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i to node j is produced, and hence the rank of node j increases.
In (Agirre and Soroa, 2009), a possible, and more accurate alternative, is also presented called PPRword2word (PPRw2w) where a different personalization vector is used for each word in a sentence. $$$$$ The algorithm converges very quickly: one sole iteration suffices for achieving a relatively high performance, and 20 iterations are enough for achieving convergence.

The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ The table shows that Ppr w2w is consistently the best method in both datasets and for all LKBs.
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable after the initial iterations (cf.
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ As before, the algorithm is applied over the MCR17 + Xwn LKB, and evaluated on the Senseval-2 all words dataset.
The intuition is that distributional evidence is able to cover the gap between word oriented usages of the PPR as for the PPRw2w defined in (Agirre and Soroa, 2009), and its sentence oriented counterpart. $$$$$ In this paper, we will use traditional PageRank to refer to the case when a uniform v vector is used in Eq.

Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.
Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ In this paper we propose a new graphbased method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
Many algorithms (as well as the one proposed by (Agirre and Soroa, 2009)) initialize the ranks of the vertex at a uniform value (usually 1/N for a graph with N vertices). $$$$$ We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS baseline.

In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ Here the differences are in many cases significant.
In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ Note that, in order to discard the effect of dangling nodes (i.e. nodes without outlinks) we slightly modified Eq.
In order to address the above problems, in line with the notion of topic-sensitive PageRank, a personalized PageRank approach has been recently devised (Agirre and Soroa, 2009) as discussed in the next section. $$$$$ Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable after the initial iterations (cf.

In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. $$$$$ In our experiments we have tried our algorithms using three different LKBs: Given an input text, we extract the list Wi i = 1... m of content words (i.e. nouns, verbs, adjectives and adverbs) which have an entry in the dictionary, and thus can be related to LKB concepts.
In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. $$$$$ Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable after the initial iterations (cf.
In (Agirre and Soroa, 2009), a novel use of PageRank for word sense disambiguation is presented. $$$$$ Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have.

The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. $$$$$ For this reason we place this method close to the MFS baseline in Table 2.
The alternative proposed in (Agirre and Soroa, 2009) allows a more static use of the full LKB. $$$$$ The TexRank algorithm (Mihalcea, 2005) for WSD creates a complete weighted graph (e.g. a graph where every pair of distinct vertices is connected by a weighted edge) formed by the synsets of the words in the input context.

A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ The next section shows how we define a modified v. PageRank is actually calculated by applying an iterative algorithm which computes Eq.
A word oriented version of the algorithm is also proposed in (Agirre and Soroa, 2009). $$$$$ The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi, and let the surrounding words decide which concept associated to Wi has more relevance.

This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ The results in Table 3 are consistent with those for English, with our algorithm approaching MFS performance.
This approach to the personalized PageRank is termed word-by-word or PPRw2w version in (Agirre and Soroa, 2009). $$$$$ Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.

The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ Regarding PageRank calculation, we have tried different numbers of iterations, and analyze the rate of convergence of the algorithm.
The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ Then, they study different graph-based centrality algorithms for deciding the relevance of the nodes on the subgraph.
The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ Table 4 shows the time spent by the different algorithms when applied to the Senseval-2 all words dataset, using the WNet17 + Xwn as LKB.
The key idea in (Agirre and Soroa, 2009) is to adapt the matrix initialization step in order to exploit the available contextual evidence. $$$$$ Such a subgraph is called a “disambiguation subgraph” GD, and it is built in the following way.

In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ As a result, every word of the context is attached to the highest ranking concept among its possible senses.
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ Table 4 shows the time spent by the different algorithms when applied to the Senseval-2 all words dataset, using the WNet17 + Xwn as LKB.
In line with the results reported in (Agirre and Soroa, 2009), experiments against two different WordNet versions, 1.7 and 3.0, have been carried out. $$$$$ Thus, we hypothesize that it captures the most relevant concepts and relations in the knowledge base for the particular input context.
