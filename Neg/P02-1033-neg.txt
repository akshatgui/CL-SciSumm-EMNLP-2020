The research most similar to ours is the work of Diab and Resnik (2002). $$$$$ Section 4 contains discussion and we conclude in Section 5.
The research most similar to ours is the work of Diab and Resnik (2002). $$$$$ Performance using this algorithm has been rigorously evaluated and is comparable with other unsupervised WSD systems, based ori fair comparison using community-wide test data.

TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). $$$$$ In of the
TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). $$$$$ Moreover, noisy annotations can serve as seeds both for monolingual supervised methods and for bootstrapping cross-linguistic sense disambiguation and sense inventories, complementing other research ori the complex problem of mapping sense tags cross linguistically (e.g.
TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). $$$$$ Moreover, it enables us to tag instances of bee with their more specific compound-noun senses when they appear within a compound that is known to the sense inventory. lit the third step, the target set is treated as a problem of monolingual sense disambiguation with respect to the target-language sense inventory.
TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.

TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). $$$$$ Consistent with Diab (2000), we added one more variant for each language in order to more closely approach the variability associated with multiple translations: in Step 2 we combined the target sets from the two MT systems.
TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.
TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). $$$$$ In of the
TransCont is based on the WSD system SALAAM (Diab and Resnik, 2002), henceforth (DR02). $$$$$ For example, the target set for French canon would have two coherent sub-clusters containing {cannon, cannonball} and {canon, theologian)}, respectively.

The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al, 2001), and Seneval 3 English AW (SV3AW) data set. $$$$$ tion using statistical models of Roget's categories on large corpora.
The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al, 2001), and Seneval 3 English AW (SV3AW) data set. $$$$$ This work has been supported, in part, by ONR MiTRI Contract FCP0.810548265, NSA RD-02-5700, and DARPA/ITO Cooperative Agreement N660010028910.
The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al, 2001), and Seneval 3 English AW (SV3AW) data set. $$$$$ The same intuitive idea is exploited by Resnik's (1999b) algorithm for disambiguating groups of related nouns, which we apply here.
The parallel data we experiment with are the same standard data sets as in (Diab and Resnik, 2002), namely, Senseval 2 English AW data sets (SV2AW) (Palmer et al, 2001), and Seneval 3 English AW (SV3AW) data set. $$$$$ Some of the sentences in the test corpus could not be automatically aligned because our aligner discards sentence pairs that are longer than a pre-defined limit.

Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ In of the
Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ Of the systems that are unsupervised, and can therefore be included in a fair comparison, only one is clearly better ort both precision and recall.
Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ tion using statistical models of Roget's categories on large corpora.
Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ tion using statistical models of Roget's categories on large corpora.

Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. $$$$$ Conversely, there are some parallel corpora large enough for training alignment models, but to our knowledge none of these have been even partially sense tagged.
Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. $$$$$ In of the
Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. $$$$$ Nouns were identified on the basis of human-assigned partof-speech tags where available (BC, WSJ and 5V2-AW) and using the Brill tagger elsewhere (Brill, 1993).
Diab and Resnik (2002) presented an unsupervised method for WSD using the same type of resource. $$$$$ In of the

Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. $$$$$ tion using statistical models of Roget's categories on large corpora.
Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. $$$$$ Having done so, we can go further: we can project the English word sense chosen for this instance of tragedy to the French word catastrophe in this context, thus tagging the two languages in tandem with a single sense inventory.
Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. $$$$$ In of the

This assumption of similar meaning when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language. $$$$$ This work has been supported, in part, by ONR MiTRI Contract FCP0.810548265, NSA RD-02-5700, and DARPA/ITO Cooperative Agreement N660010028910.
This assumption of similar meaning when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language. $$$$$ English to the corresponding words in French.
This assumption of similar meaning when multiple phrases map onto a single foreign language phrase is the converse of the assumption made in the word sense disambiguation work of Diab and Resnik (2002) which posits different word senses when a single English word maps onto different words in the foreign language. $$$$$ In of the

As mentioned in Section 1, the way that we extract paraphrases is the converse of the methodology employed in word sense disambiguation work that uses parallel corpora (Diab and Resnik, 2002). $$$$$ Table 1 shows the sizes of the component corpora.
As mentioned in Section 1, the way that we extract paraphrases is the converse of the methodology employed in word sense disambiguation work that uses parallel corpora (Diab and Resnik, 2002). $$$$$ On the other hand, that handful of words is rarely a singleton set evert for a single word/sense, because the preferences of different translators and the demands of context produce semantically similar words that differ in their nuances.
As mentioned in Section 1, the way that we extract paraphrases is the converse of the methodology employed in word sense disambiguation work that uses parallel corpora (Diab and Resnik, 2002). $$$$$ In of the
As mentioned in Section 1, the way that we extract paraphrases is the converse of the methodology employed in word sense disambiguation work that uses parallel corpora (Diab and Resnik, 2002). $$$$$ In of the

Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. $$$$$ On inspecting the target sets qualitatively, we find that they contain many outliers, largely owing to noisy alignment.
Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. $$$$$ For example, in a French-English parallel corpus, the French word catastrophe could be found in correspondence to English disaster in one instance, and to tragedy in another.
Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. $$$$$ (Alonge et al., 1998; Rodriguez et al., 1998; Vossen et al., 1999)).
Diab and Resnik (2002) use multilingual information to create an English sense tagged corpus to train a monolingual WSD approach. $$$$$ The remainder of this paper is organized as follows.

In order to accomplish these steps, Bhattacharya et al (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. $$$$$ tion using statistical models of Roget's categories on large corpora.
In order to accomplish these steps, Bhattacharya et al (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. $$$$$ Because the algorithm for disambiguating noun groupings returns a confidence value for every sense of a word, some threshold or other criterion is needed to decide which sense or senses to actually assign.
In order to accomplish these steps, Bhattacharya et al (2004) used the pseudo-translation approach of Diab and Resnik (2002): they created the model using an English-Spanish parallel corpus constructed by using Systran to translate a large collection of English text, and they obtained parallel Spanish text for the test items in the same fashion. $$$$$ Although there is no necessary assumption of directionality in translation, we will sometimes refer to the English language corpus as the target corpus and the French corpus as the source corpus, which corresponds to the characterization, in the previous section, of the French word (catastrophe) being translated into two different words (disaster and tragedy) in two diferent contexts.

Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. $$$$$ In of the
Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. $$$$$ Of the English nouns that are aligned with sourcelanguage words, approximately 35% are always aligned with the same word, rendering them untaggable using an approach based ort semantic similarity within target sets.
Pivot Language methods were also used for translation dictionary induction (Schafer and Yarowsky, 2002), word sense disambiguation (Diab and Resnik, 2002), and so on. $$$$$ The observation behind the approach, that words having the same translation often share some dimension of meaning, leads to art algorithm in which the correct sense of a word is reinforced by the semantic similarity of other words with which it shares those dimensions of meaning.

A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. $$$$$ Although there is no necessary assumption of directionality in translation, we will sometimes refer to the English language corpus as the target corpus and the French corpus as the source corpus, which corresponds to the characterization, in the previous section, of the French word (catastrophe) being translated into two different words (disaster and tragedy) in two diferent contexts.
A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. $$$$$ Because it achieves this performance using crosslanguage data alone, it is likely that improved results can be obtained by also taking advantage of monolingual contextual evidence.
A wide range of annotations from part of speech (Hi and Hwa, 2005) and chunks (Yarowsky et al, 2001) to word senses (Diab and Resnik, 2002), dependencies (Hwa et al, 2002) and semantic roles (Pado and Lapata, 2009) have been successfully transferred between languages. $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.

They have been employed in word sense disambiguation (Diab and Resnik, 2002), automatic construction of bilingual dictionaries (McEwan et al, 2002), and inducing statistical machine translation models (Koehn et al., 2003). $$$$$ The entire corpus, including the subset, is translated using commercial MT technology, producing an artificial parallel corpus.
They have been employed in word sense disambiguation (Diab and Resnik, 2002), automatic construction of bilingual dictionaries (McEwan et al, 2002), and inducing statistical machine translation models (Koehn et al., 2003). $$$$$ The choice of 5V2-AW as our gold standard corpus determined our choice of sense inventory: SENSEVAL-2 produced a gold standard for the English &quot;all words&quot; task using a pre-release of WordNet 1.7 (Fellbaum, 1998), and we restricted our attention to the noun taxonomy.
They have been employed in word sense disambiguation (Diab and Resnik, 2002), automatic construction of bilingual dictionaries (McEwan et al, 2002), and inducing statistical machine translation models (Koehn et al., 2003). $$$$$ For each French word instance f, we collect the word instance e with which it is aligned.

Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. $$$$$ Manual inspection of target sets in our experiments suggests that when target sets are semantically coherent e.g. adversaires (antagonists, opponents, contestants), accident: (accident, crash, wreck) sense assignment is generally highly accurate.
Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.
Cross-language tagging is the goal of the work by Diab and Resnik (2002), who present a method for word sense tagging both the source and target texts of parallel bilingual corpora with the WordNet sense inventory. $$$$$ Two different commercially available MT systems were used for the pseudo-translations: Globalink Pro 6.4 (GL) and Systran Professional Premium (SYS).

Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ As a result, a large number of French words will receive tags from the English sense inventory.
Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ tion using statistical models of Roget's categories on large corpora.
Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ This paper presents art unsupervised approach to word sense disambiguation that exploits translations as a proxy for semantic annotation across languages.
Word-level alignment is a critical component of a wide range of NLP applications, such as construction of bilingual lexicons (Melamed, 2000), word sense disambiguation (Diab and Resnik, 2002), projection of language resources (Yarowsky et al, 2001), and statistical machine translation. $$$$$ This paper presents art unsupervised approach to word sense disambiguation that exploits translations as a proxy for semantic annotation across languages.

The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. $$$$$ For example, in a French-English parallel corpus, the French word catastrophe could be found in correspondence to English disaster in one instance, and to tragedy in another.
The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.
The first model, which we call the Sense model, builds on the work of Diab and Resnik (2002) that uses both parallel text and a sense inventory for the target language, and recasts their approach in a probabilistic framework. $$$$$ Because it achieves this performance using crosslanguage data alone, it is likely that improved results can be obtained by also taking advantage of monolingual contextual evidence.

The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. $$$$$ After identifying and tokenizing sentences, we obtain word-level alignments for the parallel corpus using the GIZA++ implementation of the IBM statistical MT models (Och and Ney, 2000).
The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.
The main inspiration for our work is Diab and Resnik (2002), who use translations and linguistic knowledge for disambiguation and automatic sense tagging. $$$$$ Performance using this algorithm has been rigorously evaluated and is comparable with other unsupervised WSD systems, based ori fair comparison using community-wide test data.

Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.
Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). $$$$$ (Alonge et al., 1998; Rodriguez et al., 1998; Vossen et al., 1999)).
Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). $$$$$ In of the
Bengio and Kermorvant (2003) present a graphical model that is an attempt to formalize probabilistically the main ideas in Diab and Resnik (2002). $$$$$ The authors would like to thank the anonymous reviewers for their comments, Rebecca Hwa and Okan Kolak for helpful assistance and discussion, Franz Josef Och for his help with GIZA++, Adwait Ratnaparkhi for the use of MXTERMINATOR, and our collaborators at Johns Hopkins for the use of their computing facilities in parts of this work.

We show that this improves on the results of Diab and Resnik (2002). $$$$$ (Alonge et al., 1998; Rodriguez et al., 1998; Vossen et al., 1999)).
We show that this improves on the results of Diab and Resnik (2002). $$$$$ Here we briefly consider issues that bear on recall and precision, respectively.
We show that this improves on the results of Diab and Resnik (2002). $$$$$ For a target set {el, ..., en}, the algorithm considers each pair of words (e,, ei)(j and identifies which senses of the two words are most similar semantically.
We show that this improves on the results of Diab and Resnik (2002). $$$$$ Although in the end all unsupervised systems are likely to produce precision results inferior to the best supervised algorithms, they are often more practical to apply in a broad-vocabulary setting.
