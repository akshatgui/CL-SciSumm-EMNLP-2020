Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ (2008).
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ The emulated version has the same computational complexity as the original, so there is no practical reason to prefer it over the original.
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ A common strategy, and one which forms the focus of this paper, is to factor each dependency tree into small parts, which can be scored in isolation.
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ (2009, Tables 3–6). must be balanced against any resulting increase in the computational cost of the parsing algorithm.

For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. $$$$$ 06/21/98).
For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. $$$$$ All three of the “†” models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research.
For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.
For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. $$$$$ The automata are thus analogous to the rules of a CFG and attempts to use them to model grandparent indices would face difficulties similar to those already described for grammar transformations in CFGs.

Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations.
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions.
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ Our new third-order dependency parsers build on ideas from existing parsing algorithms.
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).

 $$$$$ Figure 7(b) shows how an incomplete s-span can be converted into an incomplete g-span by exchanging the internal sibling index for an external grandparent index; in the process, grand-sibling parts (g, h, m, s) are enumerated.
 $$$$$ We train each parser for 10 iterations and select paon English parsing.
 $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.
 $$$$$ Figure 3 provides a graphical specification of the second-order parsing algorithm.

Kooand Collins (2010) propose a third-order graph based parser. $$$$$ We briefly outline a few extensions to our algorithms; we hope to explore these in future work.
Kooand Collins (2010) propose a third-order graph based parser. $$$$$ We briefly outline a few extensions to our algorithms; we hope to explore these in future work.
Kooand Collins (2010) propose a third-order graph based parser. $$$$$ The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).
Kooand Collins (2010) propose a third-order graph based parser. $$$$$ Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).

In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ We now describe our first third-order parsing algorithm.
In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.
In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ Each additional ancestor lengthens the vertical scope of the factorization (e.g., from grand-siblings to “great-grandsiblings”), while increasing complexity by a factor of O(n).
In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ However, note that the head automata are defined in a sentenceindependent manner, with two automata per word in the vocabulary (ibid., Section 2).

S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3).
S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).
S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling spans, which represent the region between successive modifiers of some head.
S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.

We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ For each beam value, parsers were trained on the English training set and evaluated on the English validation set; the same beam value was applied to both training and validation data.
We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).
We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ We chose the averaged structured perceptron (Freund and Schapire, 1999; Collins, 2002) as it combines highly competitive performance with fast training times, typically converging in 5–10 iterations.
We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ 06/21/98).

For further details about the parser, see Koo and Collins (2010). $$$$$ Eisner (2000) defines dependency parsing models where each word has a set of possible “senses” and the parser recovers the best joint assignment of syntax and senses.
For further details about the parser, see Koo and Collins (2010). $$$$$ In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons.
For further details about the parser, see Koo and Collins (2010). $$$$$ If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by adapting methods in Eisner (2000).
For further details about the parser, see Koo and Collins (2010). $$$$$ For certain factorizations, efficient parsing algorithms exist for solving Eq.

The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.
The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ Incomplete g-spans are defined analogously and are denoted as Ih,..
The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ A common strategy, and one which forms the focus of this paper, is to factor each dependency tree into small parts, which can be scored in isolation.
The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008).

We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ In this section, we provide background on two relevant parsers from previous work.
We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ We evaluate our parsers on the Penn WSJ Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc et al., 2001), achieving unlabeled attachment scores of 93.04% and 87.38%.
We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.
We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ In our experiments, we find that grandchild interactions make important contributions to parsing performance (see Table 3).

As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data.
As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons.
As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ Modified versions of sibling spans will play an important role in the new parsing algorithms described in Section 4.
As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3).

 $$$$$ Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data.
 $$$$$ In the remainder of this paper, we focus on factorizations utilizing the following parts: Specifically, Sections 4.1, 4.2, and 4.3 describe parsers that, respectively, factor trees into grandchild parts, grand-sibling parts, and a mixture of grand-sibling and tri-sibling parts.
 $$$$$ Straightforward adaptations of the inside-outside algorithm (Baker, 1979) to our dynamic-programming structures would suffice to compute these quantities.
 $$$$$ We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.

We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions.
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ Since each derivation is defined by two fixed indices (the boundaries of the span) and a third free index (the split point), the parsing algorithm requires O(n3) time and O(n2) space (Eisner, 1996; McAllester, 1999).
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ Our third-order feature mappings fgsib and ftsib consist of four types of features.
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ We briefly outline a few extensions to our algorithms; we hope to explore these in future work.

Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ Figure 6 provides a graphical specification of the dynamic-programming algorithm for Model 1.
Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).
Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ First, we define 4-gram features that characterize the four relevant indices using words and POS tags; examples include POS 4-grams and mixed 4-grams with one word and three POS tags.
Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ For example, Models 1 and 2 obtain results comparable to the semi-supervised parsers of Koo et al. (2008), and additive gains might be realized by applying their cluster-based feature sets to our enriched factorizations.

While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ In summary, we make three main contributions: The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes.
While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ The first type of parser we describe uses a “firstorder” factorization, which decomposes a dependency tree into its individual dependencies.
While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons.
While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ Figure 7 provides a graphical specification of the Model 2 parsing algorithm.

 $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.
 $$$$$ Consequently, recent work in dependency parsing has been restricted to applications of secondorder parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts.
 $$$$$ In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3).
 $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.

Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ There are several possibilities for further research involving our third-order parsing algorithms.
Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ These parsing algorithms share an important characteristic: they factor dependency trees into sets of parts that have limited interactions.
Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ The authors gratefully acknowledge the following sources of support: Terry Koo and Michael Collins were both funded by a DARPA subcontract under SRI (#27-001343), and Michael Collins was additionally supported by NTT (Agmt. dtd.
Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.

Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions.
Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ If more vertical context is desired, the dynamicprogramming structures can be extended with additional ancestor indices, resulting in a “spine” of ancestors above each span.
Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ We would like to thank the anonymous reviewers for their helpful comments and suggestions.
Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ In summary, we make three main contributions: The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes.

Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. $$$$$ Finally, we make two brief remarks regarding the use of POS tags.
Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. $$$$$ The point of concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction.
