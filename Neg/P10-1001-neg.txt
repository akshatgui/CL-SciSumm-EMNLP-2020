Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ As another example, our second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams only.
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ Unfortunately, designing efficient higherorder non-projective parsers is likely to be challenging, based on recent hardness results (McDonald and Pereira, 2006; McDonald and Satta, 2007).
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons.
Regarding dependency parsing of the English PTB, currently Koo and Collins (2010) and Zhang and Nivre (2011) hold the best results, with 93.0 and 92.9 unlabeled attachment score, respectively. $$$$$ The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren.

For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. $$$$$ By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly.
For example, Koo and Collins (2010) and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.

Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ By exploiting the additional constraints arising from the factorization, maximizations or summations over the set of possible dependency trees can be performed efficiently and exactly.
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ In summary, we make three main contributions: The remainder of this paper is divided as follows: Sections 2 and 3 give background, Sections 4 and 5 describe our new parsing algorithms, Section 6 discusses related work, Section 7 presents our experimental results, and Section 8 concludes.
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).
Previous work on graph-based dependency parsing mostly adopts linear models and perceptron-based training procedures, which lack probabilistic explanations of dependency trees and do not need to compute likelihood of labeled training 1Higher-order models of Carreras (2007) and Koo and Collins (2010) can achieve higher accuracy, but has much higher time cost (O (n4)). $$$$$ A common strategy, and one which forms the focus of this paper, is to factor each dependency tree into small parts, which can be scored in isolation.

 $$$$$ We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.
 $$$$$ In brief, we train a log-linear first-order parser11 and for every sentence x in training, validation, and test data we compute the marginal probability P(h, m  |x) of each dependency.
 $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.

Kooand Collins (2010) propose a third-order graph based parser. $$$$$ Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8
Kooand Collins (2010) propose a third-order graph based parser. $$$$$ As another example, our second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams only.
Kooand Collins (2010) propose a third-order graph based parser. $$$$$ The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).
Kooand Collins (2010) propose a third-order graph based parser. $$$$$ 06/21/98).

In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ The Eisner (2000) algorithm is based on two interrelated types of dynamic-programming structures: complete spans, which consist of a headword and its descendents on one side, and incomplete spans, which consist of a dependency and the region between the head and modifier.
In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ Horizontal context can also be increased by adding internal sibling indices; each additional sibling widens the scope of the factorization (e.g., from grand-siblings to “grand-tri-siblings”), while increasing complexity by a factor of O(n).
In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ For each beam value, parsers were trained on the English training set and evaluated on the English validation set; the same beam value was applied to both training and validation data.
In the second column, [Ma08] denotes Martins et al (2008), [KC10] is Koo and Collins (2010), [Ma10] is Martins et al (2010), and [Ko10] is Koo et al (2010). $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.

S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Factored parsing can be formalized as follows: That is, we treat the dependency tree y as a set of parts p, each of which makes a separate contribution to the score of y.
S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Our new parsing algorithms could be implemented by defining the “sense” of each word as the index of its head.
S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ This can be accomplished by adapting standard chart-parsing techniques (Cocke and Schwartz, 1970; Younger, 1967; Kasami, 1965) to the recursive derivations defined in Figure 2.
S (g, h, m, s) (4) For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ One idea would be to consider extensions and modifications of our parsers, some of which have been suggested in Sections 5 and 7.4.

We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ As another example, our second-order mappings fsib and fgch define lexical trigram features, while previous work has generally used POS trigrams only.
We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ In order to evaluate the effectiveness of our parsers in practice, we apply them to the Penn WSJ Treebank (Marcus et al., 1993) and the Prague Dependency Treebank (Hajiˇc et al., 2001; Hajiˇc, 1998).6 We use standard training, validation, and test splits7 to facilitate comparisons.
We use the Model 1 version of dpo3, a state-of-the art third-order dependency parser (Koo and Collins, 2010)). $$$$$ A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree.

For further details about the parser, see Koo and Collins (2010). $$$$$ The authors gratefully acknowledge the following sources of support: Terry Koo and Michael Collins were both funded by a DARPA subcontract under SRI (#27-001343), and Michael Collins was additionally supported by NTT (Agmt. dtd.
For further details about the parser, see Koo and Collins (2010). $$$$$ A complete analysis of a sentence is given by a dependency tree: a set of dependencies that forms a rooted, directed tree spanning the words of the sentence.
For further details about the parser, see Koo and Collins (2010). $$$$$ Specifically, a sibling part consists of a triple of indices (h, m, s) where (h, m) and (h, s) are dependencies, and where s and m are successive modifiers to the same side of h. In order to parse this factorization, the secondorder parser introduces a third type of dynamicprogramming structure: sibling spans, which represent the region between successive modifiers of some head.

The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ While we have evaluated our new algorithms on standard parsing benchmarks, there are a wide variety of tasks that may benefit from the extended context offered by our thirdorder factorizations; for example, the 4-gram substructures enabled by our approach may be useful for dependency-based language modeling in machine translation (Shen et al., 2008).
The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ In this section, we provide background on two relevant parsers from previous work.
The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ A crucial limitation of factored parsing algorithms is that the associated parts are typically quite small, losing much of the contextual information within the dependency tree.
The set of potential edges was pruned using the marginals produced by a first-order parser trained using exponentiated gradient descent (Collins et al, 2008) as in Koo and Collins (2010). $$$$$ In the remainder of this paper, we focus on factorizations utilizing the following parts: Specifically, Sections 4.1, 4.2, and 4.3 describe parsers that, respectively, factor trees into grandchild parts, grand-sibling parts, and a mixture of grand-sibling and tri-sibling parts.

We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8
We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data.
We also ran MST Parser (McDonald and Pereira, 2006), the Berkeley constituency parser (Petrov and Klein, 2007), and the unmodified dpo3 Model 1 (Koo and Collins, 2010) using Conversion 2 (the current recommendations) for comparison. $$$$$ We have presented new parsing algorithms that are capable of efficiently parsing third-order factorizations, including both grandchild and sibling interactions.

As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ The term no-G indicates a parser that was trained and tested with the grandchild-based feature mappings fgch and fgsib deactivated; note that “Model 2, no-G” emulates the third-order sibling parser proposed by McDonald and Pereira (2006).
As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.
As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ 1 is impractical.
As some important relationships were represented as siblings and some as grandparents, there was a need to develop third-order parsers which could exploit both simultaneously (Koo and Collins, 2010). $$$$$ For certain factorizations, efficient parsing algorithms exist for solving Eq.

 $$$$$ The authors gratefully acknowledge the following sources of support: Terry Koo and Michael Collins were both funded by a DARPA subcontract under SRI (#27-001343), and Michael Collins was additionally supported by NTT (Agmt. dtd.
 $$$$$ Model 1 decomposes each tree into a set of grand-sibling parts—combinations of sibling parts and grandchild parts.
 $$$$$ We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.
 $$$$$ Due to space restrictions, we have been necessarily brief at some points in this paper; some additional details can be found in Koo (2010). on validation data.

We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren.
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations.
We extend Carreras (2007) graph based model with factors involving three edges similar to that of Koo and Collins (2010). $$$$$ Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8

Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ Despite the use of second-order parts, each derivation is still defined by a span and split point, so the parser requires O(n3) time and O(n2) space.
Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ We briefly outline a few extensions to our algorithms; we hope to explore these in future work.
Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ If each word in x has a set of possible “senses,” our parsers can be modified to recover the best joint assignment of syntax and senses for x, by adapting methods in Eisner (2000).
Figure 5 and 6 illustrate the third order factors, which are similar to the factors of Koo and Collins (2010). $$$$$ Third, we define backed-offfeatures that track bigram and trigram interactions which are absent in the lower-order feature mappings: for example, ftsib(x, h, m, s, t) contains features predicated on the trigram (m, s, t) and the bigram (m, t), neither of which exist in any lower-order part.

While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ Our third-order feature mappings fgsib and ftsib consist of four types of features.
While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.
While previous work using third order factors, cf. Koo and Collins (2010), was restricted to unlabeled and projective trees, our parser can produce labeled and non-projective dependency trees. $$$$$ Every dependency tree is rooted at a special “*” token, allowing the selection of the sentential head to be modeled as if it were a dependency.

 $$$$$ We also thank Regina Barzilay and Alexander Rush for their much-appreciated input during the writing process.
 $$$$$ Our new third-order dependency parsers build on ideas from existing parsing algorithms.
 $$$$$ A beam of 0.0001 was used in all experiments outside this table. rameters from the iteration that achieves the best score on the validation set.
 $$$$$ Eisner (2000) introduced a widely-used dynamicprogramming algorithm for first-order parsing; as it is the basis for many parsers, including our new algorithms, we summarize its design here.

Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.
Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ Direct integration of labels into Models 1 and 2 would result in third-order parts composed of three labeled dependencies, at the cost of increasing the time and space complexities by factors of O(L3) and O(L2), respectively, where L bounds the number of labels per dependency.
Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ We define the order of a part according to the number of dependencies it contains, with analogous terminology for factorizations and parsing algorithms.
Koo and Collins (2010) further propose a third-order model that uses third-order features. $$$$$ Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8

Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ Accuracy is measured with unlabeled attachment score (UAS): the percentage of words with the correct head.8
Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that “Model 2, no-3rd” is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren.
Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ The point of concatenation in each construction—m in Figure 2(a) or r in Figure 2(b)—is the split point, a free index that must be enumerated to find the optimal construction.
Table 7 shows the performance of the graph-based systems that were compared, where McDonald06 refers to the second-order parser of McDonald and Pereira (2006), Koo08-standard refers to the second-order parser with the features defined in Koo et al (2008), Koo10-model1 refers to the third-order parser with model1 of Koo and Collins (2010), Koo08-dep2c refers to the second-order parser with cluster-based features of (Koo et al, 2008), Suzuki09 refers to the parser of Suzuki et al. $$$$$ Our parsers are easily extended to labeled dependencies.

Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. $$$$$ We define the order of a part according to the number of dependencies it contains, with analogous terminology for factorizations and parsing algorithms.
Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. $$$$$ Our third-order feature mappings fgsib and ftsib consist of four types of features.
Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. $$$$$ In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras (2007) algorithm by “demoting” each third-order part into a second-order part: SCOREGS(x, g, h, m, s) = SCOREG(x, g, h, m) SCORETS(x, h, m, s, t) = SCORES(x, h, m, s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively.
Koo10-model1 (Koo and Collins, 2010) used the third-order features and achieved the best reported result among the supervised parsers. $$$$$ We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they reonly Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.
