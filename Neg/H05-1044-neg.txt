Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. $$$$$ However, tasks suchas multi-perspective question answering and sum marization, opinion-oriented information extraction, and mining product reviews require sentence-levelor even phrase-level sentiment analysis.
Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. $$$$$ To measure the reliability of the polarity annotation scheme, we conducted an agreement study with two annotators, using 10 documents from the MPQA Corpus.
Wilson et al (2005) proposed supervised learning, dividing the resources into prior polarity and context polarity, which are similar to polar atoms and syntactic patterns in this paper, respectively. $$$$$ The word ?Trust?

For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. $$$$$ In this paper, we present a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions.
For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. $$$$$ Of these sen tences, 28% contain no subjective expressions, 25% contain only one, and 47% contain two or more.
For example, AbuJbara et al (2013) and Jochim and Schutze (2012) find the list of polar words from Wilson et al (2005) to be useful, and neither study lists dependency relations as significant features. $$$$$ To compile the lexicon, we began with a list of subjectivity clues from (Riloff and Wiebe, 2003).

In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). $$$$$ (2) A succession of officers filled the TV screen to say they supported+ the people and that the killings were ?not tolerable?.?
In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). $$$$$ With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.
In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). $$$$$ value of 0.72.
In addition, we used subjectivity clues extracted from the lexicon by Wilson et al (2005). $$$$$ Of these sen tences, 28% contain no subjective expressions, 25% contain only one, and 47% contain two or more.

Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. $$$$$ If we consider these cases to be borderline and exclude them from the study, percent agreement increases to 90% and Kappa rises to 0.84.
Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. $$$$$ Their system classifies a much smaller proportion of the sentiment ex pressions in a corpus than ours does.
Wilson et al (2005) present a two-step process to recognize contextual polarity that employs machine learning and a variety of features. $$$$$ To make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity areincluded in our prior-polarity lexicon (General Inquirer lists (General-Inquirer, 2000) used for evaluation by Turney, and lists of manually identified pos itive and negative adjectives, used for evaluation by Hatzivassiloglou and McKeown).Some research classifies the sentiments of sen tences.

Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005). $$$$$ To compile the lexicon, we began with a list of subjectivity clues from (Riloff and Wiebe, 2003).
Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005). $$$$$ The first (66 documents/1,373 sentences/2,808 subjective expressions) is a development set, used for data exploration and feature development.
Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al, 2005). $$$$$ This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.

 $$$$$ In a dependency representation, every node in the tree structure is a surface word (i.e., there areno abstract nodes such as NP or VP).
 $$$$$ These words are included because, although their prior polarity is neu tral, they are good clues that a sentiment is beingexpressed (e.g., feels slighted, look forward to).
 $$$$$ In addition, their systems assign one sen timent per sentence; our system assigns contextual polarity to individual expressions.
 $$$$$ This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.

Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. $$$$$ We expanded the list using a dictionary and a thesaurus, and also added words from the GeneralInquirer positive and negative word lists (General Inquirer, 2000) which we judged to be potentially subjective.
Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. $$$$$ With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.
Further, there are analyses (Wiebe et al, 2005) and experiments (Wilson et al, 2005) that indicate that lexicon-lookup approaches to subjectivity analysis will have limited success on general texts. $$$$$ Thus, the annotator agreement is especially high when both are certain.

The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. $$$$$ he says, ?but we hate?
The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. $$$$$ In this paper, we present a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions.
The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. $$$$$ This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.
The former were based on the General Inquirer lexicon (Wilson et al, 2005), the MontyLingua part-of-speech tagger (Liu, 2004) and co-occurrence statistics of words with a set of predefined reference words. $$$$$ Yu and Hatzivassiloglou (2003), Kim and Hovy (2004), Hu and Liu (2004), and Grefenstette et al.

More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). $$$$$ Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.
More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). $$$$$ it.
More specifically, we use the terms in the lexicon constructed from (Wilson et al, 2005) as the indicators to identify the substructures for the convolution kernels, and extract different sub-structures according to these indicators for various types of parse trees (Section 3). $$$$$ This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.

To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). $$$$$ changes the polarity of the proposition that follows; because ?reasonable?
To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). $$$$$ With thisapproach, the system is able to automat ically identify the contextual polarity for a large subset of sentiment expressions,achieving results that are significantly bet ter than baseline.
To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). $$$$$ These patterns are high-quality, yielding quite highprecision, but very low recall.
To solve this problem, we define the indicators in this task as subjective words in a polarity lexicon (Wilson et al, 2005). $$$$$ With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.

We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). $$$$$ Adding a feature for the prior 351 Word Features word token word prior polarity: positive, negative, both, neutral Polarity Features negated: binary negated subject: binary modifies polarity: positive, negative, neutral, both, notmod modified by polarity: positive, negative, neutral, both, notmod conj polarity: positive, negative, neutral, both, notmod general polarity shifter: binary negative polarity shifter: binary positive polarity shifter: binary Table 6: Features for polarity classification polarity improves recall so that it is only 4.4% lower, but this hurts precision, which drops to 4.2% lower than the 28-feature classifier?s precision.
We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). $$$$$ Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.
We use a manually constructed polarity lexicon (Wilson et al, 2005), in which each entry is annotated with its degree of subjectivity (strong, weak), as well as its sentiment polarity (positive, negative and neutral). $$$$$ In this paper, we present a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions.

 $$$$$ To make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity areincluded in our prior-polarity lexicon (General Inquirer lists (General-Inquirer, 2000) used for evaluation by Turney, and lists of manually identified pos itive and negative adjectives, used for evaluation by Hatzivassiloglou and McKeown).Some research classifies the sentiments of sen tences.
 $$$$$ Table 1 shows the contingency table for the two annotators?
 $$$$$ To make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity areincluded in our prior-polarity lexicon (General Inquirer lists (General-Inquirer, 2000) used for evaluation by Turney, and lists of manually identified pos itive and negative adjectives, used for evaluation by Hatzivassiloglou and McKeown).Some research classifies the sentiments of sen tences.
 $$$$$ The word ?Trust?

PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005). $$$$$ falls within thisproposition, its contextual polarity becomes nega tive.
PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005). $$$$$ With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.
PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005). $$$$$ Thus, the annotator agreement is especially high when both are certain.
PRIOR-POLARITY & PRIOR-INTENSITY: We obtain these prior-attributes from the polarity lexicon populated by Wilson et al (2005). $$$$$ (2001)4 all begin by first creating prior-polaritylexicons.

 $$$$$ judgments.
 $$$$$ They also consider local negation to reverse polarity.
 $$$$$ To create a corpus for the experiments below, weadded contextual polarity judgments to existing annotations in the Multi-perspective Question Answering (MPQA) Opinion Corpus1, namely to the an notations of subjective expressions2.
 $$$$$ 2In the MPQA Corpus, subjective expressions are direct subjective expressions with non-neutral expression intensity, plus all the expressive subjective elements.

Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). $$$$$ 2In the MPQA Corpus, subjective expressions are direct subjective expressions with non-neutral expression intensity, plus all the expressive subjective elements.
Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). $$$$$ The modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence (Collins, 1997) and then converting the tree into its dependency representation (Xia and Palmer, 2001).
Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). $$$$$ Of these sen tences, 28% contain no subjective expressions, 25% contain only one, and 47% contain two or more.
Polar word Count (PC) Number of words that are polar (strong subjective words from the lexicon (Wilson et al, 2005)). $$$$$ These words are included because, although their prior polarity is neu tral, they are good clues that a sentiment is beingexpressed (e.g., feels slighted, look forward to).

We also used two datasets for the evaluation purpose: the MPQA (Wilson et al, 2005) and IQAPs (Marneffe et al, 2010) datasets. $$$$$ With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.
We also used two datasets for the evaluation purpose: the MPQA (Wilson et al, 2005) and IQAPs (Marneffe et al, 2010) datasets. $$$$$ The next step was to tag the clues in the lexicon with their prior polarity.

In this work we use MPQA (Wilson et al, 2005). $$$$$ With this approach, we are able to automatically identify the contextual polarity for a large subset ofsentiment expressions, achieving results that are sig nificantly better than baseline.
In this work we use MPQA (Wilson et al, 2005). $$$$$ value of 0.72.

The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. $$$$$ This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.
The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. $$$$$ Examples of these are verbs such as feel, look, and think, and intensifiers such asdeeply, entirely, and practically.
The MPQA lexicon contains separate lexicons for subjectivity clues, intensifiers and valence shifters (Wilson et al, 2005), which are used for identifying opinion roots, modifiers and negation words. $$$$$ This paper presents a new approach to phrase-level sentiment analysis that firstdetermines whether an expression is neu tral or polar and then disambiguates the polarity of the polar expressions.

They use the Opinion Finder lexicon (Wilson et al, 2005) and two bilingual English-Romanian dictionaries to translate the words in the lexicon. $$$$$ The words in this list were grouped in previous work according to their reliability as subjectivity clues.

To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005). $$$$$ Neutral Positive Negative Both Total Neutral 123 14 24 0 161 Positive 16 73 5 2 96 Negative 14 2 167 1 184 Both 0 3 0 3 6 Total 153 92 196 6 447 Table 1: Agreement for Subjective Expressions (Agreement: 82%, ?: 0.72) For 18% of the subjective expressions, at least oneannotator used an uncertain tag when marking po larity.
To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005). $$$$$ The annotated documents are divided into two sets.
To generate the initial explanations, one can use an off-the shelf sentiment classifier such as OpinionFinder2 (Wilson et al, 2005). $$$$$ The first (66 documents/1,373 sentences/2,808 subjective expressions) is a development set, used for data exploration and feature development.
