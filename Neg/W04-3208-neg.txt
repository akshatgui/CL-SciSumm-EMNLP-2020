 $$$$$ This is a completely unsupervised method.
 $$$$$ We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence.
 $$$$$ Our proposed method is a first approach that can extract bilingual sentence pairs from this type of very-non-parallel corpus.
 $$$$$ The word alignment principles used in previous work are as follows: Different sentence alignment algorithms based on the above principles can be found in Manning and Schűtze (1999), Somers (2001), Wu (2000), and Veronis (2002).

We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. $$$$$ Human evaluators manually check whether the matched sentence pairs are indeed parallel.
We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. $$$$$ The aim of this step is to roughly match the Chinese-English documents pairs that have the same topic, in order to extract parallel sentences from them.
We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. $$$$$ In this corpus, there are about 7,500 Chinese and 12,400 English documents, covering more around 60 different topics.

However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. $$$$$ They all use lexical information (e.g. word overlap, cosine similarity) to match documents first, before extracting sentences from these documents.
However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. $$$$$ The IBM models are commonly used for word alignment in statistical MT systems.

Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ Few of the sentence pairs turn out to be exact translations of each other, but many are bilingual paraphrases.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ It is well known that existing work on sentence alignment from parallel corpus makes use of one or multiple of the following principles (Manning and Schűtze, 1999, Somers 2001): more words that are translations of each other tend to be translations themselves.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ We found that sentence pairs with high alignment scores are not necessarily more similar than others.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ The word alignment principles used in previous work are as follows: Different sentence alignment algorithms based on the above principles can be found in Manning and Schűtze (1999), Somers (2001), Wu (2000), and Veronis (2002).

The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ Figure1.
The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ We can iterate this whole process for improved results using a Bootstrapping method.
The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ We have also learned that as bilingual corpora become less parallel, it is better to rely on lexical information rather than sentence length and position information.

Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ These methods have also been applied recently in a sentence alignment shared task at NAACL 20031.
Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Evaluation results show that our approach achieves 65.7% accuracy and a 50% relative improvement from baseline.
Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ This very-non-parallel corpus, TDT3 data, includes documents that are off-topic, i.e. documents with no corresponding topic in the other language.
Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Matching bilingual sentence pairs are extracted from different corpora using existing and the proposed methods.

Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ A few of the Chinese and English passages are almost translations of each other.
Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ In addition, we also found that the precision of parallel sentence pair extraction increases steadily over each iteration, until convergence.
Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ We further find other documents that are similar to each of the monolingual documents found.

Another approach focused on sentence extraction (Fung and Cheung, 2004). $$$$$ Munteanu et al., (2004) constructed a comparable corpus of Arabic and English news stories by matching the publishing dates of the articles.
Another approach focused on sentence extraction (Fung and Cheung, 2004). $$$$$ We investigate the performance of the IBM Model 4 EM lexical learner on data from very-non-parallel corpus, and evaluate how our method can boost its performance.
Another approach focused on sentence extraction (Fung and Cheung, 2004). $$$$$ We suggest the “find-one-get-more” principle, which claims that as long as two documents are found to contain one pair of parallel sentence, they must contain others as well.

A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004) $$$$$ The word alignment principles used in previous work are as follows: Different sentence alignment algorithms based on the above principles can be found in Manning and Schűtze (1999), Somers (2001), Wu (2000), and Veronis (2002).
A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004) $$$$$ 6.4.
A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004) $$$$$ For comparable corpora, the alignment principle made in previous work is as follows: with high similarity scores – “find-topic-extract-sentence” We take a step further and propose a new principle for our task:

We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ Munteanu et al., (2004) constructed a comparable corpus of Arabic and English news stories by matching the publishing dates of the articles.
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration.
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ However, our experiments showed that adding parallel corpus gives no improvement on the final output.
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ We show that this scores increases as the parallel-ness or comparability of the corpus increases.

The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ Based on the above, we conclude that EM lexical learning has little effect on the overall parallel sentence extraction output.
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ We also apply the IBM Model 4 EM lexical learning to find unknown word translations from the extracted parallel sentences from our system.
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ They can be described as on-topic documents.
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ It is well known that existing work on sentence alignment from parallel corpus makes use of one or multiple of the following principles (Manning and Schűtze, 1999, Somers 2001): more words that are translations of each other tend to be translations themselves.

 $$$$$ Various methods have been previously proposed to extract parallel sentences from multilingual corpora.

Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. $$$$$ This initial step is based on the same “find-topic-extract-sentence” principle as in earlier works.
Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. $$$$$ A corpus such as Hong Kong News contains documents that are in fact rough translations of each other, focused on the same thematic topics, with some insertions and deletions of paragraphs.
Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. $$$$$ These methods have also been applied recently in a sentence alignment shared task at NAACL 20031.
Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. $$$$$ Conversely, the context sentences of translated word pairs are similar.

For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ We have also learned that as bilingual corpora become less parallel, it is better to rely on lexical information rather than sentence length and position information.
For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ Evaluation results show that our approach achieves 65.7% accuracy and a 50% relative improvement from baseline.
For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ Figure1.
For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ There are 110,000 Chinese sentences and 290,000 English sentences in TDT3, which lead to more than 30 billion possible sentence pairs.
