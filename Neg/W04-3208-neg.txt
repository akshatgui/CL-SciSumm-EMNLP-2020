 $$$$$ Finally, a very-non-parallel corpus is one that contains far more disparate, very-non-parallel bilingual documents that could either be on the same topic (in-topic) or not (off-topic).
 $$$$$ Conversely, the context sentences of translated word pairs are similar.
 $$$$$ We have also learned that as bilingual corpora become less parallel, it is better to rely on lexical information rather than sentence length and position information.

We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. $$$$$ Based on the above, we conclude that EM lexical learning has little effect on the overall parallel sentence extraction output.
We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. $$$$$ For comparable corpora, the alignment principle made in previous work is as follows: with high similarity scores – “find-topic-extract-sentence” We take a step further and propose a new principle for our task:
We base our candidate generation on a method that Fung and Cheung (2004) developed for extracting loose translations (comparable sentence pairs) from quasi-comparable corpora [9], as shown in Figure 2. $$$$$ These methods have also been applied recently in a sentence alignment shared task at NAACL 20031.

However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. $$$$$ However, we implement a baseline method that follows the same “find-topic-extract-sentence” principle as in earlier work.
However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. $$$$$ It is well known that existing work on sentence alignment from parallel corpus makes use of one or multiple of the following principles (Manning and Schűtze, 1999, Somers 2001): more words that are translations of each other tend to be translations themselves.
However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. $$$$$ The parallel-ness of each type of corpus is quantified by a lexical matching score calculated for the bi-lexicon pair distributed in the aligned bilingual sentence pairs.
However, we can understand the improvement by comparing against scores obtained using the cosine-based lexical similarity metric which is typical of the majority of previous methods for mining non-parallel corpora, including that of Fung and Cheung (2004) [9]. $$$$$ This is probably due to the fact that whereas EM does find new word translations (such as A诺切特/Pinochet), this has little effect on the overall glossing of the Chinese document since such new words are rare.

Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ From the in-topic documents, most are found to have high similarity.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ A noisy parallel corpus, sometimes also called a “comparable” corpus, contains non-aligned sentences that are nevertheless mostly bilingual translations of the same document.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents. $$$$$ Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data.

The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ In fact, both Zhao and Vogel (2002) and Barzilay and Elhadad (2003) assume similar sentence orders and applied dynamic programming in their work.
The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ For comparable corpora, the alignment principle made in previous work is as follows: with high similarity scores – “find-topic-extract-sentence” We take a step further and propose a new principle for our task:
The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ Some of the bilingual sentences are translations of each other, while some others are bilingual paraphrases.
The degree of parallelism can vary greatly, ranging from noisy parallel documents that contain many parallel sentences, to quasi parallel documents that may cover different topics (Fung and Cheung, 2004). $$$$$ It is well known that existing work on sentence alignment from parallel corpus makes use of one or multiple of the following principles (Manning and Schűtze, 1999, Somers 2001): more words that are translations of each other tend to be translations themselves.

Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ All these corpora have documents in the same, matching topics.
Sample comparable sentences that contain parallel phrases other similarity measures (Fung and Cheung, 2004) have been used for the document alignment task. $$$$$ Barzilay and Elhadad (2003) mined paraphrasing sentences from weather reports.

Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ Few of the sentence pairs turn out to be exact translations of each other, but many are bilingual paraphrases.
Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents.
Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ The Hong Kong Laws Corpus is a parallel corpus with manually aligned sentences, and is used as a parallel sentence resource for statistical machine translation systems.
Based on this observation, dynamic program ming (Yang and Li, 2003), similarity measures such as Cosine (Fung and Cheung, 2004) or word and translation error ratios (Abdul-Rauf and Schwenk, 2009), or maximum entropy classifier (Munteanu and Marcu, 2005) are used for discovering parallel sentences. $$$$$ For noisy parallel corpora, sentence alignment is based on embedded content words.

Another approach focused on sentence extraction (Fung and Cheung, 2004). $$$$$ We show that this scores increases as the parallel-ness or comparability of the corpus increases.
Another approach focused on sentence extraction (Fung and Cheung, 2004). $$$$$ In fact, both Zhao and Vogel (2002) and Barzilay and Elhadad (2003) assume similar sentence orders and applied dynamic programming in their work.
Another approach focused on sentence extraction (Fung and Cheung, 2004). $$$$$ There have been conflicting definitions of the term “comparable corpora” in the research community.

A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004): parallel indicates sentence-aligned texts that are in translation relation; noisy characterizes two documents that are never the less mostly bilingual translations of each other; topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts. $$$$$ Since the IBM model parameters can be better estimated if the input sentences are more parallel, we have tried to add parallel sentences to the extracted sentence pairs in each iteration step, as proposed by Zhao and Vogel (2002).
A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004): parallel indicates sentence-aligned texts that are in translation relation; noisy characterizes two documents that are never the less mostly bilingual translations of each other; topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts. $$$$$ They all use lexical information (e.g. word overlap, cosine similarity) to match documents first, before extracting sentences from these documents.
A web interface was developed in order to annotate each pair, following the distinction introduced by Fung and Cheung (2004): parallel indicates sentence-aligned texts that are in translation relation; noisy characterizes two documents that are never the less mostly bilingual translations of each other; topic corresponds to documents which share similar topics, but that are not translation of each others and very-non that stands for rather unrelated texts. $$$$$ Since many more multilingual texts available today contain documents that do not have matching documents in the other language, we propose finding more parallel sentences from off-topic documents, as well as on-topic documents.

We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ Since many more multilingual texts available today contain documents that do not have matching documents in the other language, we propose finding more parallel sentences from off-topic documents, as well as on-topic documents.
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ For noisy parallel corpora, sentence alignment is based on embedded content words.
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ We show that this scores increases as the parallel-ness or comparability of the corpus increases.
We would like to stress that, while conducting the manual annotation, we frequently found difficult to label pairs of articles with the classes proposed by Fung and Cheung (2004). $$$$$ We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.

The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ For example, in the following extracted sentence pair, the English sentence has the extra phrase “under the agreement”, which is missing from the Chinese sentence: The precision of parallel sentences extraction is 65.7% for the top 2,500 pairs using our method, which has a 50% improvement over the baseline.
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ We present a method capable of extracting parallel sentences from far more disparate “very-non-parallel corpora” than previous “comparable corpora” methods, by exploiting bootstrapping on top of IBM Model 4 EM.
The last case study of document and sentence alignment from ―very-non-parallel corpora is the work from Fung and Cheung (2004). $$$$$ We then identify bilingual word pairs that appear in the matched sentence pairs by using a bilingual lexicon (bilexicon).

 $$$$$ From the in-topic documents, most are found to have high similarity.
 $$$$$ We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.
 $$$$$ This novel principle allows us to add parallel sentences from documents, to the baseline set.
 $$$$$ For comparable corpora, the alignment principle made in previous work is as follows: with high similarity scores – “find-topic-extract-sentence” We take a step further and propose a new principle for our task:

Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. $$$$$ Rapp (1995), Grefenstette (1998), Fung and Lo (1998), and Kaji (2003) derived bilingual lexicons or word senses from such corpora.
Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel. $$$$$ We investigate the performance of the IBM Model 4 EM lexical learner on data from very-non-parallel corpus, and evaluate how our method can boost its performance.

For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ The precision of the parallel sentences extracted is 42.8% for the top 2,500 pairs, ranked by sentence similarity scores.
For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ Similar to previous work, comparability is defined by cosine similarity between document vectors.
For example, Munteanu and Mar cu (2005) apply the Lemur IR toolkit, Utiyama and Isahara (2003) use the BM25 similarity measure, and Fung and Cheung (2004) use cosine similarity. $$$$$ In the following section, we discuss how to find new word translations.
