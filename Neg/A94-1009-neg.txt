Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ The experiments were conducted using two taggers, one written in C at Cambridge University Computer Laboratory, and the other in C++ at Sharp Laboratories.
Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased.
Elworthy (1994), in contrast, reports an accuracy of 75.49%, 80.87% and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ The general pattern of the results presented does not vary greatly with the corpus and tagset used.

While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). $$$$$ While these may be useful heuristics from a practical point of view, the next step forward is to look for an automatic way of predicting the accuracy of the tagging process given a corpus and a model.
While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). $$$$$ With a good lexicon but either degraded transitions or a test corpus differing from the training corpus, the pattern tends to be early maximum.
While this observation confirms Elworthy (1994), the impact of error reduction is much less than reported there for English about 70% (79? 94). $$$$$ The results were confirmed and extended at Sharp Laboratories of Europe.

Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.
Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. $$$$$ The work described here was carried out at the Cambridge University Computer Laboratory as part of Esprit BR Project 7315 &quot;The Acquisition of Lexical Knowledge&quot; (Acquilex-II).
Experimental results confirming this wisdom have been presented ,e.g., by Elworthy (1994) and Pereira and Schabes (1992) for EM training of Hidden Markov Models and PCFGs. $$$$$ I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.

Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful. $$$$$ The results are summarised in table 1, for various corpora, where F denotes the &quot;most frequent tag&quot; test.
Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful. $$$$$ A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows:
Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful. $$$$$ The patterns are summarised in table 3, each entry in the table showing the patterns for the two tests under the given conditions.
Although [Kupiec, 1992] presented a very sophisticated method of unsupervised learning, [Elworthy, 1994] reported that re-estimation is not always helpful. $$$$$ We could expect to achieve D1 from, say, a printed dictionary listing parts of speech in order of frequency.

 $$$$$ In two of the patterns, the re-estimation ultimately reduces the accuracy of the tagging rather than improving it.
 $$$$$ For example, the LOB tagset used 134 tags, while the Penn treebank tagset has 48.
 $$$$$ It indicates that the model is converging towards an optimum which is better than its starting point.
 $$$$$ Various methods of quoting accuracy have been used in the literature, the most common being the proport ion of words (tokens) receiving the correct tag.

However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. $$$$$ Alternatively, a procedure called BaumWelch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial model, and the resulting probabilities used to determine new values for the lexical and transition probabilities.
However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. $$$$$ The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation.
However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. $$$$$ The results were confirmed and extended at Sharp Laboratories of Europe.
However, Merialdo (1994) and Elworthy (1994) have criticized methods of estimation from an untagged corpus based on the maximum likelihood principle. $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.

Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. $$$$$ An example of each of the three behaviours is shown in figure 1.
Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. $$$$$ 1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus.
Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. $$$$$ A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows:
Discuss ion and Future Work Merialdo (1994) and Elworthy (1994) have insisted, based on their experimental results, that the maximum likelihood training using an untagged corpus does not necessarily improve tagging accuracy. $$$$$ I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.

On this point, I agree with Merialdo (1994) and Elworthy (1994). $$$$$ The patterns are summarised in table 3, each entry in the table showing the patterns for the two tests under the given conditions.
On this point, I agree with Merialdo (1994) and Elworthy (1994). $$$$$ Clearly, if the expected pattern is initial maximum, we should not use BW at all, if early maximum, we should halt the process after a few iterations, and if classical, we should halt the process in a &quot;standard&quot; way, such as comparing the perplexity of successive models.
On this point, I agree with Merialdo (1994) and Elworthy (1994). $$$$$ Early maximum Rising accuracy for a small number of iterations (2-4), and then falling as in initial maximum.

 $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.
 $$$$$ The training and test corpora were drawn from the LOB corpus and the Penn treebank.
 $$$$$ Any transitions not seen in the training corpus are given a small, non-zero probability.

Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting. $$$$$ Alternatively, a procedure called BaumWelch (BW) re-estimation may be used, in which an untagged corpus is passed through the FB algorithm with some initial model, and the resulting probabilities used to determine new values for the lexical and transition probabilities.
Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting. $$$$$ For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-l3-3 from parts B to J inclusive.
Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting. $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.
Elworthy (1994) and Merialdo (1994) demonstrated that Baum-Welch does not necessarily improve the performance of an HMM part-of speech tagger when deployed in an unsupervised or semi-supervised setting. $$$$$ A model was trained from a handtagged corpus in the manner described above, and then degraded in various ways to simulate the effect of poorer training, as follows:

 $$$$$ I thank Ted Briscoe for his guidance and advice, and the AN LP referees for their comments.
 $$$$$ D1 Lexical probabilities are correctly ordered, so that the most frequent tag has the highest lexical probability and so on, but the absolute values are otherwise unreliable.
 $$$$$ Convergence of the model is improved by keeping the number of parameters in the model down.

Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). $$$$$ To assist in this, low frequency items in the lexicon are grouped together into equivalence classes, such that all words in a given equivalence class have the same tags and lexical probabilities, and whenever one of the words is looked up, then the data common to all of them is used.
Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). $$$$$ As in the experiments above, BW reestimation gave a decrease in accuracy when the starting point was derived from a significant amount of hand-tagged text.
Our findings support those of Elworthy (1994) and Merialdo (1994) for POS tagging and suggest that EM is not always the most suitable semi-supervised training method (especially when some in-domain training data is available). $$$$$ The second experiment reveals that there are three distinct patterns of Baum-Welch reestimation.

Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ A better measure is the proportion of ambiguous words which are given the correct tag, where by ambiguous we mean that more than one tag was hypothesised.
Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ During the first experiment, it became apparent that Baum-Welch re-estimation sometimes decreases the accuracy as the iteration progresses.
Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ 1 Background Part-of-speech tagging is the process of assigning grammatical categories to individual words in a corpus.
Elworthy (1994), in contrast, reports accuracy of 75.49%, 80.87%, and 79.12% for unsupervised word-based HMM trained on parts of the LOB corpora, with a tag set of 134 tags. $$$$$ In this paper, I report two experiments designed to determine how much manual training information is needed.

We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced. $$$$$ Early work in the field relied on a corpus which had been tagged by a human annotator to train the model.
We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced. $$$$$ FB assigns a probability to every tag on every word, while Viterbi prunes tags which cannot be chosen because their probability is lower than the ones of competing hypotheses, with a corresponding gain in computational efficiency.
We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced. $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.
We considered three taggers: the El worthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al, 2006), an enhanced. $$$$$ The results were confirmed and extended at Sharp Laboratories of Europe.

In [Elworthy, 1994], similar experiments were run. $$$$$ Any transitions not seen in the training corpus are given a small, non-zero probability.
In [Elworthy, 1994], similar experiments were run. $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.
In [Elworthy, 1994], similar experiments were run. $$$$$ The first experiment concerned the effect of the initial conditions on the accuracy using Baum-Welch re-estimation.

The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy. $$$$$ Early work in the field relied on a corpus which had been tagged by a human annotator to train the model.
The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy. $$$$$ Writing these as f(i, j), f(i) and f(i, w) respectively, the transition probability from tag i to tag j is estimated as f (i, j)/ f (i) and the lexical probability as f(i, w)/ f (i).
The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy. $$$$$ The step forward taken in the work here is to show that there are three patterns of reestimation behaviour, with differing guidelines for how to use BW effectively, and that to obtain a good starting point when a hand-tagged corpus is not available or is too small, either the lexicon or the transitions must be biased.
The advantage of combining unsupervised and supervised learning over using supervised in [Elworthy, 1994] quotes accuracy on ambiguous words, which we have converted to overall accuracy. $$$$$ From the observations in the previous section, we propose the following guidelines for how to train a HMM for use in tagging: able, use BW re-estimation with standard convergence tests such as perplexity.
