Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. $$$$$ Furthermore, extending the grammar base of the deep system results in much better accuracy at a cost of a factor of 5 in speed.
Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. $$$$$ The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers.
Accuracy for XLE is not given, because the results reported by Kaplan et al (2004) compare labeled functional dependencies drawn from LFG f-structures with equivalents derived automatically from Collins outputs. $$$$$ 2), and a regularizer penalty of 10 was found optimal for the Bl prior used in stochastic disambiguation.

For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). $$$$$ The stochastic component is also “ambiguity-enabled” in the sense that the computations for statistical estimation and selection of the most probable analyses are done efficiently by dynamic programming, avoiding the need to unpack the parse forests and enumerate individual analyses.
For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). $$$$$ (The number of words indicated at the beginning of the input string was also reduced accordingly.)
For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). $$$$$ A certain amount of effort was required to remove the part-ofspeech tags and labeled brackets of the WSJ corpus in a way that restored the sentences to a standard English format (for example, to remove the space between wo and n’t that remains when the POS tags are removed).
For estimation and best-parse searching, efficient dynamic programming techniques over features forests are employed (see Kaplan et al (2004)). $$$$$ From this perspective, the nearly 75% F-score that is achieved for our deterministic rewriting of Collins’ trees into dependencies is remarkable, even if the results are not directly comparable.

Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. $$$$$ This multiword analysis was chosen because many applications do not require the internal structure of names, and the identification of named entities is now typically carried out by a separate non-syntactic pre-processing module.
Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. $$$$$ Time spent for finite-state morphology and dictionary lookup for XLE is part of the measure of its timing performance.
Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. $$$$$ The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers.
Kaplan et al (2004) report high parsing speeds for a deep parsing system which uses an LFG grammar: 1.9 sentences per second for 560 sentences from section 23 of the Penn Treebank. $$$$$ For example, the limit on the length of medial phrases was set to 20 words for the XLE system (see Sec.

We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al, 2004). $$$$$ Our experiment is comparable to recent work on reading off Propbank-style (Kingsbury and Palmer, 2002) predicate-argument relations from gold-standard treebank trees and automatic parses of the Collins parser.
We have developed a parsing system that explores this space, in the vein of systems like (Kaplan et al, 2004). $$$$$ We also employed a combined i1 regularization and feature selection technique described in Riezler and Vasserman (2004) that considerably speeds up estimation and guarantees small feature sets for stochastic disambiguation.

We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. $$$$$ For both XLE and the Collins parser we wrote conversion programs to transform the normal (tree or fstructure) output into the corresponding relations of the dependency bank.
We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. $$$$$ For our experiments we tried to define a conversion that gives appropriate credit to the dependencies that can be read from the trees without relying on an undue amount of sophisticated linguistic knowledge5.
We use the same 560 sentence subset from the DepBank utilised by Kaplan et al (2004) in their study of parser accuracy and efficiency. $$$$$ These are parsed by the grammar as described in section 2.

Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. $$$$$ However, in other experiments with an automatic finite-state extractor, we have found that the time for named-entity recognition is negligible (on the order of seconds across the entire corpus) and makes relatively few errors, so that the results reported here are good approximations of what might be expected in more realistic situations.
Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. $$$$$ We did not include the time for dependency extraction or stemming the Collins output.
Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. $$$$$ There are two operations involved in stochastic disambiguation, namely calculating feature-values from a parse forest and calculating node weights from a feature forest.
Kaplan et al (2004) compare the Collins (2003) parser with the Parc LFG parser by mapping LFG F structures and Penn Treebank parses into DepBank dependencies, claiming that the LFG parser is considerably more accurate with only a slight reduction in speed. $$$$$ Contrary to conventional wisdom, we found that the shallow system was not substantially faster than the deep parser operating on a core grammar, while the deep system was significantly more accurate.

Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. $$$$$ We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times.
Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. $$$$$ The Collins parser is thought to be accurate and fast and thus to represent a reasonable trade-off between “good-enough” output, speed, and robustness.
Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. $$$$$ Since the PARC 700 treats proper names as multiword expressions, we then augmented the input strings with XML markup of the named entities.
Kaplan et al (2004) clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies, but they also note that 'This conversion was relatively straightforward for LFG structures. $$$$$ In the notation of Miyao and Tsujii (2002), such a feature forest Φ is defined as a tuple (C, D, r, γ, S) where C is a set of conjunctive nodes, D is a set of disjunctive nodes, r E C is the root node, γ : D → 2C is a conjunctive daughter function, and S : C → 2D is a disjunctive daughter function.

In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. $$$$$ NPs under VPs are read off either as objects or adjuncts, depending on whether or not the NP is annotated with the argument indicator (-A) as in this example; the -A presumably would be missing in a sentence like John arrived Friday, and Friday would be treated as an ADJUNCT.
In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. $$$$$ For the Collins parser, a beam size of 1000 was found to improve speed considerably at little cost in accuracy.
In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. $$$$$ The predicates of LFG f-structures are already represented as citation forms; for a fair comparison we ran the leaves of the Collins tree through the same stemmer modules as part of the tree-to-dependency translation.
In the case of Kaplan et al (2004), the testing procedure would include running their con version process on Section 23 of the Penn Treebank and evaluating the output against DepBank. $$$$$ Furthermore, the np-bracketing flag (npbflag) was set to 0 to produce an extended set of NP levels for improved argument/adjunct distinction6.

Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). $$$$$ This was done by putting underscores between the parts of the named entity and changing the final part of speech tag to the appropriate one (usually NNP) if necessary.
Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). $$$$$ The complete version gives an overall improvement in F-score of 5% over the Collins parser at a cost of a factor of 5 in parsing time.
Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). $$$$$ This paper reports some experiments that compare the accuracy and performance of two stochastic parsing systems.
Note that statistical parsers can equally suffer from this problem, see e.g. (Kaplan et al., 2004). $$$$$ In the experiments reported in this paper, we used a threshold on feature-extraction that allowed us to cut off feature-extraction in 3% of the cases at no loss in accuracy.

they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). $$$$$ We also adjust for systematic differences in the choice of heads.
they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). $$$$$ We also note that proper names appear in the DEPBANK as single multi-word expressions without any internal structure.
they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). $$$$$ However, in other experiments with an automatic finite-state extractor, we have found that the time for named-entity recognition is negligible (on the order of seconds across the entire corpus) and makes relatively few errors, so that the results reported here are good approximations of what might be expected in more realistic situations.
they can be used to disprefer (actually ignore) rarely-applicable rules, in order to reduce parse time (Kaplan et al 2004). $$$$$ The stochastic disambiguation model we employ defines an exponential (a.k.a. log-linear or maximum-entropy) probability model over the parses of the LFG grammar.

The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. $$$$$ From this perspective, the nearly 75% F-score that is achieved for our deterministic rewriting of Collins’ trees into dependencies is remarkable, even if the results are not directly comparable.
The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. $$$$$ Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses.
The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. $$$$$ 2), and a regularizer penalty of 10 was found optimal for the Bl prior used in stochastic disambiguation.
The second extrapolation is to the LFG XLE parser (Kaplan et al 2004) for English, consisting of a highly developed symbolic parser and grammar, an OT-based preference component, and a stochastic back end to select among remaining alternative parser outputs. $$$$$ In this format each triple specifies that a particular relation holds between a head and either another head or a feature value, for example, that the SUBJ relation holds between the heads run and dog in the sentence The dog ran.

The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004). $$$$$ We presented some experiments that compare the accuracy and performance of two stochastic parsing systems, the shallow Collins parser and the deep-grammar-based XLE system.
The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004). $$$$$ In this display we have eliminated the head lexical items that appear redundantly at all the nonterminals in a head chain, instead indicating by a single number which daughter is the head.

The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). $$$$$ We presented some experiments that compare the accuracy and performance of two stochastic parsing systems, the shallow Collins parser and the deep-grammar-based XLE system.
The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). $$$$$ Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses.
The disadvantage of such parsers is that they are typically not very efficient, parsing a few sentences per second on commodity hardware (Kaplan et al, 2004). $$$$$ This was done by putting underscores between the parts of the named entity and changing the final part of speech tag to the appropriate one (usually NNP) if necessary.

Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). $$$$$ This was captured for the LFG parser by using named entity markup and for the Collins parser by creating complex word forms with a single POS tag (section 5).
Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). $$$$$ To control the cost of this computation, our stochastic disambiguation system includes a user-specified parameter for bounding the amount of work that is done in calculating feature-values.
Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). $$$$$ The first conjunct tends to be marked as the head of a coordination in Model 3 output, whereas the dependency bank has a more symmetric representation: it introduces a new COORD head and connects that up to the conjunction, and it uses a separate CONJ relation for each of the coordinated items.
Currently our best automatically induced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II tree bank and evaluating against the DCU1051 and 80.24% against the PARC 700 Dependency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004). $$$$$ This multiword analysis was chosen because many applications do not require the internal structure of names, and the identification of named entities is now typically carried out by a separate non-syntactic pre-processing module.

 $$$$$ An additional feature of the DEPBANK that is relevant to our comparisons is that dependency heads are represented by their standard citation forms (e.g. the verb swam in a sentence appears as swim in its dependencies).
 $$$$$ But common wisdom has it that parsing systems based on deep linguistic grammars are too difficult to produce, lack coverage and robustness, and also have poor run-time performance.
 $$$$$ This was captured for the LFG parser by using named entity markup and for the Collins parser by creating complex word forms with a single POS tag (section 5).
 $$$$$ As a concrete example, the dependency list in Fig.

We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). $$$$$ The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers.
We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). $$$$$ Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses.
We achieve between 77.68% and 80.24% against the PARC 700 following the experiments in (Kaplan et al, 2004). $$$$$ In our system, tokenization and morphological analysis are performed by finite-state transductions arranged in a compositional cascade.

Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%. $$$$$ This multiword analysis was chosen because many applications do not require the internal structure of names, and the identification of named entities is now typically carried out by a separate non-syntactic pre-processing module.
Against the PARC 700, the hand-crafted LFG grammar reported in (Kaplan et al, 2004) achieves an f score of 79.6%. $$$$$ This conversion was relatively straightforward for LFG structures (King et al., 2003).

Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). $$$$$ However, in other experiments with an automatic finite-state extractor, we have found that the time for named-entity recognition is negligible (on the order of seconds across the entire corpus) and makes relatively few errors, so that the results reported here are good approximations of what might be expected in more realistic situations.
Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). $$$$$ Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses.
Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). $$$$$ To prepare a true gold standard of dependencies, the tentative set of dependencies produced by the robust parser was then corrected and extended by human validators2.
Evaluating against the PARC 700 Dependency Bank, the P-PCFG achieves an f-score of 80.24%, an overall improvement of approximately 0.6% on the result reported for the best hand-crafted grammars in (Kaplan et al, 2004). $$$$$ The deletions involved features produced by the grammar but not included in the PARC 700 such as negative values of PASS, PERF, and PROG and the feature MEASURE used to mark measure phrases.

Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). $$$$$ The XLE parser is a deep-parsing system that couples a Lexical Functional Grammar to a loglinear disambiguation component and provides much richer representations theory.
Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). $$$$$ The corpus is freely available for research and evaluation, as are documentation and tools for displaying and pruning structures.3 In our experiments we used a Reduced version of the DEPBANK, including just the minimum set of dependencies necessary for reading out the central semantic relations and properties of a sentence.
Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). $$$$$ We measured the accuracy of both systems against a gold standard of the PARC 700 dependency bank, and also measured their processing times.
Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). $$$$$ The currently popular Collins parser is a shallow parser whose output contains more detailed semanticallyrelevant information than other such parsers.

The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004). $$$$$ Average sentence length of sentences in DEPBANK is 19.8 words, and the average number of dependencies per sentence is 65.4.
The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004). $$$$$ But common wisdom has it that parsing systems based on deep linguistic grammars are too difficult to produce, lack coverage and robustness, and also have poor run-time performance.
The data is divided into two sets, a 140-sentence development set and a test set of 560 sentences (Kaplan et al, 2004). $$$$$ As described by (King et al., 2003), the annotations were boot-strapped by parsing the sentences with a LFG grammar and transforming the resulting f-structures to a collection of dependency triples in the DEPBANK format.
