We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ Still, even if the tensor- and matrix-based decompositions turned out to have comparable effects, tensor-based smoothing is more attractive in the DM framework because we could perform the decomposition once, and use the smoothed tensor as our stable underlying DM (modulo, of course, memory problems with computing such a large tensor decomposition).

Several approaches have investigated the above mentioned problem $$$$$ Tensors are multi-way arrays, conventionally denoted by boldface Euler script letters: X (Turney 2007; Kolda and Bader 2009).
Several approaches have investigated the above mentioned problem $$$$$ The lexical information encoded in the LexDM suffixes, however, is not used to generate different links, but to implement a different counting scheme as part of a different scoring function.
Several approaches have investigated the above mentioned problem $$$$$ Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.
Several approaches have investigated the above mentioned problem $$$$$ We consider a further strength of the DM approach that it naturally encourages us to think, as we did in these cases, of ways to tackle apparently unrelated tasks with the existing resources, rather than devising unrelated approaches to deal with them.

Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ For example, we can hypothesize that dog–tail is more similar to car–wheel than to dog–animal, because the patterns connecting dog and tail (of, have, etc.) are more like those of car–wheel than like those of dog–animal (is a, such as, etc.).
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Another issue we completely ignored but which will be of fundamental importance in applications is how a DM-based system can deal with outof-vocabulary items.
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.

Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ We propose a similar split for the role of supervision in DSMs.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ Tucker decomposition applied to the mode-3 tuple tensor could capture patterns of higher order co-occurrence for each of the modes.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ A general framework for distributional semantics should satisfy the following two requirements: (1) representing corpus-derived data in such a way as to capture aspects of meaning that have so far been modeled with different, prima facie incompatible data structures; (2) using this common representation to address a large battery of semantic experiments, achieving a performance at least comparable to that of state-of-art, taskspecific DSMs.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ At the other end, DM provides sets of vectors in different vector spaces, but it is agnostic about how they are used (measuring similarity via cosines or other measures, reducing the matrices with SVD, etc.).

More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ In this respect, we think that our general results are clear even where they are not supported by statistical inference, or interpretation of the latter is problematic.
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ With this aim in mind, we introduce Distributional Memory (DM), a generalized framework for distributional semantics.
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ We will conclude, instead, by discussing some general advantages that follow from the DM approach of separating corpus-based model building, the multi-purpose long term distributional memory, and different views of the memory data to accomplish different semantic tasks, without resorting to the source corpus again.
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ The results are reported in Table 16.

The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ Thus, although other parameter combinations might lead to dramatic changes in Tucker performance, the best SVD performance in the table is probably close to the SVD performance upper bound.
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ Turney generalizes DSMs to tackle relational similarity and represents pairs of words in the space of the patterns that connect them in the corpus.
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ Of course, much of the interesting progress in distributional semantics will occur at the two ends of our tensor, with better tuple extraction and weighting techniques on one side, and better matrix manipulation and similarity measurement on the other.

We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ We can use the W1LxW2 space to explore the semantic properties of syntactic frames, and to extract generalizations about the inner structure of lexico-semantic representations of the sort formal semanticists have traditionally been interested in.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ ), and make a decision about a potential synonym pair based on their occurrence in similar contexts.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ See Turney and Pantel (2010) for references and discussion.

In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ Relations among entities can be represented by ternary tuples, or triples.
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ As an alternative to this “one task, one model” approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ For instance, language is full of productive semantic phenomena, such as the selectional preferences of verbs with respect to unseen arguments (eating topinambur vs. eating sympathy).
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ Although we stop at this point, the procedure can be seen as a DM version of popular iterative bootstrapping algorithms such as Espresso (Pantel and Pennacchiotti 2006): Start with some examples of the target relation, find links that are typical of these examples, use the links to find new examples, and so on.

 $$$$$ We leave the exploration of parameter space in DM for future research.
 $$$$$ Our focus is entirely on the second aspect.
 $$$$$ The third-order tensor formalization of distributional data allows DM to fully exploit the potential of corpus-derived tuples.
 $$$$$ As in our simple example of smoothing relational pairs with attributional neighbors, more complex tasks may be tackled by combining different views of DM, and/or resorting to different (sub)spaces within the same view, as in our approach to selectional preferences.

Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ He observes that tasks that are traditionally solved with an attributional similarity approach can be recast as relational similarity tasks.
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ Words like dog and puppy, for example, are attributionally similar in the sense that their meanings share a large number of attributes: They are animals, they bark, and so on.
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.

We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ Therefore, given a weighted tuple structure TW extracted from a corpus and subject to the constraints we just mentioned, by matricizing the corresponding labeled third-order tensor Xλ we obtain the following four distinct semantic vector spaces: word by link–word (W1xLW2): vectors are labeled with words w1, and vector dimensions are labeled with tuples of type (l, w2); word–word by link (W1W2xL): vectors are labeled with tuples of type (w1, w2), and vector dimensions are labeled with links l; word–link by word (W1LxW2): vectors are labeled with tuples of type (w1,l), and vector dimensions are labeled with words w2; link by word–word (LxW1W2): vectors are labeled with links l and vector dimensions are labeled with tuples of type (w1,w2).
We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ However, Tucker decomposition and SVD are computationally intensive procedures, and, at least with our current computational resources, we are not able to decompose even the smallest DM tensor (similarly, we cannot apply SVD to a full matricization).

LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ We can now safely claim that DM satisfies both these desiderata, and thereby represents a genuine step forward in the quest for a general purpose approach to distributional semantics.
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ Most problems encountered by DSMs in tackling this challenge are specific instances of more general issues concerning the possibility of representing symbolic operations with distributed, vector-based data structures (Markman 1999).
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ For the parameters concerning the reduced tensor fitting process, we accept the default values of the Tensor Toolbox.
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.

A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ Crucially, these on-demand matrices are derived from the same underlying resource (the tensor) and correspond to different “views” of the same data, extracted once and for all from a corpus.
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ For instance, the Sketch Engine1 builds “word sketches” consisting of triples extracted from parsed corpora and formed by two words linked by a grammatical relation (Kilgarriff et al. 2004).
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ Although we see the importance of these issues, we believe that a real breakthrough in DSMs can only be achieved by overcoming the limits of current two-way models of distributional data.
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ Our focus is entirely on the second aspect.

Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ At least as a trend, tensor decomposition appears to be better than matrix decomposition, but only marginally so (Turney does not perform this comparison).
Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.
Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ For example, if we apply mode-1 matricization to the tensor of dimensionality 3 x 2 x 3 in Table 2, we obtain the matrix A3x6 in Table 3 (ignore the labels for now).
Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ In turn, both these models differ from those used to explore concept properties or argument alternations.

we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ The main element of novelty is the fact that methods originally developed to resort to ad hoc distributional spaces are now adapted to fit into the unified DM framework.
we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ This is the only experiment in which we operate on the tensor directly, rather than on the matrices derived from it, paving the way to a more active role for the underlying tensor in the DM approach to semantics.
we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ Distributional categorization has been investigated for other POS as well, most notably verbs (Merlo and Stevenson 2001; Schulte im Walde 2006).

The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ We present a pilot study in this direction in Section 6.5.
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ Given the sentence The teacher eats a red apple, structured DSMs would not consider eat as a legitimate context for red and would distinguish the object relation connecting eat and apple as a different type of co-occurrence from the modifier relation linking red and apple.

Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ Observe that each column of the matrix is a mode-1 fiber of the tensor: The first column is the x*11 fiber; the second column is the x*21 fiber, and so on.
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ Here, we start with examples of noun–quale pairs (n, qr) that instantiate a role r. We project all LxW1W2 vectors in a subspace where only dimensions corresponding to one of the example pairs are non-zero.
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ The two words can be concatenated, treating the links as basis elements, in order to model relational similarity (Pantel and Pennacchiotti 2006; Turney 2006b).

Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., finding parts).
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ After framing our proposal within the general debate on co-occurrence modeling in distributional semantics (Section 2), we introduce the DM framework in Section 3 and compare it to other unified approaches in Section 4.
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ The third-order tensor formalization of distributional data allows DM to fully exploit the potential of corpus-derived tuples.
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ Although they are not explicitly expressed in these terms, relation extraction algorithms (Hearst 1992, 1998; Girju, Badulescu, and Moldovan 2006; Pantel and Pennacchiotti 2006) also rely on relational similarity, and focus on learning one relation type at a time (e.g., finding parts).

The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ Thus, a trained DM can become a general-purpose resource and be used by researchers beyond the realms of the NLP community, whereas applying PairClass requires a good understanding of various aspects of computational linguistics.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ The third-order tensor formalization of distributional data allows DM to fully exploit the potential of corpus-derived tuples.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ While some of the ad hoc models that were developed to tackle specific tasks do outperform our most successful DM implementation, the latter is never too far from the top, without any task-specific tuning.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ LexDM.

Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ Moreover, DSMs for attributional and relational similarity are widely used for the semi-automatic bootstrapping or extension of terminological repositories, computational lexicons (e.g., WordNet), and ontologies (Buitelaar, Cimiano, and Magnini 2005; Lenci 2010).
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ In a smoothed tensor, by the influence of tuples such as (city, obj, destroy) and (city, sbj tr, destroy), these tuples will get some non-0 weight that, hopefully, will make the object relation between city and destruction emerge.
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ Existing structured DSMs still couch this information directly in binary structures, namely, co-occurrence matrices, thereby giving rise to different semantic spaces and losing sight of the fact that such spaces share the same kind of distributional information.
