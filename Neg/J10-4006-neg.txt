We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ Pad´o and Lapata (2007), partly inspired by Lowe (2001), have proposed an interesting general formalization of DSMs.
We use a distributional resource for French, built on a 200M word corpus extracted from the French Wikipedia, following principles laid out in (Bourigault, 2002) from a structured model (Baroni and Lenci, 2010), i.e. using syntactic contexts. $$$$$ Still, given the same underlying DM and a certain task, much work can be done to exploit the DM optimally in the task, with no need to go back to corpus-based resource construction.

Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. $$$$$ The entry (i, j) in the i-th row and j-th column of a matrix A is denoted by aij.
Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. $$$$$ Specifically, we extract from our best tensor TypeDM the values labeled by tuples (wAP, l, w2) where wAP is in the AP set, l is one of the 100 most common links occurring in tuples with a wAP, and w2 is one of the 1,000 most common words occurring in tuples with a wAP and a l. The resulting (sub-)tensor, APTypeDM, has dimensionality 402 x 100 x 1, 000 with 1,318,214 non-zero entries (density: 3%).
Several approaches have investigated the above mentioned problem: (Baroni and Lenci, 2010) use a representation based on third order tensors and provide a general framework for distributional semantics in which it is possible to represent several aspects of meaning using a single data structure. $$$$$ It is hard to see how such integration could be pursued within generalized systems, such as PairClass (Turney 2008), that require keeping a full corpus around and corpus-processing know-how on behalf of interested researchers from outside the NLP community (see discussion in Section 4 above).

Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Thus, before selecting the top 20 links ranked by PMI, we filter out those links that do not have at least 10 non-zero dimensions in the positive subspace.
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ However, if we looked from a distance at the whole field of DSMs we would see that, besides the general assumption shared by all models that information about the context of a word is an important key in grasping its meaning, the elements of difference overcome the commonalities.
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ Although we see the importance of these issues, we believe that a real breakthrough in DSMs can only be achieved by overcoming the limits of current two-way models of distributional data.
Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived. $$$$$ We will point out connections to related research specific to the various tasks in the sections devoted to describing their reinterpretation in DM.

Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ The suffix substring n-the-j encodes the information that w1 is a singular noun (n), is definite (the), and has an adjective (j) that does not belong to the list of high frequency adjectives.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ This is not surprising (better distributional tuples should still be better when seen from different views), but it is good to have an empirical confirmation of the a priori intuition.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.
Example entries in Baroni and Lenci (2010)'s tensor is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. $$$$$ We leave the investigation of all these possibilities to further studies.

More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ The difference between Strudel, the best model from the earlier literature, and TypeDM is not statistically significant, according to a paired t-test across the target concepts (t = 1.1, df = 43, p = 0.27).
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ Where other models outperform TypeDM by a large margin, there are typically obvious reasons for this: The rivals have been trained on much larger corpora, or they rely on special knowledge resources, or on sophisticated machine learning algorithms.
More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w, v and a connecting link-word l. $$$$$ Thus, a trained DM can become a general-purpose resource and be used by researchers beyond the realms of the NLP community, whereas applying PairClass requires a good understanding of various aspects of computational linguistics.

The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.
The 11 most frequent contexts in Baroni and Lenci (2010)'s tensor (v and j represent verbs and adjectives, respectively). $$$$$ A mode-n fiber is obtained by fixing the values of two indices, and by taking the tensor entries corresponding to the full range of values of the third index.

We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ In DV, we would derive a single gun feature with frequency 40.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ The weighted tuple structures differ for the choice of links in L and/or for the scoring function σ. DepDM.
We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional sub space, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC). $$$$$ At the other end, DM provides sets of vectors in different vector spaces, but it is agnostic about how they are used (measuring similarity via cosines or other measures, reducing the matrices with SVD, etc.).

In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ Other possibilities, such as graph-based algorithms operating directly on the graph defined by the tensor (Baroni and Lenci 2009), or deriving unstructured semantic spaces from the tensor by removing one of the indices, are left to future work.
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ Still, given the same underlying DM and a certain task, much work can be done to exploit the DM optimally in the task, with no need to go back to corpus-based resource construction.
In the case of vectors obtained from Baroni and Lenci (2010)'s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1). $$$$$ The other two distinct spaces generated by tensor matricization look less familiar, and yet we argue that they allow us to subsume under the same general DM framework other interesting semantic phenomena.

 $$$$$ Differently from other current proposals that share similar aims, we believe that the lack of generalization in corpus-based semantics stems from the choice of representing co-occurrence statistics directly as matrices—geometrical objects that model distributional data in terms of binary relations between target items (the matrix rows) and their contexts (the matrix columns).
 $$$$$ This pattern connects adjectives and nouns that occur in the templates (the) attribute noun of (a|the) NOUN is ADJ (Almuhareb and Poesio 2004) and (a|the) ADJ attribute noun of NOUN (Veale and Hao 2008): the color of strawberries is red -4 (red, color+j+ns, strawberry); the autumnal color of the forest -4 (autumnal, color+j+n-the, forest); as adj as: this pattern links an adjective and a noun that match the template as ADJ as (a|the) NOUN (Veale and Hao 2008): as sharp as a knife -4 (sharp, as adj as+j+n-a, knife); such as: links two nouns occurring in the templates NOUN such as NOUN and such NOUN as NOUN (Hearst 1992, 1998): animals such as cats-4 (animal, such as+ns+ns, cat); such vehicles as cars -4 (vehicle, such as+ns+ns, car).
 $$$$$ DepDM is a 30,693 x 796 x 30,693 tensor with density 0.0149% (the proportion of non-zero entries in the tensor).

Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ Although it is still unclear which is the best way to compose the representation of content words in vector spaces, it is nowadays widely assumed that structured representations like those adopted by DM are in the right direction towards a solution to this issue, exactly because they allow distributional representations to become sensitive to syntactic structures (Erk and Pad´o 2008).
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ We propose a similar split for the role of supervision in DSMs.
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ The possible combinations of target lower n-ranks constitute a large tridimensional parameter space, and we leave its systematic exploration to further work.
Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010). $$$$$ Turney’s approach amounts to picking a task (identifying pairs expressing the same relation) and reinterpreting other tasks as its particular instances.

We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ The weighted tuple structures differ for the choice of links in L and/or for the scoring function σ. DepDM.
We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ Cimiano and Wenderoth asked 30 subjects to produce qualia for these words (each word was rated by at least three subjects), obtaining a total of 1,487 word– quale pairs, instantiating the four roles postulated by Pustejovsky: Formal (the category of the object: door–barrier), Constitutive (constitutive parts, materials the object is made of: food–fat), Agentive (what brings the object about: letter–write), and Telic (the function of the object: novel–entertain).
We add to the models we constructed the freely available Distributional Memory (DM) model, that has been shown to reach state-of-the-art performance in many semantic tasks (Baroni and Lenci,2010). $$$$$ The differences between TypeDM and the other, generally worse-performing DM models remind us that the idea of a shared distributional memory per se is not enough to obtain good results, and the extraction of an ideal DM from the corpus certainly demands further attention.

LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ We define a labeled tensor Xλ as a tensor such that for each of its indices there is a one-to-one mapping of the integers from 1 to I (the dimensionality of the index) to I distinct strings, that we call the labels of the index.
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ In linguistics, cognitive science, and AI, semantic and conceptual knowledge is represented in terms of symbolic structures built around typed relations between elements, such as synsets, concepts, properties, and so forth.
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ The number of shared triples is then used to measure the attributional similarity between word pairs.
LMI proved to be a good measure for different semantic tasks, see for example the work of Baroni and Lenci, 2010. $$$$$ We can use the W1LxW2 space to explore the semantic properties of syntactic frames, and to extract generalizations about the inner structure of lexico-semantic representations of the sort formal semanticists have traditionally been interested in.

A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ We need to reach a better understanding of which pieces of distributional information to extract, and whether different semantic tasks require focusing on specific subsets of distributional data.
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods.
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ A weighted tuple structure TW built from W1, L, and W2 can be represented by a labeled third-order tensor Xλ with its three indices labeled by W1, L, and W2, respectively, and such that for each weighted tuple t E TW = ((w1,l,w2),vt) there is a tensor entry (i : w1, j : l,k : w2)= vt.
A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)). $$$$$ This is at the moment just a conjecture, but it constitutes an exciting direction for further work focusing on tensor decomposition within the DM framework.

Their cognitive relevance for language has been supported by studies of child lexical development (Li et al, 2004), category-related deficits (Vigliocco et al, 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al (2007) and Baroni and Lenci (2010) for a review). $$$$$ We omit discussion of our own work that the DM framework is an extension and generalization of Baroni et al. (2010) and Baroni and Lenci (2009).

we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature.
we used local mutual information (LMI) as proposed by Baroni and Lenci (2010). $$$$$ In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.

The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ Moreover, Turney only derives the W1×LW2 space from the tensor, and does not discuss the possibility of using the tensor-based formalization to unify different views of semantic data, which is instead our main point.
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ Distributional triples are also used in computational lexicography to identify the grammatical and collocational behavior of a word and to define its semantic similarity spaces.
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ Another issue we completely ignored but which will be of fundamental importance in applications is how a DM-based system can deal with outof-vocabulary items.
The DM model is described in detail by Baroni and Lenci (2010), where it is referred to as TypeDM. $$$$$ In our experiments in Section 6.1 herein, the performance of unstructured and structured models trained on the same corpus is in general comparable.

Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ Van de Cruys (2009) used tensor decomposition to find commonalities in latent dimensions across the fiber labels (in the DM formalism, this would amount to finding commonalities across w1, l, and w2 elements).
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ .
Refer to Baroni and Lenci (2010) for how the surface realizations of a feature are determined. $$$$$ This severely limits its interdisciplinary appeal.

Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ The W1xLW2 matricization of APTypeDM results in a 402 x 1, 000, 000 matrix with 66,026 non-zero columns and the same number of non-zero entries and density as the tensor.
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ The suffix is in turn formed by two substrings separated by a +, each respectively encoding the following features of w1 and w2: their POS and morphological features (number for nouns, number and tense for verbs); the presence of an article (further specified with its definiteness value) and of adjectives for nouns; the presence of adverbs for adjectives; and the presence of adverbs, modals, and auxiliaries for verbs, together with their diatheses (for passive only).
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ Further work should investigate how we could improve TypeDM by exploring various parameter settings (many of which do not require going back to the corpus: feature selection and reweighting, SVD, etc.).
Baroni and Lenci (2010) make the vectors of their best-performing Distributional Memory (dm) model available. $$$$$ We chose the parameter values of Turney’s main model (his “baseline LRA system”).

The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ We will illustrate in Section 6.4 one function of this space, namely, to perform feature selection, picking links that can then be used to determine a meaningful subspace of the W1W2xL space.
The mcrae set (McRae et al, 1998) consists of 100 noun-verb pairs, with top performance reached by the DepDM system of Baroni and Lenci (2010), a count DSM relying on syntactic information. $$$$$ We leave the exploration of parameter space in DM for future research.

Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ In this space, nouns are represented as vectors with dimensions that are labeled with (link, word) tuples, where the word might be a verb, and the link might stand for, among other things, syntactic relations such as obj (or, in the LexDM model, an expansion thereof, such as obj+the-j).
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes.
Indeed, in several cases they are close, or even better than those attained by dm, a linguistically-sophisticated count based approach that was shown to reach top performance across a variety of tasks by Baroni and Lenci (2010). $$$$$ Observe that each column of the matrix is a mode-1 fiber of the tensor: The first column is the x*11 fiber; the second column is the x*21 fiber, and so on.
