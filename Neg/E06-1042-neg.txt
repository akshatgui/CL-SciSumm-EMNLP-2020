Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ These sets contain feature lists consisting of the stemmed nouns and verbs in a sentence, with target or seed words and frequent words removed.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ Annotations are from testing or from active learning during example-base construction.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ To add context, we simply group the sentence containing the target word with a specified number of surrounding sentences and turn the whole group into a single feature set.

The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ If the similarity is above this threshold, we label a target-word sentence as literal or nonliteral.
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ We suggest that TroFi is applicable to all sorts of nonliteral language, and that, although it is currently focused on English verbs, it could be adapted to other parts of speech and other languages.
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ In addition, we can either move the offending item to the opposite feedback set or remove it altogether.
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ Finally, we used our optimal configuration of TroFi, together with active learning and iterative augmentation, to build the TroFi Example Base, a publicly available, expandable resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing, but also as training data for other statistical NLP tasks.

Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ We reduce the problem of nonliteral language recognition to one of word-sense disambiguation by redefining literal and nonliteral as two different senses of the same word, and we adapt an existing similarity-based word-sense disambiguation method to the task of separating usages of verbs into literal and nonliteral clusters.
Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ Consider an example based on a similar example from an automated medical claims processing system.
Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.
Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ After an initial run for a particular target word, we have the cluster results plus a record of the feedback sets augmented with the newly clustered sentences.

A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ We used the iterative augmentation process to build a small example base consisting of the target words from Table 1, as well as another 25 words drawn from the examples of scholars whose work was reviewed in Section 2.
A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ We must determine that the sentence “she hit the ceiling” is meant literally before it can be marked up as an ACCIDENT claim.
A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ An excerpt from the example base is shown in Figure 2.

In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ In terms of metonymy, TroFi may cluster a verb used in a metonymic expression such as “I read Keats” as nonliteral, but we make no strong claims about this.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ N4 She had a hand in his fully comprehending the quandary.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ If there are no literals, literal recall is 100%; literal precision is 100% if there are no nonliterals in the literal cluster and 0% otherwise.

In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ Nissim & Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim & Markert, 2003), p. 56).
In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ For all our models and algorithms, we carried out detailed experiments on hand-annotated data, both to fully evaluate the system and to arrive at an optimal configuration.
In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ We suggest that TroFi is applicable to all sorts of nonliteral language, and that, although it is currently focused on English verbs, it could be adapted to other parts of speech and other languages.

Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). $$$$$ This paper focuses on the algorithmic enhancements necessary to facilitate this transformation from word-sense disambiguation to nonliteral language recognition.
Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). $$$$$ It also provides the feedback set sizes for each target word.
Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). $$$$$ Even before voting, we attempt to improve the correctness of initial attractions through the use of SuperTags, which allows us to add internal structure information to the bag-of-words feature lists.
Birke and Sarkar (2006) explain their scoring as follows: Literal recall is defined as (correct literals in literal cluster/ total correct literals); Literal precision is defined as (correct literals in literal cluster/ size of literal cluster). $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.

Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ The results are shown in Figure 1.
Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ We call our method nearly unsupervised.
Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ Literal precision is defined as (correct literals in literal cluster / size of literal cluster).
Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ Finally, we used our optimal configuration of TroFi, together with active learning and iterative augmentation, to build the TroFi Example Base, a publicly available, expandable resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing, but also as training data for other statistical NLP tasks.

NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ We further motivate the usefulness of the ability to recognize literal vs. nonliteral usages using an example from the Recognizing Textual Entailment (RTE-1) challenge of 2005.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ Sentences attracted to neither, or equally to both, sets are put in the opposite cluster to where they belong.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ (Dolan, 1995)).

Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.
Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ Many systems that use NLP methods – such as dialogue systems, paraphrasing and summarization, language generation, information extraction, machine translation, etc.
Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.
Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ The main difference between the Nissim & Markert algorithm and the TroFi algorithm – besides the fact that Nissim & Markert deal with specific types of metonymy and not a generalized category of nonliteral language – is that Nissim & Markert use a supervised machine learning algorithm, as opposed to the primarily unsupervised algorithm used by TroFi.

 $$$$$ → Kerry shot Bush.
 $$$$$ The work on supervised metonymy resolution by Nissim & Markert and the work on conceptual metaphors by Mason come closest to what we are trying to do with TroFi.
 $$$$$ In this paper, we propose TroFi (Trope Finder), a nearly unsupervised clustering method for separating literal and nonliteral usages of verbs.
 $$$$$ This paper focuses on the algorithmic enhancements necessary to facilitate this transformation from word-sense disambiguation to nonliteral language recognition.

Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ Since this baseline actually attempts to distinguish between literal and nonliteral and uses all the data used by the TroFi algorithm, it is the one we will refer to in our discussion below.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ Using majority-rules voting with Learners A and D doubled, we were able to obtain an average f-score of 48.4%, showing that voting does to an extent balance out the learners’ varying results on different words.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ We further motivate the usefulness of the ability to recognize literal vs. nonliteral usages using an example from the Recognizing Textual Entailment (RTE-1) challenge of 2005.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ Each feedback set sentence is saved with a classifier weight, with newly clustered sentences receiving a weight of 1.0.

Birke and Sarkar (2006) also used WSD. $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.
Birke and Sarkar (2006) also used WSD. $$$$$ For example, given the target verb “pour”, we would expect TroFi to cluster the sentence “Custom demands that cognac be poured from a freshly opened bottle” as literal, and the sentence “Salsa and rap music pour out of the windows” as nonliteral, which, indeed, it does.
Birke and Sarkar (2006) also used WSD. $$$$$ The main difference between the Nissim & Markert algorithm and the TroFi algorithm – besides the fact that Nissim & Markert deal with specific types of metonymy and not a generalized category of nonliteral language – is that Nissim & Markert use a supervised machine learning algorithm, as opposed to the primarily unsupervised algorithm used by TroFi.
Birke and Sarkar (2006) also used WSD. $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.

Birke and Sarkar (2006) requires WordNet. $$$$$ The TroFi Example Base is available at http://www.cs.sfu.ca/˜anoop/students/jbirke/.
Birke and Sarkar (2006) requires WordNet. $$$$$ We used the iterative augmentation process to build a small example base consisting of the target words from Table 1, as well as another 25 words drawn from the examples of scholars whose work was reviewed in Section 2.
Birke and Sarkar (2006) requires WordNet. $$$$$ (This is just an example; we do not compute entailments.)
Birke and Sarkar (2006) requires WordNet. $$$$$ This paper focuses on the algorithmic enhancements necessary to facilitate this transformation from word-sense disambiguation to nonliteral language recognition.

Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ Each feedback set sentence is saved with a classifier weight, with newly clustered sentences receiving a weight of 1.0.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ First, we examine iterative augmentation.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.

Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ Nissim & Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim & Markert, 2003), p. 56).
Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.
Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004).

Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. $$$$$ The use of additional context was responsible for our second largest leap in performance after sum of similarities.
Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. $$$$$ We suggest that TroFi is applicable to all sorts of nonliteral language, and that, although it is currently focused on English verbs, it could be adapted to other parts of speech and other languages.
Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification: literal vs. non-literal, and they do not clearly define the kinds of tropes they aim to discover. $$$$$ For all our models and algorithms, we carried out detailed experiments on hand-annotated data, both to fully evaluate the system and to arrive at an optimal configuration.

Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ “grasp his hand”).
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ Using the TroFi algorithm, we also build the TroFi Example Base, an extensible resource of annotated literal/nonliteral examples which is freely available to the NLP research community.
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ Nonliteral precision and recall are defined similarly.

Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ Consider an example based on a similar example from an automated medical claims processing system.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.

Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ Worth noting is that the target words exhibiting the most significant improvement, “drown” and “grasp”, had some of the smallest target and feedback set feature sets, supporting the theory that adding cogent features may improve performance.
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ Finally, we used our optimal configuration of TroFi, together with active learning and iterative augmentation, to build the TroFi Example Base, a publicly available, expandable resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing, but also as training data for other statistical NLP tasks.
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.
