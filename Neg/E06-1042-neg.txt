Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ We adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners, a voting schema, and additional features like SuperTags and extrasentential context.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ An excerpt from the example base is shown in Figure 2.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birke and Sarkar, 2006), or mainly focus on their noncompositionality (Katz and Giesbrecht, 2006). $$$$$ TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.

The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ As per ((Di Eugenio & Glass, 2004), cf. refs therein), the standard assessment for n values is that tentative conclusions on agreement exists when .67 ≤ n < .8, and a definite conclusion on agreement exists when n ≥ .8.
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ For example, turning “It’s hard to kick a habit like drinking” into “habit drink kick/B nx0Vpls1 habit/A NXN,” results in a higher attraction to sentences about “kicking habits” than to sentences like “She has a habit of kicking me when she’s been drinking.” Note that the creation of Learners A and B changes if SuperTags are used.
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ Metaphor processing has even been approached with connectionist systems storing world-knowledge as probabilistic dependencies (Narayanan, 1999).
The idiomatic/literal token classification methods of Birke and Sarkar (2006) and Katz and Giesbrecht (2006) rely primarily on the local context of a token, and fail to exploit specific linguistic properties of non-literal language. $$$$$ We must determine that the sentence “she hit the ceiling” is meant literally before it can be marked up as an ACCIDENT claim.

Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.
Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ Only using the context, “She broke her thumb while she was cheering for the Patriots and, in her excitement, she hit the ceiling,” can we decide.
Past work on the problem of distinguishing literal and metaphorical senses has approached it as a classical word sense disambiguation (WSD) task (Birke and Sarkar, 2006). $$$$$ There can be as many runs as desired; hence iterative augmentation.

A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.
A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.
A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ In the next section we look at the Core TroFi algorithm and its use of the above data sources.
A subset of twenty-five of the fifty verbs was used by Birke and Sarkar (2006). $$$$$ Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.

In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ Annotations are from testing or from active learning during example-base construction.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.
In our second experiment, we duplicate the setup of Birke and Sarkar (2006) so that we can compare our results with theirs. $$$$$ Running TroFi produces new clusters and re-weighted classifiers augmented with newly clustered sentences.

In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ In this paper, we propose TroFi (Trope Finder), a nearly unsupervised clustering method for separating literal and nonliteral usages of verbs.
In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ The KE algorithm is based on the principle of attraction: similarities are calculated between sentences containing the word we wish to disambiguate (the target word) and collections of seed sentences (feedback sets) (see also Section 3.1).
In the third experiment, we train the algorithm on the twenty-five new verbs that were not used by Birke and Sarkar (2006) and then we test it on the old verbs. $$$$$ We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.

Birke and Sarkar (2006) explain their scoring as follows $$$$$ In addition, we can either move the offending item to the opposite feedback set or remove it altogether.
Birke and Sarkar (2006) explain their scoring as follows $$$$$ We suggest that TroFi is applicable to all sorts of nonliteral language, and that, although it is currently focused on English verbs, it could be adapted to other parts of speech and other languages.
Birke and Sarkar (2006) explain their scoring as follows $$$$$ We adapted an existing word-sense disambiguation algorithm to literal/nonliteral clustering through the redefinition of literal and nonliteral as word senses, the alteration of the similarity scores used, and the addition of learners and voting, SuperTags, and additional context.

Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.
Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ Two sentences are similar if they contain similar words and two words are similar if they are contained in similar sentences.
Birke-Sarkar refers to the best result reported by Birke and Sarkar (2006), using a form of active learning. $$$$$ There can be as many runs as desired; hence iterative augmentation.

NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ First, we examine iterative augmentation.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ In the case of a larger scale annotation effort, having the person leading the effort provide one or two examples of literal and nonliteral usages for each target verb to each annotator would almost certainly improve inter-annotator agreement.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types.
NA indicates scores that were not calculated by Birke and Sarkar (2006). $$$$$ TroFi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies.

Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ Our inter-annotator agreement on the annotations used as test data in the experiments in this paper is quite high. n (Cohen) and n (S&C) on a random sample of 200 annotated examples annotated by two different annotators was found to be 0.77.
Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ Each learner has benefits and shortcomings.
Instead of ten-fold cross-validation, we used the twenty-five verbs in Birke and Sarkar (2006) for testing (we call these the old verbs) and the other twenty-five verbs (the new verbs) for training. $$$$$ Evaluation results were recorded as recall, precision, and f-score values.

 $$$$$ It does not claim to interpret metonymy and it will not tell you what a given idiom means.
 $$$$$ They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types.
 $$$$$ In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.

Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ Then we discuss the structure and contents of the example base and the potential for expansion.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ → Kerry shot Bush.
Birke and Sarkar (2006) automatically constructed a corpus of English idiomatic expressions (words that can be used non-literally). $$$$$ Finally, we used our optimal configuration of TroFi, together with active learning and iterative augmentation, to build the TroFi Example Base, a publicly available, expandable resource of literal/nonliteral usage clusters that we hope will be useful not only for future research in the field of nonliteral language processing, but also as training data for other statistical NLP tasks.

Birke and Sarkar (2006) also used WSD. $$$$$ The first was a simple majority-rules baseline.
Birke and Sarkar (2006) also used WSD. $$$$$ It is important to note that in building the example base, we used TroFi with an Active Learning component (see (Birke, 2005)) which improved our average f-score from 53.8% to 64.9% on the original 25 target words.
Birke and Sarkar (2006) also used WSD. $$$$$ Detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4%.

Birke and Sarkar (2006) requires WordNet. $$$$$ The target set consists of sentences from the corpus containing the target word.
Birke and Sarkar (2006) requires WordNet. $$$$$ It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.
Birke and Sarkar (2006) requires WordNet. $$$$$ We calculated two baselines for each word.
Birke and Sarkar (2006) requires WordNet. $$$$$ Mason (2004) presents CorMet, “a corpusbased system for discovering metaphorical mappings between concepts” ((Mason, 2004), p. 23).

Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ If there are no literals, literal recall is 100%; literal precision is 100% if there are no nonliterals in the literal cluster and 0% otherwise.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ First, we examine iterative augmentation.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two automatically constructed seed sets (one with literal and one with non-literal expressions), assigning the label of the closest set. $$$$$ The last column in the graph shows the average across all the target verbs.

Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ The output of TroFi is an expandable example base of literal/nonliteral clusters which is freely available to the research community.
Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ For all our models and algorithms, we carried out detailed experiments on hand-annotated data, both to fully evaluate the system and to arrive at an optimal configuration.
Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction.
Birke and Sarkar (2006) present a sentence clustering approach for non-literal language recognition implemented in the TroFi system (Trope Finder). $$$$$ We suggest that TroFi is applicable to all sorts of nonliteral language, and that, although it is currently focused on English verbs, it could be adapted to other parts of speech and other languages.

Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification $$$$$ With an average of 53.8%, all words but one lie well above our simple-attraction baseline, and some even achieve much higher results than the majority-rules baseline.
Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification $$$$$ Due to the imbalance of literal and nonliteral examples, this baseline ranges from 60.9% to 66.7% with an average of 63.6%.
Birke and Sarkar (2006) adapt this algorithm to perform a two-way classification $$$$$ For our sum of similarities enhancement, all the individual target word results except for “examine” sit above the baseline.

Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ Like CorMet, TroFi uses contextual evidence taken from a large corpus and also uses WordNet as a primary knowledge source, but unlike CorMet, TroFi does not use selectional preferences.
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ → Kerry shot Bush.
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ Sentences that were attracted to neither cluster or were equally attracted to both were put in the opposite set from their label, making a failure to cluster a sentence an incorrect clustering.
Both Birke and Sarkar (2006) and Gedigan et al. (2006) focus only on metaphors expressed by a verb. $$$$$ In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.

Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ We used the iterative augmentation process to build a small example base consisting of the target words from Table 1, as well as another 25 words drawn from the examples of scholars whose work was reviewed in Section 2.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ In this paper we present TroFi (Trope Finder), a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ It also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning.
Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguation task and use a clustering algorithm which compares test instances to two seed sets (one with literal and one with non-literal expressions), as signing the label of the closest set. $$$$$ For example, given the target verb “pour”, we would expect TroFi to cluster the sentence “Custom demands that cognac be poured from a freshly opened bottle” as literal, and the sentence “Salsa and rap music pour out of the windows” as nonliteral, which, indeed, it does.

Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ Only using the context, “She broke her thumb while she was cheering for the Patriots and, in her excitement, she hit the ceiling,” can we decide.
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004).
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ In this paper we presented TroFi, a system for separating literal and nonliteral usages of verbs through statistical word-sense disambiguation and clustering techniques.
Birke and Sarkar (2006) use a clustering algorithm which compares test instances to two automatically constructed seed sets (one literal and one non literal), assigning the label of the closest set. $$$$$ Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004).
