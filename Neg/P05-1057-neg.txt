These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ A gain is defined as follows: exp[EMm= 1 λmhm(a, e, f)] where l = (i, j) is a link added to a.
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation (Och, 2003).
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ In this framework, we have a set of M feature functions hm(a, e, f), m = 1, ... , M. For each feature function, there exists a model parameter Am, m = 1, ... , M. The direct (2) This approach has been suggested by (Papineni et al., 1997) for a natural language understanding task and successfully applied to statistical machine translation by (Och and Ney, 2002).
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ We manually aligned 935 sentences, in which we selected 500 sentences as test corpus.

(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ Our log-linear models still make use of the parameters generated by GIZA++.
(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ IBM translation models try to model the translation probability Pr(fJ1 |eI 1), which describes the relationship between a source language sentence eI1 and a target language sentence fJ1 .
(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation (Och, 2003).

We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ In statistical alignment models Pr(fJ1 , aJ1 |eI1), a ’hidden’ alignment a = aJ1 is introduced, which describes a mapping from a target position j to a source position i = aj.
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ It is unrealistic to enumerate all possible alignments when lm is very large.
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ A terminal state is a state in which no more links can be added to increase the probability of the current alignment.
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ 12.

(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ The base feature of our log-linear models, IBM Model 3, takes the parameters generated by GIZA++ as parameters for itself.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ The training scheme is 15H5354555.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ We define a bilingual dictionary as a set of entries: D = {(e, f, conf)}. e is a source language word, f is a target langauge word, and conf is a positive real-valued number (usually, conf = 1.0) assigned by lexicographers to evaluate the validity of the entry.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems.

The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ Cherry and Lin (2003) developed a statistical model to find word alignments, which allow easy integration of context-specific features.
The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ We present a framework for word alignment based on log-linear models.
The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ It is important to make use of linguistic information to improve alignment strategies.

Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ Figure 1 shows how gain threshold has an effect on precision, recall and AER with fixed model scaling factors.
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ Therefore, the feature function using a bilingual dictionary is: where
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ An alignment a is defined as a subset of the Cartesian product of the word positions: We define the alignment problem as finding the alignment a that maximizes Pr(a  |e, f) given e and f. We directly model the probability Pr(a  |e, f).
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ By applying suitable transformations, the GIS algorithm is able to handle any type of real-valued features.

 $$$$$ Although Model 6 yields better results than naive IBM models, it fails to include dependencies other than IBM models and HMM model.
 $$$$$ In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
 $$$$$ 2004AA114010).
 $$$$$ We use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972) to train the model parameters λM1 of the log-linear models according to Eq.

Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ Various methods have been proposed for finding word alignments between parallel texts.
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ Given a source (’English’) sentence e = eI1 = e1, ... , ei, ..., eI and a target language (’French’) sentence f = fJ1 = f1, ..., fj, ..., fJ.
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ In this paper, we present a framework for word alignment based on log-linear models, allowing statistical models to be easily extended by incorporating additional syntactic dependencies.
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ 13 is not equivalent to the one in Eq.

Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ The gain threshold t depends on the added link l. We remove this dependency for simplicity when using it in search algorithm by treating it as a fixed real-valued number.
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models.
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation (Och, 2003).
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ We have presented a framework for word alignment based on log-linear models between parallel texts.

To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ In practice, We use YASMET 2 written by Franz J. Och for performing training.
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ This can happen because with the modified model scaling factors the n-best list can change significantly and can include alignments that have not been taken into account in training.
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ After the bilingual sentences in the development corpus are tokenized (or segmented) and POS tagged, they can be used to train POS tags transition probabilities by counting relative frequencies: N(eHere, NA(fT, eT) is the frequency that the POS tag fT is aligned to POS tag eT and N(eT) is the frequency of eT in the development corpus.
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ An especially well-founded framework is maximum entropy (Berger et al., 1996).

 $$$$$ Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.
 $$$$$ In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.
 $$$$$ The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLAS (Zhang et al., 2003).
 $$$$$ 12.

Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ It is unrealistic to enumerate all possible alignments when lm is very large.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ Linguistic tools such as part-of-speech taggers, parsers, namedentity recognizers have become more and more robust and available for many languages by now.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ In addition, we also make use of syntactic information such as part-of-speech tags and bilingual dictionaries.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ Clues combination is done by disjunction of single clues, which are defined as probabilities of associations.

A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ 2004AA114010).
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ 2004AA114010).

For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ In our ChineseEnglish corpus, only one type of alignment was marked, meaning that S = P. In the following, we present the results of loglinear models for word alignment.
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ It is promising to optimize the model parameters directly with respect to AER as suggested in statistical machine translation (Och, 2003).
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ In this paper, we use IBM translation Model 3 as the base feature of our log-linear models.
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ If e has length l and f has length m, there are possible 2lm alignments between e and f (Brown et al., 1993).

This is a key difference between our model and (Liu et al, 2005). $$$$$ Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models.
This is a key difference between our model and (Liu et al, 2005). $$$$$ It is unrealistic to enumerate all possible alignments when lm is very large.
This is a key difference between our model and (Liu et al, 2005). $$$$$ We could use a feature that counts how many entries of a conventional lexicon co-occur in a given alignment between the source sentence and the target sentence.
