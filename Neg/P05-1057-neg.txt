These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ We use the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff, 1972) to train the model parameters λM1 of the log-linear models according to Eq.
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ We take IBM Model 3 as base feature and use syntactic information such as POS tags and bilingual dictionary.
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ Given a source (’English’) sentence e = eI1 = e1, ... , ei, ..., eI and a target language (’French’) sentence f = fJ1 = f1, ..., fj, ..., fJ.
These approaches include an enhanced HMM alignment model that uses part-of speech tags (Toutanova et al, 2002), a log-linear combination of IBM translation models and HMM models (Och and Ney, 2003), techniques that rely on dependency relations (Cherry and Lin, 2003), and a log-linear combination of IBM Model 3 alignment probabilities, POS tags, and bilingual dictionary coverage (Liu et al, 2005). $$$$$ Currently, we only employ three types of knowledge sources as feature functions.

(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ We present a framework for word alignment based on log-linear models.
(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ It allows statistical models easily extended by incorporating syntactic information.
(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ 4 requires a sum over a large number of possible alignments.
(Liu et al., 2005) uses a log-linear model with a greedy search. $$$$$ However, treated as a method for performing symmetrization, log-linear combination alone yields better results than intersection, union, and refined methods.

We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ This work is supported by National High Technology Research and Development Program contract ”Generally Technical Research and Basic Database Establishment of Chinese Platform” (Subject No.
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ In practice, We use YASMET 2 written by Franz J. Och for performing training.
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ The English sentences are tokenized by a simple tokenizer of ours and POS tagged by a rule-based tagger written by Eric Brill (Brill, 1995).
We will retrain the Chinese parser on Penn Chinese Treebank version 5.0 and try to improve word alignment quality using log-linear models as suggested in (Liu et al, 2005). $$$$$ In this paper, we use IBM Model 3 alignment probabilities, POS correspondence, and bilingual dictionary coverage as features.

(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ The greedy search algorithm for general loglinear models is formally described as follows: Input: e, f, eT, fT, and D Output: a The above search algorithm, however, is not efficient for our log-linear models.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ However, the search algorithm we proposed is supervised, relying on a hand-aligned bilingual corpus, while the baseline approach of IBM alignments is unsupervised.
(Liu et al, 2005) presented a log-linear model combining IBM Model 3 trained in both directions with heuristic features which resulted in a 1-to-1 alignment. $$$$$ Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models.

The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ It is unrealistic to enumerate all possible alignments when lm is very large.
The F-measures for Chinese-English and Arabic-English are usually around 80% (Liu et al, 2005) and 70% (Fraser and Marcu, 2007), respectively. $$$$$ We train model parameters on a development corpus, which consists of hundreds of manually-aligned bilingual sentence pairs.

Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ We can compute gain, which is a heuristic function, instead of probability for efficiency.
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ We present a framework for word alignment based on log-linear models.
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ We take IBM Model 3 as base feature and use syntactic information such as POS tags and bilingual dictionary.
Liu et al (2005) used a conditional log-linear model with similar features to those we have employed. $$$$$ Table 3 compares the results of log-linear models with IBM Model 5.

 $$$$$ We take IBM Model 3 as base feature and use syntactic information such as POS tags and bilingual dictionary.
 $$$$$ Our experiments show that log-linear models significantly outperform IBM translation models.
 $$$$$ Therefore, it is necessary to incorporate all useful linguistic information to alleviate these problems.

Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ 2004AA114010).
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ It is unrealistic to enumerate all possible alignments when lm is very large.
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ 2004AA114010).
Liu et al (2005) propose a log-linear model for the alignment between two sentences, in which different features can be used to describe the alignment quality. $$$$$ For log-linear models, POS information and an additional dictionary are used, which is not the case for GIZA++/IBM models.

Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ Given a source (’English’) sentence e = eI1 = e1, ... , ei, ..., eI and a target language (’French’) sentence f = fJ1 = f1, ..., fj, ..., fJ.
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ We define the null link l = (i, 0) to exist if ei does not correspond to a translation for any French word in f. The null link l = (0, j) is defined similarly.
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ This work is supported by National High Technology Research and Development Program contract ”Generally Technical Research and Basic Database Establishment of Chinese Platform” (Subject No.
Analternative ME approach models alignment directly as a log-linear combination of feature functions (Liu et al., 2005). $$$$$ 2004AA114010).

To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ 13 is not equivalent to the one in Eq.
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ We have presented a framework for word alignment based on log-linear models between parallel texts.
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ We have presented a framework for word alignment based on log-linear models between parallel texts.
To make more confident conclusions, we also did tests on a larger hand-aligned data set used in Liu et al (2005). $$$$$ 2004AA114010).

 $$$$$ In IBM models as well as HMM models, when one needs the model to take new information into account, one must create an extended model which can base its parameters on the previous model.
 $$$$$ All knowledge sources are treated as feature functions, which depend on the source langauge sentence, the target language sentence and possible additional variables.
 $$$$$ In log-linear models, however, new information can be easily incorporated.
 $$$$$ For our models, gain(a, l) can be obtained in a more efficient way 3: 3We still call the new heuristic function gain to reduce notational overhead, although the gain in Eq.

Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ We see that adding new features also has an effect on the other model scaling factors.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ Our start state is the empty alignment, where all words in e and f are assigned to null.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ Linguistic data, which can be used to identify associations between lexical items are often ignored by traditional word alignment approaches.
Liu et al (2005) also develop a log-linear model, based on IBM Model 3. $$$$$ This work is supported by National High Technology Research and Development Program contract ”Generally Technical Research and Basic Database Establishment of Chinese Platform” (Subject No.

A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ We can compute gain, which is a heuristic function, instead of probability for efficiency.
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ Tiedemann (2003) introduced a word alignment approach based on combination of association clues.
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ In order to incorporate a new dependency which contains extra information other than the bilingual sentence pair, we modify Eq.2 by adding a new variable v: Note that our log-linear models are different from Model 6 proposed by Och and Ney (2003), which defines the alignment problem as finding the alignment a that maximizes Pr(f, a  |e) given e.
A straightforward approach to the alignment matrix is to build a log linear model (Liu et al, 2005) for the probability of the alignment A. $$$$$ Log-linear models allow statistical alignment models to be easily extended by incorporating syntactic information.

For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ 4 requires a sum over a large number of possible alignments.
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ We will follow with our experimental results and conclusion and close with a discussion of possible future directions.
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ Linguistic data, which can be used to identify associations between lexical items are often ignored by traditional word alignment approaches.
For example, the sum over all alignments may be restricted to a sum over the n-best list from other aligners (Liu et al, 2005). $$$$$ We can compute gain, which is a heuristic function, instead of probability for efficiency.

This is a key difference between our model and (Liu et al, 2005). $$$$$ Table 3 compares the results of log-linear models with IBM Model 5.
This is a key difference between our model and (Liu et al, 2005). $$$$$ We used GIZA++ package (Och and Ney, 2003) to train IBM translation models.
This is a key difference between our model and (Liu et al, 2005). $$$$$ Syntax-based translation models, such as tree-to-string model (Yamada and Knight, 2001) and tree-to-tree model (Gildea, 2003), may be very suitable to be added into log-linear models.
This is a key difference between our model and (Liu et al, 2005). $$$$$ Treated as feature functions, syntactic dependencies can be easily incorporated into log-linear models.
