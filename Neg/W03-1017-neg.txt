 $$$$$ To determine the orientation of an opinion sentence, all that remains is to specify cutoffs and so that sentences for which the average log-likelihood score exceeds are classified as positive opinions, sentences with scores lower than are classified as negative opinions, and sentences with in-between scores are treated as neutral opinions.
 $$$$$ If a document happened to have less than four sentences, additional documents from the same topic were retrieved to supply the missing sentences.
 $$$$$ The aggregate collection covers six different newswire sources including 173,252 Wall Street Journal (WSJ) articles from 1987 to 1992.
 $$$$$ Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect ARDA’s views.

(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system $$$$$ Therefore, we include in our features the counts of positive and negative words in the sentence (which are obtained with the method of Section 5.1), as well as counts of the polarities of sequences of semantically oriented words (e.g., “++” for two consecutive positively oriented words).
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system $$$$$ Opinion question answering is a challenging task for natural language processing.
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system $$$$$ In such a context, it is imperative that the question answering system can discriminate between opinions and facts, and either use the appropriate type depending on the question or combine them in a meaningful presentation.
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system $$$$$ We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles.

We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ Those articles were used for both document and sentence level opinion/fact classification.
We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ At the document level, a fairly straightforward Bayesian classifier using lexical information can distinguish between mostly factual and mostly opinion documents with very high precision and recall (F-measure of 97%).
We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ We also used the ANEW list which was constructed during psycholinguistic experiments (Bradley and Lang, 1999) and contains 1,031 words of all four open classes.
We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ We intend to cluster together sentences from the same perspective and present them in summary form as answers to subjective questions.

Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ Psychological studies (Bradley and Lang, 1999) found measurable associations between words and human emotions.
Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ Given separate sets of features , we train separate Naive Bayes classifiers , corresponding to each feature set.
Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ The three modules of the system discussed here provide the basis for ongoing work for further classification of opinions according to subject and opinion holder and for refining the original positive/negative attitude determination.
Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.

Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ Turney (2002) showed that it is possible to use only a few of those semantically oriented words (namely, “excellent” and “poor”) to label other phrases co-occuring with them as positive or negative.
Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes', a commonly used supervised machine-learning algorithm.
Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes', a commonly used supervised machine-learning algorithm.
Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ Perspective information can also help highlight contrasts and contradictions between different sources—there will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example.

Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ For each topic, we randomly selected 25 articles from the entire combined TREC corpus (not just the WSJ portion); these were articles matching the corresponding topical phrase given above as determined by the Lucene search engine.4 From each of these documents we randomly selected four sentences.
Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ This approach presupposes the availability of at least a collection of articles with pre-assigned opinion and fact labels at the document level; fortunately, Wall Street Journal articles contain such metadata by identifying the type of each article as Editorial, Letter to editor, Business and News.
Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes', a commonly used supervised machine-learning algorithm.
Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.

Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain $$$$$ We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions.
Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain $$$$$ The approach is based on the hypothesis that positive words co-occur more than expected by chance, and so do negative words; this hypothesis was validated, at least for strong positive/negative words, in (Turney, 2002).
Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain $$$$$ We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions.
Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain $$$$$ We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles.

Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ Then we assign the sentence to the category for which the average is higher (we call this approach the “score” variant).
Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ We therefore asked human evaluators to classify a set of sentences between facts and opinions as well as determine the type of opinions.
Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ We base this decision on the number and strength of semantically oriented words (either positive or negative) in the sentence.
Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ Perspective information can also help highlight contrasts and contradictions between different sources—there will be significant disparity in the material collected for the question mentioned above between Fox News and the Independent, for example.

For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions.
For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ Bruce and Wiebe (1999) annotated 1,001 sentences as subjective or objective, and Wiebe et al. (1999) described a sentence-level Naive Bayes classifier using as features the presence or absence of particular syntactic classes (pronouns, adjectives, cardinal numbers, modal verbs, adverbs), punctuation, and sentence position.
For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ More recently, Wiebe et al. (2002) report on document-level subjectivity classification, using a k-nearest neighbor algorithm based on the total count of subjective words and phrases within each document.

There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ We intend to cluster together sentences from the same perspective and present them in summary form as answers to subjective questions.
There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ We used Brill’s tagger (Brill, 1995) to obtain part-of-speech information.
There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ For classification tasks (i.e., classifying between facts and opinions and identifying the semantic orientation of sentences), we measured our system’s performance by standard recall and precision.
There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ In this paper, we are presenting three of the components of our opinion detection and organization subsystem, which have already been integrated into our larger question-answering system.

 $$$$$ We cannot automatically select a sentence-level gold standard discriminating between facts and opinions, or between positive and negative opinions.
 $$$$$ In earlier work (Turney, 2002) only singletons were used as seed words; varying their number allows us to test whether multiple seed words have a positive effect in detection performance.
 $$$$$ This approach presupposes the availability of at least a collection of articles with pre-assigned opinion and fact labels at the document level; fortunately, Wall Street Journal articles contain such metadata by identifying the type of each article as Editorial, Letter to editor, Business and News.
 $$$$$ In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.

Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).
Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ We then average its SIMFINDER-provided similarities with each sentence in those documents.
Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ Psychological studies (Bradley and Lang, 1999) found measurable associations between words and human emotions.
Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ We used as features single words, without stemming or stopword removal.

Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. $$$$$ Unlike questions like “Who was the first man on the moon?” which can be answered with a simple phrase, more intricate questions such as “What are the reasons for the US-Iraq war?” require long answers that must be constructed from multiple sources.
Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. $$$$$ Optimal values for and are obtained from the training data via density estimation—using a small, hand-labeled subset of sentences we estimate the proportion of sentences that are positive or negative.

At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.
At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ The results show the classifier achieved 97% F-measure, which is comparable or higher than the 93% accuracy reported by (Wiebe et al., 2002), who evaluated their work based on a similar set of WSJ articles.
At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ Our second method trains a Naive Bayes classifier (see Section 3), using the sentences in opinion and fact documents as the examples of the two categories.
At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ The agreement across the 100 sentences for all seven choices was 55%; if we group together the five subtypes of opinion sentences, the overall agreement rises to 82%.

 $$$$$ At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g., filtering or retrieving only editorials in favor of a particular policy decision).
 $$$$$ We used as features single words, without stemming or stopword removal.
 $$$$$ We experimented with different combinations of part-ofspeech classes for calculating the aggregate polarity scores, and found that the combined evidence from adjectives, adverbs, and verbs achieves the highest accuracy (90% over a baseline of 48%).
 $$$$$ We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.

 $$$$$ For classification tasks (i.e., classifying between facts and opinions and identifying the semantic orientation of sentences), we measured our system’s performance by standard recall and precision.
 $$$$$ Polarity Classification Using the method of Section 5.1, we automatically identified a total of 39,652 (65,773), 3,128 (4,426), 144,238 (195,984), and 22,279 (30,609) positive (negative) adjectives, adverbs, nouns, and verbs, respectively.
 $$$$$ To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes', a commonly used supervised machine-learning algorithm.

 $$$$$ Our sentence-level classifiers introduce additional criteria for detecting subjective material (opinions), including methods based on sentence similarity within a topic and an approach that relies on multiple classifiers.
 $$$$$ Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect ARDA’s views.
 $$$$$ We first discuss how such words are automatically found by our system, and then describe the method by which we aggregate this information across the sentence.
 $$$$$ We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles.

The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ Negative ones include depraved, disastrously, problem, and depress.
The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.
The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ Then we assign the sentence to the category for which the average is higher (we call this approach the “score” variant).

These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ Both recall and precision increase as the seed set becomes larger.
These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ We randomly selected 2,000 articles3 from each category so that our data set was approximate evenly divided between fact and opinion articles.
These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ In general, the additional features helped the classifier; the best performance is achieved when words, bigrams, trigrams, part-of-speech, and polarity are included in the feature set.
These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ Further, we achieved such high performance with Naive Bayes (see Section 8) that exploring additional techniques for this task seemed unnecessary.

(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ Our second method trains a Naive Bayes classifier (see Section 3), using the sentences in opinion and fact documents as the examples of the two categories.
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles.
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ Our work so far has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about.
