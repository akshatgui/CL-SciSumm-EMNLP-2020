 $$$$$ Naive Bayes assigns a document to the class that maximizes by applying Bayes’ rule and assuming conditional independence of the features.
 $$$$$ At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g., filtering or retrieving only editorials in favor of a particular policy decision).
 $$$$$ Some of the WSJ articles have structured headings that include Editorial, Letter to editor, Business, and News (2,877, 1,695, 2,009 and 3,714 articles, respectively).
 $$$$$ Our work so far has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about.

(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. $$$$$ We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions.
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. $$$$$ Results from a large collection of news stories and a human evaluation of 400 sentences are reported, indicating that we achieve very high performance in document classification (upwards of 97% precision and recall), and respectable performance in detecting opinions and classifying them at the sentence level as positive, negative, or neutral (up to 91% accuracy).
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. $$$$$ Fully analyzing and classifying opinions involves tasks that relate to some fairly deep semantic and syntactic analysis of the text.
(Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. $$$$$ We developed three different approaches to classify opinions from facts at the sentence level.

We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ We presented several models for distinguishing between opinions and facts, and between positive and negative opinions.
We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ This work was supported by ARDA under AQUAINT project MDA908-02-C-0008.
We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ As seed words, we used subsets of the 1,336 adjectives that were manually classified as positive (657) or negative (679) by Hatzivassiloglou and McKeown (1997).
We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yuand Hatzivassiloglou, 2003)). $$$$$ We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.

Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ This work was supported by ARDA under AQUAINT project MDA908-02-C-0008.
Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ We presented several models for distinguishing between opinions and facts, and between positive and negative opinions.
Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ We used SIMFINDER (Hatzivassiloglou et al., 2001), a state-of-the-art system for measuring sentence similarity based on shared words, phrases, and WordNet synsets.
Our work is similar to Yu and Hatzivassiloglou (2003) and Wiebe et al (1999) in that we use lexical and POS features. $$$$$ Then we assign the sentence to the category for which the average is higher (we call this approach the “score” variant).

Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles.
Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ The aggregate collection covers six different newswire sources including 173,252 Wall Street Journal (WSJ) articles from 1987 to 1992.
Turney proposed the unsupervised method for sentiment classification (Turney, 2002), and similar method is utilized by many other researchers (Yu and Hatzivassiloglou, 2003). $$$$$ To separate documents that contain primarily opinions from documents that report mainly facts, we applied Naive Bayes', a commonly used supervised machine-learning algorithm.

Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ To determine the orientation of an opinion sentence, all that remains is to specify cutoffs and so that sentences for which the average log-likelihood score exceeds are classified as positive opinions, sentences with scores lower than are classified as negative opinions, and sentences with in-between scores are treated as neutral opinions.
Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ Optimal values for and are obtained from the training data via density estimation—using a small, hand-labeled subset of sentences we estimate the proportion of sentences that are positive or negative.
Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ We report results using five feature sets, starting from words alone and adding in bigrams, trigrams, part-of-speech, and polarity.
Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. $$$$$ Document Classification We trained our Bayes classifier for documents on 4,000 articles from the WSJ portion of our combined TREC collection, and evaluated on 4,000 other articles also from the WSJ part.

Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. $$$$$ Our sentence-level classifiers introduce additional criteria for detecting subjective material (opinions), including methods based on sentence similarity within a topic and an approach that relies on multiple classifiers.
Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. $$$$$ More recently, Wiebe et al. (2002) report on document-level subjectivity classification, using a k-nearest neighbor algorithm based on the total count of subjective words and phrases within each document.
Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. $$$$$ These labels are used only to provide the correct classification labels during training and evaluation, and are not included in the feature space.
Yu and Hatzivassiloglou (2003) addressed three challenges in the news article domain: discriminating between objective documents and subjective documents such as editorials, detecting subjectivity at the sentence level, and determining polarity at the sentence level. $$$$$ Our approach to document and sentence classification of opinions builds upon the earlier work by using extended lexical models with additional features.

Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives.
Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ We first discuss how such words are automatically found by our system, and then describe the method by which we aggregate this information across the sentence.
Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ We evaluated the quality of semantically oriented words by mapping the extracted words and labels to an external gold standard.
Pang and Riloff (2005) and Yu and Hatzivassiloglou (2003) trained sentence-level subjectivity classifiers and proved that performing sentiment analysis targeting selected subjective sentences only gets higher results. $$$$$ For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives.

For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ For determining whether an opinion sentence is positive or negative, we have used seed words similar to those produced by (Hatzivassiloglou and McKeown, 1997) and extended them to construct a much larger set of semantically oriented words with a method similar to that proposed by (Turney, 2002).
For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions.
For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ Note that we trained and evaluated only on WSJ articles for which we can obtain article class metadata, so the classifier may perform less accurately when used for other newswire articles.
For example, Yu and Hatzivassiloglou (2003) separated facts from opinions and assigned polarities only to opinions. $$$$$ In both cases, the results are better when we evaluate against Standard B, containing the sentences for which two humans assign the same label; obviously, it is easier for the automatic system to produce the correct label in these more clear-cut cases.

There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ Opinion question answering is a challenging task for natural language processing.
There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ We experimented with seed sets containing 1, 20, 100 and over 600 positive and negative pairs of adjectives.
There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ Our work so far has focused on characterizing opinions and facts in a generic manner, without examining who the opinion holder is or what the opinion is about.
There have been attempt son tackling this so-called document-level subjectivity classification task, with very encouraging results (see Yu and Hatzivassiloglou (2003) and Wiebe et al (2004) for details). $$$$$ We used the TREC2 8, 9, and 11 collections, which consist of more than 1.7 million newswire articles.

 $$$$$ These include not only recognizing that the text is subjective, but also determining who the holder of the opinion is, what the opinion is about, and which of many possible positions the holder of the opinion expresses regarding that subject.
 $$$$$ At the document level, information retrieval systems can target particular types of articles and even utilize perspectives in focusing queries (e.g., filtering or retrieving only editorials in favor of a particular policy decision).
 $$$$$ For example, information extraction applications may target factual statements rather than subjective opinions, and summarization systems may list separately factual information and aggregate opinions according to distinct perspectives.
 $$$$$ We wish to thank Eugene Agichtein, Sasha BlairGoldensohn, Roy Byrd, John Chen, Noemie Elhadad, Kathy McKeown, Becky Passonneau, and the anonymous reviewers for valuable input on earlier versions of this paper.

Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ As our measure of semantic orientation across an entire sentence we used the average per word loglikelihood scores defined in the preceding section.
Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ In this paper, we discuss a necessary component for an opinion question answering system: separating opinions from fact, at both the document and sentence level.
Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ The features include words, bigrams, and trigrams, as well as the parts of speech in each sentence.
Indeed, recent work has shown that benefits can be made by first separating facts from opinions in a document (e.g, Yu and Hatzivassiloglou (2003)) and classifying the polarity based solely on the subjective portions of the document (e.g., Pang and Lee (2004)). $$$$$ Some of the WSJ articles have structured headings that include Editorial, Letter to editor, Business, and News (2,877, 1,695, 2,009 and 3,714 articles, respectively).

Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. $$$$$ Some of the WSJ articles have structured headings that include Editorial, Letter to editor, Business, and News (2,877, 1,695, 2,009 and 3,714 articles, respectively).
Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. $$$$$ Having distinguished whether a sentence is a fact or opinion, we separate positive, negative, and neutral opinions into three classes.
Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. $$$$$ Opinion question answering is a challenging task for natural language processing.
Yu and Hatzivassiloglou (2003) use semantically oriented words for identification of polarity at the sentence level. $$$$$ Assuming as ground truth the information provided by the document labels and that all sentences inherit the status of their document as opinions or facts, we first train on the entire training set, then use the resulting classifier to predict labels for the training set.

At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ The aggregate collection covers six different newswire sources including 173,252 Wall Street Journal (WSJ) articles from 1987 to 1992.
At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ The sentences that receive a label different from the assumed truth are then removed, and we train on the remaining sentences.
At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ He then used these phrases to automatically separate positive and negative movie and product reviews, with accuracy of 66–84%.
At sentence level, Yu and Hatzivassiloglou (2003) propose to classify opinion sentences as positive or negative in terms of the main perspective being expressed in opinionated sentences. $$$$$ This work was supported by ARDA under AQUAINT project MDA908-02-C-0008.

 $$$$$ Our sentence-level classifiers introduce additional criteria for detecting subjective material (opinions), including methods based on sentence similarity within a topic and an approach that relies on multiple classifiers.
 $$$$$ This work was supported by ARDA under AQUAINT project MDA908-02-C-0008.
 $$$$$ Those articles were used for both document and sentence level opinion/fact classification.
 $$$$$ Our sentence-level classifiers introduce additional criteria for detecting subjective material (opinions), including methods based on sentence similarity within a topic and an approach that relies on multiple classifiers.

 $$$$$ Those articles were used for both document and sentence level opinion/fact classification.
 $$$$$ The goal is to reduce the training set to the sentences that are most likely to be correctly labeled, thus boosting classification accuracy.
 $$$$$ In general, the additional features helped the classifier; the best performance is achieved when words, bigrams, trigrams, part-of-speech, and polarity are included in the feature set.
 $$$$$ Those articles were used for both document and sentence level opinion/fact classification.

 $$$$$ Psychological studies (Bradley and Lang, 1999) found measurable associations between words and human emotions.
 $$$$$ Naive Bayes assigns a document to the class that maximizes by applying Bayes’ rule and assuming conditional independence of the features.
 $$$$$ We evaluated the quality of semantically oriented words by mapping the extracted words and labels to an external gold standard.

The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ We are grateful to the graduate students at Columbia University who participated in our evaluation of sentence-level opinions.
The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ The results, presented in Section 8, indicate that we achieve very high performance (more than 97%) at document-level classification and respectable performance (86–91%) at detecting opinion sentences and classifying them according to orientation.
The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ We therefore asked human evaluators to classify a set of sentences between facts and opinions as well as determine the type of opinions.
The annotations in Yu and Hatzivassiloglou (2003) are sentence-level subjective vs. objective and polarity judgments. $$$$$ We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.

These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ We review related work in Section 2, and then present our document-level classifier for opinion or factual articles (Section 3), three implemented techniques for detecting opinions at the sentence level (Section 4), and our approach for rating an opinion as positive or negative (Section 5).
These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ We used as features single words, without stemming or stopword removal.
These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ We present a Bayesian classifier for discriminating between documents with a preponderance of opinions such as editorials from regular news stories, and describe three unsupervised, statistical techniques for the significantly harder task of detecting opinions at the sentence level.
These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). $$$$$ For a given seed set size, we denote the set of positive seeds as ADJ and the set of negative seeds as ADJ.

(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ Any opinions, findings, or recommendations are those of the authors and do not necessarily reflect ARDA’s views.
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ While words and n-grams had little performance effect for the opinion class, they increased the recall for the fact class around five fold compared to the approach by Wiebe et al. (1999).
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ Optimal values for and are obtained from the training data via density estimation—using a small, hand-labeled subset of sentences we estimate the proportion of sentences that are positive or negative.
(Wiebe et al2001, Yu and Hatzivassiloglou 2003), a task that is not relevant for the processing of very brief pieces of direct customer feedback. $$$$$ We also examined an automatic method for assigning polarity information to single words and sentences, accurately discriminating between positive, negative, and neutral opinions in 90% of the cases.
