Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ This might be especially useful for structured query models generated from the translation lattices.
Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ It is the optimal hypothesis the statistical machine translation system can generate using the given translation and language model, and restricted by the applied pruning strategy.
Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ This also means TMQ is subject to more noise.

Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ This query language essentially enables the use of proximity operators (ordered and unordered windows) in queries, so that it is possible to model the syntactic and semantic information encoded in phrases, n-grams, and co-occurred word pairs.
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ The InQuery implementation (Lemur 2003) is applied.
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ Back off of domain specific probabilities with those of a specific model (Besling, Meier, 1995).
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ Using these language models, interpolated with the background language model gave a NIST score of 8.67, and a Bleu score of 0.2228.

individual target hypotheses (Zhao et al, 2004). $$$$$ (J. Bellegarda, 2000).
individual target hypotheses (Zhao et al, 2004). $$$$$ As introduced in section 1, the translation model represents the full knowledge of translating words, as it encodes all possible translations candidates for a given source sentence.
individual target hypotheses (Zhao et al, 2004). $$$$$ Smoothing and adaptation in a dual space via latent semantic analysis, modeling long-term semantic dependencies, and trigger combinations.

Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ We explore unsupervised language model adaptation techniques for Statistical Machine Translation.
Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ P(t) is the target language model and P(s|t) is the translation model.
Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ The given baseline results are the best results achieved from the corresponding bag-of-words query models.

(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ A small domain specific language model is build using the retrieved sentences and linearly interpolated with the background language model.
(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ A small domain specific language model is build using the retrieved sentences and linearly interpolated with the background language model.

Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ },,,{ 21 Isssst.
Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ 2.2.2 N-Best Hypothesis List as a Query Model Similar to the first-best hypothesis, the n-best hypothesis list is converted into a bag-of-words representation.
Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ In the current study we modify the target language model P(t), to represent the test data better, and thereby improve the translation quality.

Refinements of this approach are described in (Zhao et al., 2004). $$$$$ Back off of domain specific probabilities with those of a specific model (Besling, Meier, 1995).
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ The belief values provided by the arguments of the sum are averaged to produce the belief value of the #sum node.
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ Back off of domain specific probabilities with those of a specific model (Besling, Meier, 1995).
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ Each node corresponds to a source word.

These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ },,,{ 21 Isssst.
These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ The quality of the query models is crucial to the adapted language model?s performance.
These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ Extracting sentences most similar to the initial translations, building specific language models for each sentence to be translated, and interpolating those with the background language models gives significant improvement in translation quality.

This adaptation technique was first proposed by Zhao et al (2004). $$$$$ For the baseline system, we built a translation model using 284K parallel sentence pairs, and a trigram language model from a 160 million words general English news text collection.

Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ 2.2.3 Translation Model as a Query Model To fully leverage the available knowledge from the translation system, the translation model can be used to guide the language model adaptation process.
Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ Smoothing and adaptation in a dual space via latent semantic analysis, modeling long-term semantic dependencies, and trigger combinations.
Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ Ignoring word order, the hypothesis is converted into a bag-of-words representation, which is then used as a query: }|),{(),,( 1211 TiiilT VwfwwwwQ ?== L where iw is a word in the vocabulary 1TV of the Top 1 hypothesis.
