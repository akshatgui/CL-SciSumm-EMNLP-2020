Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ Weighted Sum Operator: #wsum( 11 : tw , ?) The terms or nodes ( 1t ? nt ) contribute unequally to the final result according to the weight ( iw ) associated with each it . Ordered Distance Operator: #N( 1t ? nt ) The terms must be found within N words of each other in the text in order to contribute to the document's belief value.
Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ Back off of domain specific probabilities with those of a specific model (Besling, Meier, 1995).
Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target language corpus. $$$$$ Words which occurred in several translation hypotheses are simply repeated in the bag-of-words representations.

Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ jsiw , is a candidate target word as translation of is . Thus the translation model is converted into a collection of target words as a bag-of-word query model.
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ Specific language models are then build from the retrieved data and interpolated with a general background model.
Adaptation techniques have been shown to improve language modeling performance based on perplexity (Rosenfeld, 1996) and in application areas such as speech transcription (Bacchiani and Roark, 2003) and machine translation (Zhao et al, 2004), though no previous research has examined the language model domain adaptation problem for text simplification. $$$$$ In our experiments we use different numbers of similar sentences, ranting from one to several thousand.

individual target hypotheses (Zhao et al, 2004). $$$$$ Hyp2 Currently, police have blockade on the scene of the explosion.
individual target hypotheses (Zhao et al, 2004). $$$$$ Using structured query models, which capture word order information, leads to better results that plain bag of words models.

Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ An n-gram phrase can be modeled as an ordered distance operator with N=n. Unordered Distance Operator: #uwN( 1t ? nt ) The terms contained must be found in any order within a window of N words in order for this operator to contribute to the belief value of the document.
Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ We explore unsupervised language model adaptation techniques for Statistical Machine Translation.
Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ Ignoring word order, the hypothesis is converted into a bag-of-words representation, which is then used as a query: }|),{(),,( 1211 TiiilT VwfwwwwQ ?== L where iw is a word in the vocabulary 1TV of the Top 1 hypothesis.
Zhao et al (2004) construct a baseline SMT system using a large background language model and use it to retrieve relevant documents from large monolingual corpora and subsequently interpolate the resulting small domain-specific language model with the background language model. $$$$$ We follow (Eck, et al, 2004) in considering each sentence in the monolingual corpus as a document, as they have shown that this gives better results compared to retrieving entire news stories.

(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ Word proximity and word order is closely related to syntactic and semantic characteristics.
(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ The next series of experiments was done to study if using word order information in constructing the queries could help to generate more effective adapted language models.
(Zhao et al, 2004) constructed specific language models by using machine translation output as queries to extract similar sentences from large monolingual corpora. $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.

Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ 4.3.1 Results for Query 1TQ In the first experiment we used the first-best translations to generate the queries.
Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ 3.1 Structured Query Language.
Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ As introduced in section 1, the translation model represents the full knowledge of translating words, as it encodes all possible translations candidates for a given source sentence.
Zhao et al (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. $$$$$ Experiments show significant improvements when translating with these adapted language models.

Refinements of this approach are described in (Zhao et al., 2004). $$$$$ In our sentence retrieval process, the standard tf/idf (term frequency and inverse document frequency) term weighting scheme is used.
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ Using structured query models, which capture word order information, leads to better results that plain bag of words models.
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ We explore unsupervised language model adaptation techniques for Statistical Machine Translation.
Refinements of this approach are described in (Zhao et al., 2004). $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.

These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ 2.2.1 First-best Hypothesis as a Query Model The first-best hypothesis is the Viterbi path in the search space returned from the statistical machine translation decoder.
These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ (Janiszek, et al 2001) list the following approaches to language model adaptation: ? Linear interpolation of a general and a domain specific model (Seymore, Rosenfeld, 1997).
These schemes are overall limited by the quality of the translation hypotheses (Tam et al2007 and 2008), and better initial translation hypotheses lead to better selected sentences (Zhao et al., 2004). $$$$$ hwPhwPhwP iAiBi ??

This adaptation technique was first proposed by Zhao et al (2004). $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
This adaptation technique was first proposed by Zhao et al (2004). $$$$$ The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
This adaptation technique was first proposed by Zhao et al (2004). $$$$$ Specific language models are then build from the retrieved data and interpolated with a general background model.
This adaptation technique was first proposed by Zhao et al (2004). $$$$$ The 2-grams and 3-grams can be encoded using this operator too.

Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ Therefore, the potential of adaptation techniques needs to be explored for machine translation applications.
Zhao et al (2004) and Eck et al (2004) introduce information retrieval method for language model adaptation. $$$$$ Experiments show significant improvements when translating with these adapted language models.
