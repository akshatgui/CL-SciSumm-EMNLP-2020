(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ (2), for which the maximum-likelihood probability estimates for the n-grams are given by their relative frequencies: where N(w) again denotes the number of occurrences of the word w in the training corpus and N(v, c) The very last summation of Eq.
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ We trained a number of predictive class-based language models on different Arabic and English corpora using clusterings trained on the complete data of the same corpus.
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ For efficiency reasons, an exchange of a word between two clusters is separated into a remove and a move procedure.
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ In this paper we introduce a modification of the exchange algorithm with improved efficiency and then present a distributed version of the modified algorithm, which makes it feasible to obtain word classifications using billions of tokens of training data.

We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ The resulting clusterings are then used in training partially class-based language models.
We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ (10) are affected.
We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ Once a partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure.

However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001).
However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ This procedure is then repeated until the algorithm reaches a local optimum.
However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes.

By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ We performed experiments to eliminate the possibility of data overlap between the training data and the machine translation test data as cause for the large improvements.
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ Once a partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure.
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ This algorithm fits well into the MapReduce programming model (Dean and Ghemawat, 2004) that we used for our implementation.
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ The dev and test data sets contain parts of the 2003, 2004 and 2005 Arabic NIST MT evaluation sets among other parallel data.

While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). $$$$$ Yet each iteration is sped up by approximately a factor of two.
While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). $$$$$ In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model.

We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ With each word w in one of these sets, all words v preceding w in the corpus are stored with the respective bigram count N(v, w).
We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ Another interesting direction of further research is to evaluate the use of the presented clustering technique for language model size reduction.
We use the following fairly standard features in our tagger: current word, suffixes and prefixes of length 1, 2 and 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ Once a partition of the words in the vocabulary is obtained, two-sided class-based models can be built just like word-based n-gram models using existing infrastructure.

We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ Another interesting direction of further research is to evaluate the use of the presented clustering technique for language model size reduction.
We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ We conclude that even despite the large amounts of data used to train the large word-based model in our second experiment, class-based language models are still an effective tool to ease the effects of data sparsity.
We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ Two-sided class-based models received most attention in the literature.
We use the following features for our tagger: current word, suffixes and prefixes of length 1 to 3; additionally we use word cluster features (Uszkoreit and Brants, 2008) for the current word, and transition features of the cluster of the current and previous word. $$$$$ For the first experiment we trained predictive class-based 5-gram models using clusterings with 64, 128, 256 and 512 clusters1 on the en target data.

Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.
Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ In the following, we will call this type of model a two-sided class-based model, as both the history of each n-gram, the sequence of words conditioned on, as well as the predicted word are replaced by their class.
Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ In the case of n-gram language models this is done by factoring the probability: do not differ in the last n − 1 words, one problem ngram language models suffer from is that the training data is too sparse to reliably estimate all conditional probabilities P(wi|wi−1 Class-based n-gram models are intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model (Brown et al., 1990).
Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ (9), for a given word v only the set of classes which contain at least one word w for which N(v, w) > 0 must be considered, denoted by suc(v).

To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ (8) now effectively sums over all occurrences of all words and thus cancels out with the first summation of (8) which leads to: In the first summation of Eq.
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ However, several different types of mixed word and class models have been proposed for the purpose of improving the performance of the model (Goodman, 2000), reducing its size (Goodman and Gao, 2000) as well as lowering the complexity of related clustering algorithms (Whittaker and Woodland, 2001).
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ We also do not employ this optimization in our implementation.
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ We furthermore expect to be able to increase the gains resulting from using class-based models by using more sophisticated techniques for combining them with word-based models such as linear interpolations of word- and class-based models with coefficients depending on the frequency of the history.

However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ The resulting clusterings are then used in training partially class-based language models.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ For the first experiment we trained predictive class-based 5-gram models using clusterings with 64, 128, 256 and 512 clusters1 on the en target data.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.
