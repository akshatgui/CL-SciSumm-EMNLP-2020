(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ Two-sided class-based models received most attention in the literature.
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ At the end of the iteration all deltas are merged and applied to the previous clustering, resulting in the complete clustering loaded in the next iteration.
(Uszkoreit and Brants, 2008) used partially class based LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. $$$$$ The experiments presented show that predictive class-based models trained using the obtained word classifications can improve the quality of a state-ofthe-art machine translation system as indicated by the BLEU score in both translation tasks.

We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ The dev and test data sets contain parts of the 2003, 2004 and 2005 Arabic NIST MT evaluation sets among other parallel data.
We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ When scaling up the vocabulary and corpus sizes, the current bottleneck of our implementation is loading the current clustering into memory.
We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ For the sake of simplicity, we omit this detail in the following complexity analysis.
We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. $$$$$ Throughout one iteration of the algorithm, in which for each word in the vocabulary each possible move to another class is evaluated, this amounts to the number of distinct bigrams in the training corpus B, times the number of clusters N,.

However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ Table 4 shows some examples of the generated clusters.
However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ Table 1 shows the BLEU scores reached by the translation system when combining the different class-based models with the word-based model in comparison to the BLEU scores by a system using only the word-based model on the Arabic-English translation task.
However, rather than the more commonly used model of Brown et al (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). $$$$$ For our experiment with the English Arabic translation task we trained two 5-gram predictive classbased models with 512 clusters on the Arabic ar gigaword and ar webnews data sets.

By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ We introduce a modification of the exchange clustering algorithm with improved efficiency for certain partially class-based models and a distributed version of this algorithm to efficiently obtain automatic word classifications large vocabularies million words) ussuch large training corpora billion tokens).
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ In (Emami and Jelinek, 2005) a clustering algorithm is introduced which outputs a separate clustering for each word position in a trigram model.
By ignoring class-to-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). $$$$$ We also do not employ this optimization in our implementation.

While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). $$$$$ The worst case complexity of the exchange algorithm is quadratic in the number of classes.
While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). $$$$$ The word-based language model used by the system in these experiments is a 5-gram model also trained on the en target data set.
While the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large datasets we can get reliable statistics directly on the word to-class transitions (Uszkoreit and Brants, 2008). $$$$$ We furthermore expect to be able to increase the gains resulting from using class-based models by using more sophisticated techniques for combining them with word-based models such as linear interpolations of word- and class-based models with coefficients depending on the frequency of the history.

We use the following fairly standard features in our tagger $$$$$ (8) now effectively sums over all occurrences of all words and thus cancels out with the first summation of (8) which leads to: In the first summation of Eq.
We use the following fairly standard features in our tagger $$$$$ In our experiments this heuristic lead to almost monotone convergence in log-likelihood.

We use the following features for our tagger $$$$$ In a clustering of the en target data set with 1,024 clusters, the cluster sizes follow a typical longtailed distribution with the smallest cluster containing 13 words and the largest cluster containing 20,396 words.
We use the following features for our tagger $$$$$ Two-sided class-based models received most attention in the literature.
We use the following features for our tagger $$$$$ In statistical language modeling, one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes.

Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ For the first experiment we trained predictive class-based 5-gram models using clusterings with 64, 128, 256 and 512 clusters1 on the en target data.
Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006).
Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm. $$$$$ The remove procedure is similar.

To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification.
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ One of the frequently used algorithms for automatically obtaining partitions of the vocabulary is the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998).
To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. $$$$$ (2), for which the maximum-likelihood probability estimates for the n-grams are given by their relative frequencies: where N(w) again denotes the number of occurrences of the word w in the training corpus and N(v, c) The very last summation of Eq.

However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ In the experimental evaluation, the authors observe the largest improvements using a specific clustering for the last word of each trigram but no clustering at all for the first two word positions.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ We show that combining them with wordmodels in the log-linear model of a state-of-the-art statistical machine translation system leads to improvements in translation quality as indicated by the BLEU score.
However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. $$$$$ When training class-based n-gram models on large corpora and large vocabularies, one of the problems arising is the scalability of the typical clustering algorithms used for obtaining the word classification.
