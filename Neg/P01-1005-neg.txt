However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus. $$$$$ The more recent set of techniques includes multiplicative weightupdate algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).
However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus. $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus. $$$$$ Of course for many problems, additional training data has a non-zero cost.
However, corpus size is no longer a limiting factor: whereas up to now people have typically worked with corpora of around one million words, it has become feasible to build much larger document collections; for example, Banko and Brill (2001) report on experiments with a one billion word corpus. $$$$$ In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.

Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora.
Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.
Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora.
Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy.

Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a). $$$$$ While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing.
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a). $$$$$ By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used.
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a). $$$$$ Voting can be effective in reducing both the bias of a particular training corpus and the bias of a specific learner.
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001) is presented in Table 6 and illustrated in Figure 7 (a). $$$$$ When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective.

Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ We ran three active learning experiments, increasing the size of the total unlabeled training corpus from which we can pick samples to be annotated.
Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.
Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g.
Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ We collected a 1-billion-word training corpus from a variety of English texts, including news articles, scientific abstracts, government transcripts, literature and other varied forms of prose.

So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). $$$$$ Each point in the graph is the average performance over ten confusion sets for that size training corpus.
So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.
So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). $$$$$ In all three cases, sample selection outperforms sequential sampling.

Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ Each point in the graph is the average performance over ten confusion sets for that size training corpus.
Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.
Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ Confusion set disambiguation is the problem of choosing the correct use of a word, given a set of words with which it is commonly confused.

Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ In such cases, one could look at numerous methods for compressing data (e.g.
Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ For the first three learners, we used the standard collection of features employed for this problem: the set of words within a window of the target word, and collocations containing words and/or parts of speech.
Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.

Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ Whereas with active learning we want to choose the most uncertain instances for human annotation, with unsupervised learning we want to choose the instances that have the highest probability of being correct for automatic labeling and inclusion in our labeled training data.
Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.
Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.
Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ To examine the impact of voting when using a significantly larger training corpus, we ran 3 out of the 4 learners on our set of 10 confusable pairs, excluding the memory-based learner.

In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ The more recent set of techniques includes multiplicative weightupdate algorithms (Golding and Roth, 1998), latent semantic analysis (Jones and Martin, 1997), transformation-based learning (Mangu and Brill, 1997), differential grammars (Powers, 1997), decision lists (Yarowsky, 1994), and a variety of Bayesian classifiers (Gale et al., 1993, Golding, 1995, Golding and Schabes, 1996).
In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ The memory-based learner used only the word before and word after as features.
In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.
In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ Example confusion sets include: {principle , principal}, {then, than}, {to,two,too}, and {weather,whether}.

Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). $$$$$ But does voting still offer performance gains when classifiers are trained on much larger corpora?
Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). $$$$$ We collected a 1-billion-word training corpus from a variety of English texts, including news articles, scientific abstracts, government transcripts, literature and other varied forms of prose.

It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ Doing so gave a small improvement over just using the manually parsed data.
It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ This work was partially motivated by the desire to develop an improved grammar checker.
It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ Active learning involves intelligently selecting a portion of samples for annotation from a pool of as-yet unannotated training samples.
It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.

We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b). $$$$$ Very few problems exist for which annotated data of this size is available for free.
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b). $$$$$ In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g.
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b). $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results: (a) target language candidate selection for machine translation (Grefenstette, 1998) and (b) context sensitive spelling correction (Banko and Brill, 2001a, b). $$$$$ In part, this is due to the standardization of data sets used within the field, as well as the potentially large cost of annotating data for those learning methods that rely on labeled text.

Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001). $$$$$ In Figure 3, we show the accuracy obtained from voting, along with the single best learner accuracy at each training set size.
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001). $$$$$ We are fortunate that for this particular application, correctly labeled training data is free.
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001). $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
Self-training with bagging: The general self training with bagging algorithm (Banko and Brill, 2001). $$$$$ In all of these approaches, the problem is formulated as follows: Given a specific confusion set (e.g.

As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ For example, Dagan and Engelson (1995) describe a committee-based sampling technique where a part of speech tagger is trained using an annotated seed corpus.
As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ Next we tested whether this decrease in complementarity meant that voting loses its effectiveness as the training set increases.
As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.
As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.

Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ As a result of comparing a sample of two learners as a function of increasingly large training sets, we see in Table 1 that complementarity does indeed decrease as training size increases.
Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ Doing so gave a small improvement over just using the manually parsed data.
Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ As training size increases significantly, we would expect complementarity between classifiers to decrease.
Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.

More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ Next we tested whether this decrease in complementarity meant that voting loses its effectiveness as the training set increases.
More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ Other such problems include word sense disambiguation, part of speech tagging and some formulations of phrasal chunking.
More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ Very few problems exist for which annotated data of this size is available for free.
More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ In Table 3 we show the results from these unsupervised learning experiments for two confusion sets.

Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem.
Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not.
Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective.

In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. $$$$$ Beyond 1 million words, little is gained by voting, and indeed on the
In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.
In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. $$$$$ Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.
In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. $$$$$ We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.

We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ But for others, where space comes at a premium, obtaining the gains that come with a billion words of training data may not be viable without an effort made to compress information.
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation.
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ In Table 3 we show the results from these unsupervised learning experiments for two confusion sets.

As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing.
As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora.
As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ Active learning involves intelligently selecting a portion of samples for annotation from a pool of as-yet unannotated training samples.
As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.
