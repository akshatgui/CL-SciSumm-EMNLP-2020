However, corpus size is no longer a limiting factor $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.
However, corpus size is no longer a limiting factor $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.
However, corpus size is no longer a limiting factor $$$$$ We are able to attain improvements in accuracy for free using unsupervised learning, but unlike our learning curve experiments using correctly labeled data, accuracy does not continue to improve with additional data.

Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem.
Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ While it is encouraging that there is a vast amount of on-line text, much work remains to be done if we are to learn how best to exploit this resource to improve natural language processing.
Banko and Brill, (2001) also found this trend for the task of confusion set disambiguation on corpora of up to one billion words. $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.

Self-training with bagging $$$$$ Figure 2 shows the size of learned representations as a function of training data size.
Self-training with bagging $$$$$ The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy.

Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ In order to avoid training biases that may result from merely concatenating the different data sources to form a larger training corpus, we constructed each consecutive training corpus by probabilistically sampling sentences from the different sources weighted by the size of each source.
Figure 7 (b) illustrates the algorithm and Figure 8 describes the algorithm, also known as committee based active-learning (Banko and Brill, 2001). $$$$$ Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.

So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). $$$$$ To examine the impact of voting when using a significantly larger training corpus, we ran 3 out of the 4 learners on our set of 10 confusable pairs, excluding the memory-based learner.
So the creation of more annotated data is necesssary and will certainly cause major improvements of current WSD systems and NLP systems in general (see also (Banko and Brill, 2001)). $$$$$ Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.

Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ Example confusion sets include: {principle , principal}, {then, than}, {to,two,too}, and {weather,whether}.
Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ The amount of readily available on-line text has reached hundreds of billions of words and continues to grow.
Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ Confusion set disambiguation is one of a class of natural language problems involving disambiguation from a relatively small set of alternatives based upon the string context in which the ambiguity site appears.
Banko and Brill (2001) suggested that the development of very large training corpora may be more effective for progress in empirical Natural Language Processing than improving methods that use existing smaller training corpora. $$$$$ Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.

Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.
Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ Dagan and Engleson, 1995, Weng, et al, 1998).
Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ In Table 3 we show the results from these unsupervised learning experiments for two confusion sets.
Banko and Brill (2001) report on confusion set disambiguation experiments where they apply relatively simple learning methods to a one billion word training corpus. $$$$$ Since the instances in which all bags agree have the highest probability of being correct, we attempted to automatically grow our labeled training set using the 1-million-word labeled seed corpus along with the collection of na√Øve Bayes classifiers described above.

Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ In particular, we study the problem of selection among confusable words, using orders of magnitude more training data than has ever been applied to this problem.
Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.
Recent work has replicated the Banko and Brill (2001) results on the much more complex task of automatic thesaurus extraction, showing that contextual statistics, collected over a very large corpus, significantly improve system performance (Curran and Moens, 2002). $$$$$ Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost.

In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ When a training corpus is very small, there is much more room for these biases to surface and therefore for voting to be effective.
In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ We are fortunate that for this particular application, correctly labeled training data is free.
In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used.
In related work (Ng and Cardie, 2003), we compare the performance of the Blum and Mitchell co training algorithm with that of two existing single view bootstrapping algorithms - self-training with bagging (Banko and Brill, 2001) and EM (Nigam et al., 2000) - on coreference resolution, and show that single-view weakly supervised learners are a viable alternative to co-training for the task. $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.

Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). $$$$$ In applying unsupervised learning to improve upon a seed-trained method, we consistently saw an improvement in performance followed by a decline.
Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). $$$$$ In such cases, one could look at numerous methods for compressing data (e.g.
Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). $$$$$ Numerous approaches have been explored for exploiting situations where some amount of annotated data is available and a much larger amount of data exists unannotated, e.g.

It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ We have also shown that both active learning and unsupervised learning can be used to attain at least some of the advantage that comes with additional training data, while minimizing the cost of additional human annotation.
It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ Doing so gave a small improvement over just using the manually parsed data.
It has been well-established that statistical models improve as the size of the training data increases (Banko and Brill 2001a, 2001b). $$$$$ First we show learning curves for four different machine learning algorithms.

We start by using web counts for two generation tasks for which the use of large data sets has shown promising results $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results $$$$$ In order to avoid training biases that may result from merely concatenating the different data sources to form a larger training corpus, we constructed each consecutive training corpus by probabilistically sampling sentences from the different sources weighted by the size of each source.
We start by using web counts for two generation tasks for which the use of large data sets has shown promising results $$$$$ In this paper, we present a study of the effects of data size on machine learning for natural language disambiguation.

Self-training with bagging $$$$$ In such cases, one could look at numerous methods for compressing data (e.g.
Self-training with bagging $$$$$ The empirical NLP community has put substantial effort into evaluating performance of a large number of machine learning methods over fixed, and relatively small, data sets.
Self-training with bagging $$$$$ Numerous methods have been presented for confusable disambiguation.
Self-training with bagging $$$$$ We have shown that for a prototypical natural language classification task, the performance of learners can benefit significantly from much larger training sets.

As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.
As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ In part, this is due to the standardization of data sets used within the field, as well as the potentially large cost of annotating data for those learning methods that rely on labeled text.
As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ Next, we consider the efficacy of voting, sample selection and partially unsupervised learning with large training corpora, in hopes of being able to obtain the benefits that come from significantly larger training corpora without incurring too large a cost.
As expected, word alignment, like many other NLP tasks (Banko and Brill, 2001), highly benefits from large amounts of training data. $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.

Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ We used 1 million words of Wall Street Journal text as our test set, and no data from the Wall Street Journal was used when constructing the training corpus.
Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ It may be possible to combine active learning with unsupervised learning as a way to reduce this sample bias and gain the benefits of both approaches.
Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.
Additionally, for certain applications in natural language processing (NLP), it has been noted that the particular algorithms or feature sets used tend to become irrelevant as the size of the corpus increases (Banko and Brill 2001). $$$$$ We used 1 million words of Wall Street Journal text as our test set, and no data from the Wall Street Journal was used when constructing the training corpus.

More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ One advantageous aspect of confusion set disambiguation, which allows us to study the effects of large data sets on performance, is that labeled training data is essentially free, since the correct answer is surface apparent in any collection of reasonably well-edited text.
More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora.
More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ We show the results from sample selection for confusion set disambiguation in Figure 4.
More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. $$$$$ This training corpus is three orders of magnitude greater than the largest training corpus previously used for this problem.

Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used.
Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ We propose that a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections, while deemphasizing the focus on comparing different learning techniques trained only on small training corpora.
Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy.
Banko and Brill (2001) show that even using a very simple algorithm, the results continue to improve log-linearly with more training data, even out to a billion words. $$$$$ In this section we turn to unsupervised learning in an attempt to achieve this goal.

In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. $$$$$ In this paper, we have looked into what happens when we begin to take advantage of the large amounts of text that are now readily available.
In this paper we describe two techniques - surface features and paraphrases - that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. $$$$$ This is likely due to eventualy having reached a point where the gains from additional training data are offset by the sample bias in mining these instances.

We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not.
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ A large percentage of papers published in this area involve comparisons of different learning approaches trained and tested with commonly used corpora.
We regard the results in Figure 2 as a companion to Banko and Brill (2001)'s work on exponentially increasing the amount of labeled training data. $$$$$ Therefore, we wish to explore situations where we have, or can afford, a nonnegligible sized training corpus (such as for part-of-speech tagging) and have access to very large amounts of unlabeled data.

As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.
As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ Other such problems include word sense disambiguation, part of speech tagging and some formulations of phrasal chunking.
As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less.
As others have pointed out (Banko and Brill, 2001), with enough data the complex algorithms with their tricks cease to have an advantage over the simpler methods. $$$$$ While the amount of available online text has been increasing at a dramatic rate, the size of training corpora typically used for learning has not.
