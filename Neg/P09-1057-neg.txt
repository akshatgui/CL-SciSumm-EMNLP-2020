Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary.
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset.
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ Also, they make other manual adjustments to reduce noise from the word/tag dictionary (e.g., reducing the number of tags for “the” from six to just one).

Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ In the process of minimizing the grammar size, IP ends up removing many good tag bigrams from our grammar set (as seen from the low measured recall of 0.57 for the observed grammar).
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ For example, “to” is tagged as IN by our model sometimes when it occurs in the context of a preposition, whereas in the gold tagging it is always tagged as TO.
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models.

The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset.
The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes.
The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset.

Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ Goldberg et al. (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions.
Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.
Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ Next, we proceed to recover some good tag bigrams and expand the grammar in a restricted fashion by making use of the higher-quality dictionary produced by the IP+EM method.
Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%.

However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ They further improve this to 86.8% by using priors that favor sparse distributions.
However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each tag label whereas the real tag distribution is highly skewed.
However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ In addition to removing many bad tag bigrams from the grammar, IP minimization also removes some of the good ones, leading to lower recall (EM = 0.87, IP+EM = 0.57).

This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ The IP+EM tagging (with 84.5% accuracy) has some interesting properties.
This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.
This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence.
This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007).

Ravi and Knight (2009) exploited this to iteratively improve their POS tag model $$$$$ This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary.
Ravi and Knight (2009) exploited this to iteratively improve their POS tag model $$$$$ With these improvements in mind, we embark on an alternating scheme to find better models and taggings.
Ravi and Knight (2009) exploited this to iteratively improve their POS tag model $$$$$ We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word.

Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results.
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%.
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ We obtain this answer by formulating the problem in an integer programming (IP) framework.
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models.

(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training.
(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ While the methods are quite different, they all make use of two common model elements.
(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.

The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ Because EM is efficient, we can extend our word-sequence trainModel 1 Model 2 Model 3 Model 4 Model 5 ing data from the 24,115-token set to the entire Penn Treebank (973k tokens).
The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models.
The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word.
The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS).

The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ We presented a novel method for attacking dictionary-based unsupervised part-of-speech tagging.
The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ Recall that our original EM tagging with a fully-connected 2-gram tag model was 81.7% accurate.
The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ In addition, we do not require any complex parameter estimation techniques; we train our models using simple EM, which proves to be efficient for this task.

Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ The IP solver finds the smallest grammar set that can explain the given word sequence.
Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods.
Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results.
Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ But we do note that it is possible to obtain less than optimal solutions faster by interrupting the CPLEX solver.

A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus.
A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ We found that it is the elimination of incorrect entries from the dictionary (and grammar) and not necessarily the initialization weights from previous EM training, that results in the tagging improvements.
A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data.

To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence.
To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.
To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence.

Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.
Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ As noted by Toutanova and Johnson (2008), there is no reason to limit the amount of unlabeled data used for training the models.
Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ Figure 4 shows some examples.

This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.

 $$$$$ During EM training, the smaller grammar with fewer bad tag bigrams helps to restrict the dictionary model from making too many bad choices that EM made earlier.
 $$$$$ Figure 4 shows some examples.
 $$$$$ While the methods are quite different, they all make use of two common model elements.

Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ The IP solver finds the smallest grammar set that can explain the given word sequence.
Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ Likewise, when we extend our alternating EM scheme to 100 random restarts at each step, we improve our tagging accuracy from 91.6% to 91.8% (Figure 8).
Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ The LDA+AC model from Toutanova and Johnson (2008) has a strong ambiguity class component and uses more features to handle the unknown words better, and this contributes to the slightly higher performance in the incomplete dictionary cases, when compared to the IP+EM model.

Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ We also look at things more globally.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ We do this for every unknown word, and eventually we have a dictionary containing entries for all the words.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems in both settings.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ We see how the rare tag labels (like FW, SYM, etc.) are abused by EM.

For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ Previously, researchers working on this task have also reported results for unsupervised tagging with a smaller tagset (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008).
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ The method proposed in this paper is simple— once an integer program is produced, there are solvers available which directly give us the solution.
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence).
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ The method works by explicitly minimizing the grammar size using integer programming, and then using EM to estimate parameter values.
