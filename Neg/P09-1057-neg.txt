Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ The model also gets penalized for tagging the word “U.S.” as an adjective (JJ), which might be considered valid in some cases such as “the U.S. State Department”.
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ The entire process is fully automated and yields better performance than any existing state-of-the-art system, even though our models were not provided with any additional linguistic knowledge (for example, explicit syntactic constraints to avoid certain tag combinations such as “V V”, etc.).
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ We leave this as part of future work.
Bayesian methods (85.2% from Goldwater and Griffiths (2007), who use a trigram model) and close to the best accuracy reported on this task (91.8% from Ravi and Knight (2009b), who use an integer linear program to minimize the model directly). $$$$$ We also note that it might be possible to replicate our models in a Bayesian framework similar to that proposed in (Goldwater and Griffiths, 2007).

Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ We find that the observed grammar also improves, growing from 459 entries to 603 entries, with precision increasing from 0.94 to 0.96, and recall increasing from 0.57 to 0.76.
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ With these improvements in mind, we embark on an alternating scheme to find better models and taggings.
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ But it is not easy to model such priors into EM learning.
Ravi and Knight (2009) achieved the best results thus far (92.3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. $$$$$ In this set-up, there are no unknown words.

The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ In comparison, the Bayesian HMM (BHMM) model from Goldwater et al. (2007) and the CE+spl model (Contrastive Estimation with a spelling model) from Smith and Eisner (2005) report lower accuracies (87.3% and 88.7%, respectively).
The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ In contrast, we keep all the original dictionary entries derived from the Penn Treebank data for our experiments.
The strategies employed in Ravi and Knight (2009) and Baldridge (2008) are complementary. $$$$$ We still give EM the full word/tag dictionary, but now we constrain its initial grammar model to the 459 tag bigrams identified by IP.

Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ So we see a benefit to our explicit small-model approach.
Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ Note that we used a very small IP-grammar (containing only 459 tag bigrams) during EM training.
Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. $$$$$ The goal is to tag each word token so as to maximize accuracy against a gold tag sequence.

However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes.
However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ For direct comparison to previous works, we also presented results for the case when the dictionaries are incomplete and find the performance of our system to be comparable with current best results reported for the same task.
However, the original IP method of Ravi and Knight (2009) is intractable for supertagging, so we propose a new two-stage method that scales to the larger tag sets and data involved. $$$$$ In recent years, we have seen increased interest in using unsupervised methods for attacking different NLP tasks like part-of-speech (POS) tagging.

This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ One is a probabilistic n-gram tag model P(ti|ti−n+1...ti−1), which we call the grammar.
This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ For solving the integer program, we use CPLEX software (a commercial IP solver package).
This stage uses the original minimization formulation for the supertagging problem I Poriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). $$$$$ We also include the accuracies for our IP+EM model when using only the 24,115 token test corpus for EM estimation (shown within parenthesis in second column of the table in Figure 9).

Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy gram mar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. $$$$$ In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities).
Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy gram mar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. $$$$$ However, there are actually 104378 tag sequences compatible with the 459-sized grammar, and our IP solver just selects one at random.
Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy gram mar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. $$$$$ Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset.
Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first minimization procedure is seeded with a noisy gram mar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. $$$$$ We call this the observed grammar size, and it is 915.

Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ In other cases, the model clearly produces incorrect tags (e.g., “New” gets tagged incorrectly as NNPS).
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ In more recent work, Toutanova and Johnson (2008) propose a Bayesian LDA-based generative model that in addition to using sparse priors, explicitly groups words into ambiguity classes.
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ The goal is to tag each word token so as to maximize accuracy against a gold tag sequence.
Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009). $$$$$ Our method resembles the classic Minimum Description Length (MDL) approach for model selection (Barron et al., 1998).

(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ Our method achieves a very high accuracy (92.3%) on the 45-tagset and a higher (96.8%) accuracy on a smaller 17-tagset.
(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: Goldwater and Griffiths (2007) report 74.5% accuracy for EM with a 3-gram tag model, which we confirm by replication.
(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank.
(Ravi and Knight, 2009) focus on the POS tag collection to find the smallest POS model that explain the data. $$$$$ We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values.

The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ We run EM training again for Model 5 (the best model from Figure 5) but this time using 973k word tokens, and further increase our accuracy to 92.3%.
The abuse of the rare tags is presented in Table 5 in a similar fashion with (Ravi and Knight, 2009). $$$$$ Their systems were shown to obtain considerable improvements in accuracy when using a 17-tagset (a coarsergrained version of the tag labels from the Penn Treebank) instead of the 45-tagset.

The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence “V V” is disallowed).
The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ But it is not easy to model such priors into EM learning.
The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ We leave this as part of future work.
The IP+EM system constructs a model that describes the data by using minimum number of bi gram POS tags then uses this model to reduce the dictionary size (Ravi and Knight, 2009). $$$$$ Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data.

Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ Figure 1 shows prior results for this problem.
Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ We also measure the quality of the two observed grammars/dictionaries by computing their precision and recall against the grammar/dictionary we observe in the gold tagging.4 We find that precision of the observed grammar increases from 0.73 (EM) to 0.94 (IP+EM).
Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. $$$$$ So we can safely say that EM is learning a grammar that is too big, still abusing its freedom.

A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ The table in Figure 9 shows a comparison of different systems for which tagging accuracies have been reported previously for the 17-tagset case (Goldberg et al., 2008).
A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ The precision/recall of the observed dictionary on the other hand, is not affected by much.
A more rigid mechanism for modeling sparsity is proposed by Ravi and Knight (2009), who minimize the size of tagging grammar as measured by the number of transition types. $$$$$ The accuracy numbers reported for Init-HMM and LDA+AC are for models that are trained on all the available unlabeled data from the Penn Treebank.

To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ This is our final result on the 45-tagset, and we note that it is higher than previously reported results.
To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ We do this for every unknown word, and eventually we have a dictionary containing entries for all the words.
To avoid the need for manually pruning the tag dictionary, Ravi and Knight (2009) proposed that low-probability tags might be automatically filtered from the tag dictionary through a model minimization procedure applied to the raw text and constrained by the full tag dictionary. $$$$$ They use language specific information (like word contexts, syntax and morphology) for learning initial P(t|w) distributions and also use linguistic knowledge to apply constraints on the tag sequences allowed by their models (e.g., the tag sequence “V V” is disallowed).

Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ The first row in the table compares tagging results when using a full dictionary (i.e., a lexicon containing entries for 49,206 word types).
Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data.
Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ We presented a novel method for attacking dictionary-based unsupervised part-of-speech tagging.
Ravi and Knight (2009) use a dictionary and an MDL inspired modification to the EM algorithm. $$$$$ But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data.

This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ The Bayesian methods overcome this effect by using priors which favor sparser distributions.
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ The procedure is simple and proceeds as follows: We notice significant gains in tagging performance when applying this technique.
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ This yields lexicons containing 2,141 and 1,249 words respectively, which are much smaller compared to the original 49,206 word dictionary.
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ We find that of all those tag sequences, the worst gives an accuracy of 50.8%, and the best gives an accuracy of 90.3%.

 $$$$$ But previous results from Smith and Eisner (2005) and Goldwater and Griffiths (2007) show that their models do not benefit from using more unlabeled training data.
 $$$$$ The classic Expectation Maximization (EM) algorithm has been shown to perform poorly on POS tagging, when compared to other techniques, such as Bayesian methods.

Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ However, the search procedure for MDL is usually non-trivial, and for our task of unsupervised tagging, we have not found a direct objective function which we can optimize and produce good tagging results.
Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ Our IP formulation can find us a small model, but it does not attempt to fit the model to the data.
Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). $$$$$ The entire process is fully automated and yields better performance than any existing state-of-the-art system, even though our models were not provided with any additional linguistic knowledge (for example, explicit syntactic constraints to avoid certain tag combinations such as “V V”, etc.).

Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ When we execute 100 random restarts and select the model with the highest data likelihood, we get 83.8% accuracy.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ The IP+EM tagging (with 84.5% accuracy) has some interesting properties.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ While the methods are quite different, they all make use of two common model elements.
Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. $$$$$ We can compare the tag assignments from the gold tagging and the EM tagging (Viterbi tag sequence).

For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ We see how the rare tag labels (like FW, SYM, etc.) are abused by EM.
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence.
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ Figure 9 shows comparative results for the 17tagset case when the dictionary is incomplete.
For example, the work of Ravi and Knight (2009) minimizes the number of possible tag-tag transitions in the HMM via an integer program, hence discarding unlikely transitions that would confuse the model. $$$$$ This in turn helps improve the tagging accuracy from 81.7% to 84.5%.
