Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. $$$$$ Different systems typically vary along three dimensions: choice of features, choice of classifier, and choice of training data.
Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. $$$$$ How effective is a meta-classification approach that combines language modeling and error-specific classification to the detection and correction of preposition and article errors by non-native speakers?
Gamon (2010) proposes a hybrid system for preposition and article correction, by incorporating the scores of a language model and class probabilities of a maximum entropy model, both trained on native data, into a meta-classifier that is trained on a smaller amount of annotated ESL data. $$$$$ Since the language model is generally more accurate than the error-specific classifier, the meta-classifier tends to trust its score more than that of the classifier.

In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. $$$$$ Atwell (1987) uses a part-of-speech tag language model to detect errors, Chodorow and Leacock (2000) use mutual information and chi square statistics to identify unlikely function word and part-of-speech tag sequences, Turner and Charniak (2007) employ a language model based on a generative statistical parser, and Stehouwer and van Zaanen (2009) investigate a diverse set of language models with different backoff strategies to determine which choice, from a set of confusable words, is most likely in a given context.
In contrast to Gamon (2010) and Han et al (2010) that use annotated data for training, the system is trained on native data, but the native data are transformed to be more like L1 data through artificial article errors that mimic the error rates and error patterns of non-native writers. $$$$$ An example involving deletion is in Someone came to sort of it.

Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. $$$$$ For example, an annotated error &quot;take a picture&quot; with the correction &quot;take pictures&quot; is annotated as two consecutive errors: &quot;delete a&quot; and &quot;rewrite picture as pictures&quot;.
Gamon (2010) shows precision/recall curves on the combined task of detecting missing, extraneous and confused prepositions. $$$$$ Using a meta-classifier for ensemble learning has been proven effective for many machine learning problems (see e.g.

Gamon (2010) also considers missing and extraneous preposition errors. $$$$$ Dietterich 1997), especially when the combined models are sufficiently different to make distinct kinds of errors.
Gamon (2010) also considers missing and extraneous preposition errors. $$$$$ Consider the CLEC sentence I get to know the world outside the campus by newspaper and television.
Gamon (2010) also considers missing and extraneous preposition errors. $$$$$ Candidates for additional primary models include (1) more classifiers trained either on different data sets or with a different classification algorithm, and (2) more language models, such as skip models or part-of-speech n-gram language models.
Gamon (2010) also considers missing and extraneous preposition errors. $$$$$ For the purpose of this paper, we restrict ourselves to article and preposition errors.

Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ As in the previous experiments, however, we found that we are not able to outperform the baseline by using just 1% of annotated data.
Some recent work includes Chodorow et al (2007), De Felice and Pulman (2008), Gamon (2010), Han et al (2010), Izumi et al (2004), Tetreault and Chodorow (2008), Rozovskaya and Roth (2010a, 2010b). $$$$$ Here the bigram in books has a very high probability and the system incorrectly suggests replacing on with in.

Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. $$$$$ We have shown that a meta-classifier approach outperforms using a language model or a classifier alone.
Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. $$$$$ (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates.
Gamon et al (2008) and Gamon (2010) used a language model in addition to a classifier and combined the classifier output and language model scores in a meta classifier. $$$$$ Still, the overall amount of expensive error-annotated data is relatively small, and the meta-classification approach makes it possible to leverage large amounts of wellformed text in the primary models, tuning to the non-native domain in the meta-classifier.

Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). $$$$$ Contextual classification methods represent the context of a preposition or article as a feature vector gleaned from a window of a few words around the preposition/article.
Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). $$$$$ This allows us to address the problem of domain mismatch: We can leverage large well-formed data sets that are substantially different from real-life learner language for the primary models, and then fine-tune the output to learner English using a much smaller set of expensive and hard-to-come-by annotated learner writing.
Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). $$$$$ This came as a surprise to us, given the recent prevalence of classification approaches in this area of research and the fact that our classifiers produce state-of-the art performance when compared to other systems, on well-formed data.
Note that this use of a host of language model features is substantially different from using a single language model score on hypothesized error and potential correction to filter out unlikely correction candidates as in Gamon et al (2008) and Gamon (2010). $$$$$ (2008) use n-gram counts from the web as a language model approximation to identify likely errors and correction candidates.

In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set. $$$$$ To control the precision-recall tradeoff for the language model, we calculate the difference between the log probabilities of the original user input and the suggested correction.
In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set. $$$$$ Gamon et al. (2009), for example manually evaluate preposition suggestions as belonging to one of three categories: (a) properly correcting an existing error, (b) offering a suggestion that neither improves nor degrades the user sentence, (c) offering a suggestion that would degrade the user input.
In Figure 4 we compare the sequence modeling results for prepositions with results from the preposition component of the current version of the system described in Gamon (2010) on the same test set. $$$$$ The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.

 $$$$$ How much error-annotated data is sufficient to produce positive results using that approach?
 $$$$$ How effective is a meta-classification approach that combines language modeling and error-specific classification to the detection and correction of preposition and article errors by non-native speakers?

The heuristics are based on those used in Gamon (2010) (personal communication). $$$$$ Dietterich 1997), especially when the combined models are sufficiently different to make distinct kinds of errors.
The heuristics are based on those used in Gamon (2010) (personal communication). $$$$$ Training data requirements for the meta-classifier vary significantly between article and preposition error detection.

Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). $$$$$ We then combine the language model and the classifiers in a meta-classification approach by combining evidence from the classifiers and the language model as input features to the metaclassifier.
Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). $$$$$ Reducing the training data to 1% (580 annotated errors, 7,400 sentences) still outperforms the language model alone.
Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). $$$$$ Training data requirements for the meta-classifier vary significantly between article and preposition error detection.
Obtaining better-quality training data is a major issue for machine learning applied to learner language, as the domain of writing is different from news-heavy training domains (Gamon, 2010). $$$$$ The system suggests deleting by.

This is a baseline run that represents the language model approach proposed by Gamon (2010). $$$$$ We first compare a language model and errorspecific classifiers (all trained on large English corpora) with respect to their performance in error detection and correction.
This is a baseline run that represents the language model approach proposed by Gamon (2010). $$$$$ Since the meta-classifier requires error-annotated data for training, we investigate how much training data is needed to improve results over the baseline of not using a meta-classifier.
This is a baseline run that represents the language model approach proposed by Gamon (2010). $$$$$ With articles, the system is much less data-hungry.

Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). $$$$$ We now turn to the question of the required amount of annotated training data for the metaclassifier.
Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). $$$$$ For the purpose of this paper, we restrict ourselves to article and preposition errors.
Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). $$$$$ With articles, the system is much less data-hungry.
Correcting preposition errors requires more data to achieve performance comparable to article error correction, due to the task complexity (Gamon, 2010). $$$$$ As a result we see this kind of error quite frequently.

A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. $$$$$ To find out if it is possible to reduce the required amount of annotated preposition errors for a system that still covers more than one third of the preposition errors, we ran the same learning curve experiments but now only taking the four most frequent prepositions into account: to, of, in, for.
A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. $$$$$ Note, though, that the manual tuning was performed to optimize performance against a different data set (the Chinese Learners of English Corpus: CLEC), so the latter point is not really comparable and hence is not included in the charts.
A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. $$$$$ We have shown that a meta-classifier approach outperforms using a language model or a classifier alone.
A third alternative, that of selectively removing or correcting errors, is something of a middle road, and has been used in other work using the CLC data: in particular, Gamon (2010) removes from the data sentences where some other error appears immediately next to a preposition or determiner error. $$$$$ The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.

Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010). $$$$$ The metaclassifier, in contrast, is trained on a smaller set of error-annotated learner data.
Features used in classication include surrounding words, part-of-speech tags, language model scores (Gamon, 2010), and parse tree structures (Tetreault et al, 2010). $$$$$ We believe that the logical next step is to combine more primary models in the meta-classifier.
