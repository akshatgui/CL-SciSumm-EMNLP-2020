CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al, 1998). $$$$$ Let be the number of words in segment , and let be the -th word in .
CWM is described in (Choi, 2001), U00 in (Utiyama and Isahara, 2001), C99 in (Choi, 2000), DotPlot in (Reynar, 1998) and Segmenter in (Kan et al, 1998). $$$$$ In this paper, we have used an artificial corpus to evaluate our system.

Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs. $$$$$ We say that covers (16) where is the number of different words in .
Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs. $$$$$ We propose a statistical method that finds the maximum-probability segmentation of a given text.
Misra et al (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) using TMs. $$$$$ This method finds the maximum-probability segmentation of a given text.

 $$$$$ By using both and , we can get a reasonable number of segments.
 $$$$$ A summary of such a document can be composed of summaries of the component topics.
 $$$$$ The method has been shown to be more accurate than or at least as accurate as previous methods.
 $$$$$ If a text segmentation system divides a given text into a relatively small number of segments, then a summary of the original text can be composed by combining summaries of the component segments (Kan et al., 1998; Nakao, 2000).

(Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. $$$$$ Documents usually include various topics.
(Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. $$$$$ This is a new approach for domain-independent text segmentation.
(Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. $$$$$ Therefore, it can be applied to any text in any domain.
(Utiyama and Isahara, 2001) models the problem of TS as a problem of finding the minimum cost path in a graph and therefore adopts a dynamic programming algorithm. $$$$$ The algorithm automatically determines the number of segments.

The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. $$$$$ This means that our system is more accurate than or at least as accurate as previous domainindependent text segmentation systems, because has been shown to be more accurate than previous domain-independent text segmentation systems.10
The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. $$$$$ Reynar (1994) used word repetition as a measure of cohesion.
The problem of finding thematic boundaries other than sentence boundaries automatically (e.g. Utiyama and Isahara (2001)) is thus not addressed in this work. $$$$$ We propose a statistical method that finds the maximum-probability segmentation of a given text.

U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). $$$$$ The segmentations were evaluated by applying an evaluation program in Choi’s package.
U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). $$$$$ This means that our system is more accurate than or at least as accurate as previous domainindependent text segmentation systems, because has been shown to be more accurate than previous domain-independent text segmentation systems.10
U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). $$$$$ We propose a statistical method that finds the maximum-probability segmentation of a given text.
U00 is the system described in (Utiyama and Isahara, 2001), C99 the one proposed in (Choi, 2000) and LCseg is presented in (Galley et al, 2003). $$$$$ Therefore, it can be applied to any text in any domain.

In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. $$$$$ This is due to the term in Equation (11).
In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. $$$$$ We have proposed a statistical model for domainindependent text segmentation.
In more recent work, Utiyama and Isahara (2001) combine a statistical segmentation model with a graph search algorithm to find the segmentation with the maximum probability. $$$$$ As a result, their model can not straightforwardly incorporate features pertaining to a segment itself, such as the average length of segments.

Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. $$$$$ The texts were then segmented by the systems listed in Tables 2 and 3.
Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. $$$$$ An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.
Due to lack of space we do not describe previous work in text segmentation here in detail; we refer the reader to Utiyama and Isahara (2001) and Pevzener and Hearst (2002) for a comprehensive overview. $$$$$ Next, we define a graph , where is a set of nodes and is a set of edges. is defined as can be decomposed as follows: and we then minimize to obtain , because where the edges are ordered; the initial vertex and the terminal vertex of are and , respectively.

This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. $$$$$ We propose a statistical method that finds the maximum-probability segmentation of a given text.
This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. $$$$$ Their model is basically the same as that used for HMM part-of-speech (POS) taggers (Manning and Sch¨utze, 1999), if we regard topics as POS tags.11 Finding topic boundaries is equivalent to finding topic transitions; i.e., a continuous topic or segment is a sequence of words with the same topic.
This algorithm (Utiyama and Isahara, 2001) (UI) computes the optimal segmentation by estimating changes in the language model predictions over different partitions. $$$$$ This method does not require training data because it estimates probabilities from the given text.

In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. $$$$$ An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.
In addition, we show that the benchmark segmentation system of Utiyama and Isahara (2001) can be viewed as another special case of our Bayesian model. $$$$$ In Figure 1, for example, if is the minimum-cost path, then is the minimum-cost segmentation.

Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. $$$$$ It can, however, incorporate training data when they are available, as discussed in Section 5.
Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. $$$$$ Therefore, it can be applied to any text in any domain.
Most similar to our work is the approach of Utiyama and Isahara (2001), who search for segmentations with compact language models; as shown in Section 3.1.1, this can be viewed as a special case of our model. $$$$$ An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.

Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. $$$$$ We have proposed a statistical model for domainindependent text segmentation.
Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. $$$$$ In the next section, we then describe the algorithm for selecting the most likely segmentation.
Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. $$$$$ This method does not require training data because it estimates probabilities from the given text.

 $$$$$ For example, Our assumption, however, is that we do not have such prior information.
 $$$$$ Their approach is indirect compared with our approach, which directly finds the maximumprobability segmentation.
 $$$$$ Generally speaking, the number of segments obtained by our algorithm is not sensitive to the length of a given text, which is counted in words.

 $$$$$ An experiment showed that the method is more accurate than or at least as accurate as a state-of-the-art text segmentation system.
 $$$$$ We are planning to build a segmentation corpus for Japanese and evaluate our method against this corpus.
 $$$$$ This means that our system is more accurate than or at least as accurate as previous domainindependent text segmentation systems, because has been shown to be more accurate than previous domain-independent text segmentation systems.10
 $$$$$ For example, Our assumption, however, is that we do not have such prior information.

Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). $$$$$ Our proposed algorithm finds the maximumprobability segmentation of a given text.
Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). $$$$$ Further discussion and our conclusions are given in Sections 5 and 6, respectively.
Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). $$$$$ Our proposed algorithm naturally estimates the probabilities of words in segments.
Lexical cohesion was placed in a probabilistic (though not Bayesian) framework by Utiyama and Isahara (2001). $$$$$ This method does not require training data because it estimates probabilities from the given text.

Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). $$$$$ These systems, however, can not be applied to domains for which no training data exist.
Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). $$$$$ But we use in this paper, because it is easily interpreted as a description length and the experimental results obtained by using are slightly better than those obtained by using .
Two frequently-cited systems are LCSEG (Galley et al, 2003) and TEXTSEG (Utiyama and Isahara, 2001). $$$$$ Evaluation of the output of text segmentation systems is difficult because the required segmentations depend on the application.

A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). $$$$$ Consequently, these methods can be applied to any text in any domain, even if training data do not exist.
A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). $$$$$ We found empirically that segments obtained by recursive segmentation were better than those obtained by minimum-cost segmentation when the specified number of segments was somewhat larger than that of the minimum-cost path, whose number of segments was automatically determined by the algorithm.
A set of stop-words is also removed, using the same list originally employed by several competitive systems (Utiyama and Isahara, 2001). $$$$$ If only is used to segment the text, then the resulting segmentation will have too many segments.

It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). $$$$$ This method finds the maximum-probability segmentation of a given text.
It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). $$$$$ Thus, we have to use some uninformative prior probability.
It is not clear whether our algorithm is better than (Utiyama and Isahara, 2001) (U00). $$$$$ We are planning to build a segmentation corpus for Japanese and evaluate our method against this corpus.

In this paper, we selected for comparison three systems based merely on the lexical reiteration feature $$$$$ To find the maximum-probability segmentation , we first define the cost of segmentation as 2‘Log’ denotes the logarithm to the base 2.
In this paper, we selected for comparison three systems based merely on the lexical reiteration feature $$$$$ Equation (17) favors segments whose lengths are similar to the average length (in words).

The TextSeg algorithm (Utiyama and Isahara,2001) implements a probabilistic approach to determine the most likely segmentation, as briefly described below. $$$$$ We assume that different topics have different word distributions.
