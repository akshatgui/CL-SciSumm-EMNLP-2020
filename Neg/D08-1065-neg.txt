We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ We note that Lattice MBR is operating over lattices which are gigantic in comparison to the number of paths in an N-best list.
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ Statistical MT (Brown et al., 1990; Och and Ney, 2004) can be described as a mapping of a word sequence F in the source language to a word sequence E in the target language; this mapping is produced by the MT decoder S(F).
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ Since each constituent adds a new 4-gram to the existing translation, this approach approximately maximizes the expected BLEU.

We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ In Section 3, we describe the conditions on the gain function for efficient decoding over a lattice.
We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ In experiments not reported here, we found that taking into account the brevity penalty at the sentence level can cause large fluctuations in lattice MBR performance on different test sets.
We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ We note that £ represents the space of translations.

We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ These factors could be obtained from a decoding run on a development set.
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ Each arc is labeled with a word and a weight.
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ In this paper, we are primarily concerned with local gain functions that weight n-grams.
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ Since each constituent adds a new 4-gram to the existing translation, this approach approximately maximizes the expected BLEU.

Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system.
Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ This will enable us to rewrite the log(BLEU) as a linear function of n-gram matches and the hypothesis length.
Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ There are two potential reasons why Lattice MBR decoding could outperform N-best MBR: a larger hypothesis space from which translations could be selected or a larger evidence space for computing the expected loss.

 $$$$$ In contrast, these systems are evaluated using metrics based on string-edit distance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations.
 $$$$$ Our Lattice MBR decoding is realized using Weighted Finite State Automata.
 $$$$$ Minimum Bayes-Risk (MBR) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model (Bickel and Doksum, 1977).

Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). $$$$$ The rest of the paper is organized as follows.
Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). $$$$$ Each state q in the automaton has a corresponding set of n-grams Nq ending there.
Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). $$$$$ In general, MBR decoding can use different spaces for hypothesis selection and risk computation: argmax and the sum in Equation 1 (Goel, 2001).
Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning (Sixtus and Ortmanns, 1999), which has also been used in previous work using phrase lattices (Tromble et al., 2008). $$$$$ An important aspect of our lattice MBR is the linear approximation to the BLEU score.

Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ In contrast to a phrase-based SMT system, a syntax based SMT system (e.g.
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ In contrast, these systems are evaluated using metrics based on string-edit distance (Word Error Rate), ngram overlap (BLEU score (Papineni et al., 2001)), or precision/recall relative to human annotations.
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ The MBR scaling parameter (α in Equation 3) is tuned on the dev2 development set.
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ Computing the set of n-grams N that occur in a finite automaton requires a traversal, in topological order, of all the arcs in the automaton.

For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ Formally, it is an acyclic Weighted Finite State Acceptor (WFSA) (Mohri, 2002) consisting of states and arcs representing transitions between states.
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ We compute the posterior probability of each n-gram w as: where Z(£) = EE'∈E exp(αH(E0, F)) (denominator in Equation 3) and Z(£w) = EE'∈Ew exp(αH(E0, F)).

In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ We measure statistical significance using 95% confidence intervals computed with paired bootstrap resampling (Koehn, 2004).
In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ Since each constituent adds a new 4-gram to the existing translation, this approach approximately maximizes the expected BLEU.
In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ We have shown the effect of the MBR scaling factor on the performance of lattice MBR.
In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ A translation word lattice is a compact representation for very large N-best lists of translation hypotheses and their likelihoods.

Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ These factors could be obtained from a decoding run on a development set.
Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ We conduct a range of experiments to understand why Lattice MBR improves upon N-best MBR and study the impact of various parameters on MBR performance.

up to 1081 as per Tromble et al (2008). $$$$$ We will investigate the use of a translation lattice for MBR decoding; in this case, £ will represent the set of candidates encoded in the lattice.
up to 1081 as per Tromble et al (2008). $$$$$ We would like to speed up the Lattice MBR computation (Section 4) by restricting the maximum order of the n-grams in the procedure.
up to 1081 as per Tromble et al (2008). $$$$$ We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.

Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ Then, the corpus BLEU score B(r, c0, cn) can be defined as follows (Papineni et al., 2001): where we have ignored On, the difference between the number of words in the candidate and the numthe (max, +) semiri ber of n-grams.
Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ We here describe a linear approximation to the log(BLEU score) (Papineni et al., 2001) which allows such a decomposition.
Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ We present lattice MBR experiments in Section 7.
Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ Finally, we extract the best path from the resulting automaton4, giving the lattice MBR candidate translation according to the gain function (Equation 6).

The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ It is known that the average ngram precisions decay approximately exponentially with n (Papineni et al., 2001).
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ We now present MBR decoding on translation lattices.
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ Aggregation of the weights along the path1 produces the weight of the path’s candidate H(E, F) according to the model.
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ Minimum Bayes-Risk (MBR) decoding aims to find the candidate hypothesis that has the least expected loss under the probability model (Bickel and Doksum, 1977).

Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ For N-best MBR, this space £ is the N-best list produced by a baseline decoder.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ We will introduce conditions on the loss functions that can be incorporated in Lattice MBR decoding.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ Our experiments show that the main improvement comes from the larger evidence space: a larger set of translations in the lattice provides a better estimate of the expected BLEU score.

Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ The lattice density is the average number of arcs per word and can be varied using Forward-Backward pruning (Sixtus and Ortmanns, 1999).
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ However, bcde gets a higher expected gain (Equation 6) than abde since it shares more n-grams with the Rank-3 hypothesis (bcda).
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ This illustrates how a lattice can help select MBR translations that can differ from the MAP translation.

Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ For N-best MBR, we use N-best lists of size 1000.
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ More generally, we have found a component in machine translation where the posterior distribution over hypotheses plays a crucial role.
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ A path in this automaton that corresponds to the word sequence E' has cost: θ0jE'j+EwcAr θw#w(E)p(wj£) (expression within the curly brackets in Equation 6).
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ Figure 3 shows the variation in BLEU scores on eval08 as this parameter is varied.

Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ Figure 2 reports the average number of lattice paths and BLEU scores as a function of lattice density.
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ It is known that the average ngram precisions decay approximately exponentially with n (Papineni et al., 2001).
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ Then, the corpus BLEU score B(r, c0, cn) can be defined as follows (Papineni et al., 2001): where we have ignored On, the difference between the number of words in the candidate and the numthe (max, +) semiri ber of n-grams.

The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al (2008) replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function. $$$$$ Such an approach is also followed in Dreyer et al. (2007).
The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al (2008) replace the clipped n-gram count with the product of an n gram count and an n-gram indicator function. $$$$$ Substituting the derivatives in Equation 8 gives

Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ Our experiments show that the main improvement comes from the larger evidence space: a larger set of translations in the lattice provides a better estimate of the expected BLEU score.
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ Substituting the derivatives in Equation 8 gives
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ Each path in the lattice, consisting of consecutive transitions beginning at the distinguished initial state and ending at a final state, expresses a candidate translation.
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ However, bcde gets a higher expected gain (Equation 6) than abde since it shares more n-grams with the Rank-3 hypothesis (bcda).

Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ This illustrates how a lattice can help select MBR translations that can differ from the MAP translation.
Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ Figure 1 shows a toy lattice and the final MBR automaton (Section 4) for BLEU with a maximum ngram order of 2.
Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ However, bcde gets a higher expected gain (Equation 6) than abde since it shares more n-grams with the Rank-3 hypothesis (bcda).
