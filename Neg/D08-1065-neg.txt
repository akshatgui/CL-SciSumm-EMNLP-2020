We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ This is a model-independent approach in that the lattices could be produced by any statistical MT system — both phrase-based and syntaxbased systems would work in this framework.
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ This is a significant development in that the MBR decoder operates over a very large number of translations.
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ Figure 1 shows a toy lattice and the final MBR automaton (Section 4) for BLEU with a maximum ngram order of 2.
We also analytically show that interpolating these n-gram models for different n is similar to minimum risk decoding for BLEU (Tromble et al,2008). $$$$$ This is a novel insight into the workings of MBR decoding.

We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ Finally, we show how the performance of lattice MBR changes as a function of the lattice density.
We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ This is a model-independent approach in that the lattices could be produced by any statistical MT system — both phrase-based and syntaxbased systems would work in this framework.
We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ Then, the corpus BLEU score B(r, c0, cn) can be defined as follows (Papineni et al., 2001): where we have ignored On, the difference between the number of words in the candidate and the numthe (max, +) semiri ber of n-grams.
We geometrically interpolate the resulting approximations q with one another (and with the original distribution p), justifying this interpolation as similar to the minimum-risk decoding for BLEU proposed by Tromble et al (2008). $$$$$ For each n-gram w E N, we then construct an automaton that accepts an input E with weight equal to the product of the number of times the ngram occurs in the input (#w(E)), the n-gram factor θw from Equation 6, and the posterior probability p(wj£).

We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ For each n-gram w E N, we then construct an automaton that accepts an input E with weight equal to the product of the number of times the ngram occurs in the input (#w(E)), the n-gram factor θw from Equation 6, and the posterior probability p(wj£).
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ Our strategy will be to use a first order Taylor-series approximation to what we call the corpus log(BLEU) gain: the change in corpus log(BLEU) contributed by the sentence relative to not including that sentence in the corpus.
We now observe that our variational decoding resembles the MBR decoding of Tromble et al (2008). $$$$$ Therefore, the MBR decoder can be more generally written as follows: where £h refers to the Hypothesis space from where the translations are chosen, and £e refers to the Evidence space that is used for computing the Bayesrisk.

Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ We present Minimum Bayes-Risk (MBR) decoding over translation lattices that compactly encode a huge number of translation hypotheses.
Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ Then, the corpus BLEU score B(r, c0, cn) can be defined as follows (Papineni et al., 2001): where we have ignored On, the difference between the number of words in the candidate and the numthe (max, +) semiri ber of n-grams.
Note that Tromble et al (2008) only consider MBR for a lattice without hidden structures, though their method can be in principle applied in a hyper graph with spurious ambiguity. $$$$$ Kumar and Byrne (2004) show that MBR decoding gives optimal performance when the loss function is matched to the evaluation criterion; in particular, MBR under the sentence-level BLEU loss function (Papineni et al., 2001) gives gains on BLEU.

 $$$$$ To achieve this, we make use of the properties of n-gram matches.
 $$$$$ Substituting the above factors in Equation 6, we find that the MBR decision does not depend on T; therefore any value of T can be used.
 $$$$$ In contrast, the current N-best implementation of MBR can be scaled to, at most, a few thousands of hypotheses.
 $$$$$ We note that the MBR hypothesis (bcde) has a higher decoder cost relative to the MAP hypothesis (abde).

Pass 1 $$$$$ This score is therefore a linear function in counts of words Δc0 and n-gram matches Δcn.
Pass 1 $$$$$ For N-best MBR, we use N-best lists of size 1000.
Pass 1 $$$$$ The translation lattices are pruned using Forward-Backward pruning (Sixtus and Ortmanns, 1999) so that the average numbers of arcs per word (lattice density) is 30.

Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ As an example, the hypothesis could be selected from the N-best list while the risk is computed based on the entire lattice.
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ The scale factor determines the flatness of the posterior distribution over translation hypotheses.
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ To do that, we carry out lattice MBR when either the hypothesis or the evidence space in Equation 2 is restricted to 1000-best hypotheses (Table 4).
Tromble et al (2008) proposed a linear approximation to BLEU score (log-BLEU) as a new loss function in MBR decoding and extended it from N-best lists to lattices, and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hyper graphs to alleviate the high computational cost problem in Tromble et al's work. $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.

For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ Because the lattice is acyclic, this is possible.
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ This is a novel insight into the workings of MBR decoding.
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ We introduce an approximation to the BLEU score (Papineni et al., 2001) that satisfies these conditions.
For lattice MBR decoding, we optimized the lattice density and set the p and r parameters as per Tromble et al (2008). $$$$$ If the reference translation E is known, the decoder performance can be measured by the loss function L(E, S(F)).

In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ An example of lattice MBR with a toy lattice is presented in Section 6.
In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ If the reference translation E is known, the decoder performance can be measured by the loss function L(E, S(F)).
In the lattice MBR experiments of Tromble et al (2008), it is shown that this size of hypothesis set is sufficient. $$$$$ The MBR decoder on lattices (Equation 1) can therefore be written as Here p(w|£) = EE∈Ew P(E|F) is the posterior probability of the n-gram w in the lattice.

Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.
Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ We will present experiments (Section 7) to show the relative importance of these two spaces.
Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ We describe conditions on the loss function that will enable efficient implementation of MBR decoders on lattices.
Since BLEU does not factorize over the search graph, they use the linear approximation of Tromble et al (2008) instead. $$$$$ We describe an approximation to the BLEU score (Papineni et al., 2001) that will satisfy these conditions.

up to 1081 as per Tromble et al (2008). $$$$$ As a result, it is necessary to flatten the probability distribution so that MBR decoding can select hypotheses other than the MAP hypothesis.
up to 1081 as per Tromble et al (2008). $$$$$ We now assume that the number of matches of each n-gram is a constant ratio r times the matches of the corresponding n −1 gram.
up to 1081 as per Tromble et al (2008). $$$$$ We therefore treat only cns as variables.
up to 1081 as per Tromble et al (2008). $$$$$ A translation word lattice is a compact representation for very large N-best lists of translation hypotheses and their likelihoods.

Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ We will show that MBR decoding under this score achieves a performance that is at least as good as the performance obtained under sentence-level BLEU score.
Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ We note that the MBR hypothesis (bcde) has a higher decoder cost relative to the MAP hypothesis (abde).
Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ In general, MBR decoding can use different spaces for hypothesis selection and risk computation: argmax and the sum in Equation 1 (Goel, 2001).
Also, since we maintain a probabilistic formulation across training and decoding, our approach does not require a grid-search for a scaling factor as in Tromble et al (2008). $$$$$ This illustrates how a lattice can help select MBR translations that can differ from the MAP translation.

The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ We conduct a range of translation experiments to analyze lattice MBR and compare it with N-best MBR.
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ Because the lattice is acyclic, this is possible.
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ That is, gw is θw times the number of occurrences of w in E0, or zero if w does not occur in E. We first assume that the overall gain function G(E, E0) can then be written as a sum of local gain functions and a constant θ0 times the length of the hypothesis E0.
The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed lattice based MBR (Tromble et al, 2008). $$$$$ Substituting the derivatives in Equation 8 gives

Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ Our Lattice MBR decoding is realized using Weighted Finite State Automata.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ The resulting MBR automaton computes the total expected gain of each path.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ Our experiments show that the main improvement comes from the larger evidence space: a larger set of translations in the lattice provides a better estimate of the expected BLEU score.
Other linear functions have been explored for MBR, including Taylor approximations to the logarithm of BLEU (Tromble et al, 2008) and counts of matching constituents (Zhang and Gildea, 2008), which are discussed further in Section 3.3. $$$$$ We now present MBR decoding on translation lattices.

Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ The MBR scaling parameter (α in Equation 3) is tuned on the dev2 development set.
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ We note that the MBR hypothesis (bcde) has a higher decoder cost relative to the MAP hypothesis (abde).
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ However, bcde gets a higher expected gain (Equation 6) than abde since it shares more n-grams with the Rank-3 hypothesis (bcda).
Tromble et al (2008) describe a similar approach using MBR with a linear similarity measure. $$$$$ This linearization technique has been applied elsewhere when working with BLEU: Smith and Eisner (2006) approximate the expectation of log BLEU score.

Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ However, bcde gets a higher expected gain (Equation 6) than abde since it shares more n-grams with the Rank-3 hypothesis (bcda).
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ In Section 3, we describe the conditions on the gain function for efficient decoding over a lattice.
Using G, Tromble et al (2008) extend MBR to word lattices, which improves performance over k-best list MBR. $$$$$ The lattice MBR technique is efficient when performed over enormous number of hypotheses (up to 1080) since it takes advantage of the compact structure of the lattice.

Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ Thus it directly incorporates the loss function into the decision criterion.
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ We will present experiments (Section 7) to show the relative importance of these two spaces.
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ The resulting MBR automaton computes the total expected gain of each path.
Our approach differs from Tromble et al (2008) primarily in that we propose decoding with an alternative to MBR using BLEU, while they propose decoding with MBR using a linear alternative to BLEU. $$$$$ The MBR decoding under this approximate BLEU is realized using Weighted Finite State Automata.

The log-BLEU function must be modified slightly to yield a linear Taylor approximation $$$$$ Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with words and hidden tree structures.
The log-BLEU function must be modified slightly to yield a linear Taylor approximation $$$$$ The factors depend on a set of n-gram matches and counts (cn; n E 10, 1, 2, 3, 4}).
The log-BLEU function must be modified slightly to yield a linear Taylor approximation $$$$$ In Section 3, we describe the conditions on the gain function for efficient decoding over a lattice.
The log-BLEU function must be modified slightly to yield a linear Taylor approximation $$$$$ This illustrates how a lattice can help select MBR translations that can differ from the MAP translation.

Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ We loosely call a gain function local if it can be applied to all paths in the lattice via WFSA intersection (Mohri, 2002) without significantly multiplying the number of states.
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ This will enable us to rewrite the log(BLEU) as a linear function of n-gram matches and the hypothesis length.
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks.
Second, rather than use BLEU as a sentence level similarity measure directly, Tromble et al (2008) approximate corpus BLEU with G above. $$$$$ Our experiments show that the Lattice MBR decoder yields moderate, consistent gains in translation performance over N-best MBR decoding on Arabicto-English, Chinese-to-English and Englishto-Chinese translation tasks.

Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ This is a novel insight into the workings of MBR decoding.
Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system.
Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ There are four steps involved in decoding starting from weighted finite-state automata representing the candidate outputs of a translation system.
Tromble et al (2008) compute expected feature values by intersecting the translation lattice with a lattices for each n-gram t. $$$$$ We now present experiments to tease apart these factors.
