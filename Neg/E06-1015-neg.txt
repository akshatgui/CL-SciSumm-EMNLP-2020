The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ In our study, we consider syntactic parse trees, consequently, each node with its children is associated with a grammar production rule, where the symbol at left-hand side corresponds to the parent node and the symbols at right-hand side are associated with its children.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ With all the training data FTK terminated in 6 hours whereas QTK required more than 1 week.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ Finally, Section 5 discusses the related work whereas Section 6 summarizes the conclusions.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982).

 $$$$$ Table 4 shows the results of four kernel combinations.
 $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.
 $$$$$ Moreover, we note that the above tree kernels are not convolution kernels as those proposed in this article.
 $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.

 $$$$$ Thus, Eq.
 $$$$$ On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003).
 $$$$$ From the FrameNet corpus (http://www.icsi .berkeley.edu/∼framenet), we extracted all 24,558 sentences of the 40 Frames selected for the Automatic Labeling of Semantic Roles task of Senseval 3 (www.senseval.org).
 $$$$$ In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al., 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004).

 $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.
 $$$$$ In (Cumby and Roth, 2003), a feature description language was used to extract structural features from the syntactic shallow parse trees associated with named entities.
 $$$$$ In this section we compare our Fast Tree Kernel (FTK) approach with the Quadratic Tree Kernel (QTK) algorithm.
 $$$$$ The main arguments against their use are their efficiency and accuracy lower than traditional feature based approaches.

 $$$$$ In (Collins and Duffy, 2002), the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task.
 $$$$$ In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
 $$$$$ In (Vishwanathan and Smola, 2002), a linear complexity algorithm for the computation of the ST kernel is provided (in the worst case).
 $$$$$ Frame elements or semantic roles are arguments of predicates called target words.

SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ By recursively applying this property, it follows that the subtrees in n1 and n2 are identical.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ When the productions associated with n1 and n2 are different, we can avoid to evaluate A(n1, n2) since it is 0. n2=get next elem(L2); /*get the head element and move the pointer to the next element*/ Thus, we look for a node pair set Np ={(n1, n2)E NT1 x NT2 : p(n1) = p(n2)1, where p(n) returns the production rule associated with n. To efficiently build Np, we (i) extract the L1 and L2 lists of the production rules from T1 and T2, (ii) sort them in the alphanumeric order and (iii) scan them to find the node pairs (n1, n2) such that (p(n1) = p(n2)) E L1nL2.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ Section 4 shows the comparative performance in term of the execution time and accuracy.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ Our FTK, adapted for the LTAG tree kernel, would have allowed SVMs to be trained on the whole data.

 $$$$$ ARG0), according to different percentages of training data.
 $$$$$ On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003).
 $$$$$ In contrast, their major drawback are (a) the computational time complexity which is superlinear in the number of tree nodes and (b) the accuracy that they produce is often lower than the one provided by linear models on manually designed features.
 $$$$$ Since QTK was used for the kernel computation, the high learning complexity forced the authors to train different SVMs on different slices of training data.

The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.
The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ We adopted the OVA approach as it is simple and effective as showed in (Pradhan et al., 2004).
The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.

PTFs have been defined in (Moschitti, 2006a). $$$$$ We note that, with 70% of the training data, FTK is about 10 times faster than QTK.
PTFs have been defined in (Moschitti, 2006a). $$$$$ Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins’ parser (Collins, 1997), consequently, the experiments on FrameNet relate to automatic syntactic parse trees.
PTFs have been defined in (Moschitti, 2006a). $$$$$ The task of selecting the most relevant substructures is carried out by the kernel machines themselves.

Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ Additionally, it was alluded that the average execution time depends on the number of repeated productions.
Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ This research is partially supported by the Presto Space EU Project#: FP6-507336.
Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ In contrast, two identical parse trees may generate a linear number of non-null pairs if there are few groups of nodes associated with the same production rule.

The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ The main arguments against their use are their efficiency and accuracy lower than traditional feature based approaches.
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ The results show that, on both PropBank and FrameNet datasets, the SST-based kernel, i.e. the richest in terms of substructures, produces the highest SVM accuracy.
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ The results show an increase of the speed similar to the one produced by our method.
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ In this paper, we aim to solve the above problems.

It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). $$$$$ The high different number of substructures gives an intuitive quantification of the different information level between the two tree-based representations.
It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). $$$$$ In (Kazama and Torisawa, 2005), an interesting algorithm that speeds up the average running time is presented.
It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.

For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ In this paper, we have shown that tree kernels can effectively be adopted in practical natural language applications.
For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ This research is partially supported by the Presto Space EU Project#: FP6-507336.
For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.
For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.

The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). $$$$$ In (Zelenko et al., 2003), two kernels over syntactic shallow parser structures were devised for the extraction of linguistic relations, e.g. personaffiliation.
The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.
The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). $$$$$ When σ = 0, A(n1, n2) is equal 1 only if bj A(cjn1, cjn2) = 1, i.e. all the productions associated with the children are identical.

 $$$$$ We will refer to such extended kernels as ST+bow and SST+bow (bag-ofwords).
 $$$$$ Linear takes advantage by the richer feature set of the SSTs.
 $$$$$ The results show an increase of the speed similar to the one produced by our method.
 $$$$$ The experiments on the named entity categorization showed that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy.

Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ Finally, Section 5 discusses the related work whereas Section 6 summarizes the conclusions.
Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ 87.1%6 in (Pradhan et al., 2004).
Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ This low complexity allows SVMs to carry out experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial kernel, widely used on large experimentation e.g.
Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ In this paper, we have shown that tree kernels can effectively be adopted in practical natural language applications.

More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ (Gildea and Palmer, 2002; Pradhan et al., 2004).
More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ The task of selecting the most relevant substructures is carried out by the kernel machines themselves.
More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ In (Collins and Duffy, 2002), the SST tree kernel was experimented with the Voted Perceptron for the parse-tree reranking task.
More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ We mapped together the semantic roles having the same name and we considered only the 18 most frequent roles associated with verbal predicates, for a total of 37,948 arguments.

 $$$$$ (Pradhan et al., 2004).
 $$$$$ For example, the Parse Tree Path of the pair (brought, ARG1) in the syntactic tree of Figure 4 is V T VP 1 NP.
 $$$$$ We note that, (a) STs and SSTs improve Poly (about 0.5 and 2 percent points on PropBank and FrameNet, respectively) and (b) the linear kernel, which uses fewer features than Poly, is more enhanced by the SSTs than STs (for example on PropBank we have 89.4% and 88.6% vs. 87.6%), i.e.
 $$$$$ To measure the similarity between two nodes, the contiguous string kernel and the sparse string kernel (Lodhi et al., 2000) were used.

 $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.
 $$$$$ The results show an increase of the speed similar to the one produced by our method.
 $$$$$ Each row of the two tables shows the F1 measure of the individual classifiers using different kernels whereas the last column illustrates the global accuracy of the multiclassifier.

Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement $$$$$ Unfortunately, the ST set is rather poorer than the one generated by the subset tree (SST) kernel designed in (Collins and Duffy, 2002).
Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement $$$$$ SST), the higher the accuracy is, (b) tree kernels are effective also in case of automatic parse trees and (c) as kernel combinations always improve traditional feature models, the best approach is to combine scalar-based and structured based kernels.
Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement $$$$$ SST), the higher the accuracy is, (b) tree kernels are effective also in case of automatic parse trees and (c) as kernel combinations always improve traditional feature models, the best approach is to combine scalar-based and structured based kernels.
