The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ We have shown that a fast algorithm (FTK) can evaluate tree kernels in a linear average running time and also that the overall converging time required by SVMs is compatible with very large data sets.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ Only verbs are considered as predicates whereas arguments are labeled sequentially from ARG0 to ARG9.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ The difference with the subtrees is that the leaves can be associated with non-terminal symbols.
The time needed for tree kernel function was not so problematic as we could use the fast evaluation proposed in (Moschitti, 2006). $$$$$ First, we run ST, SST, ST+bow, SST+bow, Linear and Poly kernels over different training-set size of PropBank.

 $$$$$ It should be noted that our results of kernel combinations on FrameNet are in contrast with (Moschitti, 2004), where no improvement was obtained.
 $$$$$ Finally, to study the combined kernels, we applied the K1 + γK2 formula, where K1 is either the Linear or the Poly kernel and K2 is the ST or the SST kernel.
 $$$$$ Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.
 $$$$$ Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.

 $$$$$ In this paper, we have shown that tree kernels can effectively be adopted in practical natural language applications.
 $$$$$ In this paper, we aim to solve the above problems.
 $$$$$ Figure 4 shows the parse tree of the sentence: &quot;Mary brought a cat to school&quot; along with the predicate argument annotation proposed in the PropBank project.
 $$$$$ In our study, we consider syntactic parse trees, consequently, each node with its children is associated with a grammar production rule, where the symbol at left-hand side corresponds to the parent node and the symbols at right-hand side are associated with its children.

 $$$$$ In principle, the designer should have had to select and experiment all possible tree subparts.
 $$$$$ Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.
 $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.
 $$$$$ This can be used to compute the ST fragments once the tree is converted into a string.

 $$$$$ Only verbs are considered as predicates whereas arguments are labeled sequentially from ARG0 to ARG9.
 $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.

SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ The difference with the subtrees is that the leaves can be associated with non-terminal symbols.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ The main arguments against their use are their efficiency and accuracy lower than traditional feature based approaches.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ Note that, since the FrameNet data does not include deep syntactic tree annotation, we processed the FrameNet data with Collins’ parser (Collins, 1997), consequently, the experiments on FrameNet relate to automatic syntactic parse trees.
SVMLight (Joachims,1999), in the SVMLight/TK (Moschitti, 2006) variant, allows to use tree-valued features. $$$$$ We have shown that a fast algorithm (FTK) can evaluate tree kernels in a linear average running time and also that the overall converging time required by SVMs is compatible with very large data sets.

 $$$$$ For such purpose, the fragment types need to be described.
 $$$$$ This does not hold for the SSTs whose leaves can be internal nodes.
 $$$$$ First, we run ST, SST, ST+bow, SST+bow, Linear and Poly kernels over different training-set size of PropBank.
 $$$$$ Frame elements or semantic roles are arguments of predicates called target words.

The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ This low complexity allows SVMs to carry out experiments on hundreds of thousands of training instances since it is not higher than the complexity of the polynomial kernel, widely used on large experimentation e.g.
The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ This research is partially supported by the Presto Space EU Project#: FP6-507336.
The algorithm for the efficient evaluation of ? for the syntactic tree kernel (STK) has been widely discussed in (Collins and Duffy, 2002) whereas its fast evaluation is proposed in (Moschitti, 2006b), so we only describe the equations of the partial tree kernel (PTK). $$$$$ On the other hand, if the SST space contains too many irrelevant features, overfitting may occur and decrease the classification accuracy (Cumby and Roth, 2003).

PTFs have been defined in (Moschitti, 2006a). $$$$$ 1).
PTFs have been defined in (Moschitti, 2006a). $$$$$ 1 evaluates the subtree (ST) kernel.
PTFs have been defined in (Moschitti, 2006a). $$$$$ As a final decision of the multiclassifier, we select the argument type ARGt associated with the maximum value among the scores provided by the Ci, i.e. t = argmaxi∈S score(Ci), where S is the set of argument types.
PTFs have been defined in (Moschitti, 2006a). $$$$$ When the productions associated with n1 and n2 are different, we can avoid to evaluate A(n1, n2) since it is 0. n2=get next elem(L2); /*get the head element and move the pointer to the next element*/ Thus, we look for a node pair set Np ={(n1, n2)E NT1 x NT2 : p(n1) = p(n2)1, where p(n) returns the production rule associated with n. To efficiently build Np, we (i) extract the L1 and L2 lists of the production rules from T1 and T2, (ii) sort them in the alphanumeric order and (iii) scan them to find the node pairs (n1, n2) such that (p(n1) = p(n2)) E L1nL2.

Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches, e.g.
Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ (Pradhan et al., 2004).
Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ In recent years tree kernels have been proposed for the automatic learning of natural language applications.
Miwa et al (2009a) proposed a hybrid kernel, which combines the all-paths graph (APG) kernel (Airola et al 2008), the bag-of-words kernel, and the subset tree kernel (Moschitti, 2006) (applied on the shortest dependency paths between target protein pairs). $$$$$ In this paper, we have shown that tree kernels can effectively be adopted in practical natural language applications.

The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ To understand such point, we should make a step back before Gildea and Jurafsky defined the first set of features for Semantic Role Labeling (SRL).
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ To solve the problem (b), a study on different tree substructure spaces should be carried out to derive the tree kernel that provide the highest accuracy.
The function can be computed recursively in closed form, and quite efficient implementations are available (Moschitti, 2006). $$$$$ Moreover, we add the decay factor λ by modifying steps (2) and (3) as follows1: The computational complexity of Eq.

It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). $$$$$ Additionally, 30% of training was used as a validationset.
It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). $$$$$ In (Gildea and Jurafsky, 2002) seven different features2, which aim to capture the relation between the predicate and its arguments, were proposed.
It should be stressed that we are comparing against a fast TK implementation that is almost linear in time with respect to the number of tree nodes (Moschitti, 2006). $$$$$ The experiments on the named entity categorization showed that when the description language selects an adequate set of tree fragments the Voted Perceptron algorithm increases its classification accuracy.

For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
For classification we applied the updated tree-kernel package (Moschitti 2006), distributed with the svm-light tool (Joachims 1999) for learning Support Vector Machines (SVMs). $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.

The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). $$$$$ For example the following sentence is annotated according to the ARREST frame: [Time One Saturday night] [ Authorities police in Brooklyn ] [Target apprehended ] [Suspect sixteen teenagers].
The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). $$$$$ (c) Such approach is perfectly compatible with the dynamic programming algorithm which computes A.
The worse case is not really informative since as shown in (Moschitti, 2006), we can design fast algorithm with a linear average running time (we use such algorithm for SSTK). $$$$$ Thus, Eq.

 $$$$$ In this paper, we show that tree kernels are very helpful in the processing of natural language as (a) we provide a simple algorithm to compute tree kernels in linear average running time and (b) our study on the classification properties of diverse tree kernels show that kernel combinations always improve the traditional methods.
 $$$$$ Many thanks to the EACL 2006 anonymous reviewers, Roberto Basili and Giorgio Satta who provided me with valuable suggestions.
 $$$$$ I would like to thank the AI group at the University of Rome ”Tor Vergata”.
 $$$$$ We will refer to this basic implementation as the Quadratic Tree Kernel (QTK).

Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982).
Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ Third, we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort (Gildea and Jurafsky, 2002).
Collins and Duffy (Collins and Duffy, 2002) suggested to employ convolution kernels to measure similarity between two trees in terms of their sub structures, and more recently, Moschitti (Moschitti, 2006) described in details a fast implementation of tree kernels. $$$$$ To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982).

More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ Given a syntactic tree we can use as feature representation the set of all its STs or SSTs.
More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ Experiments with Support Vector Machines on the predicate argument classification task provide empirical support to our thesis.
More recently, Moschitti (Moschitti, 2006) introduced in details a fast implementation of tree kernels, where a node pair set is first constructed for those associated with same production rules. $$$$$ 87.1%6 in (Pradhan et al., 2004).

 $$$$$ Our fast algorithm computes the kernels between two syntactic parse trees in O(m + n) average time, where m and n are the number of nodes in the two trees.
 $$$$$ In recent years tree kernels have been proposed for the automatic learning of natural language applications.
 $$$$$ This research is partially supported by the Presto Space EU Project#: FP6-507336.
 $$$$$ When SSTs are combined with the manual designed features, we always obtain the best figure classifier.

 $$$$$ On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown, for example, in (Zelenko et al., 2003; Moschitti, 2004).
 $$$$$ However, as observed in (Collins and Duffy, 2002) this worst case is quite unlikely for the syntactic trees of natural language sentences, thus, we can design algorithms that run in linear time on average.
 $$$$$ In fact, the only difference with the original approach is that the matrix entries corresponding to pairs of different production rules are not considered.
 $$$$$ This research is partially supported by the Presto Space EU Project#: FP6-507336.

Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a). $$$$$ Unfortunately, they show (a) an inherent super linear complexity and (b) a lower accuracy than traditional attribute/value methods.
Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a). $$$$$ To understand such point, we should make a step back before Gildea and Jurafsky defined the first set of features for Semantic Role Labeling (SRL).
Although this kernel achieves state-of-the-art performance in NLP tasks, such as Question Classification (Bloehdorn and Moschitti, 2007b) and Textual Entailment (Mehdad et al, 2010), it offers clearly possibility of improvement: (i) better possibility to exploit semantic smoothing since, e.g., trivially STK only matches the syntactic structure apple/orange when comparing the big beautiful apple to a nice large orange; and (ii) STK cannot be effectively applied to dependency structures, e.g. see experiments and motivation in (Moschitti, 2006a). $$$$$ To confirm such hypothesis, we measured the impact of the algorithm on the time required by SVMs for the learning of about 122,774 predicate argument examples annotated in PropBank (Kingsbury and Palmer, 2002) and 37,948 instances annotated in FrameNet (Fillmore, 1982).
