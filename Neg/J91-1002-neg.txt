Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ We suspect that the chain-forming parameter settings (regarding transitivity and distances between words) will be shown to vary slightly according to author's style and the type of text.
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ Hence, computing the chains is useful, since they will have a correspondence to the structure of the text.
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ It would be useful to automate this tool and run a large corpus of text through it.

(Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. $$$$$ It does not have to name or classify the idea or relationship.
(Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. $$$$$ The lexical chains also provide a semantic context for interpreting words, concepts, and sentences.
(Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. $$$$$ Also, we believe that chain strength analysis will be required for this purpose.
(Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. $$$$$ Since the lexical chains are computable, and exist in nonâ€“domain-specific text, they provide a valuable indicator of text structure.

An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). $$$$$ He uses a formal definition of coherence relations, an extensive knowledge base of assertions and properties of objects and actions, and a mechanism that searches this knowledge source and makes simple inferences.
An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). $$$$$ It is anticipated that chain return analysis would become integrated with other text processing tools in order to prevent this.

Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). $$$$$ Thanks to Chrysanne DiMarco, Mark Ryan, and John Morris for commenting on earlier drafts.
Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). $$$$$ In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning.
Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). $$$$$ In the structure of traditional artificial intelligence knowledge bases, such as frames or semantic networks, words or ideas that are related are actually &quot;physically close&quot; in the representation.

Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). $$$$$ When a unit of text is about the same thing there is a strong tendency for semantically related words to be used within that unit.
Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). $$$$$ Also, the returns coincided with intentional boundaries.
Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). $$$$$ This was found to be true.

Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). $$$$$ Thanks to Robin Cohen, Jerry Hobbs, Eduard Hovy, Ian Lancashire, and anonymous referees for valuable discussions of the ideas in this paper.
Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). $$$$$ This work was financially assisted by the Government of Ontario, the Department of Computer Science of the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.
Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). $$$$$ When a discourse segment purpose contributes to a discourse segment purpose of the immediately preceding discourse segment, the new focus space is pushed onto the stack.

Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ If it is hard to classify a relationship in a systematic semantic way, it will be hard to represent the relationship in a traditional frame or semantic network formalism.
Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ In this paper, a thesaurus is used as the major knowledge base for computing lexical chains.
Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ Overall, the lexical chains found in this example provide a good clue for the determination of the intentional structure.
Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;

The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;
The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ The lexical chains also provide a semantic context for interpreting words, concepts, and sentences.
The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ This work was financially assisted by the Government of Ontario, the Department of Computer Science of the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.
The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ Therefore the them in example 11 is an instance of it.

In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. $$$$$ Thanks to Robin Cohen, Jerry Hobbs, Eduard Hovy, Ian Lancashire, and anonymous referees for valuable discussions of the ideas in this paper.
In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. $$$$$ The five texts were analyzed with respect to distance between clearly related words.
In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. $$$$$ There is a distance relation between each word in the chain, and the words co-occur within a given span.
In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. $$$$$ 3.2.3 Notation and Data Structures.

Global context where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an en tire text (Morris and Hirst, 1991). $$$$$ Correspondences between lexical chains and structural elements are shown to exist.
Global context where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an en tire text (Morris and Hirst, 1991). $$$$$ In Section 1, different types of word relationships were discussed: systematic semantic, nonsystematic semantic, word association, and words related by a common situation.

The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ Correspondences between lexical chains and structural elements are shown to exist.
The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ This is important, since Grosz and Sidner give no method for computing the intentions or linguistic segments that make up the structure that they propose.
The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ The topmost level consists of eight major classes developed by Roget in 1852: abstract relations, space, physics, matter, sensation, intellect, volition, and affections.

More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ Figure 2 shows the index entry for lid.
More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ It is likely that it varies slightly with respect to style, author, or type of text.
More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ The thesaurus was conceived by Peter Mark Roget, who described it as being the &quot;converse&quot; of a dictionary.
More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;

The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ Also, the returns coincided with intentional boundaries.
The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ There are three factors contributing to chain strength.
The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ There is a clear correspondence between chain 1, {.

Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ .
Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ Chain 2.1, {afflicted, darkness, .
Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ In the computation of lexical chains, the following information is kept for each word in a chain: * T stands for transitively related * q is the word number through which the transitive relation is formed.

WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;
WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). $$$$$ The lexical chains computed by the algorithm given in Section 3.2.3 correspond closely to the intentional structure produced from the structural analysis method of Grosz and Sidner (1986).
WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). $$$$$ But it could just as easily be explanation.

Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. $$$$$ Lexical chains do not stop at sentence boundaries.
Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. $$$$$ In this paper, a thesaurus is used as the major knowledge base for computing lexical chains.
Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. $$$$$ In this paper, a thesaurus is used as the major knowledge base for computing lexical chains.

Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ Determining the structure of text is an essential step in determining the deep meaning of the text.
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ Hence, computing the chains is useful, since they will have a correspondence to the structure of the text.
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ Ventola (1987) analyzed lexical cohesion and text structure within the framework of systemic linguistics and the specific domain of service encounters such as the exchange of words between a client at a post office and a postal worker.
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ These lexical chains are a direct result of units of text being &quot;about the same thing,&quot; and finding text structure involves finding units of text that are about the same thing.

The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). $$$$$ In this paper, a thesaurus is used as the major knowledge base for computing lexical chains.
The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). $$$$$ Given a copy, implementation would clearly be straightforward.
The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). $$$$$ In these cases, verification of the possible correspondence must be accomplished through the use of other textual information such as semantics or pragmatics.

Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. $$$$$ Also, the returns coincided with intentional boundaries.
Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;

Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ Three structure levels are above the category level.
Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning.
Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ There have been various attempts to classify all possible coherence relations, but there is as yet no widespread agreement.
