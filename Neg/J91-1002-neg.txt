Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ This represents an 87% hit rate (but not a big sample space).
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ The program to implement the algorithm given in Section 3.2.3 would also be straightforward.
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ 42.
Paradigmatic similarity is based on association data extracted from thesauri [Morris and Hirst, 1991], psychological experiments [Osgood, 1952], and so on. $$$$$ Correspondences between lexical chains and structural elements are shown to exist.

(Xiong et al, 2013b) incorporated lexical-chain-based models (Morris and Hirst, 1991) into machine translation. $$$$$ The methodology we used in our analyses was as follows: 3.

An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). $$$$$ The fact that they are short and overlapping suggests that they could be taken together as a whole.
An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). $$$$$ This section will concentrate on analyzing correspondences between lexical chains and structural units of text, including: The text structure theory chosen for this analysis was that of Grosz and Sidner (1986).
An algorithm for computing lexical chains was first given by (Morris and Hirst, 1991) using the Roget's Thesaurus (Kirkpatrick, 1998). $$$$$ This work was financially assisted by the Government of Ontario, the Department of Computer Science of the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.

Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). $$$$$ Thanks to Robin Cohen, Jerry Hobbs, Eduard Hovy, Ian Lancashire, and anonymous referees for valuable discussions of the ideas in this paper.
Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). $$$$$ Since language is not static, a thesaurus would have to be continually updated to remain current.
Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976). $$$$$ It was found that without some concept of chain strength, the semantic relatedness of these neighborhoods decays, partly due to homographs.

Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). $$$$$ This work was financially assisted by the Government of Ontario, the Department of Computer Science of the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.
Usually a lexical chain is obtained in a bottom-up fashion, by taking each candidate word of a text, and finding an appropriate relation offered by a thesaurus as Rodget (Morris and Hirst, 1991) or WordNet (Barzilay and Elhadad, 1999). $$$$$ Such collocation relationships exist between words that tend to occur in similar lexical environments.

Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). $$$$$ .
Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). $$$$$ Thanks to Robin Cohen, Jerry Hobbs, Eduard Hovy, Ian Lancashire, and anonymous referees for valuable discussions of the ideas in this paper.
Lexical cohesion analysis has been used in such NLP applications as determining the structure of text (Morris and Hirst, 1991) and automatic text summarization (Barzilay and Elhadad, 1999). $$$$$ This distinction depends on context, knowledge, and beliefs.

Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ A dictionary explains the meaning of words, whereas a thesaurus aids in finding the words that best express an idea or meaning.
Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;
Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ There is a clear correspondence between chain 1, {.
Roget's Thesaurus, which was used to form the lexical chains in Morris and Hirst (1991), also gives non classically related word groups. $$$$$ It should also be stressed, however, that the lexical structures cannot be used on their own to predict an exact structural partitioning of the text.

The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ Even though the chain is a lot shorter in length than the intention, its presence is a clue to the existence of a separate intention in its textual vicinity.
The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ They collectively correspond to intention 1.1.1 (hatred of commuting).
The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ The question of chain returns and when they can occur requires further research.
The lexical chains of Morris and Hirst (1991) had no such restriction, and frequently nouns, verbs ,adjectives, adverbs, and verbs were joined together in one chain. $$$$$ Hence, computing the chains is useful, since they will have a correspondence to the structure of the text.

In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. $$$$$ These in turn are divided into the basic categories.
In fact, each column of the matrix corresponds to a lexical chain (Morris and Hirst, 1991) for a particular term across the whole text. $$$$$ Correspondences between lexical chains and structural elements are shown to exist.

Global context where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an en tire text (Morris and Hirst, 1991). $$$$$ Since cohesion is well defined, one might expect that it would be computationally easier to identify, because the identification of ellipsis, reference, substitution, conjunction, and lexical cohesion is a straightforward task for people.
Global context where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an en tire text (Morris and Hirst, 1991). $$$$$ In the example given, there were cases where a lexical chain did correspond to an intention, but the sentences spanned by the lexical chain and the intention differed by more than two.

The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;
The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ Thanks to Chrysanne DiMarco, Mark Ryan, and John Morris for commenting on earlier drafts.
The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning.
The most closely related method is perhaps the lexical chains algorithm (Morris and Hirst, 1991) where threads of meaning are identified throughout a text. $$$$$ Determining the structure of text is an essential step in determining the deep meaning of the text.

More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ The thesaurus was conceived by Peter Mark Roget, who described it as being the &quot;converse&quot; of a dictionary.
More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ In traditional knowledge bases, the relationships must be named.
More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ Intuitively, some lexical chains are &quot;stronger&quot; than others, and possibly only strong chains can be returned to.
More recently, Clarke and Lapata (2007) use Centering Theory (Grosz et al, 1995) and Lexical Chains (Morris and Hirst, 1991) to identify which information to prune. $$$$$ But note that there will be cases in which lexical chains should be merged as a result of the intentional merging of ideas or concepts in the text.

The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ Correspondences between lexical chains and structural elements are shown to exist.
The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ }.
The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ The motivation behind this work was that lexical cohesion in text should correspond in some way to the structure of the text.
The idea of using lexical chains as indicators of lexical cohesion goes back to Morris and Hirst (1991). $$$$$ Lexical chains do not always correspond exactly to intentional structure, and when they do not, other textual information is needed to obtain the correct correspondences.

Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ The third component of this theory is the attentional state.
Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ A factor common to all but situational relationships is that there is a strong tendency for the word relationships to be captured in the thesaurus.
Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ Determining the structure of text is an essential step in determining the deep meaning of the text.
Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). $$$$$ Cruel is the third reiteration of the word in chain 2.

WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). $$$$$ Since the lexical chains are computable, and exist in nonâ€“domain-specific text, they provide a valuable indicator of text structure.
WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). $$$$$ Correspondences between lexical chains and structural elements are shown to exist.
WordNet is the main resource for detecting the cohesive relationships between words and their relevance to a given chain (Morris and Hirst, 1991). $$$$$ Determining the structure of text is an essential step in determining the deep meaning of the text.

Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. $$$$$ We are grateful to Jay Teitel for allowing us to reprint text from his article &quot;Outland.&quot;
Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. $$$$$ One significant fact emerged from this analysis: returns consisting of one word only were always made with a repetition of one of the words in the returned-to chain.
Ever since Morris and Hirst (1991)' s ground breaking paper, topic segmentation has been a steadily growing research area in computational linguistics. $$$$$ Thanks to Chrysanne DiMarco, Mark Ryan, and John Morris for commenting on earlier drafts.

Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ Zayde was lucid to the end, but a few years before he died the family assigned me the task of talking to him about his problem with alcohol.
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ In Figure 6 we show the intentional structure of the text of Section 4.2, and in Figure 7 we show the correspondences between the lexical chains and intentions of the example.
Lexical cohesion techniques include similarity measures between adjacent blocks of text, as in TextTiling (Hearst, 1994, 1997) and lexical chains based on recurrences of a term or related terms, as in Morris and Hirst (1991). $$$$$ Ventola's chain-building rule was that each lexical item is &quot;taken back once to the nearest preceding lexically cohesive item regardless of distance&quot; (p. 131).

The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). $$$$$ A paragraph contains words of only one syntactic category.
The relation between entities (noun phrases) in adjacent sentences could be of type center-reference (pronoun reference or reiteration), or based on semantic relatedness (Morris and Hirst,1991). $$$$$ Of course, most sentences that relate coherently do exhibit cohesion as well) Halliday and Hasan (1976) give two examples of lexical cohesion involving identity of reference: Example 11 Reichman (1985, p. 180) writes &quot;It is not the use of a pronoun that gives cohesion to the wash-and-core-apples text.

Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. $$$$$ Hence the concept of lexical cohesion, defined originally by Halliday and Hasan (1976) and expanded in this work, has a definite use in an automated text understanding system.
Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. $$$$$ Grosz and Sidner propose a structure common to all discourse, which could be used along with a structurally dependent focus of attention to delineate and constrain referring expressions.
Morris and Hirst (1991) suggested building lexical chains is important in the resolution of lexical ambiguity and the determination of coherence and discourse structure. $$$$$ In the example from Hobbs above, safe and combination are lexically related, which in a general sense means they &quot;are about the same thing in some way.&quot; In example 8, bought and shopping are lexically related, as are raincoat and rained.

Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ These lexical chains are a direct result of units of text being &quot;about the same thing,&quot; and finding text structure involves finding units of text that are about the same thing.
Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ Specifically, sentences 40 to 44 form intention 1.3 (why new suburbs exist), and hence a strong portion of the chain would correspond exactly to a structural unit.
Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ In the context {hair, curl, comb, wave} (Halliday and Hasan 1976), wave means a hair wave, not a water wave, a physics wave, or a friendly hand wave.
Morris and Hirst (1991) were the first to propose the concept of Lexical Chains to explore the discourse structure of a text. $$$$$ If the sprawling agglomeration of people known as Toronto has boomed in the past 10 years it has boomed outside the traditional city confines in a totally new city, a new suburbia containing one and a quarter million people.
