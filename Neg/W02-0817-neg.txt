The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. $$$$$ Open Mind Word Expert pursues the potential of creating a large tagged corpus.
The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. $$$$$ The basic idea behind Open Mind is to use the information and knowledge that may be collected from the existing millions of Web users, to the end of creating more intelligent software.
The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. $$$$$ The test corpus will be released during SENSEVAL-3.
The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted to Romanian. $$$$$ We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.

The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). $$$$$ We expect the system to yield a large volume of high-quality training data at a much lower cost than the traditional method of hiring lexicographers.
The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). $$$$$ These are the examples that are presented to the users for tagging in Stage 2.
The use of collaborative contributions from volunteers has been previously shown to be beneficial in the Open Mind Word Expert project (Chklovski and Mihalcea, 2002). $$$$$ We are also grateful to Adam Kilgarriff for valuable suggestions and interesting discussions, to Randall Davis and to the anonymous reviewers for useful comments on an earlier version of this paper, and to all the Open Mind Word Expert users who have emailed us with their feedback and suggestions, helping us improve this activity.

Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). $$$$$ Open Mind Word Expert pursues the potential of creating a large tagged corpus.
Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). $$$$$ WSD can also benefit in other ways from the Open Mind approach.
Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002). $$$$$ We would like to thank the Open Mind Word Expert contributors who are making all this work possible.

Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. $$$$$ We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.
Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. $$$$$ We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.
Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. $$$$$ From these, the users will be presented only with those examples where there is a disagreement between the labels assigned by the two classifiers.
Due to the expensive annotation process, only a handful of manually sense-tagged corpora are available. An effort to alleviate the training data bottle neck is the Open Mind Word Expert (OMWE) project (Chklovski and Mihalcea, 2002) to collect sense-tagged data from Internet users. $$$$$ One of the next steps we plan to take is to replace the ”two tags per item” scheme with the ”tag until at least two tags agree” scheme proposed and used during the SENSEVAL-2 tagging (Kilgarriff, 2002).

The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. $$$$$ Estimations made in (Ng, 1997) indicated that a high accuracy domain independent system for WSD would probably need a corpus of about 3.2 million sense tagged words.
The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. $$$$$ The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora.
The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. $$$$$ Web contributors can help this process by creating the initial set of seeds, and exercising control over the quality of the automatically generated seeds.
The annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002), adapted for multilingual annotations. $$$$$ The set of tagged words consists of the 191 most frequently occurring nouns and verbs.

Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. $$$$$ In all, automatic assessment of the quality of tagging seems possible, and, based on the experience of prior volunteer contribution projects (Singh, 2002), the rate of maliciously misleading or incorrect contributions is surprisingly low.
Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. $$$$$ We are also grateful to Adam Kilgarriff for valuable suggestions and interesting discussions, to Randall Davis and to the anonymous reviewers for useful comments on an earlier version of this paper, and to all the Open Mind Word Expert users who have emailed us with their feedback and suggestions, helping us improve this activity.
Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. $$$$$ We would like to thank the Open Mind Word Expert contributors who are making all this work possible.
Finally, in an effort related to the Wikipedia collection process, (Chklovski and Mihalcea, 2002) have implemented the Open Mind Word Expert system for collecting sense annotations from volunteer contributors over the Web. $$$$$ The final corpus for each ambiguous word will be created with (1) the original set of tagged examples, plus (2) the examples selected by the active learning component, sense tagged by users.

Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ Most of the efforts in the Word Sense Disambiguation (WSD) field have concentrated on supervised learning algorithms.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ We are also grateful to Adam Kilgarriff for valuable suggestions and interesting discussions, to Randall Davis and to the anonymous reviewers for useful comments on an earlier version of this paper, and to all the Open Mind Word Expert users who have emailed us with their feedback and suggestions, helping us improve this activity.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ We believe that making the contribution process as engaging and as “game-like” for the contributors as possible is crucial to collecting a large volume of data.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ The starting corpus we use is formed by a mix of three different sources of data, namely the Penn Treebank corpus (Marcus et al., 1993), the Los Angeles Times collection, as provided during TREC conferences1, and Open Mind Common Sense2, a collection of about 400,000 commonsense assertions in English as contributed by volunteers over the Web.

The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). $$$$$ We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.
The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). $$$$$ If successful, the collection process can be extended to create the definitive corpus of word sense information.
The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). $$$$$ Yet, as of today, only few sense tagged corpora are publicly available.
The extension consisted in extending the training data set so as to include a selection of WordNet examples (full sentences containing a main verb) and the Open Mind Word Expert corpus (Chklovski and Mihalcea 2002). $$$$$ The low precision on the instances in the disagreement set justifies referring to these as “hard to tag”.

For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). $$$$$ We would like to thank the Open Mind Word Expert contributors who are making all this work possible.
For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). $$$$$ For each word with sense tagged data created with Open Mind Word Expert, a test corpus will be built by trained human taggers, starting with examples extracted from the corpus mentioned in Section 3.1.
For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). $$$$$ Based on early feedback from both researchers and contributors, a future version of Open Mind Word Expert may allow contributors to specify more than one sense for any word.
For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al, 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)). $$$$$ It is available at http://teach-computers.org.

We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. $$$$$ WSD can also benefit in other ways from the Open Mind approach.
We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. $$$$$ We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.
We use SemCor1, OMWE 1.0 (Chklovski and Mihalcea, 2002), and example sentences in Word Net as the training corpus. $$$$$ We are also grateful to Adam Kilgarriff for valuable suggestions and interesting discussions, to Randall Davis and to the anonymous reviewers for useful comments on an earlier version of this paper, and to all the Open Mind Word Expert users who have emailed us with their feedback and suggestions, helping us improve this activity.

Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. $$$$$ We would like to thank the Open Mind Word Expert contributors who are making all this work possible.
Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. $$$$$ With Open Mind Word Expert we aim at creating a very large sense tagged corpus, by making use of the incredible resource of knowledge constituted by the millions of Web users, combined with techniques for active learning.
Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. $$$$$ For details on the data and how it has been collected, see (Singh, 2002).
Chklovski and Mihalcea (2002) presented another interesting proposal which turns to Web users to produce sense-tagged corpora. $$$$$ The size of the tagged corpus increased with SENSEVAL-2 (Kilgarriff, 2001), when 13,000 additional examples were released for 73 polysemous words.

Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. $$$$$ This will be implemented if there are indications of need for this in the pilot; it will help screen out contributors who, for example, always select the first sense (and are in high agreement with other contributors who do the same).
Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. $$$$$ It relies on the fact that different sets of features have different effects depending on the ambiguous word considered.
Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. $$$$$ The size of the tagged corpus increased with SENSEVAL-2 (Kilgarriff, 2001), when 13,000 additional examples were released for 73 polysemous words.
Open Mind Word Expert (Chklovski and Mihalcea, 2002) was a real application of active learning for WSD. $$$$$ WSD can also benefit in other ways from the Open Mind approach.

Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002). $$$$$ We thus propose a Senseval-3 lexical sample activity where the training data is collected via Open Mind Word Expert.
Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002). $$$$$ Open Mind Word Expert is an implemented active learning system for collecting word sense tagging from the general public over the Web.
Lately, many such corpora have been developed in different languages, including SemCor (Miller et al, 1993), LDC-DSO (Ng and Lee, 1996), Hinoki (Kasahara et al, 2004), and the sense annotated corpora with the help of Web users (Chklovski and Mihalcea, 2002). $$$$$ WSD can also benefit in other ways from the Open Mind approach.
