Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ However, for this initial work we assume P(o4) = 0.4 Let P(oi  |A) be the probability that oi is applied to A, and P(a  |A, oi) be the probability of A –* a given that the operation is oi.
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ While our initial implementation had fewer parameters than the method sketched above, it tended to terminate quickly, so that not many new simplifications were found; so, again, we do not report results here.
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ Nothing is more simple than greatness; indeed, to be simple is to be great.

(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.
(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.
(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.

Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ Systems that can rewrite text into simpler versions promise to make information available to a broader audience, such as non-native speakers, children, laypeople, and so on.
Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ There are at least four possible edit operations: fix (o1), simplify (o2), no-op (o3), or spam (o4).
Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ We therefore sought to use comments to identify “trusted” revisions wherein the extracted lexical edit instances (see §3.1) would be likely to be simplifications.

The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ The first two labels correspond to simplifications for the orientations A → a and a → A, respectively.
The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ The key quantities of interest are P(o2  |A) in S, which is the probability that A should be simplified, and P(a  |A, o2), which yields proper simplifications of A.
The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.
The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ Nothing is more simple than greatness; indeed, to be simple is to be great.

More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ Furthermore, the edit model yielded higher precision than SIMPL for the top 100 pairs.
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.

In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ Dictionary of simplifications The SimpleEW editor “Spencerk” (Spencer Kelly) has assembled a list of simple words and simplifications using a combination of dictionaries and manual effort9.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ Wiki editors have the option of associating a comment with each revision, and such comments sometimes indicate the intent of the revision.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ The nativespeaker majority label was used in our evaluations.

Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. $$$$$ Here, we consider an important dimension of style, namely, simplicity.
Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.

Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ For ComplexEW, we processed —16M revisions for 19407 articles.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ Simplification can also be considered to be a form of MT in which the two “languages” in question are highly related.

In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ Our second idea was to iteratively expand the set of trusted revisions, adding those that contain already highly ranked simplifications.
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.

Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ Unfortunately, our initial implementations involved many parameters (upper and lower commentfrequency thresholds, number of highly ranked simplifications to consider, number of comments to add per iteration), making it relatively difficult to tune; we thus omit its results.
Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ Here, we consider an important dimension of style, namely, simplicity.
Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ Dictionary of simplifications The SimpleEW editor “Spencerk” (Spencer Kelly) has assembled a list of simple words and simplifications using a combination of dictionaries and manual effort9.

(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ There are at least four possible edit operations: fix (o1), simplify (o2), no-op (o3), or spam (o4).
(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ We therefore sought to use comments to identify “trusted” revisions wherein the extracted lexical edit instances (see §3.1) would be likely to be simplifications.
(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ If we assume that the probability of any particular fix operation being applied in SimpleEW is proportional to that in ComplexEW— e.g., the SimpleEW fix rate might be dampened because already-edited ComplexEW articles are copied over — we have6 Pb(o1  |A) = αfC(A) where 0 < α < 1.

The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ —Emerson, Literary Ethics Style is an important aspect of information presentation; indeed, different contexts call for different styles.
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ He provides a list of 17,900 simple words — words that do not need further simplification — and a list of 2000 transformation pairs.
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ Let ik = (rk, ... , rk, ...) be the sequence of revisions for the kth article in SimpleEW, where rk is the set of lexical edit instances (A —* a) extracted from the ith modification of the document.

Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ Since the focus of our study was not word alignment, we used a simple method that identified the longest differing segments (based on word boundaries) between each sentence, except that to prevent the extraction of entire (highly nonmatching) sentences, we filtered out A —* a pairs if either A or a contained more than five words.
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ .
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ It would be interesting to use our proposed estimates as initialization for EM-style iterative re-estimation.
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (§2.1), and the use of metadata to filter out undesirable revisions (§2.2).

Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ Extracting lexical edit instances.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.

Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ For a given lexical item or ~ phrase A, we write A E dk if there is any version ~ ~ in dk that contains A.
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ Instead, we measured our agreement with the list of transformations he assembled (SPLIST).
