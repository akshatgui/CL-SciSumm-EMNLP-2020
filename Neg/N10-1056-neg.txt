Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ .
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ Baselines RANDOM returns lexical edit instances drawn uniformly at random from among those extracted from SimpleEW.
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ Since the focus of our study was not word alignment, we used a simple method that identified the longest differing segments (based on word boundaries) between each sentence, except that to prevent the extraction of entire (highly nonmatching) sentences, we filtered out A —* a pairs if either A or a contained more than five words.
Yatskar et al (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. $$$$$ For each article, we aligned sentences in each pair of adjacent versions using tf-idf scores in a way similar to Nelken and Shieber [6] (this produced satisfying results because revisions tended to represent small changes).

(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ The crux of our proposal is to learn lexical simplifications from SimpleEW edit histories, thus leveraging the efforts of the 18K pseudonymous individuals who work on SimpleEW.
(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ As mentioned above, a key idea in our work is to utilize SimpleEW edits.
(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ An important direction for future work is to differentially weight the edit instances within a revision, as opposed to placing equal trust in all of them; this could prevent our bootstrapping methods from giving common fixes (e.g., “a” —* “the”) high scores.
(Yatskar et al., 2010) use an unsupervised learning method and meta data from the Simple English Wikipedia. $$$$$ Our second idea was to iteratively expand the set of trusted revisions, adding those that contain already highly ranked simplifications.

Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ For a given lexical item or ~ phrase A, we write A E dk if there is any version ~ ~ in dk that contains A.
Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.
Simple English Wikipedia has been used before in simplicity analysis, as described in (Yatskar et al, 2010). $$$$$ Since the focus of our study was not word alignment, we used a simple method that identified the longest differing segments (based on word boundaries) between each sentence, except that to prevent the extraction of entire (highly nonmatching) sentences, we filtered out A —* a pairs if either A or a contained more than five words.

The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ Simplification is strongly related to but distinct from paraphrasing and machine translation (MT).
The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ He provides a list of 17,900 simple words — words that do not need further simplification — and a list of 2000 transformation pairs.
The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
The sequence of article edits can be used as training data for data-driven NLP algorithms, such as vandalism detection (Chin et al, 2010), text summarization (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), unsupervised extraction of lexical simplifications (Yatskar et al, 2010), the expansion of textual entailment corpora (Zanzotto and Pennacchiotti, 2010), or assesing the trustworthiness of Wikipedia articles (Zeng et al, 2006). $$$$$ We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ “Cat”) and a sequence dk of article versions caused by successive edits.

More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ Our work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8].
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ For each article, we aligned sentences in each pair of adjacent versions using tf-idf scores in a way similar to Nelken and Shieber [6] (this produced satisfying results because revisions tended to represent small changes).
More recently, Yatskar et al (2010) explore data-driven methods to learn lexical simplifications from Wikipedia revision histories. $$$$$ Collapsing the 5 labels into “simplification”, “not a simplification”, and “?” yields reasonable agreement among the 3 native speakers (n = 0.69; 75.3% of the time all three agreed on the same label).

In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ While it can be considered a directional form of the former, it differs in spirit because simplification must trade off meaning preservation (central to paraphrasing) against complexity reduction (not a consideration in paraphrasing).
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ Simplification can also be considered to be a form of MT in which the two “languages” in question are highly related.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ We believe that these two approaches provide a good starting point for further explorations.
In contrast to Yatskar et al (2010) and Zhu et al (2010), simplification operations (e.g., substitution or splitting) are not modeled explicitly; instead, we leave it up to our grammar extraction algorithm to learn appropriate rules that reflect the training data. $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.

Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
Indeed, Yatskar et al (2010) learn lexical simplifications without taking syntactic context into account. $$$$$ Two ideas for bootstrapping We also considered bootstrapping as a way to be able to utilize revisions whose comments are not in the initial Seed set.

Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ It would be interesting to use our proposed estimates as initialization for EM-style iterative re-estimation.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ We start with a seed set of trusted comments, Seed.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edit changes identified in both Wikipedias revision histories, though they only provide a list of the top phrasal rules and do not utilize them in an end-to-end simplification system. $$$$$ We consider two main approaches: (1) deriving simplification probabilities via an edit model that accounts for a mixture of different operations, and (2) using metadata to focus on edits that are more likely to be simplification operations.

In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ Note that in SimpleEW, where P(o1 V o2  |A) is the probability that A is changed to abdifferent word in SimpleEW, which we estimate as P(o1 V o2  |A) = fS(A).
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ Instead, we measured our agreement with the list of transformations he assembled (SPLIST).
In particular Yatskar et al (2010) leverage the relations between Simple Wikipedia and English Wikipedia to extract simplification pairs. $$$$$ Here are the results; “-x-y” means that x and y are the number of instances discarded from the precision calculation for having no majority label or majority label “?”, respectively: Both baselines yielded very low precisions — clearly not all (frequent) edits in SimpleEW were simplifications.

Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ Collapsing the 5 labels into “simplification”, “not a simplification”, and “?” yields reasonable agreement among the 3 native speakers (n = 0.69; 75.3% of the time all three agreed on the same label).
Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ —Emerson, Literary Ethics Style is an important aspect of information presentation; indeed, different contexts call for different styles.
Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.
Revisions on Wikipedia have been shown useful for various applications, including spelling correction (Zesch, 2012), sentence compression (Yamangil and Nelken, 2008), text simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzottoand Pennacchiotti, 2010). $$$$$ Note that in SimpleEW, where P(o1 V o2  |A) is the probability that A is changed to abdifferent word in SimpleEW, which we estimate as P(o1 V o2  |A) = fS(A).

(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.
(Yatskar et al, 2010) focus on using edit histories in Simple English Wikipedia to extract lexical simplifications. $$$$$ (We defer detailed description of how we extract lexical edit instances from data to §3.1.)

The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ Our work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8].
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ Since we are also currently ignoring edit spam, we thus assume that only o1 edits occur in ComplexEW.5 Let fC(A) be the fraction of ~dk in C containing A in which A is modified: A natural estimate for the conditional probability of A being rewritten to a under any operation type is based on observations of A –* a in SimpleEW, since that is the corpus wherein both operations are assumed to occur: Thus, from (1) we get that for A =� a:
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ The SIMPL method Given a set of trusted revisions TRev (in our case TRev = R(Seed)), we score each A —* a E TRev by the point-wise mutual information (PMI) between A and a.7 We write RANK(TRev) to denote the PMI-based ranking of A —* a E TRev, and use SIMPL to denote our most basic ranking method, RANK(R(Seed)).
The Wikipedia revision history has been used for spelling correction, text summarization (Nelken and Yamangil, 2008), lexical simplification (Yatskar et al, 2010), paraphrasing (Max and Wisniewski, 2010), and textual entailment (Zanzotto and Pennacchiotti, 2010). $$$$$ Wiki editors have the option of associating a comment with each revision, and such comments sometimes indicate the intent of the revision.

Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ Further evaluation could include comparison with machine-translation and paraphrasing algorithms.
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ There are at least four possible edit operations: fix (o1), simplify (o2), no-op (o3), or spam (o4).
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ The SIMPL method Given a set of trusted revisions TRev (in our case TRev = R(Seed)), we score each A —* a E TRev by the point-wise mutual information (PMI) between A and a.7 We write RANK(TRev) to denote the PMI-based ranking of A —* a E TRev, and use SIMPL to denote our most basic ranking method, RANK(R(Seed)).
Another useful resource is the edit history of Simple Wikipedia, from which simplifications can be learned (Yatskar et al, 2010). $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.

Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ Our work joins others in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, “eggcorns”3 [7] and entailments [8].
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ We report on work in progress on extractlexical simplifications (e.g., focusing on utilizing edit histories in Simple English Wikipedia for this task.
Yatskar et al (2010) learn a set of candidate phrase simplification rules based on edits identified in the revision histories of both Simple English Wikipedia and English Wikipedia. $$$$$ We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.

Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ Another idea would be to estimate simplification priors based on a model of inherent lexical complexity; some possible starting points are number of syllables (which is used in various readability formulae) or word length.
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ Let ck be the comment that accompanies rk, and conversely, let R(Set) = {rk|ck E Set}.
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ We find our methods to outperform a reasonable baseline and yield many high-quality lexical simplifications not included in an independently-created manually prepared list.
Woodsend and Lapata (2011) and Yatskar et al (2010) use Wikipedia comments to identify relevant edits for learning sentence simplification. $$$$$ While our initial implementation had fewer parameters than the method sketched above, it tended to terminate quickly, so that not many new simplifications were found; so, again, we do not report results here.
