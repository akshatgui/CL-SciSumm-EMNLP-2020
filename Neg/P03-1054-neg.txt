We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging. $$$$$ Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexiparse on par with early lexicalized parsers.
We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging. $$$$$ However, we believe that there is a fundamental qualitative distinction, grounded in linguistic practice, between what we see as permitted in an unlexicalized PCFG as against what one finds and hopes to exploit in lexicalized PCFGs.
We fully parsed the training and testing data using the Stanford Parser of (Klein and Manning, 2003) operating on the TnT part-of-speech tagging. $$$$$ categories appearing in the Penn treebank.

At present, the Stanford Parser (Klein and Manning, 2003) is used. $$$$$ The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact.
At present, the Stanford Parser (Klein and Manning, 2003) is used. $$$$$ To 13The inability to encode distance naturally in a naive PCFG is somewhat ironic.
At present, the Stanford Parser (Klein and Manning, 2003) is used. $$$$$ We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated.
At present, the Stanford Parser (Klein and Manning, 2003) is used. $$$$$ We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated.

 $$$$$ The h < 2, v < 1 markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included.
 $$$$$ In the heart of any PCFG parser, the fundamental table entry or chart item is a label over a span, for example an NP from position 0 to position 5.
 $$$$$ Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.
 $$$$$ We took the final model and used it to parse section 23 of the treebank.

Duringthe NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). $$$$$ The resulting battle against sparsity means that we can only afford to make a few distinctions which have major distributional impact.
Duringthe NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). $$$$$ This paper is based on work supported in part by the National Science Foundation under Grant No.
Duringthe NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). $$$$$ We took the final model and used it to parse section 23 of the treebank.
Duringthe NER extraction, we also employ phrase analysis based on our phrase utility extraction method using Standford dependency parser ((Klein and Manning, 2003)). $$$$$ Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.

The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. $$$$$ The annotation SPLIT-IN does a linguistically motivated 6-way split of the IN tag, and brought the total to 81.19%.
The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. $$$$$ Both parent annotation (adding context) and RHS markovization (removing it) can be seen as two instances of the same idea.
The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. $$$$$ The advantages of unlexicalized grammars are clear enough – easy to estimate, easy to parse with, and time- and space-efficient.
The manual symbol refinement described in (Klein and Manning, 2003) was applied to an all-fragments grammar and this improved accuracy in the English WSJ parsing task. $$$$$ This paper is based on work supported in part by the National Science Foundation under Grant No.

We follow a standard procedure to extract statements, as similarly adopted by Nakashole et al (2012), using Stanford CoreNLP (Klein and Manning, 2003) to lemmatize and parse sentences. $$$$$ While these kinds of errors are undoubtedly profitable targets for lexical preference, most attachment mistakes were overly high attachments, indicating that the overall right-branching tendency of English was not being captured.
We follow a standard procedure to extract statements, as similarly adopted by Nakashole et al (2012), using Stanford CoreNLP (Klein and Manning, 2003) to lemmatize and parse sentences. $$$$$ Around this point, we must address exactly what we mean by an unlexicalized PCFG.
We follow a standard procedure to extract statements, as similarly adopted by Nakashole et al (2012), using Stanford CoreNLP (Klein and Manning, 2003) to lemmatize and parse sentences. $$$$$ IIS0085896, and in part by an IBM Faculty Partnership Award to the second author.

We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. running), or whether the sentence is passive. $$$$$ A second kind of information in the original trees is the presence of empty elements.
We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. running), or whether the sentence is passive. $$$$$ One distributionally salient tag conflation in the Penn treebank is the identification of demonstratives (that, those) and regular determiners (the, a).
We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. running), or whether the sentence is passive. $$$$$ Although it does not necessarily jump out of the grid at first glance, this point represents the best compromise between a compact grammar and useful markov histories.
We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. running), or whether the sentence is passive. $$$$$ IIS0085896, and in part by an IBM Faculty Partnership Award to the second author.

We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. $$$$$ The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively.
We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. $$$$$ An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998).
We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. $$$$$ categories appearing in the Penn treebank.
We use the Stanford Parser (Klein and Manning, 2003b) for all experiments. $$$$$ The idea that part-of-speech tags are not fine-grained enough to abstract away from specific-word behaviour is a cornerstone of lexicalization.

As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). $$$$$ This paper is based on work supported in part by the National Science Foundation under Grant No.
As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). $$$$$ The h < 2, v < 1 markovization baseline of 77.77% dropped even further, all the way to 72.87%, when these annotations were included.
As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). $$$$$ Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating “base NPs” from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP.
As the grammar becomes sparser, there are limited opportunities for the lexical dependencies to correct the output of the PCFG grammar under the factored parsing model of Klein and Manning (2003b). $$$$$ Figure 8 shows the results.

We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). $$$$$ Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope.
We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). $$$$$ In the heart of any the fundamental table entry or chart item is a label over a span, for exan position 0 to position 5.
We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). $$$$$ The annotation SPLIT-IN does a linguistically motivated 6-way split of the IN tag, and brought the total to 81.19%.
We parsed the documents into typed dependencies with the Stanford Parser (Klein and Manning, 2003). $$$$$ In isolation, this resulted in an absolute gain of 0.55% (see figure 3).

To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. $$$$$ In the case of attachment of a PP to an NP either above or inside a relative clause, the high NP is distinct from the low one in that the already modified one contains a verb (and the low one may be a base NP as well).
To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. $$$$$ This brought the total F1 to 83.06%.
To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. $$$$$ Under VP, they are n’t (3779) and not (922).
To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. $$$$$ Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.

 $$$$$ The final distance/depth feature we used was an explicit attempt to model depth, rather than use distance and linear intervention as a proxy.
 $$$$$ This paper is based on work supported in part by the National Science Foundation under Grant No.
 $$$$$ With the functional annotation left in, this drops to 71.49%.

 $$$$$ 3 External vs. Internal Annotation The two major previous annotation strategies, parent annotation and head lexicalization, can be seen as instances of external and internal annotation, respectively.
 $$$$$ history models similar in intent to those described in Ron et al. (1994).
 $$$$$ The annotation SPLIT-IN does a linguistically motivated 6-way split of the IN tag, and brought the total to 81.19%.
 $$$$$ Note that this technique of pushing the functional tags down to preterminals might be useful more generally; for example, locative PPs expand roughly the same way as all other PPs (usually as IN NP), but they do tend to have different prepositions below IN.

 $$$$$ We do not want to argue that lexical selection is not a worthwhile component of a state-ofthe-art parser – certain attachments, at least, require it – though perhaps its necessity has been overstated.
 $$$$$ However, no use is made of lexical class words, to provide either monolexical or bilexical probabilities.11 At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do markovize rules, as explained above).

To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1 best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). $$$$$ The F1 after UNARY-INTERNAL, UNARY-DT, and UNARY-RB was 78.86%.
To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1 best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). $$$$$ An unlexicalized PCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998).
To relieve the negative effect of SRL errors, we get the multiple SRL results by providing the SRL system with 3-best parse trees of Berkeley parser (Petrov and Klein, 2007), 1 best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003). $$$$$ Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.

 $$$$$ Figure 8 shows the results.
 $$$$$ We focus here on using unlexicalized, structural context because we feel that this information has been underexploited and underappreciated.
 $$$$$ However, the dismal performance of basic unannotated unlexicalized grammars has generally rendered those advantages irrelevant.
 $$$$$ However, no use is made of lexical class words, to provide either monolexical or bilexical probabilities.11 At any rate, we have kept ourselves honest by estimating our models exclusively by maximum likelihood estimation over our subcategorized grammar, without any form of interpolation or shrinkage to unsubcategorized categories (although we do markovize rules, as explained above).

Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003). $$$$$ Figure 8 shows the results.
Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003). $$$$$ Rather, we have shown ways to improve parsing, some easier than lexicalization, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.

In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). $$$$$ In the raw grammar, there are many unaries, and once any major category is constructed over a span, most others become constructible as well using unary chains (see Klein and Manning (2001) for discussion).
In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). $$$$$ The for example, showed that the determiners which occur alone are usefully distinguished from those which occur with other nomimaterial.
In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). $$$$$ In this paper, we show that the parsing performance that can be achieved by an unlexicalized PCFG is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible.
In STAN-ANNOTATION, we annotate thetreebank symbols with annotations from the Stanford parser (Klein and Manning, 2003). $$$$$ Both parent annotation (adding context) and RHS markovization (removing it) can be seen as two instances of the same idea.

For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford's (Klein and Manning, 2003) constituent parsers. $$$$$ For example, NPs with S parents (like subjects) will be marked NP&quot;S, while NPs with VP parents (like objects) will be NP&quot;VP.
For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford's (Klein and Manning, 2003) constituent parsers. $$$$$ Similarly, only the previous h horizontal ancestors matter (we assume that the head child always matters).
For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford's (Klein and Manning, 2003) constituent parsers. $$$$$ Error analysis at this point suggested that many remaining errors were attachment level and conjunction scope.
For the following ensemble experiments we make use of both (Charniak and Johnson, 2005) and Stanford's (Klein and Manning, 2003) constituent parsers. $$$$$ IIS0085896, and in part by an IBM Faculty Partnership Award to the second author.

 $$$$$ IIS0085896, and in part by an IBM Faculty Partnership Award to the second author.
 $$$$$ In such a framework, if we try to annotate categories with any detailed lexical information, many sentences either entirely fail to parse, or have only extremely weird parses.
 $$$$$ In this case, the more common nominal use of works is preferred unless the IN tag is annotated to allow if to prefer S complements.
 $$$$$ We follow this approach in our model: various closed classes are subcategorized to better represent important distinctions, and important features commonly expressed by function words are annotated onto phrasal nodes (such as whether a VP is finite, or a participle, or an infinitive clause).
