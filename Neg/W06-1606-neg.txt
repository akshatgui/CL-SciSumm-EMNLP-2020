Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. $$$$$ The seven outputs were randomly shufied and presented to three English speakers for assessment.
Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. $$$$$ When decoding the test corpus, the decoder returns the translation that has the most probable derivation; in other words, the sum operator in equation 4 is replaced with an argmax.
Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. $$$$$ Also, the SPMT models yield significantly fewer rules that the model of Galley et al. In contrast with the model proposed by Chiang, the SPMT models introduced in this paper are fully grounded in syntax; this makes them good candidates for exploring the impact that syntaxbased language models could have on translation performance.
Galley et al.(2006) propose one solution to this problem and Marcu et al (2006) propose another, both of which we explore in Sections 5.1 and 5.2. $$$$$ Also, the SPMT models yield significantly fewer rules that the model of Galley et al. In contrast with the model proposed by Chiang, the SPMT models introduced in this paper are fully grounded in syntax; this makes them good candidates for exploring the impact that syntaxbased language models could have on translation performance.

An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). $$$$$ To our knowledge, we are the first to report results that show that a syntax-based system can produce results that are better than those produced by a strong phrasebased system in experimental conditions similar to those used in large-scale, well-established independent evaluations, such as those carried out annually by NIST.
An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). $$$$$ For example, although the minimal Model 1 rules r11 and r13 are sufficient for building an English NP on top of two NPs separated by the Chinese conjunction AND, the composed rule r14 in Figure 1 accomplishes the same result in only one step.
An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
An alternative for extracting larger rules called SPMT model 1 is presented by Marcu et al (2006). $$$$$ We are interested to model a generative process that explains how English parse trees 7r and their associated English string yields E, foreign sentences, F, and word-level alignments, A, are produced.

This solution requires larger applicability contexts (Marcu et al, 2006). $$$$$ For example, if we assume that the generative process has already produced the top NP node in Figure 2, then the corresponding partial English parse tree, foreign/source string, and word-level alignment could be generated by the rule derivation r4(r1, r3(r2)), where each rule is assumed to have some probability.
This solution requires larger applicability contexts (Marcu et al, 2006). $$$$$ On a 1 to 5 quality scale, the difference between the phrase-based and syntax-based systems was, on average, between 0.2 and 0.3 points.
This solution requires larger applicability contexts (Marcu et al, 2006). $$$$$ The Syncronous Grammar formalism utilized by Chiang is stricter than SPMT since it allows only for single-level target tree annotations.
This solution requires larger applicability contexts (Marcu et al, 2006). $$$$$ The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3.

Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. $$$$$ We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences.
Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
Addressing the issues in Galley et al (2006), Marcu et al (2006) create an x Rs rule headed by a pseudo, non-syntactic non-terminal symbol that subsumes the phrase and its corresponding multi headed syntactic structure. $$$$$ As development data for the SPMT systems, we used the sentences in the 2002 NIST development corpus that are shorter than 20 words; we made this choice in order to finish all experiments in time for this submission.

Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). $$$$$ The algorithm finds for each foreign/source phrase span its projected span on the English side and then traverses the English parse tree bottom up until it finds a node that subsumes the projected span.
Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
Unlike previous work, our solution neither requires larger applicability contexts (Galley et al, 2006), nor depends on pseudo nodes (Marcu et al, 2006) or auxiliary rules (Liu et al, 2007). $$$$$ We compare these two probabilities across the submodels and we scale all model probabilities to be compatible with those of Model 2 composed.

We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. $$$$$ We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences.
We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. $$$$$ The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.
We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. $$$$$ We use the same training corpus, 138.7M words of parallel Chinese-English data released by LDC, in order to train several statistical-based MT systems: In all systems, we use a rule extraction algorithm that limits the size of the foreign/source phrases to four words.
We restrict the target side to the so called well formed dependency structures, in order to cover a large set of non-constituent transfer rules (Marcu et al., 2006), and enable efficient decoding through dynamic programming. $$$$$ Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.

Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). $$$$$ The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.
Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). $$$$$ The process creates one xRS rule that is headed by a pseudo, non-syntactic nonterminal symbol that subsumes the target phrase and corresponding multi-headed syntactic structure; and one sibling xRS rule that explains how the non-syntactic nonterminal symbol can be combined with other genuine nonterminals in order to obtain genuine parse trees.
Rule Coverage Marcu et al (2006) showed that many useful phrasal rules can not be represented as hierarchical rules with the existing representation methods, even with composed transfer rules (Galley et al, 2006). $$$$$ This feature is encouraging because it shows that the syntactified translation rules learned in the SPMT models can generalize better than the phrase-based rules.

(Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. $$$$$ We use the new weights in order to rerank the nbest outputs on the test corpus.
(Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. $$$$$ Although the number of syntax-based rules used by our models is smaller than the number of phrase-based rules used in our state-of-the-art baseline system, the SPMT models produce outputs of higher quality.
(Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. $$$$$ The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.
(Marcu et al, 2006) and (Galley et al, 2006) introduced artificial constituent nodes dominating the phrase of interest. $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.

Inside the Moses toolkit, three different statistical approaches have been implemented $$$$$ The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006).
Inside the Moses toolkit, three different statistical approaches have been implemented $$$$$ We hypothesize that composed rules, i.e., rules that can be decomposed via the application of a sequence of Model 1 rules may improve the performance of an SPMT system.
Inside the Moses toolkit, three different statistical approaches have been implemented $$$$$ This research was partially supported by the National Institute of Standards and Technology’s Advanced Technology Program Award 70NANB4H3050 to Language Weaver Inc.
Inside the Moses toolkit, three different statistical approaches have been implemented $$$$$ The reference translation presented as automatically produced output was selected from the set of four reference translations provided by NIST so as to be representative of human translation quality.

Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). $$$$$ We condition on the root node of each rule and use the rule counts f(r) and a basic maximum likelihood estimator to assign to each rule type a conditional probability (see equation 5).
Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). $$$$$ Our intuition is that composed rules that involve the application of more than two minimal rules are not reliable.
Moreover, syntax-based approaches often suffer from the rule coverage problem since syntactic constraints rule out a large portion of non syntactic phrase pairs, which might help decoders generalize well to unseen data (Marcu et al,2006). $$$$$ The reference translation presented as automatically produced output was selected from the set of four reference translations provided by NIST so as to be representative of human translation quality.

Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). $$$$$ Since these lexicalized rules are not sufficient to explain an entire (7r, F, A) tuple, we also extract the required minimal/necessary, non-lexicalized xRS rules.
Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). $$$$$ We hope that the composed rules could play in SPMT the same role that phrases play in string-based translation models.
Syntactic analysis of texts (such as Part-OfSpeech tagging and syntactic parsing) is an ex ample of such a generic analysis, and has proved useful in applications ranging from machine translation (Marcu et al, 2006) to text mining in the bio-medical domain (Cohen and Hersh, 2005). $$$$$ The CKY-style decoder computes the probability of English syntactic constituents in a bottom up fashion, by log-linearly interpolating all the submodel scores described in Section 2.3.

Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). $$$$$ Under the SPMT composed model 1, the tree in Figure 2 can be produced, for example, by the following derivation: r15(r9(r7), r14(r12(r5), r12(rs))).
Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). $$$$$ In our simplest model, we assume that each tuple (7r, F, A) in our automatically annotated corpus could be produced by applying a combination of minimally syntactified, lexicalized, phrase-based compatible xRS rules, and minimal/necessary, non-lexicalized xRS rules.
Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even out perform them in some cases (Marcu et al, 2006). $$$$$ Although the number of syntax-based rules used by our models is smaller than the number of phrase-based rules used in our state-of-the-art baseline system, the SPMT models produce outputs of higher quality.

Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ If we analyze these three models in terms of expressive power, the Galley et al. (2006) model is more expressive than the SPMT models, which in turn, are more expressive than Chiang’s model.
Following Galley et al (2006)' s work, Marcu et al (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. $$$$$ To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task.

As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ In our simplest model, we assume that each tuple (7r, F, A) in our automatically annotated corpus could be produced by applying a combination of minimally syntactified, lexicalized, phrase-based compatible xRS rules, and minimal/necessary, non-lexicalized xRS rules.
As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ This feature is encouraging because it shows that the syntactified translation rules learned in the SPMT models can generalize better than the phrase-based rules.
As shown in the following parts of this paper, it works very well with the existing techniques, such as rule com posing (Galley et al, 2006), SPMT models (Marcu et al, 2006) and rule extraction with k best parses (Venugopal et al, 2008). $$$$$ The decoder uses a binarized representation of the rules, which is obtained via a syncronous binarization procedure (Zhang et al., 2006).

In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. $$$$$ We decode with each of our SPMT models using a straightforward, bottom-up, CKY-style decoder that builds English syntactic constituents on the top of Chinese sentences.
In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. $$$$$ When decoding the test corpus, the decoder returns the translation that has the most probable derivation; in other words, the sum operator in equation 4 is replaced with an argmax.
In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. $$$$$ The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et al’s (2006) and can easily exploit and expand on previous research in phrase-based machine translation.
In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. $$$$$ We remove the Galley et al.’s lexicalized rules because they are either already accounted for by the minimally syntactified, lexicalized, phrasebased-compatible xRS rules or they subsume noncontinuous source-target phrase pairs.

In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. $$$$$ After being exposed to 100M+ words of parallel Chinese-English texts, current phrase-based statistical machine translation learners induce reasonably reliable phrase-based probabilistic dictionaries.
In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. $$$$$ The SPMT models outperform a state of the art phrase-based baseline model by 2.64 Bleu points on the NIST 2003 Chinese-English test corpus and 0.28 points on a humanbased quality metric that ranks translations on a scale from 1 to 5.
In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. $$$$$ We concatenate the lists and we learn a new combination of weights that maximizes the Bleu score of the combined nbest list using the same development corpus we used for tuning the individual systems (Och, 2003).
In this system, both of minimal GHKM (Galley et al, 2004) and SPMT rules (Marcu et al., 2006) are extracted from the bilingual corpus, and the composed rules are generated by com posing two or three minimal GHKM and SPMT rules. $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.

Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. $$$$$ The decoder is capable of producing nbest derivations and nbest lists (Knight and Graehl, 2005), which are used for Maximum Bleu training (Och, 2003).
Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. $$$$$ We introduce SPMT, a new class of statistical Translation Models that use Syntactified target language Phrases.
Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. $$$$$ We compare these two probabilities across the submodels and we scale all model probabilities to be compatible with those of Model 2 composed.
Chiang (2005) describes a procedure to extract PSCFG rules from word-aligned (Brown et al, 1993) corpora, where all nonterminals share the same generic label X. InGalley et al (2004) and Marcu et al (2006), tar get language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al (2006) use source language parse trees instead. $$$$$ Although phrase-based models yield high-quality translations for language pairs that exhibit similar word order, they fail to produce grammatical outputs for language pairs that are syntactically divergent.

The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. $$$$$ We assume that observed (7r, F, A) triplets are generated by a stochastic process similar to that used in Data Oriented Parsing models (Bonnema, 2002).
The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. $$$$$ The composed rule is obtained by extracting the rule licensed by the foreign/source phrase, alignment, English parse tree, and the first multi-child ancestor node of the root of the minimal rule.
The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. $$$$$ The judges who participated in our experiment were instructed to carefully read the three reference translations and seven machine translation outputs, and assign a score between 1 and 5 to each translation output on the basis of its quality.
The base feature set used for all systems is similar to that used in (Marcu et al 2006), including 14 base features in total such as 5-gram language model, bidirectional lexical and phrase based translation probabilities. $$$$$ Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.

Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. $$$$$ For example, our baseline statistical phrasebased system learns that, with high probabilities, the Chinese phrases “ASTRO- -NAUTS”, “FRANCE AND RUSSIA” and “COMINGFROM” can be translated into English as “astronauts”/“cosmonauts”, “france and russia”/“france and russian” and “coming from”/“from”, respectively.
Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. $$$$$ We prepared a web-based evaluation interface that showed for each input sentence: The evaluated `MT systems” were the six systems shown in Table 1 and one of the reference translations.
Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. $$$$$ We use the new weights in order to rerank the nbest outputs on the test corpus.
Zollmann and Venugopal (2006) and Marcu et al (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage. $$$$$ Recent models that exploit syntactic information of the source language (Quirk et al., 2005) have been shown to produce better outputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.

We build two translation systems $$$$$ The parameters of the SPMT models presented in this paper are easier to estimate than those of Galley et al’s (2006) and can easily exploit and expand on previous research in phrase-based machine translation.
We build two translation systems $$$$$ Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al., 2006) when significantly different engines compete for producing the best outputs.
We build two translation systems $$$$$ 1 Unfortunately, when given as input Chinese sentence 1, our phrase-based system produces the output shown in 2 and not the translation in 3, which correctly orders the phrasal translations into a grammatical sequence.
We build two translation systems $$$$$ During the last four years, various implementations and extentions to phrase-based statistical models (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) have led to significant increases in machine translation accuracy.
