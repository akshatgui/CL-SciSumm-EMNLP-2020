In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). $$$$$ While the semantic roles of FrameNet are defined at the level of the frame, in Propbank, roles are defined on a per-predicate basis.
In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). $$$$$ A more complete description of the FrameNet project can be found in (Baker et al., 1998; Johnson et al., 2001), and the ramifications for automatic classification are discussed more thoroughly in (Gildea and Jurafsky, 2002).
In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). $$$$$ Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.

Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy. $$$$$ Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.
Gildea and Palmer (2002) show that semantic role labels can be predicted given syntactic features derived from the PTB with fairly high accuracy. $$$$$ The prepositional phrase expressing the Manner relation, however, is not identified by the chunk-based system.

For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. $$$$$ Much of the evaluation of these systems has been conducted on extracting relations for specific semantic domains such as corporate acquisitions or terrorist events in the framework of the DARPA Message Understanding Conferences.
For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. $$$$$ We measure the effect of parser accuracy on semantic role prediction from parse trees, and determine whether a complete tree is indeed necessary for accurate role prediction.
For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. $$$$$ Statistical systems have been trained to automatically label semantic roles from the output of statistical parsers on unannotated text.
For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. $$$$$ The philosophy of the Propbank project can be likened to FrameNet without frames.

We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. $$$$$ In particular, sentential complements (which comprise 11% of the data) and prepositional phrases (which comprise 10%) always correspond to more than one chunk, and therefore cannot be correctly labeled by our system which assigns roles to single chunks.
We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. $$$$$ One reason for sparsity is seen in the third row: in the Treebank, the adjunction of an adverbial phrase or modal verb can cause an additional VP node to appear in our path feature.
We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. $$$$$ Not only the constituent structure but also head word information, produced as a side product, are important features.
We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. $$$$$ Propbank's annotation process has proceeded from the most to least common verbs, and all examples of each verb from the corpus are annotated.

The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. $$$$$ In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).
The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. $$$$$ This feature may overcome the shortcomings of reading grammatical function from the parse tree, as well as errors in the parser output.
The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. $$$$$ As our path feature is a somewhat unusual way of looking at parse trees, its behavior in the system warrants a closer look.
The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. $$$$$ Although results for Propbank are lower than for FrameNet, this appears to be primarily due to the smaller number of training examples for each predicate, rather than the difference in annotation style between the two corpora.

Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). $$$$$ In this representation, base-level constituent boundaries and labels are present, but there are no dependencies between constituents, as shown by the following sample sentence: (3) [NP Big investment banks] [V P refused to step] [ADVP up] [PP to] [NP the plate] [V P to support] [NP the beleaguered floor traders] [PP by] [V P buying] [NP big blocks] [PP of] [NP stock] , [NP traders] [V P say] .
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). $$$$$ Probabilities of a parse constituent belonging to a given semantic role were calculated from the following features: Phrase Type: This feature indicates the syntactic type of the phrase expressing the semantic roles: examples include noun phrase (NP), verb phrase (VP), and clause (S).
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). $$$$$ Our tree-based system assigned the following analysis: floor traders] [MNR by buying big blocks of stock] , traders say .
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). $$$$$ While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect.

These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. $$$$$ Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.
These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. $$$$$ The first collapses sequences of nodes with the same label, for example combining rows 2 and 3 of Table 3.
These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. $$$$$ Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.
These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. $$$$$ Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.

Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). $$$$$ Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations.
Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). $$$$$ Propbank's annotation process has proceeded from the most to least common verbs, and all examples of each verb from the corpus are annotated.
Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). $$$$$ With this scoring regime, the chunk-based system performs at 49.5% precision and 35.1% recall, still significantly lower than the 57.7% precision/50.0% recall for exact matches using automatically generated parses.

For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). $$$$$ Annotation takes place with reference to the Penn Treebank trees not only are annotators shown the trees when analyzing a sentence, they are constrained to assign the semantic labels to portions of the sentence corresponding to nodes in the tree.
For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). $$$$$ We also examine whether it is possible that the additional information contained in a full parse tree is negated by the errors present in automatic parser output, by testing a role-labeling system based on a flat or &quot;chunked&quot; representation of the input.
For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). $$$$$ In order to provide results comparable with the statistical parsing literature, annotations from Section 23 of the Treebank were used as the test set; all other sections were included in the training set.
For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (CallisonBurch, 2008). $$$$$ FrameNet is focused on semantic frames, which are defined as schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore, 1976).

In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. $$$$$ Thus, the experiments were carried out using &quot;goldstandard&quot; rather than automatically derived chunk boundaries, which we believe will provide an upper bound on the performance of a chunkbased system.
In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. $$$$$ It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure.
In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. $$$$$ While the goals of the two projects are similar in many respects, their methodologies are quite different.
In the last few years, many researchers (Blaheta and Charniak 2000), (Gildea and Jurafsky 2002), (Gildea and Palmer 2002), (Pradhan et al. 2003) have focused on the automatic prediction of semantic roles using statistical techniques. $$$$$ Position: This feature simply indicates whether the constituent to be labeled occurs before or after the predicate defining the semantic frame.

 $$$$$ Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations.
 $$$$$ However, the preliminary version of the data used in the experiments below are not tagged for word sense, or for the roleset used.
 $$$$$ We will examine the contribution of each of these information sources, beginning with the problem of assigning the correct role in the case where the boundaries of the arguments in the sentence are known, and then turning to the problem of finding arguments in the sentence.
 $$$$$ While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect.

The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003). $$$$$ From the perspective of an automatic classification system, the overrepresentation of rare syntactic realizations may cause the system to perform more poorly than it might on more statistically representative data.
The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003). $$$$$ Even for a single predicate, semantic arguments often have multiple syntactic realizations, as shown by the following paraphrases: Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming an important part of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization.
The first experiment compares the random forest classifier to three other classifiers, a statistical Bayesian approach with back off (Gildea and Palmer, 2002), a decision tree classifier (Surdeanu et al, 2003), and a Support Vector Machine (SVM) (Pradhan et al, 2003). $$$$$ In order to see how much of the performance degradation is caused by the difficulty of finding exact argument boundaries in the chunked representation, we can relax the scoring criteria to count as correct all cases where the system correctly identifies the first chunk belonging to an argument.

 $$$$$ When the argument boundaries are known, the grammatical relationship of the the constituent to the predicate turns out to be of little value.
 $$$$$ Since the parser used assigns each constituent a head word as an integral part of the parsing model, the head words of the constituents can be read from the parser output.

Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. $$$$$ But now, unlike in the tree-based output, the ARG0 label is mistakenly attached to a noun phrase immediately before the predicate.
Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. $$$$$ Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.
Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. $$$$$ Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.
Gildea and Palmer (2002) report F-score results in the 55% range for argument and boundary recognition based on automatic parses. $$$$$ The need for multiple rolesets is determined by the roles themselves, that is, uses of the verb with different arguments are given separate rolesets.

For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). $$$$$ Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al.
For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). $$$$$ The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate constituent.
For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). $$$$$ Our results using hand-annotated parse trees show that improvements in parsing should translate directly into better semantic interpretations.
For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002). $$$$$ He ate some pancakes Figure 2: In this example, the path from the predicate ate to the argument He can be represented as VBfVPfS�NP, with f indicating upward movement in the parse tree and � downward movement.

The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J. $$$$$ While the errors introduced by the parser no doubt negatively affected the results obtained, there was no direct way of quantifying this effect.
The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J. $$$$$ Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.
The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J. $$$$$ In previous work using the FrameNet corpus, Gildea and Jurafsky (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1997).
The Gildea and Palmer (2002) system uses the same features and the same classification mechanism used by G&J. $$$$$ Acknowledgments This work was undertaken with funding from the Institute for Research in Cognitive Science at the University of Pennsylvania and from the Propbank project, DoD Grant MDA904-00C-2136.

 $$$$$ Annotation takes place with reference to the Penn Treebank trees not only are annotators shown the trees when analyzing a sentence, they are constrained to assign the semantic labels to portions of the sentence corresponding to nodes in the tree.
 $$$$$ While the goals of the two projects are similar in many respects, their methodologies are quite different.
 $$$$$ Parsers, however, still have a long way to go.

the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). $$$$$ While the two types of label names are reminiscent of the traditional argument/adjunct distinction, this is primarily as a convenience in defining roles, and no claims are intended as to optionality or other traditional argument/adjunct tests.
the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). $$$$$ Broad-coverage corpora annotated with semantic role, or argument structure, information are becoming available for the first time.
the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). $$$$$ The information provided by the parse tree can be decomposed into three pieces: the constituent boundaries, the grammatical relationship between predicate and argument, expressed by our path feature, and the head word of each candidate constituent.
the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). $$$$$ By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data.

More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). $$$$$ Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles.
More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). $$$$$ Our chunk-based system takes the last word of the chunk as its head word for the purposes of predicting roles, but does not make use of the identities of the chunk's other words or the intervening words between a chunk and the predicate, unlike Hidden Markov Model-like systems such as Bikel et al. (1997), McCallum et al.
More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). $$$$$ As a gauge of how closely the Propbank argument labels correspond to the path feature overall, we note that by always assigning the most common role for each path, for example always assigning ARG0 to the subject position, and using no other features, we obtain the correct role 69.4% of the time, vs. 82.3% for the complete system.
More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al (2008). $$$$$ FrameNet is focused on semantic frames, which are defined as schematic representation of situations involving various participants, props, and other conceptual roles (Fillmore, 1976).

This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition. $$$$$ The tagging guidelines for a verb may contain many &quot;rolesets&quot;, corresponding to word sense at a relatively coarsegrained level.
This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition. $$$$$ Such an approach has a large advantage in speed, as the extensive search of modern statistical parsers is avoided.
This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition. $$$$$ The first collapses sequences of nodes with the same label, for example combining rows 2 and 3 of Table 3.
This is in line with results in (Gildea and Palmer, 2002), who compare the effect of manual and automatic parsing on semantic predicate argument recognition. $$$$$ By using a gold-standard chunking representation, we have obtained higher performance over what could be expected from an entirely automatic system based on a flat representation of the data.
