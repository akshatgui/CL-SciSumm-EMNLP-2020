[Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). $$$$$ The values of the class-based variables are a set of twenty-five POS tags formed, with one exception, from the first letter of the tags used in the Penn Treebank corpus.
[Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). $$$$$ A limited number of collocation-specific variables were selected, where the term collocation is used loosely to refer to a specific spelling form occurring in the same sentence as the ambiguous word.
[Interest] is a binary version of the word sense disambiguation data from (Bruce and Wiebe, 1994). $$$$$ Other researchers have proposed approaches to systematically combining information from multiple contextual features in determining the sense of an ambiguous word.

One set was extracted from a hand-tagged corpus (Bruce and Wiebe, 1994) and the other by our algorithm. $$$$$ The form of the model describes the interactions among the variables by expressing the joint distribution of the values of all contextual features and sense tags as a product of conditionally independent marginals, with each marginal being composed of non-independent variables.
One set was extracted from a hand-tagged corpus (Bruce and Wiebe, 1994) and the other by our algorithm. $$$$$ Techniques for identifying the optimum feature to use in disambiguating a word are presented in [7], [30] and [5].

 $$$$$ Schutze ([26]) derived contextual features from a singular value decomposition of a matrix of letter four-gram co-occurrence frequencies, thereby assuring the independence of all features.
 $$$$$ To test the method of model selection presented in this paper, a case study of the disambiguation of the performed. selected because it has been shown in previous studies to be a difficult word to disambiguate.
 $$$$$ In this paper, a different approach to formulating a probabilistic model is presented along with a case study of the performance of models produced in this manner for the disambiguation of the noun describe a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation, without requiring untested assumptions regarding the form of the model.

We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). $$$$$ Models of this form describe a markov field ([8], [21]) that can be represented graphically as is shown in Figure 1 for Model 4 of Table 2.
We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). $$$$$ Each of the variables rlpos, r2pos, Ilpos, and 12pos is the POS tag of the word 1 or 2 positions to the left (/) or right (r).
We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). $$$$$ Section 2 provides a more complete definition of the Abstract Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features.
We also make use of these properties in formulating the empirical classifiers as described in (Bruce and Wiebe, 1994). $$$$$ Thus, we are able to use multiple contextual features without the need for untested assumptions regarding the form of the model.

An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use. $$$$$ Section 2 provides a more complete definition of the Abstract Most probabilistic classifiers used for word-sense disambiguation have either been based on only one contextual feature or have used a model that is simply assumed to characterize the interdependencies among multiple contextual features.
An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use. $$$$$ In such cases, the fit of the model will appear to be too good, indicating that the model is in fact over constrained for the data available.
An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use. $$$$$ Unfortunately, interpreting a contextual feature that is a weighted combination of letter four-grams is difficult.
An example of the type of feature used is the part-of-speech of the word to the right; see (Bruce and Wiebe, 1994) for the other ones we use. $$$$$ In a Bayesian network, the notions of causation and influence replace the notion of conditional independence in a Markov field.

The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). $$$$$ An Experiment in Computational Discrimination of English Word Senses.
The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). $$$$$ In this paper, we presented a method for formulating probabilistic models that use multiple contextual features for word-sense disambiguation without requiring untested assumptions regarding the form of the model.
The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). $$$$$ The values of the class-based variables are a set of twenty-five POS tags formed, with one exception, from the first letter of the tags used in the Penn Treebank corpus.

The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE). $$$$$ Cambridge: The MIT Press.
The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE). $$$$$ 87, No.
The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE). $$$$$ [1] Baglivo, J., Olivier, D., and Pagano, M. (1992).
The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE). $$$$$ 418, June 1992.

 $$$$$ Assume that a random sample consisting of N independent and identical trials (i.e., all trials are described by the same probability density function) is drawn from a discrete d-variate distribution.
 $$$$$ [1] Baglivo, J., Olivier, D., and Pagano, M. (1992).
 $$$$$ The semantics of the graph topology is that all variables that are not directly connected in the graph are conditionally independent given the values of the variables mapping to the connecting nodes.

 $$$$$ The tree construction process used by Black partitions the data according to the values of one contextual feature before considering the values of the next, thereby treating all features incorporated in the tree as interdependent.
 $$$$$ We are investigating several extensions to this work.

The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). $$$$$ Both the form and the performance of the model selected for each set of variables is presented in Table 2.
The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). $$$$$ In the case of Model 4, Figure 1 graphically depicts the fact that the value of the morphological variable ending is conditionally independent of the values of all other contextual features given the sense tag of the ambiguous word.
The Interest data set developed by Bruce and Wiebe (1994) has been previously used for WSD (Ng and Lee, 1996). $$$$$ Accessing the fit 'The marginal distributions can be represented in terms of counts or relative frequencies, depending on whether the parameters are expressed as expected frequencies or probabilities, respectively. of a model in terms of the significance of its G2 statistic gives preference to models with the fewest number of interdependencies, thereby assuring the selection of a model specifying only the most systematic variable interactions.
