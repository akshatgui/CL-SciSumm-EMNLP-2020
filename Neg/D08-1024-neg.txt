To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. $$$$$ Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.
To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. $$$$$ We trained the coarse-grained distortion model on 10,000 sentences of the training data.
To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. $$$$$ The paper proceeds as follows.
To remedy this problem, Chiang et al (2008) introduce a structural distortion model, which we include in our experiment. $$$$$ We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined.

As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. $$$$$ Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost.
As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. $$$$$ Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features.
As proposed by Haddow et al (2011), BLEU is approximately computed in the local batch, since BLEU is not linearly decomposed into a sentence wise score (Chiang et al, 2008a), and optimization for sentence-BLEU does not always achieve optimal parameters for corpus-BLEU. $$$$$ Table 1 shows the results of our experiments with the training methods and features described above.

An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. $$$$$ Those hypotheses were generated from different weight vectors, but can still provide useful information.
An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. $$$$$ The p = 0 points in the upper-left corner are typical of oracle translations that would be selected under the max-BLEU policy: they indeed have a very high BLEU score, but are far removed from the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled p = 1.
An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. $$$$$ The baseline model was Hiero with the following baseline features (Chiang, 2005; Chiang, 2007): The probability features are base-100 logprobabilities.
An alternative way of accounting for phrase size is presented by Chiang et al (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. $$$$$ The first features we explore are based on a line of research introduced by Chiang (2005) and improved on by Marton and Resnik (2008).

MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ It is worth noting that this experimentation is on a larger scale than Watanabe et al.’s (2007), and considerably larger than Marton and Resnik’s (2008).
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ We also tested some of the differences between our training method and Watanabe et al.’s (2007); the results are shown in Table 2.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.
MBUU is a batch update mode which updates the weight with all training examples, but MIRA is an online one which updates with each example (Watanabe et al 2007) or part of examples (Chiang et al 2008). $$$$$ In both cases we obtain significant improvements in translation performance.

We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). $$$$$ All significance testing was performed against the first line (MERT baseline) using paired bootstrap resampling (Koehn, 2004).
We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). $$$$$ The intuition is that due to noise in the training data or reference translations, a high-BLEU translation may actually use peculiar rules which it would be undesirable to encourage the model to use.
We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al (2008). $$$$$ By capturing how reordering depends on constituent length, these features improve translation quality significantly.

The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). $$$$$ As an alternative, we introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define a separate binary feature for each value of (R, S), where R is as above and S E J*, 1, ... , 9, >_101 and * means any size.
The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). $$$$$ The sets of hypotheses thus collected are then processed as one batch.
The results are especially notable for the basic feature setting - up to 1.2 BLEU and 4.6 TER improvement over MERT - since MERT has been shown to be competitive with small numbers of features compared to high-dimensional optimizers such as MIRA (Chiang et al, 2008). $$$$$ This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: We keep track of n-grams using the same scheme used to incorporate an n-gram language model into the decoder (Wu, 1996; Chiang, 2007).

We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). $$$$$ This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT.
We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). $$$$$ Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure.
We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
We conjecture both these issues will be ameliorated with syntactic features such as those in Chiang et al (2008). $$$$$ We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αij = C for a single value of j such that eij = e∗i , and initialize αij = 0 for all other values of j.

In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008). $$$$$ Crucially, our training algorithm provides the ability to train all the fine-grained features, a total of 34 feature weights, simultaneously.
In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008). $$$$$ We can estimate P(R  |S) via relativefrequency estimation from the rules as they are extracted from the parallel text, and incorporate this probability as a new feature of the model.
In future work we also intend to explore using additional sparse features that are known to be useful in translation, e.g. syntactic features explored by Chiang et al (2008). $$$$$ This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al.

 $$$$$ We then define coarse- and fine-grained versions of the structural distortion model.
 $$$$$ Building on the work of Watanabe et al., we explore the use of the MIRA algorithm of Crammer et al. as an alternative to MERT.
 $$$$$ The rules were extracted from all the allowable parallel text from the NIST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions).
 $$$$$ In a phrase-based model, reordering is performed both within phrase pairs and by the phrasereordering model.

The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. $$$$$ The labels that were excluded were parts of speech, nonconstituent labels like FRAG, or labels that occurred only two or three times.
The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. $$$$$ This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT.
The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. $$$$$ In this paper, we have brought together two existing lines of work: the training method of Watanabe et al. (2007), and the models of Chiang (2005) and Marton and Resnik (2008).
The oracle is created, analogously Chiang et al (2008), by choosing e+j? N to maximise the sum of gain (calculated on the batch) and model score. $$$$$ (2003; 2006).

For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. $$$$$ Optimizing them in combination, for a total of 56 feature weights, improve performance by 2.6 a subset of the NIST 2006 Arabic-English evaluation data.
For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. $$$$$ Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.
For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. $$$$$ All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model.
For example, the features introduced by Chiang et al (2008) and Chiang et al (2009) for an SCFG model for Chinese/English translation are of two types: The first type explicitly counters overestimates of rule counts, or rules with bad overlap points, bad rewrites, or with undesired insertions of target-side terminals. $$$$$ Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.

The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). $$$$$ We observe similar behavior for the structural distortion features: MERT is not able to take advantage of the finer-grained features, but MIRA is.
The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). $$$$$ However, this more syntactically aware model, when tested in Chinese-English translation, did not improve translation performance.
The perceptron algorithm itself compares favorably to related learning techniques such as the MIRA adaptation of Chiang et al (2008). $$$$$ Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.

The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. $$$$$ In sum, we have shown that removing the bottleneck of MERT opens the door to many possibilities for better translation.
The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. $$$$$ The paper proceeds as follows.
The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
The algorithms described below can be straightforwardly generalized to compute oracle hypotheses under combined metrics mixing model scores and quality measures (Chiang et al 2008), by weighting each edge with its model score and by using these weights down the pipe. $$$$$ Indeed, the MIRA system scores significantly higher on the test set; but if we break the test set down by genre, we see that the MIRA system does slightly worse on newswire and better on newsgroups.

Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). $$$$$ For example, if a nonterminal with span 11 has its contents reordered, then the features (true, >_10) and (true, *) would both fire.
Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). $$$$$ So, for example, if we have rules we might expect that rule (12) is more common in general, but that rule (13) becomes more and more rare as X1 gets larger.
Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). $$$$$ Hierarchical rules were extracted from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder.
Building on this paper, the most recent work to our knowledge has been done by Chiang et al (2008). $$$$$ We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined.

We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). $$$$$ We then define coarse- and fine-grained versions of the structural distortion model.
We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). $$$$$ By contrast, when the fine-grained features are trained using MIRA, they yield substantial improvements.
We parse the English side of our parallel corpus with the Berkeley parser (Petrov et al, 2006), and tune parameters of them T system with MIRA (Chiang et al, 2008). $$$$$ Watanabe et al.’s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA’s performance and that, even using the same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost.

Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). $$$$$ By contrast, in a hierarchical model, all reordering is performed by a single mechanism, the rules of the grammar.
Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). $$$$$ Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.
Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). $$$$$ As an alternative, we introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define a separate binary feature for each value of (R, S), where R is as above and S E J*, 1, ... , 9, >_101 and * means any size.
Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al, 2008) and large-margin training (Chiang et al, 2008). $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.

Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. $$$$$ Finally, we compared our parallelization method against a simpler method in which all processors learn independently and their weight vectors are all averaged together (line 5).
Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. $$$$$ By contrast, in a hierarchical model, all reordering is performed by a single mechanism, the rules of the grammar.
Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. $$$$$ We see that sharing information among the processors makes a significant difference.
Chiang et al (2008) added structure distortion features into their decoder and showed improvements in their Chinese-English experiment. $$$$$ First of all, we find that MIRA is competitive with MERT when both use the baseline feature set.

The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. $$$$$ Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.
The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. $$$$$ (2003; 2006).
The definition of the loss function here is similar to the one used in (Chiang et al, 2008) where only the top-1 translation candidate (i.e. k= 1) is taken into account. $$$$$ There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008).

In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). $$$$$ Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features.
In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). $$$$$ For example, if a nonterminal with span 11 has its contents reordered, then the features (true, >_10) and (true, *) would both fire.
In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). $$$$$ The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002).
In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al, 2008). $$$$$ Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.

 $$$$$ Both mechanisms are able to learn that longer-distance reorderings are more costly than shorter-distance reorderings: phrase pairs, because phrases that involve more extreme reorderings will (presumably) have a lower count in the data, and phrase reordering, because models are usually explicitly dependent on distance.
 $$$$$ One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information.
 $$$$$ It appears, then, that the additional negative examples enable the algorithm to reliably learn from the enhanced oracle translations.
 $$$$$ We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined.
