Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ One way of turning a nondeterministic parser into a deter ministic one is to use a guide (or oracle) that can inform the parser at each nondeterministic choice point; cf.
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ ?wj |wi|S, I, A ? {(wi, r, wj)}?
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ One problem whendeveloping a parser for Swedish is that there is no com parable large-scale treebank available for Swedish.

The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). $$$$$ for the correspond ing undirected relations, i.e. wi ? wj iff wi ? wj or wj ? wi.
The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). $$$$$ k = 5, i.e. classification based on 5 nearest neigh bors.
The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). $$$$$ The transition Right-Arc (RA) adds an arc wi r?wj.

For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). $$$$$ For the experiments reported in this paper, we have used the software package TiMBL (Tilburg MemoryBased Learner), which provides a variety of metrics, al gorithms, and extra functions on top of the classical knearest neighbor classification kernel, such as value distance metrics and distance weighted class voting (Daele mans et al, 2003).
For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.
For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). $$$$$ This is a standard probabilistic tagger trained on the Stockholm-Umea?
For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.

Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). $$$$$ However, in order to maintain the efficiency of the parser, the classifier must also be implemented in such a way that each transition can still be performed in constant time.Previous work in this area includes the use of memory based learning to guide a standard shift-reduce parser(Veenstra and Daelemans, 2000) and the use of support vector machines to guide a deterministic depen dency parser (Yamada and Matsumoto, 2003).
Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.

Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.
Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ 1999; Charniak, 2000).
Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ Guided parsing is normally used to improve the efficiency of a nondeterministic parser,e.g. by letting a simpler (but more efficient) parser con struct a first analysis that can be used to guide the choice of the more complex (but less efficient) parser.

These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further.
These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.
These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ Section 2 gives the necessary background definitions and introduces the idea of guided parsing as well as memory-based learning.
These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.

The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ Single head (wi?wj ? wk?wj) ? wi = wk Acyclic ?(wi?wj ? wj??wi) Connected wi??wj Projective (wi?wk ? wiwjwk) ?
The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ ??wk?r?(wk, r?, wi) ? A Right-Arc ?wi|S,wj |I, A?
The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ If we compare the labeled attachmentscore to the prediction accuracy (which also takes depen dency types into account), we observe a substantial drop (from 89.7 to 81.7 for the lexical model, from 87.4 to 76.5 for the non-lexical model), which is of course onlyto be expected.

Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ Overlap metric replaced by the modified value dis tance metric (MVDM) (Stanfill and Waltz, 1986; Cost and Salzberg, 1993).
Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).
Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ 3.1 Target Function and Approximation.
Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.

This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). $$$$$ In addition, it is important to evaluate the approach with respect to other languages and corpora in order to increase the comparability with other approaches.
This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.
This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). $$$$$ Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic pars ing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003).

It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ Section 3 describes the data used in the experiments, the evaluation metrics, and the models and algorithms used in the learning process.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ Parser configurations are represented by triples ?S, I, A?, where S is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ One problem whendeveloping a parser for Swedish is that there is no com parable large-scale treebank available for Swedish.

 $$$$$ Smoothing is per formed only for zero frequency events, in which case the classifier backs off to more general models by omittingfirst the features TOP.LEFT and LOOK and then the fea tures TOP.RIGHT and NEXT.LEFT; if even this does not help, the classifier predicts Reduce if permissible and Shift otherwise.
 $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.
 $$$$$ The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further.
 $$$$$ The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further.

Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ We see that MBL outperforms the MCLE model even when limited to the same features (all differences again being significant at the .0001 level according to a paired t-test).
Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ The paper is structured as follows.
Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ For adverbial modifiers, finally, attachment accuracy is lower than for the other dependency types, which is largely due to the notorious PP-attachment problem.

Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). $$$$$ Suggestions for further research includes the further exploration of alternative models and parameter settings, but also the combination of inductive and analytical learning to impose high-level linguistic constraints, and the development of new parsing methods (e.g. involving multiple passes over the data).
Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). $$$$$ Acknowledgements The work presented in this paper was supported by agrant from the Swedish Research Council (621-20024207).
Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). $$$$$ In this paper we have shown that a combination of memory-based learning and deterministic dependency parsing can be used to construct a robust and efficient parser for unrestricted natural language text, achieving a parsing accuracy which is close to the state of the art evenwith relatively limited amounts of training data.

In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.
In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than forEnglish, where the best results are above 90% (attach ment score per word) (Collins et al, 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).
In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ Features weighted by Gain Ratio (Quinlan, 1993).
In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.

More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004). $$$$$ The linguistic tradition of dependency grammar comprises a large and fairly diverse family of theories and formalisms that share certain basic assumptions about syn tactic structure, in particular the assumption that syntacticstructure consists of lexical nodes linked by binary re lations called dependencies (see, e.g., Tesnie`re (1959), Sgall (1986), Mel?c?uk (1988), Hudson (1990)).
More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004). $$$$$ Suggestions for further research includes the further exploration of alternative models and parameter settings, but also the combination of inductive and analytical learning to impose high-level linguistic constraints, and the development of new parsing methods (e.g. involving multiple passes over the data).
More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004). $$$$$ ?S,wj |I, A ? {(wj , r, wi)}?

 $$$$$ (1991).
 $$$$$ Memory-based learning and problem solving is based ontwo fundamental principles: learning is the simple stor age of experiences in memory, and solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans, 1999).
 $$$$$ Suggestions for further research includes the further exploration of alternative models and parameter settings, but also the combination of inductive and analytical learning to impose high-level linguistic constraints, and the development of new parsing methods (e.g. involving multiple passes over the data).

To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ from the next input token wj to the token wi on top of the stack and reduces (pops) wi from the stack.
To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.
To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.

We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). $$$$$ Unlike standard shift reduce parsing, the simulation of the current algorithm is almost deterministic and is guaranteed to be correct if the input dependency tree is well-formed.The complete converted treebank contains 6316 sentences and 97623 word tokens, which gives a mean sentence length of 15.5 words.
We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). $$$$$ Clas sifiers based on memory-based learning achieve higher parsing accuracy than previous probabilistic models, and the improvement increases if lexical information is added to the model.
We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). $$$$$ (R ? {nil})Here Config is the set of all possible parser configura tions and R is the set of dependency types as before.

However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.
However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ In theexperiments reported in this paper, we apply memory based learning within a deterministic dependency parsing framework.
However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ Kay (2000), Boullier (2003).
However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.

Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ Section 3 describes the data used in the experiments, the evaluation metrics, and the models and algorithms used in the learning process.
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ whose domain is a finite space of parser states, which are abstractions over configurations.
