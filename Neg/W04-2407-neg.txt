Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic pars ing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003).
Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al, 2004). $$$$$ , rm} be the set of permissible de-.

The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). $$$$$ The output of the memory-based learner is a classifier that predicts the next transition (including dependency type), given the current state of the parser.
The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). $$$$$ Moreover, this single anal ysis is derived in a monotonic fashion with no redundancy or backtracking, which makes it possible to parse natural language sentences in linear time (Nivre, 2003).In this paper, we report experiments using memorybased learning (Daelemans, 1999) to guide the parser described in Nivre (2003), using data from a small treebank of Swedish (Einarsson, 1976).
The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively smalltreebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004). $$$$$ Moreover, the memory-based approach can easily handle multi-class classification, unlike the support vector machines used by Yamada and Matsumoto (2003).

For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). $$$$$ Left-Arc ?wi|S,wj |I, A?
For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). $$$$$ 2.4 Memory-Based Learning.

Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). $$$$$ De terministic parsing means that we always derive a singleanalysis for each input string.
Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). $$$$$ Unlike most pre vious work on data-driven dependency parsing (Eisner, 1996; Collins et al, 1999; Yamada and Matsumoto, 2003;Nivre, 2003), we assume that dependency graphs are la beled with dependency types, although the evaluationwill give results for both labeled and unlabeled represen tations.
Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). $$$$$ In addition, arcs may be labeled with specific dependency types.
Previous work on memory-based learning for deterministic parsing includes Veenstra and Daelemans (2000) and Nivre et al (2004). $$$$$ Distance weighted class voting with inverse distance weighting (Dudani, 1976).

Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ The results indicate that subjects have the highest accuracy, especially when labels are taken intoaccount.
Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ The results in the first column were obtained with the default settings of the TiMBL package, in particular: ? The IB1 classification algorithm (Aha et al, 1991).
Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ Clas sifiers based on memory-based learning achieve higher parsing accuracy than previous probabilistic models, and the improvement increases if lexical information is added to the model.
Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency tree bank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al, 2004). $$$$$ We are grateful to three anonymous reviewers for constructive com ments on the preliminary version of the paper.

These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ pendency types (arc labels).
These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ One way of doing this is to use a treebank, i.e. a corpus of analyzed sentences, to train a classifier that can predict the next transition (and dependency type) given the current configuration of the parser.
These settings are the result of extensive experiments partially reported in Nivre et al (2004). $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.

The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further.
The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.
The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ Single head (wi?wj ? wk?wj) ? wi = wk Acyclic ?(wi?wj ? wj??wi) Connected wi??wj Projective (wi?wk ? wiwjwk) ?
The portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004). $$$$$ More precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing (Eisner, 1996; Collins et al, 1999).

Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.
Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ Objects and predicative complements have comparable attachment accuracy, but are more often misclas sified with respect to dependency type.
Indirect support for this assumption can be gained from previous experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a tree bank which is much smaller but which contains proper dependency annotation (Nivre et al, 2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.

This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.
This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivreet al, 2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.

It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ One reason why our results nevertheless compare reasonably well with those obtained with the much larger training set is probably that the conversion to dependency trees is more accurate for the Swedish treebank, given theexplicit annotatation of grammatical functions.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ In addition, it is important to evaluate the approach with respect to other languages and corpora in order to increase the comparability with other approaches.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ The final feature (LOOK) is a simple looka head, using the part-of-speech of the next plus one input token.

 $$$$$ In addition, it is important to evaluate the approach with respect to other languages and corpora in order to increase the comparability with other approaches.
 $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.
 $$$$$ Moreover, this single anal ysis is derived in a monotonic fashion with no redundancy or backtracking, which makes it possible to parse natural language sentences in linear time (Nivre, 2003).In this paper, we report experiments using memorybased learning (Daelemans, 1999) to guide the parser described in Nivre (2003), using data from a small treebank of Swedish (Einarsson, 1976).
 $$$$$ The unlabeled attachment score is naturally higher, and it is worth noting that the relative differ ence between the MBL lexical model and the other twomodels is much smaller.

Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ Suggestions for further research includes the further exploration of alternative models and parameter settings, but also the combination of inductive and analytical learning to impose high-level linguistic constraints, and the development of new parsing methods (e.g. involving multiple passes over the data).
Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ , rm} be the set of permissible de-.
Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ Results from the experiments are given in section 4, while conclusions and suggestions for further research are presented in section 5.
Based on results from previous optimization experiments (Nivre et al., 2004), we use the modified value difference metric (MVDM) to determine distances between instances, and distance-weighted class voting for determining the class of a new instance. $$$$$ Memory-based learning has been successfully appliedto a number of problems in natural language processing, such as grapheme-to-phoneme conversion, part of-speech tagging, prepositional-phrase attachment, and base noun phrase chunking (Daelemans et al, 2002).Most relevant in the present context is the use of memory based learning to predict the actions of a shift-reduce parser, with promising results reported in Veenstra and Daelemans (2000).

Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). $$$$$ In addition, arcs may be labeled with specific dependency types.
Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). $$$$$ to denote the reflexive and transitive closure of the unlabeled arcrelation; and we use ? and ??
Compared to the state of the art in dependency parsing, the unlabeled attachment scores obtained for Swedish with model? 5, for both MBL and SVM, are about 1 percentage point higher than the results reported for MBL by Nivre et al (2004). $$$$$ Using data from a small treebank of Swedish, memory-based classifiers for predicting the next action of the parser are constructed.

In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ Section 3 describes the data used in the experiments, the evaluation metrics, and the models and algorithms used in the learning process.
In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ De terministic parsing means that we always derive a singleanalysis for each input string.
In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al, 2004) and English (Nivre and Scholz, 2004). $$$$$ We use McNemar?s test for statistical sig nificance.

More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004). $$$$$ Section 2 gives the necessary background definitions and introduces the idea of guided parsing as well as memory-based learning.
More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004). $$$$$ Clas sifiers based on memory-based learning achieve higher parsing accuracy than previous probabilistic models, and the improvement increases if lexical information is added to the model.
More details on the memory-based prediction can be found in Nivre et al (2004) and Nivre and Scholz (2004). $$$$$ The accuracy of a classifier as such is evaluated on held-out data derived from the treebank, and its performance as a parserguide is evaluated by parsing the held-out por tion of the treebank.

 $$$$$ No weighting of features.?
 $$$$$ The evaluation shows thatmemory-based learning gives a signficant im provement over a previous probabilistic modelbased on maximum conditional likelihood esti mation and that the inclusion of lexical features improves the accuracy even further.
 $$$$$ No weighting of features.?

To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ The unlabeled attachment score is naturally higher, and it is worth noting that the relative differ ence between the MBL lexical model and the other twomodels is much smaller.
To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ This confirms previous results from statistical parsing indicating that lex ical information is crucial for disambiguation (Collins,stances that are equally distant to the test instance.
To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ This paper reports the results of experimentsusing memory-based learning to guide a de terministic dependency parser for unrestricted natural language text.
To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al, 2004), SVMs (Nivre et al, 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). $$$$$ The memory-based classifiers used in the experi ments were constructed using the Tilburg Memory-BasedLearner (TiMBL) (Daelemans et al, 2003).

We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). $$$$$ 4.
We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). $$$$$ Features weighted by Gain Ratio (Quinlan, 1993).
We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al,2004). $$$$$ The memory-based classifiers used in the experi ments were constructed using the Tilburg Memory-BasedLearner (TiMBL) (Daelemans et al, 2003).

However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ Acknowledgements The work presented in this paper was supported by agrant from the Swedish Research Council (621-20024207).
However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ Dependency parsing means that the goal of the parsing process is to constructa dependency graph, of the kind depicted in Figure 1.
However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ Thus, in order to get a deterministic parser, we need to introduce a mechanism for resolving transition conflicts.
However, instead of performing deterministic parsing as in (Nivre et al, 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. $$$$$ k = 5, i.e. classification based on 5 nearest neigh bors.

Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ Clas sifiers based on memory-based learning achieve higher parsing accuracy than previous probabilistic models, and the improvement increases if lexical information is added to the model.
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ A dependency graph for a string of words W = w1?
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al, 2004). $$$$$ Objects and predicative complements have comparable attachment accuracy, but are more often misclas sified with respect to dependency type.
