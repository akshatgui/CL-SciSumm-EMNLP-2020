We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ This is motivated by the observation that enhancing the feature set or learning algorithm used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ While many other contextual features are often employed, it isn't clear that they offer substantial advantages.

Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ In this paper ensemble disambiguation is based on a simple majority vote of the nine member classifiers.
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ In this paper, these counts are the number of sentences in the sensetagged text where the word represented by Fi occurs within some specified window of context of the ambiguous word when it is used in sense S. Any parameter that has a value of zero indicates that the associated word never occurs with the specified sense value.
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ Claudia Leacock and Raymond Mooney provided valuable assistance with the line data.

It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results.
It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ There are three such ranges; narrow corresponds to windows 0, 1 and 2 words wide, medium to windows 3, 4, and 5 words wide, and wide to windows 10, 25, and 50 words wide.
It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ A learning algorithm induces a representative model from these features which is employed as a classifier to perform disambiguation.

The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ Finally, further experimentation with the size of the windows of context seems warranted.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ A preliminary version of this paper appears in (Pedersen, 2000).

The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ A Naive Bayesian classifier assumes that all the feature variables representing a problem are conditionally independent given the value of a classification variable.
The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998).
The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ Naive_Bayes (1,r) represents a classifier where the model parameters have been estimated based on frequency counts of shallow lexical features from two windows of context; one including 1 words to the left of the ambiguous word and the other including r words to the right.
The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.

In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ The lesson taken from these results was that an ensemble should consist of classifiers that represent as differently sized windows of context as possible; this reduces the impact of redundant errors made by classifiers that represent very similarly sized windows of context.
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ These approaches typically represent the context in which each sense—tagged instance of a word occurs with a set of linguistically motivated features.
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ This paper shows that word sense disambiguation accuracy can be improved by combining a number of simple classifiers into an ensemble.
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ A learning algorithm induces a representative model from these features which is employed as a classifier to perform disambiguation.

They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ This paper shows that word sense disambiguation accuracy can be improved by combining a number of simple classifiers into an ensemble.
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ Shallow lexical features such as co—occurrences and collocations are recognized as potent sources of disambiguation information.
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different.
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously published results.

Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ For example, an ensemble was created for interest using the nine classifiers in the range category (medium, medium).
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ This process is repeated five times so that each fold serves as the source of the test data once.
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ Some combination of stop-lists and stemming could reduce the numbers of parameters and thus improve the overall quality of the parameter estimates made from the training data.
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)).

It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ This did not arise in this study but will certainly occur as Naive Bayesian ensembles are applied to larger sets of data.
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997).
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)).
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ These studies represent the context in which an ambiguous word occurs with a wide variety of features.

Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ This work extends ideas that began in collaboration with Rebecca Bruce and Janyce Wiebe.
Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ While many other contextual features are often employed, it isn't clear that they offer substantial advantages.
Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ Thus, the boxes that subdivide each table correspond to a particular range category.

The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%.
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously published results.
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data.
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ The ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors.

Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ A number of issues have arisen in the course of this work that merit further investigation.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ The senses and their fresense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line - the experiments in this paper and previous work use a uniformly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for the use of money 1252 a share in a company or business 500 readiness to give attention 361 advantage, advancement or favor 178 activity that one gives attention to 66 causing attention to be given to 11 total 2368 Table 2: Distribution of senses for interest - the experiments in this paper and previous work use the entire corpus, where each sense occurs the number of times shown above. quency distribution are shown in Table 2.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ Punctuation and capitalization are removed from the windows of context.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ This did not arise in this study but will certainly occur as Naive Bayesian ensembles are applied to larger sets of data.

Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ (Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data.
Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ The current formulation is based on a combination of intuition and empirical study.
Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ Preliminary experiments for this paper used feature sets that included collocates, co—occurrences, part—of— speech and grammatical information for surrounding words.
Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ Unlike line, the sense distribution is skewed; the majority sense occurs in 53% of the sentences, while the smallest minority sense occurs in less than 1%.

Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ Still, the results in this paper are encouraging due to the simplicity of the approach.
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ This is discussed in the context of combining part—of—speech taggers in (Brill and Wu, 1998).
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ This work extends ideas that began in collaboration with Rebecca Bruce and Janyce Wiebe.
