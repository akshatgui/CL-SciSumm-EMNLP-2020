We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ This work extends ideas that began in collaboration with Rebecca Bruce and Janyce Wiebe.
We have presented an ensemble approach to word sense disambiguation (Pedersen, 2000) where multiple Naive Bayesian classiers, each based on co-occurrence features from varying sized windows of context, is shown to perform well on the widely studied nouns interest and line. $$$$$ The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments.

Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ This paper closes with a discussion of the choices made in formulating this methodology and plans for future work.
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ This did not arise in this study but will certainly occur as Naive Bayesian ensembles are applied to larger sets of data.
Pedersen (2000) proposed an ensemble model with multiple NB classifiers differing by context window size. $$$$$ In both cases context is represented by a set of topical and local features.

It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ These senses and their frequency distribution are shown in Table 1.
It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ Some combination of stop-lists and stemming could reduce the numbers of parameters and thus improve the overall quality of the parameter estimates made from the training data.
It is similar to the ordinary Naive Bayes model for WSD (Pedersen, 2000). $$$$$ These are distinct from co—occurrences in that they are words that occur in close proximity to the ambiguous word and do so to a degree that is judged statistically significant.

The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ The size and range of the left window of context is indicated along the horizontal margin in Tables 3 and 4 while the right window size and range is shown along the vertical margin.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously published results.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ This work extends ideas that began in collaboration with Rebecca Bruce and Janyce Wiebe.
The origins of Duluth can be found in an ensemble approach based on multiple Naive Bayesian classifiers that perform disambiguation via a majority vote (Pedersen, 2000). $$$$$ Early experiments also revealed that an ensemble based on a majority vote of all 81 classifiers performed rather poorly.

The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context.
The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ Claudia Leacock and Raymond Mooney provided valuable assistance with the line data.
The approach used, called combination approach, has known lots of success in speech recognition (Fiscus 1997, Schwenck and Gauvain 2000), part of speech tagging (Halteren and al. 1998, Brill and al. 1998, Marquez et Padro 1998), named entity recognition (Borthwick and al. 1998), word sense disambiguation (Pedersen, 2000) and recently in parsing (Henderson and Brill 1999), Inui and Inui 2000, Monceaux and Robba 2003). $$$$$ This did not arise in this study but will certainly occur as Naive Bayesian ensembles are applied to larger sets of data.

In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ There are nine possible range categories since there are separate left and right windows.
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local context while the other represents topical context.
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998).
In NLP, such methods have been applied to tasks such as POS tagging (Brill and Wu, 1998), word sense disambiguation (Pedersen, 2000), parsing (Henderson and Brill, 1999), and machine translation (Frederking and Nirenburg, 1994). $$$$$ Early experiments also revealed that an ensemble based on a majority vote of all 81 classifiers performed rather poorly.

They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships.
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships.
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ I am indebted to an anonymous reviewer who pointed out the importance of separate test and devtest data sets.
They include those using Naive Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al 2000), and Naive Bayesian Ensemble (Pedersen 2000). $$$$$ Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results.

Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ This approach was evaluated using the widely studied nouns line and interest, which are disambiguated with accuracy of 88% and 89%, which rivals the best previously published results.
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses.
Among these methods, the one using Naive Bayesian Ensemble (i.e., an ensemble of Naive Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). $$$$$ The first step in the ensemble approach is to train a separate Naive Bayesian classifier for each of the 81 possible combination of left and right window sizes.

It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997).
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results.
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ The standard deviations were between .01 and .025 and are not shown given their relative consistency.
It actually employs an ensemble of the Naive Bayesian Classifiers (NBC), because an ensemble of NBCs generally performs better than a single NBC (Pedersen 2000). $$$$$ A preliminary version of this paper appears in (Pedersen, 2000).

Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ The accuracy of this ensemble was 84%, slightly less than the most accurate individual classifiers in that range which achieved accuracy of 86%.
Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ All other lexical items are included in their original form; no stemming is performed and non-content words remain.
Table 4 shows the results achieved by some existing supervised learning methods with respect to the benchmark data (cf., Pedersen 2000). $$$$$ The senses and their fresense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line - the experiments in this paper and previous work use a uniformly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for the use of money 1252 a share in a company or business 500 readiness to give attention 361 advantage, advancement or favor 178 activity that one gives attention to 66 causing attention to be given to 11 total 2368 Table 2: Distribution of senses for interest - the experiments in this paper and previous work use the entire corpus, where each sense occurs the number of times shown above. quency distribution are shown in Table 2.

The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ The first step in the ensemble approach is to train a separate Naive Bayesian classifier for each of the 81 possible combination of left and right window sizes.
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997).
The Duluth-xLSS system was originally inspired by (Pedersen, 2000), which presents an ensemble of eighty-one Naive Bayesian classifiers based on varying sized windows of context to the left and right of the target word that define co-occurrence features. $$$$$ This was expected since slight differences in window sizes lead to roughly equivalent representations of context and classifiers that have little opportunity for collective improvement.

Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ However, when the contribution of each type of feature to overall accuracy is analyzed (eg.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ A number of issues have arisen in the course of this work that merit further investigation.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ These senses and their frequency distribution are shown in Table 1.
Pedersen (2000) built an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context. $$$$$ The line data was first studied by (Leacock et al., 1993).

Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ Punctuation and capitalization are removed from the windows of context.
Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ Finally, further experimentation with the size of the windows of context seems warranted.
Locally weighted NB (LWNB, Frank et al 2003) and Ensemble NB (ENB Pedersen 2000) are two combinational approaches. $$$$$ For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model.

Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different.
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ A methodology for formulating an ensemble of Naive Bayesian classifiers is presented, where each member classifier is based on co—occurrence features extracted from a different sized window of context.
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ Note that Naive_Bayes (0,0) includes no words to the left or right; this classifier acts as a majority classifier that assigns every instance of an ambiguous word to the most frequent sense in the training data.
Pedersen (2000) presents experiments with an ensemble of Naive Bayes classifiers, which outperform all previous published results on two ambiguous words (line and interest). $$$$$ This work extends ideas that began in collaboration with Rebecca Bruce and Janyce Wiebe.
