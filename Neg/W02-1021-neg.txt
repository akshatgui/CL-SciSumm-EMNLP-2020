The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ Figure 4 shows the change in graph error rate in relation to the average graph density.
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ Stefan Ortmanns, Hermann Ney, and Xavier Aubert.
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.

This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ Stefan Ortmanns, Hermann Ney, and Xavier Aubert.

A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.
A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.
A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.

The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ Those hypotheses that survive the pruning are called the active hypotheses.
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ In single best search, a standard trigram language model is used.
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.

The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ Stefan Ortmanns, Hermann Ney, and Xavier Aubert.
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.

Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged.
Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ We have to keep every partial hypothesis in the priority queue in order to determine the N best sentences.
Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ The calculation of the graph error rate is performed by a dynamic programming based algorithm.

The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ graphs: An efficient interface between continous speech recognition and language understanding.
The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.
The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ Word re-ordering and DP-based search in statistical matranslation.

The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ Otherwise, we might lose one of them by recombination.

We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ Experiments have shown that the graph error rate significantly decreases for rising word graph densities.
We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ Experiments have shown that the graph error rate significantly decreases for rising word graph densities.
We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse.
We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.

The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model.
The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.
The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse.

 $$$$$ The results of the comparison of the one-pass and the two-pass search are given in Section 5.
 $$$$$ Experiments have shown that the graph error rate significantly decreases for rising word graph densities.
 $$$$$ It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: After the search has finished, i.e. when all source sentence positions have been translated, we trace back the best sentence in the bookkeeping tree.
 $$$$$ The probability of the partial hypothesis is obtained by multiplying the probabilities of the edges expanded for this sentence.

The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ In '00: The 18th Int.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.

(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.
(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea $$$$$ A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.
(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea $$$$$ International Conference on Acoustics, and Signal Processing, 2, pages 119-122, Minneapolis, MN, April.
(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea $$$$$ A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.

The algorithm in (Ueffing et al, 2002) has two disadvantages $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.
The algorithm in (Ueffing et al, 2002) has two disadvantages $$$$$ Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993).
The algorithm in (Ueffing et al, 2002) has two disadvantages $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
The algorithm in (Ueffing et al, 2002) has two disadvantages $$$$$ Its space complexity is the number of graph nodes times the length of the reference translation.

We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ One efficient way to store the different alternatives is a word graph.
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged.

A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ Thus, it is a lower bound for the word error rate and gives a measurement of what can be achieved by rescoring with more complex models.
A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ We can then apply rescoring with a refined model to those hypotheses.
A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ The probability of the partial hypothesis is obtained by multiplying the probabilities of the edges expanded for this sentence.
A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ The bookkeeping structure is no longer a tree but a network where the source is the bookkeeping entry with zero covered source sentence positions and the sink is a node accounting for complete hypotheses (see Figure 3).

The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ It is widely accepted in the community that a significant improvement in translation quality will come from more sophisticated translation and language models.
The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.
The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.

In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ The calculation of the graph error rate is performed by a dynamic programming based algorithm.

For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ We use A* search for finding the N best sentences in a word graph: starting in the root of the graph, we successively expand the sentence hypotheses.
For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ Stefan Ortmanns, Hermann Ney, and Xavier Aubert.
For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse.
For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged.

For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.
For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ Therefore, we apply a twopass approach as it was widely used in speech recognition in the past (Ortmanns et al., 1997).
For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ If the pruning threshold t is zero, the word graph is not pruned at all, and if t = 1, we retain only the sentence with maximum probability.
For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ The quality of the hypotheses contained in a word graph is better than of those in an N-best list.
