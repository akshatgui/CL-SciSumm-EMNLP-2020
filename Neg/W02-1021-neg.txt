The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ 2000.
The generation of word graphs for a bottom-top search with the IBM constraints is described in (Ueffing et al, 2002). $$$$$ In '00: The 18th Int.

This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ Word re-ordering and DP-based search in statistical matranslation.
This is sometimes referred to as a word graph (Ueffing et al, 2002), although in our case the segmented phrase table also produces tokens that correspond to morphemes. $$$$$ Thus, an entry in the bookkeeping structure may have several backpointers to different preceding entries.

A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ Thus, it is a lower bound for the word error rate and gives a measurement of what can be achieved by rescoring with more complex models.
A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ The graph error rate is computed by determining that sentence in the word graph that has the minimum Levenstein distance to a given reference.
A word graph is a weighted directed acyclic graph, in which each node represents a partial translation hypothesis and each edge is labelled with a word of the target sentence and is weighted according to the scores given by an SMT model (see (Ueffing et al, 2002) for more details). $$$$$ The value of the pruning threshold is given as the negative logarithm of the probability.

The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion.
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.
The decoder involves generating a phrase lattice (Ueffing et al, 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. $$$$$ In our experiments, we varied the word graph pruning threshold in order to obtain word graphs of different densities, i.e. different numbers of hypotheses.

The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ This indicates that word graph rescoring can yield a significant gain in translation quality.
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ graphs: An efficient interface between continous speech recognition and language understanding.
The idea of n-best list extraction from a word graph for SMT was presented in (Ueffing et al, 2002). $$$$$ Search with a bigram language model is much faster, but it yields a lower translation quality.

Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ We can then apply rescoring with a refined model to those hypotheses.
Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ Experiments have shown that the graph error rate significantly decreases for rising word graph densities.
Details regarding n-best list generation from decoder output can be found in (Ueffing et al, 2002). $$$$$ This leads us to the concept of word graph nodes and edges containing the following information: — the probabilities according to the different models: the language model and the translation submodels, — the backpointer to the preceding bookkeeping entry.

The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.
The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ Figure 4 shows the change in graph error rate in relation to the average graph density.
The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ To overcome this problem, we enhance the bookkeeping concept and generate a word graph as described in Section 3.3.
The machine translation system is a graph based decoder (Ueffing et al, 2002). $$$$$ We can then apply rescoring with a refined model to those hypotheses.

The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ Christoph Tillmann and Hermann Ney.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ This indicates that word graph rescoring can yield a significant gain in translation quality.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ The results of the comparison of the one-pass and the two-pass search are given in Section 5.
The consistent N-best phrase alignment can be obtained by using A* search as described in (Ueffing et al, 2002). $$$$$ To generate the N best hypotheses after search, it is not sufficient to simply trace back the complete hypotheses with the highest probabilities in the bookkeeping, because those hypotheses have been recombined.

We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ Thus, the N best hypothesis are extracted from the graph without additional overhead of finding sentences with a lower probability.
We implemented our own decoder based on the algorithm described in (Ueffing et al, 2002). $$$$$ This method combines both advantages in the following way: a word graph is constructed using a bigram language model and is then rescored with a trigram language model.

The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.
The only publication, we are aware of, is (Ueffing et al, 2002). $$$$$ We have to keep every partial hypothesis in the priority queue in order to determine the N best sentences.

 $$$$$ To study the effect of the word graph size on the translation quality, we produce a conservatively large word graph.
 $$$$$ Its space complexity is the number of graph nodes times the length of the reference translation.
 $$$$$ This reduces the size of the bookkeeping structure significantly.

The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ The probability of the partial hypothesis is obtained by multiplying the probabilities of the edges expanded for this sentence.
The important point is that we compare the full path scores and not only partial scores as, for instance, in the beam pruning method in (Ueffing et al., 2002). $$$$$ This reduces the size of the bookkeeping structure significantly.

(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation. $$$$$ Word re-ordering and DP-based search in statistical matranslation.
(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation. $$$$$ This indicates that word graph rescoring can yield a significant gain in translation quality.
(Ueffing et al, 2002) and (Mohri and Riley, 2002) both present an algorithm based on the same idea: use a modified A* algorithm with an optimal rest cost estimation. $$$$$ whose probability is below this value multiplied with a threshold (lower than one) will not be regarded for further expansion.

The algorithm in (Ueffing et al, 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity. $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.
The algorithm in (Ueffing et al, 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity. $$$$$ As rest cost estimation, we use the probabilities determined in a backward pass as follows: for each node in the graph, we calculate the probability of a best path from this node to the goal node, i.e. the highest probability for completing a partial hypothesis.
The algorithm in (Ueffing et al, 2002) has two disadvantages: it does not care about duplicates and the rest cost computation is suboptimal as the described algorithm has an exponential worst-case complexity. $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.

We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ The word graph structure and the results presented here can easily be transferred to other search algorithms, such as A* search.
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ This rest cost estimation is perfect because it takes the exact probability as heuristic, i.e. the probability of the partial hypothesis multiplied with the rest cost estimation yields the actual probability of the complete hypothesis.
We will use the well-known graph word error rate (GWER), see also (Ueffing et al,2002). $$$$$ Thus, it is a lower bound for the word error rate and gives a measurement of what can be achieved by rescoring with more complex models.

A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.
A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ In '00: The 18th Int.
A lattice (Ueffing et al, 2002) can be viewed as a special hyper graph, in which the maximum arity is one. $$$$$ If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.

The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ This reduces the size of the bookkeeping structure significantly.
The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ Word re-ordering and DP-based search in statistical matranslation.
The objective measures used were the BLEU score (Papineni et al, 2001), the NIST score (Dod ding ton, 2002) and Multi-reference Word Error Rate (mWER) (Ueffing et al, 2002). $$$$$ After threshold and histogram pruning have been applied, we also compare all hypotheses with the same number of covered source sentence positions and apply both pruning types again.

In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ To generate the N best hypotheses after search, it is not sufficient to simply trace back the complete hypotheses with the highest probabilities in the bookkeeping, because those hypotheses have been recombined.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ A word graph algorithm for large vocabcontinuous speech recognition. and Language, January.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.
In this paper we explore a different strategy to perform MBR decoding over Translation Lattices (Ueffing et al, 2002) that compactly encode a huge number of translation alternatives relative to an N -best list. $$$$$ We have presented a concept for constructing word graphs for statistical machine translation by extending the single best search algorithm.

For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ The saturation point of the GER lies at 13% and is reached for an average graph density about 1000 which relates to a pruning threshold of 20.
For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al, 1999) and extract. best candidate translations using A* search (Ueffing et al, 2002). $$$$$ Those hypotheses that survive the pruning are called the active hypotheses.

For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ Stefan Ortmanns, Hermann Ney, and Xavier Aubert.
For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ For the future, we plan the application of refined translation and language models for rescoring on word graphs.
For a description on how to generate lattices, see (Ueffing et al, 2002). $$$$$ Histogram pruning means that all but the M best hypotheses are pruned for a fixed M. For finding the most likely partial hypotheses, first all hypotheses with the same set of covered source sentence positions are compared.
