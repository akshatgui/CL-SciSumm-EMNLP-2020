In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ Using phrases means that the length of the potential terms is not restricted to something arbitrary, rather the terms are treat as the units they are.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ The longest chunks in the test set that were correctly assigned are five tokens long.

In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ The first approach was to extract all noun phrases in the documents as judged by an NP-chunker.
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ Several runs were made for each representation, with the goal to maximise the performance as evaluated on the validation set: first the weights of the positive examples were adjusted, as the data set is unbalanced.
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ The length of the abstracts in the test set varies from 338 to 23 tokens (the median is 121 tokens).

For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ In this section, the three different term selection approaches, in other words, the three definitions of what constitutes a term in a document, are described.
For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.

It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. $$$$$ The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.
It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. $$$$$ The total number of manually assigned terms present in the abstracts is 3 816, and the mean is 7.63 terms per document. tion approaches, extracting NP-chunks gives a better precision, while extracting all words or sequences of words matching any of a set of POS tag patterns gives a higher recall compared to extracting ngrams.

In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ For the experiments described here, only the uncontrolled terms were considered, as these to a larger extent are present in the abstracts (76.2% as opposed to 18.1%).
In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ The trained model is subsequently applied to documents for which no keywords are assigned: each defined term from these documents is classified either as a keyword or a non-keyword; or—if a probabilistic model is used—the probability of the defined term being a keyword is given.
In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ Finding potential terms—when no machine learning is involved in the process—by means of POS patterns is a common approach.

In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ The final example given in this paper is Daille et al. (1994) who apply statistical filters on the extracted noun phrases.
In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ Keywords may, for example, serve as a dense summary for a document, lead to improved information retrieval, or be the entrance to a document collection.
In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.

In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ As an example, an extracted phrase like random JJ excitations NNS gets the atomic feature value JJ NNS.
In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ The highest F-score is obtained by one of the n-gram runs.

Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ As with the chunking approach, experiments with both unstemmed and stemmed terms were performed.
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ As about half of the manual keywords present in the training data were lost using the chunking approach, I decided to define another term selection approach.

In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ All unigrams, bigrams, and trigrams were extracted.
In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ Four different features are used: term frequency, collection frequency, relative position of the first occurrence, and the POS tag(s) assigned to the term.
In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ The set of abstracts was arbitrarily divided into three sets: a training set (to construct the model) consisting of 1 000 documents, a validation set (to evaluate the models, and select the best performing one) consisting of 500 documents, and a test set (to get unbiased results) with the remaining 500 abstracts.
In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ Frank et al. also discuss the addition of a fourth feature that significantly improves the algorithm, when trained and tested on domain-specific documents.

Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ Proper nouns were kept.
Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ The F-scores (F ) for these two runs are 17.6 and 33.9 respectively.
Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ Experiments with both unstemmed and stemmed terms were performed.

More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ But, the difference is shown when the POS tag feature is included: the number of correctly assigned terms is more or less the same for this approach with or without the tag feature, while the number of incorrect terms is halved.
More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ The set of abstracts was arbitrarily divided into three sets: a training set (to construct the model) consisting of 1 000 documents, a validation set (to evaluate the models, and select the best performing one) consisting of 500 documents, and a test set (to get unbiased results) with the remaining 500 abstracts.

In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.
In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ It might well be that better results are possible for any of the representations.
In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ When extracting the terms from the test set according to the n-gram approach, the data consisted of 42 159 negative examples, and 3 330 positive examples, thus in total 45 489 examples were classified by the trained model.

Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ For the experiments described here, only the uncontrolled terms were considered, as these to a larger extent are present in the abstracts (76.2% as opposed to 18.1%).
Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.
Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ Automatic keyword assignment is a research topic that has received less attention than it deserves, considering keywords’ potential usefulness.

More linguistic knowledge has been explored by Hulth (2003). $$$$$ This is the POS tag or tags assigned to the term by the same partial parser used for finding the chunks and the tag patterns.
More linguistic knowledge has been explored by Hulth (2003). $$$$$ Secondly, the user must state how many keywords to extract from each document, as both algorithms, for each potential keyword, output the probability of the term being a keyword.
More linguistic knowledge has been explored by Hulth (2003). $$$$$ A related research area is that of terminology extraction (see e.g., Bourigault et al. (2001)), where all terms describing a domain are to be extracted.
More linguistic knowledge has been explored by Hulth (2003). $$$$$ This section begins with a discussion on the different ways the data were represented: in Section 4.1 the term selection approaches are described, and in Section 4.2 the features are discussed.

Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ However, the indexers had access to the full-length documents when assigning the keywords.
Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.

As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ In this section, the three different term selection approaches, in other words, the three definitions of what constitutes a term in a document, are described.
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ As the proportion of correctly suggested keywords is considered equally important as the amount of terms assigned by a professional indexer that was detected, was assigned the value 1, thus giving precision and recall equal weights.
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ The set of abstracts was arbitrarily divided into three sets: a training set (to construct the model) consisting of 1 000 documents, a validation set (to evaluate the models, and select the best performing one) consisting of 500 documents, and a test set (to get unbiased results) with the remaining 500 abstracts.
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ One shortcoming of the work is that there is currently no relation between the different POS tag feature values.

Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ This way of defining the terms is in this paper called the chunking approach.

Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ A related research area is that of terminology extraction (see e.g., Bourigault et al. (2001)), where all terms describing a domain are to be extracted.
Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ The set of abstracts was arbitrarily divided into three sets: a training set (to construct the model) consisting of 1 000 documents, a validation set (to evaluate the models, and select the best performing one) consisting of 500 documents, and a test set (to get unbiased results) with the remaining 500 abstracts.
Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ The aim of keyword assignment is to find a small set of terms that describes a specific document, independently of the domain it belongs to.
Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ Finally all remaining tokens were stemmed using Porter’s stemmer (Porter, 1980).

This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ The collection frequency was calculated for the three data sets separately.
This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ For the experiments described here, only the uncontrolled terms were considered, as these to a larger extent are present in the abstracts (76.2% as opposed to 18.1%).
This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ In case of a draw, the first occurring one is assigned.

While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.
While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ When extracting the terms according to the stemmed pattern approach, the test data consisted of 33 507 examples.
While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ However, the indexers had access to the full-length documents when assigning the keywords.
