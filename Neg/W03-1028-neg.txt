In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ The actual number of terms assigned per document is 100 to 0 (for three documents) without the tag feature, and 46 to 0 (for four documents) with the tag feature.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ Automatic keyword assignment is a research topic that has received less attention than it deserves, considering keywords’ potential usefulness.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ This way of defining the terms is in this paper called the chunking approach.
In Hulth (2003a) an evaluation of three different methods to extract candidate terms from documents is presented. $$$$$ This way of defining the terms is in this paper called the chunking approach.

In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ When adding the fourth feature, the number of correct terms decreases slightly, while the number of incorrect terms is decreased to a third.
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ In this section, the results obtained by the best performing model for each approach—as judged on the validation set—when run on the previously unseen test set are presented.
In Hulth (2003b), experiments on how the performance of the keyword extraction can be improved by combining the judgement of three classifiers are presented. $$$$$ This figure varies slightly for the unstemmed and the stemmed data, and for the two the corresponding value is used.

For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ Both the controlled terms and the uncontrolled terms may or may not be present in the abstracts.
For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ The set of manually assigned keywords were then removed from the documents.
For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ 33.9 is the highest F-score that was achieved for the six runs presented here.
For these experiments, the same machine learning system RDS is used as for the experiments presented by Hulth (2003a). $$$$$ The median is 30 and 12 respectively.

It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. $$$$$ In the next set of experiments a partial parserl was used to select all NP-chunks from the documents.
It was noted in Hulth (2003b) that when extracting NP chunks, the accompanying determiners are also extracted (per definition), but that determiners are rarely found at the initial position of keywords. $$$$$ This procedure is repeated n times to generate n classifiers that then vote to classify an instance.

In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ If looking at the actual distribution of assigned terms for these two runs, this varies between 134(!) and 5 without the tag feature, and from 48 to 1 with the tag feature.
In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.
In the experiments presented in Hulth (2003a), only the documents present in the training, validation, and test set respectively are used for calculating the collection frequency. $$$$$ The abstracts are from the years 1998 to 2002, from journal papers, and from the disciplines Computers and Control, and Information Technology.

In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ The aim of keyword assignment is to find a small set of terms that describes a specific document, independently of the domain it belongs to.
In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ The median is 30 and 12 respectively.
In the experiments discussed so far, the weights given to the positive examples are those resulting in the best performance for each individual classifier (as described in Hulth (2003a)). $$$$$ The pattern approach is also the approach which keeps the largest number of assigned terms after that the data have been pre-processed.

In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.
In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ The abstracts are from the years 1998 to 2002, from journal papers, and from the disciplines Computers and Control, and Information Technology.
In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ In this study, the main concern is the precision and the recall for the examples that have been assigned the class positive, that is how many of the suggested keywords are correct (precision), and how many of the manually assigned keywords that are found (recall).
In the experiments presented in Hulth (2003a), the automatic keyword indexing task is treated as a binary classification task, where each candidate term is classified either as a keyword or a non-keyword. $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.

Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ When inspecting manually assigned keywords, the vast majority turn out to be nouns or noun phrases with adjectives, and as discussed in Section 2, the research on term extraction focuses on noun patterns.
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ This way of defining the terms is in this paper called the chunking approach.
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ The system used allows for different ensemble techniques to be applied, meaning that a number of classifiers are generated and then combined to predict the class.
Several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al, 1999)). $$$$$ The longest chunks in the test set that were correctly assigned are five tokens long.

In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ Both the controlled terms and the uncontrolled terms may or may not be present in the abstracts.
In the statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency? inverse document frequency (tf.idf) (Salton and Buckley, 1988), among others. $$$$$ Of these were 3 340 positive, and 30 167 negative.

Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ The set of abstracts was arbitrarily divided into three sets: a training set (to construct the model) consisting of 1 000 documents, a validation set (to evaluate the models, and select the best performing one) consisting of 500 documents, and a test set (to get unbiased results) with the remaining 500 abstracts.
Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ Keywords may, for example, serve as a dense summary for a document, lead to improved information retrieval, or be the entrance to a document collection.
Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ As stemming with few exceptions led to better results on the validation set over all runs, only these values are presented in this section.
Additional features to frequency that have been experimented are e.g. relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ In this paper I have not touched upon the more intricate aspects of evaluation, but simply treated the manually assigned keywords as the gold standard.

More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ Finally in Section 4.4, the training and the evaluation of the classifiers are discussed.
More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.
More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). $$$$$ In this work, the automatic keyword extraction is treated as a supervised machine learning task, an approach first proposed by Turney (2000).

In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ Future work will examine alternative approaches to evaluation.
In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ Two important issues are how to define the potential terms, and what features of these terms are considered discriminative, i.e., how to represent the data, and consequently what is given as input to the learning algorithm.
In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ These two different approaches mean that the length of the potential terms is not limited to something arbitrary, but reflects a linguistic property.
In statistical key phrase extraction, many variations for term frequency counts have been proposed in the literature including relative frequencies (Damerau, 1993), collection frequency (Hulth, 2003), term frequency-inverse document frequency (tfidf) (Salton and Buckley, 1988), among others. $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.

Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.
Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.
Additional features to frequency that have been experimented are e.g., relative position of the first occurrence of the term (Frank et al, 1999), importance of the sentence in which the term occurs (HaCohen-Kerner, 2003), and widely studied part-of-speech tag patterns, e.g. Hulth (2003). $$$$$ As the proportion of correctly suggested keywords is considered equally important as the amount of terms assigned by a professional indexer that was detected, was assigned the value 1, thus giving precision and recall equal weights.

More linguistic knowledge has been explored by Hulth (2003). $$$$$ Automatic keyword assignment is a research topic that has received less attention than it deserves, considering keywords’ potential usefulness.
More linguistic knowledge has been explored by Hulth (2003). $$$$$ These two different approaches mean that the length of the potential terms is not limited to something arbitrary, but reflects a linguistic property.
More linguistic knowledge has been explored by Hulth (2003). $$$$$ This section begins with a discussion on the different ways the data were represented: in Section 4.1 the term selection approaches are described, and in Section 4.2 the features are discussed.

Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ The number of terms on average per document is 16.38 without the tag feature, and 9.58 with it.
Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ or mass) Initially, the same features that Frank et al. (1999) used for their domain-independent experiments were used.
Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ The largest amount of assigned terms present in the abstracts are assigned by the pattern approach without the tag feature.
Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. $$$$$ The collection used for the experiments described in this paper consists of 2 000 abstracts in English, with their corresponding title and keywords from the Inspec database.

As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ The strategy used to construct the rules is recursive partitioning (or divide-and-conquer), which has as the goal to maximise the separation between the classes for each rule.
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ For the experiments described here, only the uncontrolled terms were considered, as these to a larger extent are present in the abstracts (76.2% as opposed to 18.1%).
As shown in (Hulth, 2003), most key phrases are noun phrases. $$$$$ If looking at the actual distribution of assigned terms for these two runs, this varies between 134(!) and 5 without the tag feature, and from 48 to 1 with the tag feature.

Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ This would hopefully lead to a better precision, while recall probably would be affected negatively; the importance of recall would then need to be reconsidered.
Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ As for when syntactic information is included as a feature (in the form of the POS tag(s) assigned to the term), it is evident from the results presented in this paper that this information is crucial for assigning an acceptable number of terms per document, independent of what term selection strategy is chosen.
Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ In this paper, experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed.
Previous work has pointed out the importance of syntactic features for supervised keyword extraction (Hulth, 2003). $$$$$ For all experiments the same training, validation, and test sets were used.

Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.
Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ The set of manually assigned keywords were then removed from the documents.
Finally, in recent work, (Hulth, 2003) proposes a system for keyword extraction from abstracts that uses supervised learning with lexical and syntactic features, which proved to improve significantly over previously published results. $$$$$ In more detail, exgives a better precithan and by adding the tag(s) assigned to the term as a feature, a dramatic improvement of the results is obtained, independent of the term selection approach applied.

This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ For the experiments described here, only the uncontrolled terms were considered, as these to a larger extent are present in the abstracts (76.2% as opposed to 18.1%).
This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ The main point of this paper is that by adding linguistic knowledge to the representation (such as syntactic features), rather than relying only on (such as term frequency and grams), a better result is obtained as measured by keywords previously assigned by professional indexers.
This is a relatively popular dataset for automatic key phrase extraction, as it was first used by Hulth (2003) and later by Mihalcea and Tarau (2004) and Liu et al (2009b). $$$$$ Another possibility would be to let several persons index each document, thus getting a larger set of acceptable terms to choose from.

While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ Each abstract has two sets of keywords—assigned by a professional indexer— associated to them: a set of controlled terms, i.e., terms restricted to the Inspec thesaurus; and a set of uncontrolled terms that can be any suitable terms.
While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ The number of terms assigned must be explicitly limited by the user for these algorithms.
While Mihalcea and Tarau (2004) and our re implementations use all of these gold-standard key phrases in our evaluation, Hulth (2003 )andLiu et al address this issue by using as gold standard key phrases only those that appear in the corresponding document when computing recall. $$$$$ Treating the automatic keyword extraction as a supervised machine learning task means that a classifier is trained by using documents with known keywords.
