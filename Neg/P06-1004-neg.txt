 $$$$$ We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.
 $$$$$ Since the minimization of the normalized cut is NP-complete in the general case, researchers in vision have to approximate this computation.
 $$$$$ We modeled text segmentation as a graphpartitioning task aiming to simultaneously optimize the total similarity within each segment and dissimilarity across various segments.
 $$$$$ Combining global analysis with advanced methods for smoothing (Ji and Zha, 2003) and weighting could further boost the segmentation performance.

 $$$$$ Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
 $$$$$ The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle.
 $$$$$ We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.
 $$$$$ This criterion measures both the similarity within each partition and the dissimilarity across different partitions.

 $$$$$ Our ultimate goal is to automatically generate tables of content for lectures.
 $$$$$ Not surprisingly, text segmentation has been extensively investigated over the last decade.
 $$$$$ These counts are weighted in accordance to their distance from the current sentence: e−α(j−i)sj, where si are vectors of word counts, and α is a parameter that controls the degree of smoothing.

Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). $$$$$ Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al., 2003; Ji and Zha, 2003).
Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). $$$$$ Shi and Malik (2000) introduced the normalized-cut criterion and demonstrated its practical benefits for segmenting static images.

In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ First, in order to make it work in the new linguistic framework, we had to redefine the underlying representation and introduce a variety of smoothing and lexical weighting techniques.
In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ These counts are weighted in accordance to their distance from the current sentence: e−α(j−i)sj, where si are vectors of word counts, and α is a parameter that controls the degree of smoothing.
In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.

Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ Our method, however, is not a simple application of the existing approach to a new task.
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments.
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ In this paper we studied the impact of long-range dependencies on the accuracy of text segmentation.

Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. $$$$$ Synthetic Corpus Also as part of our analysis, we used the synthetic corpus created by Choi (2000) which is commonly used in the evaluation of segmentation algorithms.
Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. $$$$$ Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. $$$$$ In this paper we studied the impact of long-range dependencies on the accuracy of text segmentation.

This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ Following previous studies, we quantify the level of annotator agreement with the Pk measure (Gruenstein et al., 2005).3 Table 3 shows the annotator agreement scores between different pairs of annotators.
This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.
This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ This is desirable, since the lecture transcripts lack sentence boundary markers, and short utterances can skew the cosine similarity scores.
This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ We plan to investigate strategies for generating titles that will succinctly describe the content of each segment.

We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ Our ultimate goal is to automatically generate tables of content for lectures.
We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ With this constraint, we formulate a dynamic programming algorithm for exactly finding the minimum normalized multiway cut in polynomial time: However, we need to ensure that the two partitions are not only maximally different from each other, but also that they are themselves homogeneous by accounting for intra-partition node similarity.
We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.
We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ The length of the segments in this corpus ranges from three to eleven sentences.

Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ However, considering all pairwise relations in a long text may be detrimental to segmentation accuracy.
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance (Ji and Zha, 2003; Choi et al., 2001).
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework.
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ Shi and Malik (2000) introduced the normalized-cut criterion and demonstrated its practical benefits for segmenting static images.

We evaluate the performance of APS on three tasks $$$$$ Our ultimate goal is to automatically generate tables of content for lectures.
We evaluate the performance of APS on three tasks $$$$$ The edge weights, w(u, v), define a measure of similarity between pairs of nodes u and v, where higher scores indicate higher similarity.
We evaluate the performance of APS on three tasks $$$$$ Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
We evaluate the performance of APS on three tasks $$$$$ Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

We compare APS with two recent systems $$$$$ We will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole.
We compare APS with two recent systems $$$$$ However, in our case, the multi-way cut is constrained to preserve the linearity of the segmentation.
We compare APS with two recent systems $$$$$ However, considering all pairwise relations in a long text may be detrimental to segmentation accuracy.

Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ In this paper we studied the impact of long-range dependencies on the accuracy of text segmentation.
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168, grant IIS0415865, and the NSF Graduate Fellowship).
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ We will explore how the interaction between the generation and segmentation components can improve the performance of such a system as a whole.

The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ The difficulty arises, however, when transitions between topics are smooth and distributional variations are subtle.
The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ This corpus consists of a set of concatenated segments randomly sampled from the Brown corpus.
The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ Therefore, a more refined analysis of lexical distribution is needed.
We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ Disregarding this sub-topic change would impair the high-level understanding of the structure and the content of the lecture.
We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
We compare the performance of APS with that of two state-of-the-art segmenters $$$$$ Since the minimization of the normalized cut is NP-complete in the general case, researchers in vision have to approximate this computation.

In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework.
In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ Our results demonstrate that global analysis improves the segmentation accuracy and is robust in the presence of speech recognition errors.
In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ Following the first unsupervised segmentation approach by Hearst (1994), most algorithms assume that variations in lexical distribution indicate topic changes.
In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ We use the text Dotplotting representation by (Church, 1993) and plot the cosine similarity scores between every pair of sentences in the text.

 $$$$$ The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168, grant IIS0415865, and the NSF Graduate Fellowship).
 $$$$$ We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm.
 $$$$$ We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm.

(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ We wrote a detailed instruction manual for the task, with annotation guidelines for the most part following the model used by Gruenstein et al. (2005).
(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm.
(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ Any opinions, findings, conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.

Segmentation may be particularly beneficial when working with documents without overt structure $$$$$ We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.
Segmentation may be particularly beneficial when working with documents without overt structure $$$$$ The intensity of a point (i, j) on the plot indicates the degree to which the i-th sentence in the text is similar to the j-th sentence.
Segmentation may be particularly beneficial when working with documents without overt structure $$$$$ These annotators had taken the class in the past and were familiar with the subject matter under consideration.

Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ The intensity of a point (i, j) on the plot indicates the degree to which the i-th sentence in the text is similar to the j-th sentence.
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ The same words can convey varying degrees of information across different lectures, and term weighting specific to individual lectures becomes important in the similarity computation.
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ Note that annotator A operated at a level of granularity consistent with the original reference segmentation.
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ This is desirable, since the lecture transcripts lack sentence boundary markers, and short utterances can skew the cosine similarity scores.
