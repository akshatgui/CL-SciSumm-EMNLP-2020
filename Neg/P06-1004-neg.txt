 $$$$$ Our ultimate goal is to automatically generate tables of content for lectures.
 $$$$$ The same words can convey varying degrees of information across different lectures, and term weighting specific to individual lectures becomes important in the similarity computation.
 $$$$$ The smoothing is achieved by adding counts of words that occur in adjoining sentences to the current sentence feature vector.
 $$$$$ We consider the task of unsupervised lecture segmentation.

 $$$$$ Previous research has analyzed various facets of lexical distribution, including lexical weighting, similarity computation, and smoothing (Hearst, 1994; Utiyama and Isahara, 2001; Choi, 2000; Reynar, 1998; Kehagias et al., 2003; Ji and Zha, 2003).
 $$$$$ We also note that we did not try to adjust our model to optimize its performance on the synthetic data.
 $$$$$ Cosine similarity is commonly used in text segmentation (Hearst, 1994).
 $$$$$ The attractive feature of our algorithm, however, is robustness to recognition errors — testing it on the ASR transcripts caused only 7.8% relative increase in Pk measure (from 0.298 to 0.322), compared to a 13.5% relative increase for the UI system.

 $$$$$ Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments.
 $$$$$ We consider the problem of partitioning the graph into two disjoint sets of nodes A and B.
 $$$$$ With this constraint, we formulate a dynamic programming algorithm for exactly finding the minimum normalized multiway cut in polynomial time: However, we need to ensure that the two partitions are not only maximally different from each other, but also that they are themselves homogeneous by accounting for intra-partition node similarity.
 $$$$$ Of particular concern are words that may not be common in general English discourse but that occur throughout the text for a particular lecture or subject.

Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). $$$$$ We believe that the algorithm has to produce segmentations for various levels of granularity, depending on the needs of the application that employs it.
Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). $$$$$ We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.
Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). $$$$$ The same words can convey varying degrees of information across different lectures, and term weighting specific to individual lectures becomes important in the similarity computation.
Our model for disentanglement fits into the general class of graph partitioning algorithms (Roth and Yih, 2004) which have been used for a variety of tasks in NLP, including the related task of meeting segmentation (Malioutov and Barzilay, 2006). $$$$$ We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000).

In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ A transcript is split uniformly into N chunks; each chunk serves as the equivalent of documents in the tf-idf computation.
In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ We consider the problem of partitioning the graph into two disjoint sets of nodes A and B.
In our problem, however, the solution is constrained by the linearity of segmentation on transcripts, similar to that in (Malioutov and Barzilay, 2006). $$$$$ This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework.

Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ Let G = {V, E} be an undirected, weighted graph, where V is the set of nodes corresponding to sentences in the text and E is the set of weighted edges (See Figure 2).
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ We aim to minimize the cut, which is defined to be the sum of the crossing edges between the two sets of nodes.
Malioutov and Barzilay (2006) describe a dynamic programming algorithm to conduct topic segmentation for spoken documents. $$$$$ Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance (Ji and Zha, 2003; Choi et al., 2001).

Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. $$$$$ We also would like to acknowledge the MIT NLP and Speech Groups, the three annotators, and the three anonymous reviewers for valuable comments, suggestions, and help.
Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. $$$$$ We use the Pk and WindowDiff measures to evaluate our system (Beeferman et al., 1999; Pevzner and Hearst, 2002).
Malioutov and Barzilay (2006) optimize a normalized minimum-cut criteria based on a variation of the cosine similarity between sentences. $$$$$ We use the publicly available implementation of the system and optimize the system on a range of mask-sizes and different parameter settings described in (Choi, 2000) on a heldout development set of three lectures.

This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ This is expected since the segments in the synthetic dataset are randomly selected from widely-varying documents in the Brown corpus, even spanning different genres of written language.
This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ We test the system on a range of data sets, including the Physics and AI lectures and the synthetic corpus created by Choi (2000).
This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ In this section we provide further details on the graph construction process.
This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006). $$$$$ The focus of our work, however, is on an orthogonal yet fundamental aspect of this analysis — the impact of long-range cohesion dependencies on segmentation performance.

We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ We are grateful to David Karger for an illuminating discussion on the Minimum Cut algorithm.
We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ Our approach moves beyond localized comparisons and takes into account longrange cohesion dependencies.
We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ The block-level similarity function is not as sensitive to individual word errors, because the partition volume normalization factor dampens their overall effect on the derived models.
We use the evaluation source code provided by Malioutov and Barzilay (2006). $$$$$ We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.

Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ However, we can also represent graph nodes with non-overlapping blocks of words of fixed length.
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ Equations 3 and 4 capture respectively the condition that the normalized cut value of the trivial segmentation of an empty text into one segment is zero and the constraint that the first segment starts with the first node.
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ However, in our case, the multi-way cut is constrained to preserve the linearity of the segmentation.
Our corpora do not include development sets, so tuning was performed using the lecture transcript corpus described by Malioutov and Barzilay (2006). $$$$$ We plan to investigate strategies for generating titles that will succinctly describe the content of each segment.

We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisen stein and Barzilay, 2008) and identifying chapter breaks in novels. $$$$$ Our ultimate goal is to automatically generate tables of content for lectures.
We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisen stein and Barzilay, 2008) and identifying chapter breaks in novels. $$$$$ The i-th segment, Aj,k, begins at node uj and ends at node uk.
We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisen stein and Barzilay, 2008) and identifying chapter breaks in novels. $$$$$ Most unsupervised algorithms assume that fragments of text with homogeneous lexical distribution correspond to topically coherent segments.
We evaluate the performance of APS on three tasks: finding topical boundaries in transcripts of course lectures (Malioutov and Barzilay, 2006), identifying sections in medical textbooks (Eisen stein and Barzilay, 2008) and identifying chapter breaks in novels. $$$$$ Lexical Weighting Previous research has shown that weighting schemes play an important role in segmentation performance (Ji and Zha, 2003; Choi et al., 2001).

We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ The weights are computed separately for each transcript, since topic and word distributions vary across lectures.
We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ We would like to thank Masao Utiyama for providing us with an implementation of his segmentation system and Alex Gruenstein for assisting us with the NOMOS toolkit.
We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ Therefore, we discard edges between sentences exceeding a certain threshold distance.
We compare APS with two recent systems: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (Eisenstein and Barzilay, 2008). $$$$$ We wrote a detailed instruction manual for the task, with annotation guidelines for the most part following the model used by Gruenstein et al. (2005).

Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ Our ultimate goal is to automatically generate tables of content for lectures.
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ We show that optimizing our global objective enables us to detect subtle topical changes. mentation Our work is inspired by minimum-cutbased segmentation algorithms developed for image analysis.
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ The authors acknowledge the support of the National Science Foundation (CAREER grant IIS-0448168, grant IIS0415865, and the NSF Graduate Fellowship).
Malioutov and Barzilay (2006) show that the knowledge about long-range similarities between sentences improves segmentation quality. $$$$$ As Table 4 shows, the improvement is consistent across all lecture datasets.

The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ The i-th segment, Aj,k, begins at node uj and ends at node uk.
The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ Shi and Malik (2000) introduced the normalized-cut criterion and demonstrated its practical benefits for segmenting static images.
The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ Equations 3 and 4 capture respectively the condition that the normalized cut value of the trivial segmentation of an empty text into one segment is zero and the constraint that the first segment starts with the first node.
The first, compiled by Malioutov and Barzilay (2006), consists of manually transcribed and segmented lectures on Artificial Intelligence, 3 development files and 19 test files. $$$$$ First, in order to make it work in the new linguistic framework, we had to redefine the underlying representation and introduce a variety of smoothing and lexical weighting techniques.

We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008). $$$$$ We also would like to acknowledge the MIT NLP and Speech Groups, the three annotators, and the three anonymous reviewers for valuable comments, suggestions, and help.
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008). $$$$$ Combining global analysis with advanced methods for smoothing (Ji and Zha, 2003) and weighting could further boost the segmentation performance.
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008). $$$$$ The task is to partition a text into a linear sequence of topically coherent segments and thereby induce a content structure of the text.
We compare the performance of APS with that of two state-of-the-art segmenters: the Minimum Cut segmenter (Malioutov and Barzilay, 2006) and the Bayesian segmenter (EisensteinandBarzilay, 2008). $$$$$ On average, a lecture was annotated with six segments, and a typical segment corresponds to two pages of a transcript.

In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ Since the minimization of the normalized cut is NP-complete in the general case, researchers in vision have to approximate this computation.
In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ The optimal length of the block is tuned on a heldout development set.
In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.
In situations where the document boundaries are unavailable or when finer segmentation is desired, automatic techniques for document segmentation may be applied (Malioutov and Barzilay, 2006). $$$$$ In this section we provide further details on the graph construction process.

 $$$$$ The time complexity of the dynamic programming algorithm is O(KN2), where K is the number of partitions and N is the number of nodes in the graph or sentences in the transcript.
 $$$$$ We formalize segmentation as a graph-partitioning task that optimizes the normalized cut criterion.

(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ Being less structured than written text, lecture material exhibits digressions, disfluencies, and other artifacts of spontaneous communication.
(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ The smoothing is achieved by adding counts of words that occur in adjoining sentences to the current sentence feature vector.
(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ This criterion measures both the similarity within each partition and the dissimilarity across different partitions.
(Malioutov and Barzilay, 2006) uses the minimum cut model to segment spoken lectures (i.e., monologue). $$$$$ Preprocessing Before building the graph, we apply standard text preprocessing techniques to the text.

Segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts (Malioutov and Barzilay, 2006), newswire (Misra et al, 2011) or novels (Kazantseva and Szpakowicz, 2011). $$$$$ We plan to investigate strategies for generating titles that will succinctly describe the content of each segment.
Segmentation may be particularly beneficial when working with documents without overt structure: speech transcripts (Malioutov and Barzilay, 2006), newswire (Misra et al, 2011) or novels (Kazantseva and Szpakowicz, 2011). $$$$$ Another attractive property of the algorithm is its robustness to noise: the accuracy of our algorithm does not deteriorate significantly when applied to speech recognition output.

Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ We also would like to acknowledge the MIT NLP and Speech Groups, the three annotators, and the three anonymous reviewers for valuable comments, suggestions, and help.
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ This issue has been explored in the past (Ji and Zha, 2003; Utiyama and Isahara, 2001), and we will explore the existing strategies in our framework.
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ We aim to minimize the cut, which is defined to be the sum of the crossing edges between the two sets of nodes.
Malioutov and Barzilay (2006) created a corpus of course lectures segmented by four annotators, noting that the annotators operated at different levels of granularity. $$$$$ This reduction in the graph size also provides us with computational savings.
