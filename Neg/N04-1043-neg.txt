Miller et al (2004) describe a relevant technique for the latter. $$$$$ Thus, nodes higher in the tree correspond to larger word clusters, while lower nodes correspond to smaller clusters.
Miller et al (2004) describe a relevant technique for the latter. $$$$$ With 50,000 words of training, performance for the discriminative model exceeds 90F, a level not reached by the HMM until it has observed 150,000 words of training.
Miller et al (2004) describe a relevant technique for the latter. $$$$$ To achieve reasonable performance, the HMM-based technology we presented required roughly 150,000 words of annotated examples, and over a million words to achieve peak accuracy.
Miller et al (2004) describe a relevant technique for the latter. $$$$$ Recent work in discriminative methods (Lafferty et al., 2001; Sha and Pereira, 2003, Collins 2002) suggested a framework for exploiting large numbers of arbitrary input features.

We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ Fortunately, a second line of recent research provided a potential solution.
We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word.
We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ The result of running the clustering algorithm is a binary tree, where each word occupies a single leaf node, and where each leaf node contains a single word.

By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ Compared with a state-of-the-art HMM-based name finder, the presented technique requires only 13% as much annotated data to achieve the same level of performance.
By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ The following are example bit strings from the Spatter clustering algorithm:
By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ Finally, Figure 4 shows the impact of consolidating the gains from both cluster features and active learning compared to the baseline HMM.

Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. $$$$$ The confidence for the entire sentence is then just the minimum of the scores at each word position.
Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. $$$$$ The confidence for the entire sentence is then just the minimum of the scores at each word position.
Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. $$$$$ For annotated data, we used text from Sections 02-23 of the Wall Street Journal Treebank corpus that had previously been annotated with the MUC name classes.
Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. $$$$$ For example, the string Betty Mary and Bobby Lou would be tagged as PERSON-START PERSON-START NULL-START PERSON-START PERSON-CONTINUE.

We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. $$$$$ Interior nodes represent intermediate size clusters containing all of the words that they dominate.
We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. $$$$$ The confidence score we assign to a sentence is just the un-normalized difference in perceptron scores between the highest scoring theory and the second highest scoring alternative.
We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. $$$$$ We conjecture that lack of smoothing in the discriminative tagger may account for the difference.

As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.
As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ Second, we consider the impact of word clusters.
As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ Instead, we simply iterated for 5 epochs in all cases, regardless of the training set size or number of features used.
As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance.

Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ These estimation methods do not impose the same strict independence conditions as generative models.
Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.
Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ At least for the named-entity task we studied, using the method described, a single annotator could begin work after breakfast and, by lunchtime, have enough data annotated to achieve an F-score of 90.
Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ These methods seemed to have exactly the right characteristics for incorporating the statistically-correlated hierarchical word clusters we wished to exploit.

Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ It seems likely that some additional gains could be realized by alternative discriminative methods (e.g. conditional random fields estimated with conjugate-gradient training).
Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ At 1,000,000 word of training, the final combination continues to exhibit a 25% reduction in error over the baseline system (because of limitations in the experimental framework discussed earlier, active learning can provide no additional gain at this operating point).
Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ Short prefixes specify short paths from the root node and therefore large clusters.
Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ At a recent meeting, we presented name-tagging technology to a potential user.

 $$$$$ For example, if the word reported has previously been observed to follow person names, but the word announced has not yet been seen, it may be possible to guess that the word preceding announced is a person based on the fact that reported and announced occupy the same cluster.
 $$$$$ We suspect that these simplifications may have cost several tenths of a point in performance.
 $$$$$ We define the confidence at a word to be the difference between the summed forward and backward scores of the best and second best tags for that word.
 $$$$$ On the application side, it would be interesting to apply the technique to other language problems.

Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word.
Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ We evaluate the technique for named-entity tagging.
Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ The confidence score we assign to a sentence is just the un-normalized difference in perceptron scores between the highest scoring theory and the second highest scoring alternative.

It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ Sections 02-21 were used as training material, and Section 23 was used as test (note that the syntactic trees were not used in any way).
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ In the past few years, researchers have begun to view generative models as instances of a broader class of linear (or log-linear) models, and have introduced discriminative methods (e.g. conditional random fields) to estimate the model parameters.
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ These methods seemed to have exactly the right characteristics for incorporating the statistically-correlated hierarchical word clusters we wished to exploit.
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ We evaluate the technique for named-entity tagging.

This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008). $$$$$ The confidence for the entire sentence is then just the minimum of the scores at each word position.
This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008). $$$$$ We implemented the averaged perceptron training We used only a rudimentary confidence measure to perform active learning, introducing no additional features beyond those used in training and decoding.

Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ We evaluate the technique for named-entity tagging.
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ We implemented the averaged perceptron training We used only a rudimentary confidence measure to perform active learning, introducing no additional features beyond those used in training and decoding.
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ Unfortunately, the issue often comes down to whether a specific word has previously been observed in training: if the system has seen the word, it is certain, if not, it is uncertain.

Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.
Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ For annotated data, we used text from Sections 02-23 of the Wall Street Journal Treebank corpus that had previously been annotated with the MUC name classes.
Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ We evaluate the technique for named-entity tagging.

This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ The confidence score we assign to a sentence is just the un-normalized difference in perceptron scores between the highest scoring theory and the second highest scoring alternative.
This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ We evaluate the technique for named-entity tagging.
This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ A set of 16 tags was used to tag 8 name classes (the seven MUC classes plus the additional null class).

Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ Both with and without clusters, active learning exhibits a noticeable increase in learning rates.
Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ A particular word can be assigned a binary string by following the traversal path from the root to its leaf, assigning a 0 for each left branch, and a 1 for each right branch.
Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ Third, we consider the impact of active learning.

 $$$$$ Active learning is used to select training examples.
 $$$$$ To benefit from such information, however, we would need an automatic learning mechanism that could effectively exploit it.
 $$$$$ At 1,000,000 word of training, the final combination continues to exhibit a 25% reduction in error over the baseline system (because of limitations in the experimental framework discussed earlier, active learning can provide no additional gain at this operating point).

(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ We implemented the averaged perceptron training We used only a rudimentary confidence measure to perform active learning, introducing no additional features beyond those used in training and decoding.
(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.
(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ The result of running the clustering algorithm is a binary tree, where each word occupies a single leaf node, and where each leaf node contains a single word.
(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ Our model uses a total of 19 classes of features.

In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain. $$$$$ To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word.

One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. $$$$$ Then, in the hope of achieving greater efficiency, we reverseengineered the clustering software in Spatter.
One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. $$$$$ The root node defines a cluster containing the entire vocabulary.
One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. $$$$$ Our model uses a total of 19 classes of features.
