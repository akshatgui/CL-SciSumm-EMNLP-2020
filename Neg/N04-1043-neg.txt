Miller et al (2004) describe a relevant technique for the latter. $$$$$ A particular word can be assigned a binary string by following the traversal path from the root to its leaf, assigning a 0 for each left branch, and a 1 for each right branch.
Miller et al (2004) describe a relevant technique for the latter. $$$$$ We evaluate the technique for named-entity tagging.
Miller et al (2004) describe a relevant technique for the latter. $$$$$ Because our combined method promises to require substantially less training data, it may also prove useful for so-called low-density languages, where limited resources – and even more limited numbers of native speakers – are available.

We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ Because the nested clusters surrounding each word are highly correlated, it is unreasonable to treat them as independent.
We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ The work presented here extends a substantial body of previous work (Blum and Mitchell, 1998; Riloff and Jones, 1999; Lin et al., 2003; Boschee et al, 2002; Collins and Singer, 1999; Yarowsky, 1995) that all focuses on reducing annotation requirements through a combination of (a) seed examples, (b) large unannotated corpora, and (c) training example selection.
We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ Applying it to parsing would yield a rare sense of closure, knitting together the word clustering of Magerman’s (1995) Spatter parser – arguably the first successful broadcoverage statistical parser – with structural elements of the now-dominant Collins (1997) style parsers.
We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al, 2004). $$$$$ Somewhat surprisingly, the clusters continue to provide some benefit even with 1,000,000 words of training.

By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ However, the dominant trend during the past decade toward generative models has made integration of such hierarchical clusters difficult.
By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ Somewhat surprisingly, the clusters continue to provide some benefit even with 1,000,000 words of training.
By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ Recent work in discriminative methods (Lafferty et al., 2001; Sha and Pereira, 2003, Collins 2002) suggested a framework for exploiting large numbers of arbitrary input features.
By using prefixes of various lengths, we can produce clusterings of different granularities (Miller et al, 2004). $$$$$ Somewhat surprisingly, the clusters continue to provide some benefit even with 1,000,000 words of training.

Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. $$$$$ Armed with modern discriminative training methods, it seemed reasonable to us to revisit hierarchical clustering.
Following Miller et al (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity. $$$$$ The synthesis of these techniques, nevertheless, proved highly effective in achieving our primary objective of reducing the need for annotated data.

We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. $$$$$ Second, convergence rates appeared favorable, which would facilitate multiple experiments.
We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. $$$$$ However, we did not implement cross-validation to determine when to stop training.
We found that it was nontrivial to select the proper prefix lengths for the dependency parsing task; in particular, the prefix lengths used in the Miller et al (2004) work (between 12 and 20 bits) performed poorly in dependency parsing. $$$$$ These estimation methods do not impose the same strict independence conditions as generative models.

As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ We evaluate the technique for named-entity tagging.
As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ The first seven of these correspond closely to features used in a typical HMM name tagger.
As mentioned earlier, our approach was inspired by the success of Miller et al (2004), who demonstrated the effectiveness of using word clusters as features in a discriminative learning approach. $$$$$ To apply active learning, we simply To compute the confidence scores efficiently, we use a combination of the forward Viterbi and backward Viterbi scores at each word.

Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ The remaining twelve encode cluster membership.
Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ The remaining features encode essentially the same information used in the HMM, although in a slightly different form.
Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ Both with and without clusters, active learning exhibits a noticeable increase in learning rates.
Miller et al (2004) use word clusters (Brown et al, 1992) learned from unlabeled text, resulting in a performance improvement of NER. $$$$$ We define the confidence at a word to be the difference between the summed forward and backward scores of the best and second best tags for that word.

Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ The confidence for the entire sentence is then just the minimum of the scores at each word position.
Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ Our main objective, though, was not reducing error rates but rather reducing the amount of annotation required.
Given the amount of training, our results are lower than in Miller et al (2004) (an F1 of 90 with less than 25K words of training). $$$$$ Active learning is used to select training examples.

 $$$$$ We implemented this algorithm twice as part of our work.
 $$$$$ We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.
 $$$$$ The confidence for the entire sentence is then just the minimum of the scores at each word position.

Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ This final combination achieves an F-score of 90 with less than 20,000 words of training – a quantity that can be annotated in about 4 person hours – compared to 150,000 words for the HMM – a quantity requiring nearly 4 person days to annotate.
Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ However, because the pool of pre-annotated examples is limited, the results are most meaningful for small training sets.
Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ Short prefixes specify short paths from the root node and therefore large clusters.
Though all of them used the same hierarchical word clustering algorithm for the task of name tagging and reported improvements, we noticed that the clusters used by Miller et al (2004) were quite different from that of Ratinov and Roth (2009) and Turian et al (2010). $$$$$ We present a technique for augmenting annotated training data with hierarchical word clusters that are automatically derived from a large unannotated corpus. membership is encoded in features that are incorporated in a discriminatively trained tagging model.

It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ The root node defines a cluster containing the entire vocabulary.
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ In the past few years, researchers have begun to view generative models as instances of a broader class of linear (or log-linear) models, and have introduced discriminative methods (e.g. conditional random fields) to estimate the model parameters.
It has shown promise in improving the performance of many tasks such as name tagging (Miller et al, 2004), semantic class extraction (Lin et al, 2003), chunking (Ando and Zhang, 2005), coreference resolution (Bean and Riloff, 2004) and text classification (Blum and Mitchell, 1998). $$$$$ The first implementation derived directly from the description given in the Brown paper.

This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008). $$$$$ We view clustering here as a method for estimating the probabilities of low frequency events, particularly events that are likely to go unobserved in a small annotated training corpus.
This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al, 2008). $$$$$ However, we also hoped that a third technique, active learning (Cohn et al., 1996; McCallum and Nigam, 1998), would be particularly effective when used in conjunction with hierarchical word clusters.

Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ The first seven of these correspond closely to features used in a typical HMM name tagger.
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ Active learning is used to select training examples.
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ We evaluate the technique for named-entity tagging.
Previous approaches ,e.g., (Miller et al 2004) and (Koo et al 2008), have all used the Brown algorithm for clustering (Brown et al 1992). $$$$$ To achieve reasonable performance, the HMM-based technology we presented required roughly 150,000 words of annotated examples, and over a million words to achieve peak accuracy.

Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ Our model uses a total of 19 classes of features.
Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ The following are example bit strings from the Spatter clustering algorithm:
Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ Similarly, alternative clustering techniques, perhaps based on different contextual features or different distance measures, could further improve performance.
Miller et al, (2004) augmented name tagging training data with hierarchical word clusters and encoded cluster membership in features for improving name tagging. $$$$$ Specifically, active learning attempts to select examples for annotation by estimating the system's certainty about the answer, requesting a human judgment only for those cases where it is most uncertain.

This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ The remaining twelve encode cluster membership.
This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ In practice, we observed no significant differences in accuracy when using one or the other in our experiments.
This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ Clusters of various granularity are specified by prefixes of the bit strings.
This simple solution has been shown effective for named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ However, because the pool of pre-annotated examples is limited, the results are most meaningful for small training sets.

Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ Given a large annotated training set of 1,000,000 words, the technique achieves a 25% reduction in error over the state-of-the-art HMM trained on the same material.
Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ Our decision was based on three criteria.
Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ We suspect that these simplifications may have cost several tenths of a point in performance.
Semi-supervised approach has been successfully applied to named entity recognition (Miller et al, 2004) and dependency parsing (Koo et al, 2008). $$$$$ The synthesis of these techniques, nevertheless, proved highly effective in achieving our primary objective of reducing the need for annotated data.

 $$$$$ The root node defines a cluster containing the entire vocabulary.
 $$$$$ Furthermore, we did not implement features that occurred in no training instances, as was done in (Sha and Pereira, 2003).
 $$$$$ Long prefixes specify long paths and small clusters.
 $$$$$ Furthermore, we did not implement features that occurred in no training instances, as was done in (Sha and Pereira, 2003).

(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ Long prefixes specify long paths and small clusters.
(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ Our model uses a total of 19 classes of features.
(Miller et al, 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. $$$$$ First were techniques for producing word clusters from large unannotated corpora (Brown et al., 1990; Pereira et al., 1993; Lee and Pereira, 1999).

In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain. $$$$$ Second, convergence rates appeared favorable, which would facilitate multiple experiments.
In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain. $$$$$ The following are example bit strings from the Spatter clustering algorithm:
In addition, Miller et al (2004) and Freitag (2004) employ distributional and hierarchical clustering methods to improve the performance of NER within a single domain. $$$$$ Finally, Figure 4 shows the impact of consolidating the gains from both cluster features and active learning compared to the baseline HMM.

One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. $$$$$ We suspect that these simplifications may have cost several tenths of a point in performance.
One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. $$$$$ Finally, Figure 4 shows the impact of consolidating the gains from both cluster features and active learning compared to the baseline HMM.
One approach has been that proposed in both Miller et al (2004) and Freitag (2004), uses distributional clustering to induce features from a large corpus, and then uses these features to augment the feature space of the labeled data. $$$$$ Because our combined method promises to require substantially less training data, it may also prove useful for so-called low-density languages, where limited resources – and even more limited numbers of native speakers – are available.
