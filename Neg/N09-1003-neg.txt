Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ Most of the relations in WordNet are of hierarchical nature, and although other relations exist, they are far less numerous, thus explaining the good results for both WN30 and WN30g on similarity, but the bad results of WN30 on relatedness.
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ The complementarity of our methods was exploited by a supervised learner, producing the best results so far for RG and WordSim353.

Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5
Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ Distributional similarity was effectively used to cover out-of-vocabulary items in the WordNet-based measure providing our best unsupervised results.
Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ This work pioneers cross-lingual extension and evaluation of both distributional and WordNet-based measures.

Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ For each word in the pair we first compute a personalized PageRank vector of graph G (Haveliwala, 2002).
Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ Distributional similarity was effectively used to cover out-of-vocabulary items in the WordNet-based measure providing our best unsupervised results.
Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ Previous work suggests that distributional similarities suffer from certain limitations, which make them less useful than knowledge resources for semantic similarity.

Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ All calculations are done in parallel sharding by dimension, and it is possible to calculate all pairwise similarities of the words in the test sets very quickly on this corpus using the MapReduce infrastructure.
Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ In order to calculate similarities in a cross-lingual setting, where some of the words are in a language l other than English, the following algorithm is used: models and distributional models.
Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ Most of the relations in WordNet are of hierarchical nature, and although other relations exist, they are far less numerous, thus explaining the good results for both WN30 and WN30g on similarity, but the bad results of WN30 on relatedness.
Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ While MCR16 is close to WN30g for the RG dataset, it lags well behind on WordSim353.

Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ Although the vocabulary of WordNet is very extensive, applications are bound to need the similarity between words which are not included in WordNet.
Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation.
Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ Overall the distributional context-window approach performs best in the RG, reaching 0.89 correlation, and both WN30g and the combination of context windows and syntactic context perform best on WordSim353.

In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ Each of these outputs is a ranking of word pairs, and we implemented an oracle that chooses, for each pair, the rank that is most similar to the rank of the pair in the gold-standard.
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ Once the vectors have been obtained, the frequency for each dimension in every vector is weighted using the other vectors as contrast set, with the k2 test, and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms.
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ In order to calculate the semantic similarity between the words in a set, we have used a vector space model, with the following three variations: In the bag-of-words approach, for each word w in the dataset we collect every term t that appears in a window centered in w, and add them to the vector together with its frequency.

 $$$$$ The supervised combination produces the best results reported so far.
 $$$$$ Table 7 shows the results of related work on MC that was available to us, including our own.
 $$$$$ Table 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the interval at 0.95 of confidence6.

We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ The first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms.
We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ This explains why this method performed better for the RG dataset.
We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ More details of our algorithm can be found in (Agirre and Soroa, 2009).

Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). $$$$$ We have used two standard datasets.
Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). $$$$$ In their case, the mapping function was exp (−'4 ), which was chosen empirically.

In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ We see this as an explanation why this model performed better than the context window approach for WordSim353, where annotators were instructed to provide high ratings to related terms.
In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information Retrieval or CL sponsored search.
In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5

 $$$$$ This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words.
 $$$$$ Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations.
 $$$$$ Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.
 $$$$$ In our believe Pearson is less informative, as the Pearson correlation suffers much when the scores of two systems are not linearly correlated, something which happens often given due to the different nature of the techniques applied.

 $$$$$ Each of these outputs is a ranking of word pairs, and we implemented an oracle that chooses, for each pair, the rank that is most similar to the rank of the pair in the gold-standard.
 $$$$$ The only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, probably because of the dense, manually distilled knowledge contained in Wikipedia.
 $$$$$ Once the vectors have been obtained, the frequency for each dimension in every vector is weighted using the other vectors as contrast set, with the k2 test, and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms.

While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ The complementarity of our methods was exploited by a supervised learner, producing the best results so far for RG and WordSim353.
While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ Most of the relations in WordNet are of hierarchical nature, and although other relations exist, they are far less numerous, thus explaining the good results for both WN30 and WN30g on similarity, but the bad results of WN30 on relatedness.

We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ We first present the WordNet-based method, followed by the distributional methods.
We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ Their results are probably worse due to the smaller data size (they used 270,000 documents) and the differences in the calculation of the similarities.
We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ Therefore, terms that are topically related can appear in the same textual passages and will get high values using this model.
We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ Therefore, true synonyms and hyponyms/hyperonyms will receive high similarities, whereas terms related topically or based on any other semantic relation (e.g. movie and star) will have lower scores.

This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ In order to study this effect, ideally, we would have two versions of the dataset, where annotators were given precise instructions to distinguish similarity in one case, and relatedness in the other.
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ We first present the WordNet-based method, followed by the distributional methods.
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ The complementarity of our methods was exploited by a supervised learner, producing the best results so far for RG and WordSim353.
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ Although the vocabulary of WordNet is very extensive, applications are bound to need the similarity between words which are not included in WordNet.

Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ Most similarity researchers have published their complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation.
Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ For each word w we collect a template of the syntactic context.
Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ In this case, both similarity and rell never forget the' on his face when grin,2,smile,10 he had a giant' on his face and grin,3,smile,2 room with a huge' on her face and grin,2,smile,6 the state of every' will be updated every automobile,2,car,3 repair or replace the' if it is stolen automobile,2,car,2 located on the north' of the Bay of shore,14,coast,2 areas on the eastern' of the Adriatic Sea shore,3,coast,2 Thesaurus of Current English' The Oxford Pocket Thesaurus slave,3,boy,5,shore,3,string,2 latedness are annotated without any distinction.
Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ Section 5 presents some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet similarity, the contrast between similarity and relatedness, and the combination of methods.

For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ In fact, there are nine pairs which returned null similarity for this reason.
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora.
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ Our distributional methods also outperform all other corpus-based methods.
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ Given the recent availability of the disambiguated gloss relations for WordNet 3.03, we also used a version which incorporates these relations.

Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ In the case of the distributional approaches, the fall in performance was caused by the translations, as only 61% of the words were translated into the original word in the English datasets.
Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ The results for WordSim353 show that WN30g is the best for this dataset, with the rest of the methods falling over 10 percentage points relative to the monolingual experiment.
Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.

 $$$$$ The algorithm for WordNet-base similarity and the necessary resources are publicly available8.
 $$$$$ We have used two standard datasets.
 $$$$$ Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.
 $$$$$ The bag-of-words approach tends to group together terms that can have a similar distribution of contextual terms.

 $$$$$ We show that the use of disambiguated glosses allows for the best published results for WordNet-based systems on the WordSim353 dataset, mainly due to the better modeling of relatedness (as opposed to similarity).
 $$$$$ We then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors.
 $$$$$ This paper presents and compares WordNetbased and distributional similarity approaches.
 $$$$$ Though not totally comparable, if we compute the correlation over pairs covered in WordNet alone, the correlation would drop only 2 percentage points.
