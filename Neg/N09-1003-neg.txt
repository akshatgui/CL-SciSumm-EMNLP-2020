Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ For RG, MCR16 and the context windows methods drop only 5 percentage points, showing that cross-lingual similarity is feasible, and that both cross-lingual strategies are robust.
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007).
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ Results improve from 0.5 Spearman correlation up to 0.65 when increasing the corpus size three orders of magnitude, although the effect decays at the end, which indicates that we might not get further gains going beyond the current size of the corpus.
Agirre et al (2009) split thews set into similarity (wss) and relatedness (wsr) subsets. $$$$$ This paper has presented two state-of-the-art distributional and WordNet-based similarity measures, with a study of several parameters, including performance on similarity and relatedness data.

Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ We then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors.
Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ In the case of the distributional approaches, the fall in performance was caused by the translations, as only 61% of the words were translated into the original word in the English datasets.
Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ A simple translation strategy also yields good results for distributional methods. automobile, car 3.92 62 journey, voyage 3.84 54 gem, jewel 3.84 61 boy, lad 3.76 57 coast, shore 3.7 53 asylum, madhouse 3.61 45 magician, wizard 3.5 49 midday, noon 3.42 61 furnace, stove 3.11 50 food, fruit 3.08 47 bird, cock 3.05 46 bird, crane 2.97 38 implement, tool 2.95 55 brother, monk 2.82 42 crane, implement 1.68 26 brother, lad 1.66 39 car, journey 1.16 37 monk, oracle 1.1 32 food, rooster 0.89 3 coast, hill 0.87 34 forest, graveyard 0.84 27 monk, slave 0.55 17 lad, wizard 0.42 13 coast, forest 0.42 18 cord, smile 0.13 5 glass, magician 0.11 10 rooster, voyage 0.08 1 noon, string 0.08 5
Interestingly, the same overall result pattern is observed if we limit evaluation to the WordSim subsets that Agirre et al (2009) have identified as semantically similar (e.g., synonyms or coordinate terms) and semantically related (e.g., meronyms or topically related concepts). $$$$$ Given the lack of such datasets, we devised a simpler approach in order to reuse the existing human judgements.

Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ Finally, we pioneer cross-lingual similarity, showing that our methods are easily adapted for a cross-lingual task with minor losses.
Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ Distributional similarities have proven to be competitive when compared to knowledgebased methods, with context windows being better for similarity and bag of words for relatedness.
Spearman's rank correlation (ρ) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models. $$$$$ We then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors.

Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ Using a search engine to calculate similarities between words has the drawback that the data used will always be truncated.
Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ For each word in the pair we first compute a personalized PageRank vector of graph G (Haveliwala, 2002).
Multiple prototypes improve Spearman correlation on WordSim-353 compared to previous methods using the same underlying representation (Agirre et al., 2009). $$$$$ We show that the use of disambiguated glosses allows for the best published results for WordNet-based systems on the WordSim353 dataset, mainly due to the better modeling of relatedness (as opposed to similarity).

Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ We have shown that closely aligned wordnets provide a natural and effective way to compute cross-lingual similarity with minor losses.
Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.
Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations.
Earlier work on reducing the polysemy of sense inventories has considered WordNet-based sense relatedness measures (Mihalcea and Moldovan, 2001) and corpus-based vector representations of word senses (Agirre and Lopez, 2003; McCarthy, 2006). $$$$$ We first present the WordNet-based method, followed by the distributional methods.

In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ Section 6 presents related work, and finally, Section 7 draws the conclusions and mentions future work.
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ We can then compute the personalized PageRank for each of car and coche on the same underlying graph, and then compare the similarity between both probability distributions.
In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2,000 cores. $$$$$ Section 4 is devoted to the evaluation and results on the monolingual and crosslingual tasks.

 $$$$$ In our believe Pearson is less informative, as the Pearson correlation suffers much when the scores of two systems are not linearly correlated, something which happens often given due to the different nature of the techniques applied.
 $$$$$ Table 2 illustrates a few examples of context collected.
 $$$$$ Here, results improve even more with data size, probably due to the sparse data problem collecting 8-word context windows if the corpus is not large enough.

We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ Distributional similarity was effectively used to cover out-of-vocabulary items in the WordNet-based measure providing our best unsupervised results.
We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ The inter-tagger agreement rate was 0.80, with a Kappa score of 0.77.
We also compare our results against the state-of-the-art results (Agirre) for distributional similarity (Agirre et al, 2009). $$$$$ This paper has presented two state-of-the-art distributional and WordNet-based similarity measures, with a study of several parameters, including performance on similarity and relatedness data.

Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). $$$$$ Table 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the interval at 0.95 of confidence6.
Recent systems have, however, shown improved results using extremely large corpora (Agirre et al, 2009), and existing large-scale resources such as Wikipedia (Strube and Ponzetto, 2006). $$$$$ The first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms.

In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ Given a pair of words and a graph-based representation of WordNet, our method has basically two steps: We first compute the personalized PageRank over WordNet separately for each of the words, producing a probability distribution over WordNet synsets.
In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02.
In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ The first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might appear between the terms.
In a recent paper, Agirre et al (2009) parsed 4 billion documents (1.6 Terawords) crawled from the web, and then used a search function to extract syntactic relations and context windows surrounding key words. $$$$$ The second dataset, WordSim3535 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 human judgements.

 $$$$$ We have used two standard datasets.
 $$$$$ The systems that rely on collecting snippets are also limited by the maximum number of documents returned per query, typically around a thousand.
 $$$$$ The complementarity of our methods was exploited by a supervised learner, producing the best results so far for RG and WordSim353.
 $$$$$ Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics.

 $$$$$ In this paper, we explore both families.
 $$$$$ We then compare how similar these two discrete probability distributions are by encoding them as vectors and computing the cosine between the vectors.
 $$$$$ Although the vocabulary of WordNet is very extensive, applications are bound to need the similarity between words which are not included in WordNet.
 $$$$$ For RG, MCR16 and the context windows methods drop only 5 percentage points, showing that cross-lingual similarity is feasible, and that both cross-lingual strategies are robust.

While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ The algorithm for WordNet-base similarity and the necessary resources are publicly available8.
While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ Most similarity researchers have published their complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using different correlation.
While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.
While our results are not competitive with the best corpus based methods, we can note that our current corpus is an order of magnitude smaller - 2 million sentences versus 1 million full Wikipedia articles (Gabrilovich and Markovitch, 2007) or 215MB versus 1.6 Terabyte (Agirre et al, 2009). $$$$$ Therefore, true synonyms and hyponyms/hyperonyms will receive high similarities, whereas terms related topically or based on any other semantic relation (e.g. movie and star) will have lower scores.

We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ Each of our methods independently provide the best results in their class on the RG and WordSim353 datasets, and a supervised combination of them yields the best published results on all datasets.
We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ For Syn, the window size is actually the tree-depth for the governors and descendants.
We also evaluated our scores separately on the semantically similar versus the semantically related subsets of WordSim-353 following Agirre et al (2009). $$$$$ The strengths and weaknesses of each approach regarding similarity and relatedness tasks are discussed, and a combination is presented.

This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0.
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ In this case, all punctuation symbols are replaced with a special token, to unify patterns like, the <term> said to and ’ the <term> said to.
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ Table 3 shows the results for the English-Spanish cross-lingual datasets.
This is remarkable as different methods tend to be more appropriate to calculate either one or the other (Agirre et al, 2009). $$$$$ For the results using the WordSim353 corpus, we show the results of the bag-of-words approach with context size 10.

Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ Though not totally comparable, if we compute the correlation over pairs covered in WordNet alone, the correlation would drop only 2 percentage points.
Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ In this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus.
Agirre et al (2009) compared DS approaches with WordNet-based methods in computing word similarity and relatedness; and they also studied the combination of them. $$$$$ This information is then encoded as a contextual template.

For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ This explains why this method performed better for the RG dataset.
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ Several studies indicate that the human scores consistently have very high correlations with each other (Miller and Charles, 1991; Resnik, 1995), thus validating the use of these datasets for evaluating semantic similarity.
For instance, Agirre et al (2009) derive a WordNet-based measure using PageRank and combined it with several corpus based vector space models using SVMs. $$$$$ A complete run takes around 15 minutes on 2,000 cores.

Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ The algorithm for WordNet-base similarity and the necessary resources are publicly available8.
Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ The number in parenthesis in Table 1 for WordSim353 shows the results for the 344 remaining pairs.
Examining the relations between the words in each pair, Agirre et al (2009) further split this dataset into similar pairs (WS-sim) and related pairs (WS-rel), where the former contains synonyms, antonyms, identical words and hyponyms/hypernyms and the latter capture other word relations. $$$$$ For example, a training instance using two unsupervised classifiers is 0.001364, 31, 0.327515, 64, 0.084805, 57, 0.109061, 59, negative meaning that the similarities given by the first classifier to the two pairs were 0.001364 and 0.327515 respectively, which ranked them in positions 31 and 64.

 $$$$$ The paper is structured as follows.
 $$$$$ Section 5.2 shows a proposal to overcome this limitation.
 $$$$$ All in all, our supervised combination gets the best published results on this dataset.

 $$$$$ Comparison on the WordSim353 dataset is easier, as all researchers have used Spearman.
 $$$$$ Distributional similarities have proven to be competitive when compared to knowledgebased methods, with context windows being better for similarity and bag of words for relatedness.
 $$$$$ This explains why this method performed better for the RG dataset.
 $$$$$ The paper is structured as follows.
