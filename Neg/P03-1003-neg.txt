Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ For example, given the question “Where was Sartre born?” we will select the following factoids: Up to now, we have collected about 100,000 question-factoid pairs.
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ Once one has these two modules, one has a QA system because finding the answer to a question Q amounts to selecting the sub-string SA of highest score.
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ For example, from the word “age” and its gloss “a historic period”, we create the dictionary entries “age - historic” and “age – period”.
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ Assume that we want to explain why “1977” in sentence S in Figure 1 is a good answer for the question “When did Elvis Presley die?” To do this, we build a noisy channel model that makes explicit how answer sentence parse trees are mapped into questions.

At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ For example, the systems developed at IBM and ISI map questions and answer sentences into parse trees and surfacebased semantic labels and measure the similarity between questions and answer sentences in this syntactic/semantic space, using QA-motivated metrics.
At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ This work was supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program under contract number MDA908-02-C-0007.
At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ In Section 4, we describe how we use the learned models to answer factoid questions, we evaluate the performance of our system using a variety of experimental conditions, and we compare it with a rule-based system that we have previously used in several TREC evaluations.

Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). $$$$$ We also hope that our paper will reduce the high barrier to entry that is explained by the complexity of current QA systems and increase the number of researchers working in this field: because our QA system uses only publicly available software components (an IR engine; a parser; and a statistical MT system), it can be easily reproduced by other researchers.
Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). $$$$$ Acknowledgments.
Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). $$$$$ To make use of this technique, we extend our training data set by expanding every questionanswer pair Q-SA to a list (Qr-SA), Qr ⊂ Θ where Θ is the set of question reformulations.2 We also expand in a similar way the answer candidates in the test corpus.

a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ For the KM corpus, the relatively low MRRs are explained by two factors: (i) for this corpus, each evaluation pattern consists of only one string – the original answer; (ii) the KM questions are more complex than TREC questions (What piece of furniture is associated with Modred, Percival, Gawain, Arthur, and Lancelot?).
a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ Using reformulations improved the performance of our system on the TREC 2002 test set while it was not beneficial for the KM test set (see Table 4).
a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system.

In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ It is not difficult to see by now that these logical relations can be represented graphically as alignments between question and answer terms (see Figure 4).
In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ (iii) The third data set consists of 2381 question/answer pairs collected from http://www.quiz-zone.co.uk.
In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ In Section 5, we demonstrate that the framework we propose is flexible enough to accommodate a wide range of resources and techniques that have been employed in state-of-the-art QA systems.
In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ It is remarkable that a statistical machine translation system can do so well in a totally different context, in question answering.

The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ However, building dedicated systems that employ more sophisticated, QA-motivated generative stories is likely to yield significant improvements.
The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ We use the same method to automatically enhance this set by retrieving from the web sentences containing answers to the questions.
The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ The KM questions tend to be longer and quite different in style compared to the TREC questions.
The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ In order to rewrite this tree into a question, we assume the following generative story: according to a distortion table d, in order to obtain a well-formed, grammatical question.

In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed $$$$$ In testing, for every question Q, we select factoids that overlap sufficiently enough with Q as sentences that potentially contain the answer.
In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed $$$$$ We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.

We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ The first set consists of the 500 questions used at TREC 2002; the second set consists of 500 questions that were randomly selected from the Knowledge Master (KM) repository (http://www.greatauk.com).
We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ Condition c) brings the sentence closer to the question by compacting portions that are syntactically far from question terms and answer.
We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.

As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ Here are some of the test cases we consider for the question-answer pair in Figure 1: Q: When did Elvis Presley die ?
As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ Q: When did Elvis Presley die ?
As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ We also hope that our paper will reduce the high barrier to entry that is explained by the complexity of current QA systems and increase the number of researchers working in this field: because our QA system uses only publicly available software components (an IR engine; a parser; and a statistical MT system), it can be easily reproduced by other researchers.
As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ SAj: Presley died PP PP PP , and NP return by A_NP NP .

In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ The generative story that our noisy-channel employs is rudimentary; we have chosen it only because we wanted to exploit to the best extent possible existing software components (GIZA).
In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ Each questionanswer pair results in one training example.
In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ We also hope that our paper will reduce the high barrier to entry that is explained by the complexity of current QA systems and increase the number of researchers working in this field: because our QA system uses only publicly available software components (an IR engine; a parser; and a statistical MT system), it can be easily reproduced by other researchers.

For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ Our QA system implements the full-blown Model 4 statistical model described by Brown et al. generative story.
For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ In 1867, Secretary of State William H. Seward arranged for the United-States to purchase Alaska for 2 cents per acre.
For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ However, one has to recognize that the reliance of our system on publicly available components is not ideal.
For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ They access external resources such as the WordNet (Hovy et al., 2001, Pasca and Harabagiu, 2001, Prager et al., 2001), the web (Brill et al., 2001), structured, and semistructured databases (Katz et al., 2001; Lin, 2002; Clarke, 2001).

Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ They contain feedback loops, ranking, and re-ranking modules.
Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ It is the examples generated through this procedure that we use to estimate the parameters of our model.
Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ SAj: Presley died PP PP PP , and NP return by A_NP NP .
Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ In order to rewrite this tree into a question, we assume the following generative story: according to a distortion table d, in order to obtain a well-formed, grammatical question.
