Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ And systems such as those developed by IBM and BBN map questions and answers into feature sets and compute the similarity between them using maximum entropy models that are trained on question-answer corpora.
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ Q: When did Elvis Presley die ?
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ Condition b) helps learn answer types.
Echihabi and Marcu (2003) have developed a noisy-channel model for QA, which explains how a sentence containing an answer to a given question can be rewritten into that question through a sequence of stochastic operations. $$$$$ This model explains how a given sentence SA that contains an answer sub-string A to a question Q can be rewritten into Q through a sequence of stochastic operations.

At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ Each questionanswer pair results in one training example.
At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ The KM questions tend to be longer and quite different in style compared to the TREC questions.
At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources.
At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). $$$$$ We also hope that our paper will reduce the high barrier to entry that is explained by the complexity of current QA systems and increase the number of researchers working in this field: because our QA system uses only publicly available software components (an IR engine; a parser; and a statistical MT system), it can be easily reproduced by other researchers.

Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). $$$$$ Our QA system is straightforward.
Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). $$$$$ SA: Presley died PP PP in A_DATE, and SNT.
Examples of 22 cases where the bag-of-words approach fails abound in QA literature; here we borrow an example used by Echihabi and Marcu (2003). $$$$$ We also show that the model we propose is flexible enough to accommodate within one mathematical framework many QA-specific resources and techniques, which range from the exploitation of WordNet, structured, and semi-structured databases to reasoning, and paraphrasing.

a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ Acknowledgments.
a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ We found out that these pairs cover only 24 of the 500 TREC 2002 questions.
a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ Consider, for example, the question “Who is the leader of France?” The sentence “Henri Hadjenberg, who is the leader of France’s Jewish community, endorsed confronting the specter of the Vichy past” overlaps with all question terms, but it does not contain the correct answer; while the sentence “Bush later met with French President Jacques Chirac” does not overlap with any question term, but it does contain the correct answer.
a noisy-channel model which selects the most likely answer to a question (cf. (Echihabi and Marcu, 2003)). $$$$$ Although this view is not made explicit by QA researchers, it is implicitly present in all systems we are aware of.

In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ SAj: Presley died PP PP PP , and NP return by A_NP NP .
In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ We introduce a probabilistic noisychannel model for question answering and we show how it can be exploited in the context of an end-to-end QA system.
In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ For example, the systems developed at IBM and ISI map questions and answer sentences into parse trees and surfacebased semantic labels and measure the similarity between questions and answer sentences in this syntactic/semantic space, using QA-motivated metrics.
In contrast, Echihabi and Marcu (2003) introduce an SMT-based method for extracting the concrete answer in factoid QA. $$$$$ Consider, for example, the automatically derived answer sentence parse tree in Figure 1, which associates to nodes both syntactic and shallow semantic, named-entity-specific tags.

The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ However, one has to recognize that the reliance of our system on publicly available components is not ideal.
The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ However, one has to recognize that the reliance of our system on publicly available components is not ideal.
The approach of Echihabi and Marcu (2003) that uses translation probabilities to rank the answers achieves higher results on the same data set (an MRR of 0.325 versus our 0.141). $$$$$ Adding WordNet synonyms and glosses improved slightly the performance on the KM questions.

In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. $$$$$ The readers familiar with the statistical machine translation (SMT) literature should recognize that steps 3 to 5 are nothing but a one-to-one reproduction of the generative story proposed in the SMT context by Brown et al. (see Brown et al., 1993 for a detailed mathematical description of the model and the formula for computing the probability of an alignment and target string given a source string).1 answering To simplify our work and to enable us exploit existing off-the-shelf software, in the experiments we carried out in conjunction with this paper, we assumed a flat distribution for the two steps in our 1 The distortion probabilities depicted in Figure 1 are a simplification of the distortions used in the IBM Model 4 model by Brown et al.
In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. $$$$$ Consider, for example, the following two factoid-question template pairs: Qt1: What is the capital of _c?
In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. $$$$$ However, building dedicated systems that employ more sophisticated, QA-motivated generative stories is likely to yield significant improvements.

We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ (1993).
We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ Figure 2 represents graphically the conditions that led to this training example being generated.
We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ We chose this watered down representation only for illustrative purposes.
We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). $$$$$ We believe that our work will lead to a better understanding of the similarities and differences between the approaches that make up today’s QA research landscape.

As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ We believe that our work will lead to a better understanding of the similarities and differences between the approaches that make up today’s QA research landscape.
As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ Consider, for example, the question “Who is the leader of France?” The sentence “Henri Hadjenberg, who is the leader of France’s Jewish community, endorsed confronting the specter of the Vichy past” overlaps with all question terms, but it does not contain the correct answer; while the sentence “Bush later met with French President Jacques Chirac” does not overlap with any question term, but it does contain the correct answer.
As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ The probability P(Q  |SA) is computed by multiplying the probabilities in all the steps of our generative story (Figure 1 lists some of the factors specific to this computation.)
As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. $$$$$ Voting is mandatory for all Argentines aged over 18 What is the legal age to vote in Argentina?

In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ A resolution-based module then proves that the question logically follows from the answer using a set of axioms that are automatically extracted from the WordNet glosses.
In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ In training, we simply add all these pairs to the training data set.
In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ Assume that we want to explain why “1977” in sentence S in Figure 1 is a good answer for the question “When did Elvis Presley die?” To do this, we build a noisy channel model that makes explicit how answer sentence parse trees are mapped into questions.
In (Echihabi and Marcu, 2003) a noisy channel model for Q/A was introduced. $$$$$ To circumvent this limitation of word-based similarity metrics, QA researchers have developed methods through which they first map questions and sentences that may contain answers in different spaces, and then compute the “similarity” between them there.

For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ St2: _p died of causeDeath(_p).
For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ Our dictionary creation procedure is a crude version of the axiom extraction algorithm described by Moldovan et al. (2002); and our exploitation of the glosses in the noisy-channel framework amounts to a simplified, statistical version of the semantic proofs implemented by LCC.
For the experiments, we used PropBank (www.cis.upenn.edu/? ace) along with Penn TreeBank3 2 (www.cis.upenn.edu/? tree bank) (Echihabi and Marcu, 2003). $$$$$ Assume that the question-answer pair in Figure 1 appears in our training corpus.

Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ Our baseline is a state of the art QA system, QA-base, which was ranked from second to seventh in the last 3 years at TREC.
Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ Given their complexity, it is often difficult (and sometimes impossible) to understand what contributes to the performance of a system and what doesn’t.
Echihabi and Marcu (2003) align all paths in questions with trees for heuristically pruned answers. $$$$$ Our noisy-channel system outperforms a stateof-the-art rule-based QA system that uses similar resources.
