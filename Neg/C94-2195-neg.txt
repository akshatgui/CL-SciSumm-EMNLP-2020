This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a). $$$$$ (\]iv{;\]\] a eh{}ice of features, they train ;t prol}abi/islie model For I)r(Sitclcoutext), and in {.esl.ing choose Site :-: v oP Site = n l a~ccordi\]lg I;o which has {he higher eomlitional probal)i\]ity.
This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a). $$$$$ ~' For reasons of ~: u n- t ime c\[lk:icn(:y, transfonmLl, ions tmddng re\['crence 1:o tile classes of both n l a,nd n2 were IlOI; p(~l?lXiitl, tR(I. GI;or expository purposes, the u.iqm'.

Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). $$$$$ Accuracy improved to 75.8% r using the larger training set, still significantly lower than accuracy obtained us-- lag tam tl:ansformal;ion-based approach.
Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). $$$$$ Figure 2 shows a plot of test-set accuracy as a function of the nulnber of training instances.

Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ They also report that a (leeision tree mode/ eon- st\];u(:t~d using the same features m,d I,i;aining data ac\[lieve{I I)erformanee of 77.71~, (}n t\[:e same I.est set, A llUll ll)el' o\[' other reseaPehers have exl)lored eorlms-I)ased approaches I;o l)repositional phrase attaehmet, t disaml)iguation tM~t n\]~d{c use of word classes, l"or example, Weisehed{q cl al.
Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ I}iguity.
Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ A set of silnple rules is learned au- tomatically to try to prediet proper attachment based on any of a number of possible contextual giles.
Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ sparseness i similar to tllat of (l{,es93b, li, l\[93), where {~ method ix proposed for using WordNet in conjunction with a corpus to ohtain class-based statisl, ie,q.

This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. $$$$$ In order to compare the two approaches, we reimplemen:ed the ~flgorithm fi'om (IIR.91) and tested it using the same training and test set used for the above experiments.
This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. $$$$$ plateau, suggesting that more training data wonld lead to further improvements.

Brill and Resnik (1994) used the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples. $$$$$ Since the tr;ulsformation-based al)l/roach with classes cCm gener~dize ill a way that the approach without classes is ml~l)le to, we woldd expect f'cwer l;ransf'ormal;ions to be necessary, l!;xperimeaH, ally, this is indeed the case.
Brill and Resnik (1994) used the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples. $$$$$ This exlw, r iment also demonstrates how rely \[~?~l;ul:e-based lexicon or word classiflcat, ion scheme cau triviaJly be incorlJorated into the learner, by exLencling l;ransfot'nlal,iolls to allow thent to make l'efel'eAlc(?

For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNetsynsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. $$$$$ Since llindle and Rooth's approach does not make reference to n2, we re-ran the transformation-learner disalk)wing all transforma- tions that make reference ~o n2.
For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNetsynsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. $$$$$ I:ithm begins with n,ore refiued input.
For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNetsynsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. $$$$$ Applying l.hese transt'ormai.ions to the test set l'eslllted in a.n accuracy of' 81.8%.

This huge number of tokens can be explained by the fact that the lexicon used for tokenization and tagging integrates many multi-word expressions which are not part of these mantic lexicon for (Brill and Resnik, 1994) and 0.77 for (LauerandDras, 1994)), but a direct comparison is difficult inasmuch as only three-word sequences (V N P, for (Brill and Resnik, 1994) and N N N for (Lauer and Dras, 1994)) were used for evaluation in those works, and the language studied is English. $$$$$ h> stead, semanl, ic class information is an attracLive alternative.
This huge number of tokens can be explained by the fact that the lexicon used for tokenization and tagging integrates many multi-word expressions which are not part of these mantic lexicon for (Brill and Resnik, 1994) and 0.77 for (LauerandDras, 1994)), but a direct comparison is difficult inasmuch as only three-word sequences (V N P, for (Brill and Resnik, 1994) and N N N for (Lauer and Dras, 1994)) were used for evaluation in those works, and the language studied is English. $$$$$ As in the experiments here, their statistical model also makes use of a 4-tuple context (v, c<l, p, n2), and can use the identit.ies of the words, class inl'or- marion (tbr them, wdues of any of the class bits), rThe difference between these results ~nd tile result they quoted is likely due to a much bLrger training set used in their origimd experiments.
This huge number of tokens can be explained by the fact that the lexicon used for tokenization and tagging integrates many multi-word expressions which are not part of these mantic lexicon for (Brill and Resnik, 1994) and 0.77 for (LauerandDras, 1994)), but a direct comparison is difficult inasmuch as only three-word sequences (V N P, for (Brill and Resnik, 1994) and N N N for (Lauer and Dras, 1994)) were used for evaluation in those works, and the language studied is English. $$$$$ I}iguity.

In later stages of processing, a corpus-based approach (Brill and Resnik, 1994) is used to deal with ambiguities that cannot be solved with syntactic information only, in particular attachments of prepositional phrases, gerunds and infinitive constructions. $$$$$ There are a number of ways to address the sparse data problem.

For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase $$$$$ \]u addit ion, we have shown how the l;raus\['orln~Lion-based l arner can casity be e?.- tended to incorporate word-class i/fformatiou.
For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase $$$$$ oped independently by Ratnaparkhi and I/,oukos (l{R94), since they also used training and test datt~ drawn from the Penn Treebank's Wall Street Jour- nal corpus.

A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. $$$$$ It is possihle Lo compare; the results described here with a somewhat similar approach devel-.
A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. $$$$$ Ilow- ever, this l)eeomes less of a probh'.m as atmotated eorl}ora beeolne increasingly available, and sug- gests the comhinat ion o1:' supexvised and uusuper vised methods as a.u ilfl;eresth G ave\]me \['or \['urther rese;ire\] \[.

 $$$$$ h> stead, semanl, ic class information is an attracLive alternative.

 $$$$$ oped independently by Ratnaparkhi and I/,oukos (l{R94), since they also used training and test datt~ drawn from the Penn Treebank's Wall Street Jour- nal corpus.
 $$$$$ P is i~ and N\] is 15 N1 V \[written co.mmlu~ication\] 16 N1 V l ) is wilhoul 17 N1 V P is during 18 N 1 V 19 NI V. 20 N1.

 $$$$$ Baseline l l indle and Rooth (IIR91, 1\[17{93) describe corpus-based approach to disambiguating between prepositional phrase attachlnent to the main verb and to the object nonn phrase (such as in the ex- ample sentence above).
 $$$$$ / - -F . . .

Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ They also report that a (leeision tree mode/ eon- st\];u(:t~d using the same features m,d I,i;aining data ac\[lieve{I I)erformanee of 77.71~, (}n t\[:e same I.est set, A llUll ll)el' o\[' other reseaPehers have exl)lored eorlms-I)ased approaches I;o l)repositional phrase attaehmet, t disaml)iguation tM~t n\]~d{c use of word classes, l"or example, Weisehed{q cl al.

Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. $$$$$ COl\[lD~/l:iSOll, Conc lus ions The.
Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. $$$$$ (BI}V91) bol,\]l deseril)e the use of lnanual ly coustrueted, donmhv Sl){~eitic word classes together with cori}us-tmsed si,t~tisties in o f d{2r to resolve i)rel)ositional 1)hrase a.t, taehlllellt &Ill-.
Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. $$$$$ First, the train- ing set is processed according to the start state annotator, in this case attaching all prepositional phrases low (attached to nl) . Then, in essence, each possible transtbrmation is scored by apply- ing it to the corpus and cornputing the reduction (or increase) in error rate.

The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes). $$$$$ This resulted in a slight; increase in 1)erformanee, but, more notal)\]y it resulted in a reduct;ion hy roughly half in the l;ota\[ mnnl)er of transfor- mat ion rules needed.
The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes). $$$$$ model ~(:quired (Inring training is rel)reser~ted in a huge, t~d)le of probabilities, pl:ecludiug any stra.ightf'orward analysis of its workings.

Brill and Resnik (1994) applied Error-Driven Transformation-Based Learning to this task, using the verb, noun1, preposition, and noun2 features. $$$$$ I{e(;a.llSe these papers deseril)e results ol)- tained on different corpora, however, it is (lifIicull; to II~,:'tl,:.{; a. 1)(;r\['(}rllla, iic{!
Brill and Resnik (1994) applied Error-Driven Transformation-Based Learning to this task, using the verb, noun1, preposition, and noun2 features. $$$$$ Instead of' using mammlly coustructed lexical classes, they nse word classes arrived at via mutmd information clustering in a training corpus (BDd+92), resulting in a representation i which each word is represented by a sequence of bits.

Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994). $$$$$ P is i~ and N\] is 15 N1 V \[written co.mmlu~ication\] 16 N1 V l ) is wilhoul 17 N1 V P is during 18 N 1 V 19 NI V. 20 N1.

We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems. $$$$$ 4 The transformation template is modified so that in ad- dition to asking if a nmm matches ome word W, 4Class names corresponded to unique "synonynl set" identifiers within the WordNet noun database.
We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems. $$$$$ Instead of' using mammlly coustructed lexical classes, they nse word classes arrived at via mutmd information clustering in a training corpus (BDd+92), resulting in a representation i which each word is represented by a sequence of bits.
We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems. $$$$$ First, the train- ing set is processed according to the start state annotator, in this case attaching all prepositional phrases low (attached to nl) . Then, in essence, each possible transtbrmation is scored by apply- ing it to the corpus and cornputing the reduction (or increase) in error rate.

(10) The lower bound for the B&R data is 63% (Brill and Resnik, 1994) and for the IBM data is 52% (Ratnaparkhi et al, 1994). $$$$$ \[n this e?periment (as in (\[II~,9\], I\]l{93)), tim at- tachment choice For l)repositional i)hrases was I)e- I,ween the oh.iecl~ mmn and l,he matrix verb.
(10) The lower bound for the B&R data is 63% (Brill and Resnik, 1994) and for the IBM data is 52% (Ratnaparkhi et al, 1994). $$$$$ Next, the training set was expanded to include not only the cases o\[' ambiguous attachment \]Fonnd in the parsed Wall Street Journal corpus, as before, but also all the unambiguous prepositional phrase at- tachments tbnnd in the corpus, as well (contiml- ing to exclnde the tesl, set, of course).
