This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a). $$$$$ After applying the transforma- tions, accuracy increases to 80.8%.
This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a). $$$$$ t~,atnal)~Pkhi and Roukos rel}ort an aecuraey oi' 81.6% using bot, h word and class iui'orma, tion on Wall SI;re.et 3ourna\] text,, using a t:raining COl:- pus twice as la, rgc as that used in ouP experiments.

Brill and Resnik (1994) trained a transformation-based learning algorithm on 12,766 quadruples from WSJ, with modifications similar to those by Collins and Brooks (1995). $$$$$ The transformation-based learner is I, rained on sen- tences containing v, n\[ and p, whereas the algo- r i thm describe.d by l l indle and I~,ooth ca.n zdso use sentences (;ontailfing only v and p, (n' only nl and i1.

Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ Exist ing knowl- e{lge, such as structural strategies or even a priori h;xieal l}references, (;all 1)e incorl)orated into I;he start state annotator , so theft the learning ~dgo.
Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ COl\[lD~/l:iSOll, Conc lus ions The.
Since Eric Brill first introduced the method of Transformation-Based Learning (TBL) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging [Brill, 1995], PP attachment disambiguation [Brill and Resnik, 1994], text chunking [Ramshaw and Marcus, 1995], spelling correction [Mangu and Brill, 1997], dialogue act tagging [Samuel et al, 1998] and ellipsis resolution [Hardt, 1998]. $$$$$ Baseline l l indle and Rooth (IIR91, 1\[17{93) describe corpus-based approach to disambiguating between prepositional phrase attachlnent to the main verb and to the object nonn phrase (such as in the ex- ample sentence above).

This alternative, which we have yet to try, has the advantage of fitting into the transformation-based error-driven paradigm (Brill and Resnik, 1994) more cleanly than having a translation stage. $$$$$ However, while there are certainly cases of an> biguity that seem to need some deep knowledge, either linguistic or conceptual, one might ask whag sort of performance could 1oe achieved by a sys- tem thai uses somewhat superficial knowledge au- *Parts of this work done a.t the Computer and hP lbrmation Science Department, University of Penn- sylvania were supported by by DARPA and AFOSR jointly under grant No.

Brill and Resnik (1994) used the supervised transformation-based learning method and lexical and conceptual classes derived from WordNet, achieving 82% precision on 500 randomly selected examples. $$$$$ Another prol)lem of the method, shared by ma.ny statistical approaches, is that the.

For example, in (Brill and Resnik, 1994) clustering PP heads according to WordNetsynsets produced only a 1% improvement in a PP disambiguation task, with respect to the non-clustered method. $$$$$ They then suggest using lexical preference, estimated from a large corpus of text, as a method of re- solving attachment ambiguity, a technique the}' call "lexical association."


In later stages of processing, a corpus-based approach (Brill and Resnik, 1994) is used to deal with ambiguities that cannot be solved with syntactic information only, in particular attachments of prepositional phrases, gerunds and infinitive constructions. $$$$$ _ _ 73.00 j - 72.00 l l i _ __ / __ . ,?!>2 - 70.00 69.00 68.00 67.00 64.00 0.00 5.00 I q ! i t T!aining Size x 103 10.00 Figure 2: Accuracy as a function of l;raining corpus size (no word class information).
In later stages of processing, a corpus-based approach (Brill and Resnik, 1994) is used to deal with ambiguities that cannot be solved with syntactic information only, in particular attachments of prepositional phrases, gerunds and infinitive constructions. $$$$$ Next, the training set was expanded to include not only the cases o\[' ambiguous attachment \]Fonnd in the parsed Wall Street Journal corpus, as before, but also all the unambiguous prepositional phrase at- tachments tbnnd in the corpus, as well (contiml- ing to exclnde the tesl, set, of course).
In later stages of processing, a corpus-based approach (Brill and Resnik, 1994) is used to deal with ambiguities that cannot be solved with syntactic information only, in particular attachments of prepositional phrases, gerunds and infinitive constructions. $$$$$ In the experiments reported here we used WordNet version :l.2.

For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). $$$$$ I!'or instance, a table entry is cousidered a definite instance of the prepositional phrase attaching to the noun if: '\['he noun phrase occm:s in a context where no verb could license the prepositional phrase, specifically if the noun phrase is in a subjeet or other pre-verbal position.
For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). $$$$$ \]u addit ion, we have shown how the l;raus\['orln~Lion-based l arner can casity be e?.- tended to incorporate word-class i/fformatiou.
For example, the sentence Congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment toN: accused [the president of peccadillos] attachment to V: (4) accused [the president] [of peccadillos] The UPenn Treebank-II Parsed Wall Street Journal corpus includes PP-attachment information, and PP-attachment classifiers based on this data have been previously described in Ratnaparkhi, Reynar, Roukos (1994), Brill and Resnik (1994), and Collins and Brooks (1995). $$$$$ Instead of' using mammlly coustructed lexical classes, they nse word classes arrived at via mutmd information clustering in a training corpus (BDd+92), resulting in a representation i which each word is represented by a sequence of bits.

A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. $$$$$ if" N2 is a. nomt I, hal; describes time (i.e. ix a. member of WordNet class that in- cludes tim nouns "y(;ar," "month," "week," and others), thell the preltositiomd phrase should be al;tache(\[ t,() the w;rb, since, tim(; is \]nlMl more l ikely Io modify a yet'It (e.g. le,vc lh(: re(cling iu an hour) thaJl a, lloun.
A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. $$$$$ Ilow- ever, this l)eeomes less of a probh'.m as atmotated eorl}ora beeolne increasingly available, and sug- gests the comhinat ion o1:' supexvised and uusuper vised methods as a.u ilfl;eresth G ave\]me \['or \['urther rese;ire\] \[.
A non-statistical supervised approach by Brill and Resnik (1994) yielded 81.8% accuracy using a transformation-based approach (Brill, 1995) and incorporating word-class information. $$$$$ (BI}V91) bol,\]l deseril)e the use of lnanual ly coustrueted, donmhv Sl){~eitic word classes together with cori}us-tmsed si,t~tisties in o f d{2r to resolve i)rel)ositional 1)hrase a.t, taehlllellt &Ill-.

 $$$$$ %oo!1 / I 74:001 . . .
 $$$$$ 4 N1 V P is into 5 N 1 V P is from 6 N1 V 1 ) is wilh 7 N1 V P is of P is in and NI is 8 N 1 V \[measure, quanlily, amou~l\] P is by all.el 9 N1 V N2 is \[abslraclion\] I 0 NI V P is lhro'ugh 1) is in and N I is 11 NI V \[group,group.in.g\].
 $$$$$ Doing so re- sull;ed in an attachment accuracy of 70.4%.

 $$$$$ I{e(;a.llSe these papers deseril)e results ol)- tained on different corpora, however, it is (lifIicull; to II~,:'tl,:.{; a. 1)(;r\['(}rllla, iic{!
 $$$$$ The l)reposi- tioiml phrase a.tt~Munent |ea.riter learns tra.nsfor-- Ill~ttiollS \[?onl a C,)l:l>tls O\[ 4-tuples of the \['orm (v I11 I\] 1|9), where v is ~1 w;rl), nl is the head of its objecl, llolni \]phrase, i ) is the \])l'epositioll, and 11:2 is the head of the noun phrase, governed by the prel)c, sition (for e,-:anq~le, sce/v :1~' bo:q/,l o,/p the h711/~2).

 $$$$$ oped independently by Ratnaparkhi and I/,oukos (l{R94), since they also used training and test datt~ drawn from the Penn Treebank's Wall Street Jour- nal corpus.
 $$$$$ V l ) is on and Nt is \[U~.ing\] P is after V is buy and P is for Figure 4: The first 20 transformations learned for prepositional phrase attachment, using noun classes.

Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maximum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ \[11 a (\]irect eoml);u:ison with lexical association, higher ble(;ll- vaey is achieved using words alolm (wen though at tachment inf\}rnlation is captured i*l a relatively small numl)er of simple, rea(lable rules, as opl)osed to a. large lllllll\])eF Of lexical co-oeetlrreltee l)l'o\])a -- I)ilities.

Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. $$$$$ Next, the training set was expanded to include not only the cases o\[' ambiguous attachment \]Fonnd in the parsed Wall Street Journal corpus, as before, but also all the unambiguous prepositional phrase at- tachments tbnnd in the corpus, as well (contiml- ing to exclnde the tesl, set, of course).
Brill and Resnik (1994) applied Error-Driven TransformationBased Learning, Ratnaparkhi, Reynar and Roukos (1994) applied a Maximum Entropy model, Franz (1996) used a Loglinear model, and Collins and Brooks (1995) obtained good results using a BackOff model. $$$$$ We would guess that the correct interpretation is that one should buy cars that come with steer- ing wheels, and not that one should use a steering wheel as barter for purchasing a car.

The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes). $$$$$ This resulted in a slight; increase in 1)erformanee, but, more notal)\]y it resulted in a reduct;ion hy roughly half in the l;ota\[ mnnl)er of transfor- mat ion rules needed.
The other methods for which results have been reported on this dataset include decision trees, Maximum Entropy (Ratnaparkhi, Reynar, and Roukos, 1994), and Error-Driven TransformationBased Learning (Brill and Resnik, 1994), which were clearly outperformed by both IB1 and IBI-IG, even though e.g. Brill~ Resnik used more elaborate feature sets (words and WordNet classes). $$$$$ _ _ t ....

Brill and Resnik (1994) applied Error-Driven Transformation-Based Learning to this task, using the verb, noun1, preposition, and noun2 features. $$$$$ In the experiment, a tol, al of 471 transfor- mations were learned - - Figure 3 shows the first twenty.

Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994). $$$$$ An algorithm is then specified 1,o try to extract attachment information h'om this table of co-occurrences.
Supervised methods are as varied as the Back off approach by Collins and Brooks (1995) and the Transformation-based approach by Brill and Resnik (1994). $$$$$ The rule captures the very regular appearance in the Penn Tree- bank Wall Street Journal corpus of parses like Sales for the yea," \[v'P rose \[Np5Yo\]\[pP in fiscal 1988\]\].

We use the PPA data created by (Brill and Resnik, 1994) and (Ratnaparkhi et al, 1994) to objectively compare the performances of the systems. $$$$$ Ilow- ever, this l)eeomes less of a probh'.m as atmotated eorl}ora beeolne increasingly available, and sug- gests the comhinat ion o1:' supexvised and uusuper vised methods as a.u ilfl;eresth G ave\]me \['or \['urther rese;ire\] \[.

(10) The lower bound for the B&R data is 63% (Brill and Resnik, 1994) and for the IBM data is 52% (Ratnaparkhi et al, 1994). $$$$$ (BI}V91) bol,\]l deseril)e the use of lnanual ly coustrueted, donmhv Sl){~eitic word classes together with cori}us-tmsed si,t~tisties in o f d{2r to resolve i)rel)ositional 1)hrase a.t, taehlllellt &Ill-.
