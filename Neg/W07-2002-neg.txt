The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. $$$$$ Acknowledgments We want too thank the organizers of SemEval-2007 task 17 for kindly letting us use their corpus.
The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. $$$$$ 7 77.1 80.5 73.3Table 4: Supervised evaluation as recall.
The object of the sense induction task of SENSEVAL-4 (Agirre and Soroa, 2007) was to cluster 27,132 instances of 100 different words (35 nouns and 65 verbs) into senses or classes. $$$$$ An other problem of the supervised approach is that theinventory and distribution of senses changes dra matically from one domain to the other, requiring additional hand-tagging of corpora (Mart??nez and Agirre, 2000; Koeling et al, 2005).

The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. $$$$$ The supervised evaluation introduces a mapping step which interacts with the clustering solu tion.
The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. $$$$$ Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages.
The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. $$$$$ In the unsupervised evaluation only the sense with maximum weightwas considered, but for the supervised one the whole score vector was used.
The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL 2007 WSI task (Agirre and Soroa, 2007), with some modifications. $$$$$ Acknowledgments We want too thank the organizers of SemEval-2007 task 17 for kindly letting us use their corpus.

The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. $$$$$ We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17.
The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. $$$$$ Lexicographers and semanticists have long warned about the problems of such an approach,where senses are listed separately as discrete entities, and have argued in favor of more complex rep resentations, where, for instance, senses are dense regions in a continuum (Cruse, 2000).Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of think ing, and tries to induce word senses directly fromthe corpus.
The evaluation of the collocational-graph method in the SemEval-2007 sense induction task (Agirre and Soroa, 2007) showed promising results. $$$$$ We reused the data from the SemEval 2007 English lexical sample subtask of task 17, and.

Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). $$$$$ However, all systems outperform by far the random and 1c1instbaselines, meaning that the systems are able to in duce correct senses.
Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). $$$$$ 6 systems participated, but one of them was not a sense induction system.
Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). $$$$$ We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17.
Our definition of context is equivalent to an instance of the target word in the SemEval-2007 sense induction task dataset (Agirre and Soroa, 2007). $$$$$ Then, M = {mij} 1 ? i ? m, 1 ? j ? n, and each mij = P (sj |hi), that is, mij is the probability of a word having sense jgiven that it has been assigned cluster i. This probability can be computed counting the times an occur rence with sense sj has been assigned cluster hi in the train corpus.

We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ Supervised WSD techniques are the best performing in public evaluations, butneed large amounts of hand-tagging data.
We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ The goal of this task is to allow for comparison across sense-induction and discrimination systems, and also to compare these systems to other su pervised and knowledge-based systems.
We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ was submitted by the organizers.
We followed the setting of SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ to ?word senses?.

We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task ,i.e. supervised evaluation. $$$$$ We will call this evaluation unsupervised.
We evaluate our method on the nouns of the SemEval-2007 word sense induction task (Agirre and Soroa, 2007) under the second evaluation setting of that task ,i.e. supervised evaluation. $$$$$ With this measure some of the systems3 are able to beat all baselines.

We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ 6 systems participated, but one of them was not a sense induction system.
We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ For completeness, we also computed the FScore usingthe complete corpus (both train and test).
We followed the same sense mapping method as in the SemEval-2007 sense induction task (Agirre and Soroa, 2007). $$$$$ Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages.

We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. $$$$$ 7 the task, and the official results.
We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art. The remainder of this paper is structured as follows. $$$$$ We were also surprised to see that no system could 11 System Supervised evaluation random2 78.6 random10 77.6 ramdom100 64.2 random1000 31.8 Table 7: Supervised evaluation of several random baselines.beat the ?one cluster one word?

For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). $$$$$ We also noticed that some words had hundreds of instancesand only a single sense.
For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). $$$$$ In fact, the ranking of the participating systems 3All systems in the case of a random train/test split varies according to the evaluation method used.
For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007). $$$$$ of task 17.

Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. $$$$$ Exist ing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with WordNetsenses (Fellbaum, 1998) allow for a small improve ment over the simple most frequent sense heuristic,as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004).
Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. $$$$$ Given the fact that FScore tries to balance precision (higher for large numbers of clusters) and recall (higher for small numbers of clusters), this was not expected.
Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. $$$$$ This approach has twomain disadvantages.
Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods. $$$$$ An other problem of the supervised approach is that theinventory and distribution of senses changes dra matically from one domain to the other, requiring additional hand-tagging of corpora (Mart??nez and Agirre, 2000; Koeling et al, 2005).

The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from Onto Notes. $$$$$ We reused the data from the SemEval 2007 English lexical sample subtask of task 17, and.
The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from Onto Notes. $$$$$ We also provide a compari son to the results of the systems participating in the lexical sample subtask of task 17.Evaluating clustering solutions is not straightfor ward.
The SemEval-2007 word sense induction task (Agirre and Soroa, 2007) already allows for evaluation of automatic sense induction systems, but compares output to gold-standard senses from Onto Notes. $$$$$ As shown in Table 3, no system outperforms the 1c1word baseline, which indicates that this baseline 10is quite strong, perhaps due the relatively small num ber of classes in the gold standard.

While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). $$$$$ Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages.
While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). $$$$$ In fact, the ranking of the participating systems 3All systems in the case of a random train/test split varies according to the evaluation method used.
While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). $$$$$ Section 3 presents the systems that participated in 1WSID approaches prefer the term ?word uses?
While word senses have been studied extensively in lexical semantics, research has focused on word sense disambiguation, the task of disambiguating words in context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). $$$$$ Given the fact that FScore tries to balance precision (higher for large numbers of clusters) and recall (higher for small numbers of clusters), this was not expected.

The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. $$$$$ baseline.
The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. $$$$$ Supervised WSD techniques are the best performing in public evaluations, butneed large amounts of hand-tagging data.
The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. $$$$$ paradigm, where the senses for a target wordare a closed list coming from a dictionary or lex icon.

The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). $$$$$ Note that the purity and entropy measures are not very indicative in this setting.
The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). $$$$$ One alternative is to manually de cide the correctness of the clusters assigned to each occurrence of the words.
The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). $$$$$ The entropy measure consid ers how the various classes of objects are distributedwithin each cluster.
The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). $$$$$ As shown in Table 3, no system outperforms the 1c1word baseline, which indicates that this baseline 10is quite strong, perhaps due the relatively small num ber of classes in the gold standard.

Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. $$$$$ We also provide a compari son to the results of the systems participating in the lexical sample subtask of task 17.Evaluating clustering solutions is not straightfor ward.
Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. $$$$$ The unsupervised evaluation seems to besensitive to the number of senses in the gold stan dard, and the coarse grained sense inventory usedin the gold standard had a great impact in the results.
Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. $$$$$ The context similarity matrix is pruned and the resulting associated graphis clustered by means of a random-walk type al gorithm.

Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). $$$$$ Note that UBC-AS?
Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). $$$$$ In total there were 6 participating systems.

The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). $$$$$ 500M words) would improve the perfor mance of supervised WSD, but no current projectexists to provide such an expensive resource.
The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). $$$$$ We reused the data from the SemEval 2007 English lexical sample subtask of task 17, and.
The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). $$$$$ Section 2 presents the evaluation framework used in this task.
The second type of evaluation, supervised evaluation, follows the supervised evaluation of the SemEval-2007 WSI task (Agirre and Soroa, 2007). $$$$$ We provide a comparison to the results ofthe systems participating in the lexical sam ple subtask of task 17.

This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. $$$$$ We reused the SemEval 2007 English lexical sample subtask of task17, and set up both clustering-style unsuper vised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping).
This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. $$$$$ Typical WSID systems involve clustering techniques, which group together similar examples.
This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. $$$$$ This approach has twomain disadvantages.
This evaluation follows the supervised evaluation of SemEval-2007WSI task (Agirre and Soroa, 2007), with the difference that the reported results are an average of 5 random splits. $$$$$ 5 78.5 80.7 76.0 UOY 6 77.7 81.6 73.3 UofL??

Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 20071 (Agirre and Soroa, 2007). $$$$$ The evaluation of clustering solutions is not straightforward.
Brody and Lapata (2009) (B&L herein) showed that the parametric Bayesian model, Latent Dirichlet Allocation (LDA), could be successfully employed for this task, as compared to previous results published for the WSI component of SemEval 20071 (Agirre and Soroa, 2007). $$$$$ Supervised evaluation seems to be more neutral regarding the number of clusters, as the ranking of systems according to this measure include diverse cluster averages.

The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007). $$$$$ is not a sense induction system.
The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007). $$$$$ Pantel and Lin (2002) automatically map the senses to WordNet, and then mea sure the quality of the mapping.
The evaluation data comes from the WSI task of SemEval-2007 (Agirre and Soroa, 2007). $$$$$ Each of the induced clusters is mapped into a weighted vector of senses, and thus inducing a number of clusters similar to the number of senses is not a requirement for good results.
