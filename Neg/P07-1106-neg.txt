On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ Figure 1 gives the algorithm, where N is the number of training sentences and T is the number of passes over the data.
On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ However, sub-words do not necessarily contain full word information.
On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ The output segmented sentence is compared with the original training example.
On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain // append the character to the last word word and length information.

 $$$$$ Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain // append the character to the last word word and length information.
 $$$$$ However, the correct decision can in a segmented sentence.
 $$$$$ Moreover, sub-word extraction is performed separately from feature extraction.
 $$$$$ Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus.

Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ The averaged perceptron algorithm (Collins, 2002) was proposed as a way of reducing overfitting on the training data.
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ For this particular feature set, the longest range features are word bigrams.
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ For this particular feature set, the longest range features are word bigrams.

 $$$$$ Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.
 $$$$$ Another difference from our model is the rule-based submodel, which uses a dictionary-based forward maximum match method described by Sproat et al. (1996).
 $$$$$ To guarantee reasonable running speed, the size of the target agenda is limited, keeping only the B best candidates.
 $$$$$ The key difference between our model and the above models is the wordbased nature of our system.

(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ The features are usually confined to tem is competitive with the best systems, obtaining a five-character window with the current character the highest reported F-scores on a number of the in the middle.
(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ Unlike the character-tagging models, the CRF submodel assigns tags to subwords, which include single-character words and the most frequent multiple-character words from the training corpus.
(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ The feature templates are shown in Table 1.
(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ CWS systems are evaluated by two types of tests.

All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ Table 2 shows the precision, recall and F-measure for the development set after 1 to 10 training iterations, as well as the number of mistakes made in each iteration.
All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ Any segmented sentence is mapped to a global feature vector according to these templates.
All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain // append the character to the last word word and length information.
All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ One attractive feature of the perceptron training algorithm is its simplicity, consisting of only a decoder and a trivial update process.

It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ If the output is incorrect, the parameter vector is updated by adding the global feature vector of the training example and subtracting the global feature vector of the decoder output.
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ Each character is classified independently, using information in the neighboring five-character window.
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.

In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ This work is supported by the ORS and Clarendon Fund.
In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.
In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ Any extra knowledge is not allowed, including common surnames, Chinese and Arabic numbers, European letters, lexicons, part-of-speech, semantics and so on.

 $$$$$ Therefore, given the flexibility of the feature-based perceptron model, an obvious next step is the study of open features in the segmentor.
 $$$$$ This work is supported by the ORS and Clarendon Fund.
 $$$$$ Table 3 shows the accuracies with ten different agenda sizes, each used for both training and testing.

More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score.
More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain // append the character to the last word word and length information.
More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ We proposed a word-based CWS model using the discriminative perceptron learning algorithm.
More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ We thank the anonymous reviewers for their insightful comments.

We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.
We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ Figure 1 gives the algorithm, where N is the number of training sentences and T is the number of passes over the data.
We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ The feature templates are shown in Table 1.
We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996).

We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ One attractive feature of the perceptron training algorithm is its simplicity, consisting of only a decoder and a trivial update process.
We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ The algorithm can perform multiple passes over the same training sentences.
We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ Let N be the number of training sentences, T the number of training iterations, and αn,t the parameter vector immediately after the nth sentence in the tth iteration.
We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ Out-of-vocabulary (OOV) words are a major source of ambiguity.

 $$$$$ Open tests measure a model’s capability to utilize extra information and domain knowledge, which can lead to improved performance, but since this extra information is not standardized, direct comparison between open test results is less informative.
 $$$$$ The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word.
 $$$$$ In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.
 $$$$$ The closed tests require that the system is trained only with a designated training corpus.

For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ We thank the anonymous reviewers for their insightful comments.
For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ Reducing the agenda size increases the decoding speed, but it could cause loss of accuracy by eliminating potentially good candidates.
For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ We proposed a word-based CWS model using the discriminative perceptron learning algorithm.

This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ In comparison, our system is based on a single perceptron model.
This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word.
This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ In this experiment, the agenda size was set to 16, for both training and testing.

 $$$$$ The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder.
 $$$$$ This work is supported by the ORS and Clarendon Fund.
 $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.
 $$$$$ Exam- ods for NLP tasks.

For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ Any segmented sentence is mapped to a global feature vector according to these templates.
For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ The decoder reads characters from the input sentence one at a time, and generates candidate segmentations incrementally.
For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ Initially the source agenda contains an empty sentence and the target agenda is empty.

The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature.
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ To guarantee reasonable running speed, the size of the target agenda is limited, keeping only the B best candidates.
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ For this particular feature set, the longest range features are word bigrams.
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder.

We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ We thank the anonymous reviewers for their insightful comments.
We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.
We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.
We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.

We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ We thank the anonymous reviewers for their insightful comments.
We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ Moreover, sub-word extraction is performed separately from feature extraction.
We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ This method guarantees exhaustive generation of possible segmentations for any input sentence.
We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ In this paper we propose an alternative, word-based segmentor, which uses features based on complete words and word sequences.
