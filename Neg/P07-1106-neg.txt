On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ Figure 2 gives the decoding algorithm.
On the three corpora, it also outperformed the word-based perceptron model of Zhang and Clark (2007). $$$$$ After the last character is processed, the decoder returns the candidate with the best score in the source agenda.

 $$$$$ The decoder reads characters from the input sentence one at a time, and generates candidate segmentations incrementally.
 $$$$$ Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature.
 $$$$$ Collins dimension represents the count of a particular fea(2002) proposed the perceptron as an alternative to ture in the sentence.
 $$$$$ Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.

Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ F-measure is used as the accuracy measure.
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ This work is supported by the ORS and Clarendon Fund.
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ Features 1 and 2 contain only word information, 3 to 5 contain character and length information, 6 and 7 contain only character information, 8 to 12 contain word and character information, while 13 and 14 contain // append the character to the last word word and length information.
Moreover, our model is based on our previous work, in line with Zhang and Clark (2007), which does not treat word segmentation as character sequence labeling. $$$$$ Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.

 $$$$$ There are 356,337 features with non-zero values after 6 training iterations using the development data.
 $$$$$ There are 356,337 features with non-zero values after 6 training iterations using the development data.
 $$$$$ Any segmented sentence is mapped to a global feature vector according to these templates.
 $$$$$ This reflects the fact that the best segmentation is often within the current top few candidates in the agenda.2 Since the training and testing time generally increases as N increases, the agenda size is fixed to 16 for the remaining experiments.

(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ Also, we wish to explore the possibility of incorporating POS tagging and parsing features into the discriminative model, leading to joint decoding.
(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ One existing method that is based on sub-word information, Zhang et al. (2006), combines a CRF and a rule-based model.
(Zhang and Clark, 2007) uses perceptron (Collins, 2002) to generate word candidates with both word and character features. $$$$$ Chinese character sequences are ambiguous, often requiring knowledge from a variety of sources for disambiguation.

All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ The decoder can be optimized accordingly: when an incoming character is combined with candidate items as a new word, only the best candidate is kept among those having the same last word.
All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ The averaged perceptron algorithm improves the segmentation accuracy at each iteration, compared with the nonaveraged perceptron.
All of the above models, except (Zhang and Clark, 2007), adopt the character-based discriminative approach. $$$$$ Most of the features in Table 1 are related to words.

It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ The ambiguity can be resolved with information about the neighboringn words.
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature.
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ This model is an alternative to the existing characterbased tagging models, and allows word information to be used as features.
It shows that both our joint-plus model and joint model exceed (or are comparable to) almost all e state-of-the-art systems across all corpora, except (Zhang and Clark, 2007) at PKU (ucvt.). $$$$$ Reducing the agenda size increases the decoding speed, but it could cause loss of accuracy by eliminating potentially good candidates.

In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.
In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.
In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ This work is supported by the ORS and Clarendon Fund.
In that special case, (Zhang and Clark, 2007) outperforms the joint-plus model by 0.3% on F score (0.4% for the joint model). $$$$$ The advantage is two-fold: higher level syntactic information can be used in word segmentation, while joint decoding helps to prevent bottomup error propagation among the different processing steps.

 $$$$$ We follow the format from Peng et al. (2004).
 $$$$$ The last two columns represent the average accuracy of each model over the tests it participated in (SAV), and our average over the same tests (OAV), respectively.
 $$$$$ Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature.
 $$$$$ Liang (2005) uses the discriminative perceptron algorithm (Collins, 2002) to score whole character tag sequences, finding the best candidate by the global score.

More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.
More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ This method guarantees exhaustive generation of possible segmentations for any input sentence.
More recently, Zhang and Clark (2007) reported success using a linear model trained with the average perceptron algorithm (Collins, 2002). $$$$$ The results are shown in Table 5.

We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ However, sub-words do not necessarily contain full word information.
We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ This work is supported by the ORS and Clarendon Fund.
We use the publicly available Stanford CRF segmenter (Tseng et al, 2005) as our character-based baseline model, and reproduce the perceptron-based segmenter from Zhang and Clark (2007) as our word-based baseline model. $$$$$ Two agendas are used: the source agenda and the target agenda.

We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ Therefore, given the flexibility of the feature-based perceptron model, an obvious next step is the study of open features in the segmentor.
We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ This method guarantees exhaustive generation of possible segmentations for any input sentence.
We adopted the development setting from (Zhang and Clark, 2007), and used CTB sections 1-270 for training and sections 400-931 for development in hyper-parameter setting; for all results given in tables, the models are trained and evaluated on the standard train/test split for the given dataset. $$$$$ Therefore, among partial candidates ending with the same bigram, the best one will also be in the best final candidate.

 $$$$$ This work is supported by the ORS and Clarendon Fund.
 $$$$$ The key difference between our model and the above models is the wordbased nature of our system.
 $$$$$ Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.

For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ We thank the anonymous reviewers for their insightful comments.
For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ Typical examples of unseen words include Chinese names, translated foreign names and idioms.
For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference. $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.

This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ Closed tests on the first and show that our system is competitive with the best in the literature, achieving the highest reported F-scores for a number of corpora.
This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ We thank the anonymous reviewers for their insightful comments.
This is different from the experiments reported in (Zhang and Clark, 2007). $$$$$ Let N be the number of training sentences, T the number of training iterations, and αn,t the parameter vector immediately after the nth sentence in the tth iteration.

 $$$$$ Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.
 $$$$$ This work is supported by the ORS and Clarendon Fund.
 $$$$$ Now αn,t We found that this lazy update method was significantly faster than the naive method.
 $$$$$ After each character is processed, the items in the target agenda are copied to the source agenda, and then the target agenda is cleaned, so that the newly generated candidates can be combined with the next incoming character to generate new candidates.

For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve accuracy.
For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ The generalized perceptron algorithm is used for discriminative training, and we use a beamsearch decoder.
For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ This model is an alternative to the existing characterbased tagging models, and allows word information to be used as features.
For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. $$$$$ At each processing stage, the decoder reads in a character from the input sentence, combines it with each candidate in the source agenda and puts the generated candidates onto the target agenda.

The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ Discriminatively trained models based on local character features are used to make the tagging decisions, with Viterbi decoding finding the highest scoring segmentation.
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ This work is supported by the ORS and Clarendon Fund.
The feature templates in (Zhao et al, 2006) and (Zhang and Clark, 2007) are used in training the CRFs model and Perceptrons model, respectively. $$$$$ Among the character-tagging CWS models, Li et al. (2005) uses an uneven margin alteration of the traditional perceptron classifier (Li et al., 2002).

We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ To guarantee reasonable running speed, the size of the target agenda is limited, keeping only the B best candidates.
We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ The first, used for development, was based on the part of Chinese Treebank 4 that is not in Chinese Treebank 3 (since CTB3 was used as part of the first bakeoff).
We built a two-stage baseline system, using the per ceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002). $$$$$ We proposed a word-based CWS model using the discriminative perceptron learning algorithm.

We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ Standard approaches to Chinese word segmentation treat the problem as a tagging task, assigning labels to the characters in the sequence indicating whether the character marks a word boundary.
We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ Closed tests using the first and second SIGHAN CWS bakeoff data demonstrated our system to be competitive with the best in the literature.
We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segment or to refer to the segment or from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation). $$$$$ The advantage is two-fold: higher level syntactic information can be used in word segmentation, while joint decoding helps to prevent bottomup error propagation among the different processing steps.
