For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). $$$$$ In JWS, the frequency-based pruning does not work well.
For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). $$$$$ Examples include Part-of-Speech tagging (Nakagawa et al., 2002) Text Chunking (Kudo and Matsumoto, 2001), Named Entity Recognition (Isozaki and Kazawa, 2002), and Japanese Dependency Parsing (Kudo and Matsumoto, 2000; Kudo and Matsumoto, 2002).
For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). $$$$$ The merit of the kernel is that feature combinaimplicitly expanded without loss of generality and increasing the computational costs.
For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)). $$$$$ This is critical, since, in some cases, the dot products can be evaluated by a simple Kernel Function: K(x1, x2) = φ(x1) · φ(x2).

There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). $$$$$ Although we can reduce the size of Q by half, the accuracy is also reduced (97.94%→97.83%).
There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). $$$$$ In NLP, although feature combinations are crucial to improving performance, they are heuristically selected.
There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). $$$$$ In previous research, feature combination has been selected manually, and the performance significantly depended on these selections.
There are two types of correct summary according to the character length, "long" and "short", All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). $$$$$ It is known in NLP that a combination of features, a subset of feature set F in general, contributes to overall accuracy.

We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. $$$$$ Then, we introduced two fast classification algorithms for this kernel.
We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. $$$$$ To apply the PKE, we must efficiently enumerate the effective sub-trees from a set of support examples.
We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic detection. $$$$$ Substituting kernel function into (1), we have the following decision function.

Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s $$$$$ To apply the PKE, we must efficiently enumerate the effective sub-trees from a set of support examples.
Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s $$$$$ We first divide the support examples into positive (yi > 0) and negative (yi < 0) examples, and process mining independently.
Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s $$$$$ , (xL, yL) xj E RN, yj E {+1, −1}, where xj is a feature vector of the j-th training sample, and yj is the class label associated with this training sample.
Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s $$$$$ The parameter ξ is usually referred to as the Minimum Support.

 $$$$$ Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.
 $$$$$ In this paper, we extend Mining to convert a kernel-based classifier into a simple and fast linear classifier.
 $$$$$ Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.

Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. $$$$$ We can similarly apply a sub-tree mining algorithm (Zaki, 2002) to this problem.
Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. $$$$$ In NLP, although feature combinations are crucial to improving performance, they are heuristically selected.
Following (Kudo and Matsumoto, 2003), we use a trie (hereafter, weight trie) to maintain conjunctive features. $$$$$ (B) αj, b E R, αj > 0.

PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. $$$$$ In the field of Natural Language Processing, many successes have been reported.
PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. $$$$$ The PKI leads to about 2 to 12 times improvements over the PKB.
PKI - Inverted Indexing (Kudo and Matsumoto, 2003), stores for each feature the support vectors in which it appears. $$$$$ The Polynomial Kernel of degree d can be rewritten as

PKE - Heuristic Kernel Expansion, was introduced by (Kudo and Matsumoto, 2003). $$$$$ The merit of the kernel is that feature combinaimplicitly expanded without loss of generality and increasing the computational costs.

Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. $$$$$ Japanese word segmentation is formalized as a simple classification task.
Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. $$$$$ Text Chunking is a fundamental task in NLP – dividing sentences into non-overlapping phrases.
Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. $$$$$ The calculation of w' is formulated as the following mining problem: Given a set of support examples and subset weight cd(r), extract all subsets s and their weights w(s) if w(s) holds w(s) > σpos or w(s) :5 σneg .
Our approach is similar to the PKE approach (Kudo and Matsumoto, 2003), which used a basket mining approach to prune many features from the expansion. $$$$$ The algorithm PKE is an approximation of the PKB, and changes the final accuracy according to the selection of thresholds σpos and σneg.

We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. $$$$$ Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.
We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. $$$$$ Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.
We use Yamcha (Kudo and Matsumoto, 2003), a support-vector machine-based sequence tagger. $$$$$ Tables 2, 3 and 4 show the execution time, accuracy4, and |Q |(size of extracted subsets), by changing σ from 0.01 to 0.0005.

We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. $$$$$ For instance, in the task of dependency parsing, it would be hard to confirm a correct dependency relation with only a single set of features from either a head or its modifier.
We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. $$$$$ The other is PKE (Polynomial Kernel Expanded), where all feature combinations are explicitly expanded.
We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. $$$$$ Experimental results on English BaseNP Chunking, Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel-based classifiers.
We use Yamcha (Kudo and Matsumoto, 2003), an implementation of support vector machines which includes Viterbi decoding. $$$$$ This is critical, since, in some cases, the dot products can be evaluated by a simple Kernel Function: K(x1, x2) = φ(x1) · φ(x2).

Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy. $$$$$ In this context, frequent means that there are no less than ξ transactions which contain a sub-structure.
Second, we replace the YAMCHA (Kudo and Matsumoto, 2003) implementation of Support Vector Machines (SVMs) with SVMTool (Gimenez and Marquez, 2004) as our machine learning tool, for reasons of speed, at the cost of a slight decrease in accuracy. $$$$$ This is a naive extension of Inverted Indexing in Information Retrieval.

We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. $$$$$ For instance, if we use a polynomial kernel, all feature combinations are implicitly expanded without loss of generality and increasing the computational costs.
We used YamCha (Kudo and Matsumoto, 2003) to detect named entities, and we trained it on the SemEval full-text training sets. $$$$$ They seem to use a naive exhaustive method to expand them, which is not always scalable and efficient for extracting three or more feature combinations.

readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. $$$$$ It is known in NLP that combination of features contributes to a significant improvement in accuracy.
readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. $$$$$ The explicit difference between XQK and PKE is that XQK is designed only for Quadratic Kernel.
readers may refer to Kudo and Matsumoto (2003) for the detailed computation for obtaining w. $$$$$ XQK can be subsumed into PKE.

The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. $$$$$ Kernel methods change this situation.
The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. $$$$$ It implies that XQK can only deal with feature combination of size up to two.
The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. $$$$$ In this context, frequent means that there are no less than ξ transactions which contain a sub-structure.
The number of support vectors of SVMs was 71,766 ± 9.2%, which is twice as many as those used by Kudo and Matsumoto (2003) (34,996) in their experiments on the same task. $$$$$ Although there are many implementations for TRIE, we use a Double-Array (Aoe, 1989) in our task.

This result conforms to the results reported in (Kudo and Matsumoto, 2003). $$$$$ We can similarly apply a sub-tree mining algorithm (Zaki, 2002) to this problem.
This result conforms to the results reported in (Kudo and Matsumoto, 2003). $$$$$ There have been several studies for efficient classification of SVMs.
This result conforms to the results reported in (Kudo and Matsumoto, 2003). $$$$$ In previous research, feature combination has been selected manually, and the performance significantly depended on these selections.

 $$$$$ We can similarly apply a sub-tree mining algorithm (Zaki, 2002) to this problem.

In (Kudo and Matsumoto, 2003), an extension of the PrefixSpan algorithm (Pei et al, 2001) is used to efficiently mine the features in a low degree polynomial kernel space. $$$$$ For example, an SVM-based NE-chunker runs at a rate of only 85 byte/sec, while previous rulebased system can process several kilobytes per second (Isozaki and Kazawa, 2002).

 $$$$$ This problem can be overcome by noticing that both construction of optimal parameter αi (we will omit the details of this construction here) and the calculation of the decision function only require the evaluation of dot products φ(xi)·φ(x).
 $$$$$ Suppose a feature set F = {1, 2, ... , N} and training examples Xj(j = 1, 2, ... , L), all of which are subsets of F (i.e., Xj C_ F).
 $$$$$ The algorithm PKI does not change the final accuracy of the classification.

We use the following tools for syntactic processing $$$$$ It is known in NLP that a combination of features, a subset of feature set F in general, contributes to overall accuracy.
We use the following tools for syntactic processing $$$$$ Kernel-based text analysis shows an excellent performance in terms in accuracy; however, these methods are usually too slow to apply to large-scale text analysis.
