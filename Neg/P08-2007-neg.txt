More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. $$$$$ Let a weighted sentence pair additionally include a real-valued function 0 : {eij}x{fkl} —* R, which scores links.
More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. $$$$$ In practice, however, the space of alignments has to be pruned severely using word alignments to control the running time of EM.
More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. $$$$$ An alignment is a set of links.
More recently, DeNero and Klein (2008) have proven the NP-completeness of the phrase alignment problem. $$$$$ On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.

Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). $$$$$ The existence of a polynomial time algorithm for £ implies a polynomial time algorithm for S, because A = U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}.
Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). $$$$$ SUM, S: Given (e, f, φ), compute EaEA φ(a).
Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.
Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial for a sub-class of weighted synchronous context free grammars (Wu, 1997). $$$$$ The score implies that f aligns using all oneword phrases and Vai E a, 0(ai) = 1.

Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). $$$$$ In practice, however, the space of alignments has to be pruned severely using word alignments to control the running time of EM.
Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). $$$$$ Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipartite graph G with edge set E = {(vj, vl)}, we construct e and f with n words each, and set 0(ej−1 j, fl−1 l) = 1 and 0 otherwise.
Marcu and Wong (2002) describe a joint-probability phrase-based model for alignment, but the approach is limited due to excessive complexity as Viterbi inference becomes NP-hard (DeNero and Klein, 2008). $$$$$ Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers.

Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). $$$$$ First, we introduce binary indicator variables ai,j,k,l denoting whether (eij, fkl) ∈ a.
Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). $$$$$ CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard.
Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). $$$$$ DeNero et al. (2006) instead proposes an exponential-time dynamic program to systematically explore A, which can in principle solve either O or £.
Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). $$$$$ In practice, however, the space of alignments has to be pruned severely using word alignments to control the running time of EM.

(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. $$$$$ CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard.
(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. $$$$$ EXPECTATION, £: Given a weighted sentence pair (e, f, φ) and indices i, j, k,l, compute Ea φ(a) over all a E A such that (eij, fkl) E a.
(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. $$$$$ £ arises in computing sufficient statistics for re-estimating phrase translation probabilities (Estep) when training models.
(DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. $$$$$ However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited.

Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. $$$$$ CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard.
Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. $$$$$ With another construction, we can show that S is #Phard, meaning that it is at least as hard as any #Pcomplete problem.
Recently, DeNero and Klein (2008) addressed the training problem for phrase-based models by means of integer linear programming, and proved that the problem is NP-hard. $$$$$ EXPECTATION, £: Given a weighted sentence pair (e, f, φ) and indices i, j, k,l, compute Ea φ(a) over all a E A such that (eij, fkl) E a.

While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ The number of perfect matchings in G is the sum S for this weighted sentence pair.
While theoretically sound, this approach is computationally challenging both in practice (DeNero et al, 2008) and in theory (DeNero and Klein, 2008), may suffer from reference reachability problems (DeNero et al, 2006), and in the end may lead to inferior translation quality (Koehn et al, 2003). $$$$$ Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP).

The first challenge is with inference $$$$$ The number of perfect matchings in G is the sum S for this weighted sentence pair.
The first challenge is with inference $$$$$ With another construction, we can show that S is #Phard, meaning that it is at least as hard as any #Pcomplete problem.

Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). $$$$$ Furthermore, we introduce binary indicators ei,j and fk,l that denote whether some (eij, ·) or (·, fkl) appears in a, respectively.
Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). $$$$$ A sentence pair consists of two word sequences, e and f. A set of phrases {eij} contains all spans eij from between-word positions i to j of e. A link is an aligned pair of phrases, denoted (eij, fkl).'
Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). $$$$$ 0(eij, fkl) can be sentence-specific, for example encoding the product of a translation model and a distortion model for (eij, fkl).
Our application is also atypical for an NLP application in that we use an approximate sampler not only to include Bayesian prior in formation (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). $$$$$ The score implies that f aligns using all oneword phrases and Vai E a, 0(ai) = 1.

The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). $$$$$ Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipartite graph G with edge set E = {(vj, vl)}, we construct e and f with n words each, and set 0(ej−1 j, fl−1 l) = 1 and 0 otherwise.
The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). $$$$$ This result holds despite the fact that the related problem of finding an optimal matching in a weighted bipartite graph (the ASSIGNMENT problem) is polynomialtime solvable using the Hungarian algorithm.
The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). $$$$$ The number of perfect matchings in G is the sum S for this weighted sentence pair.

However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). $$$$$ The existence of a polynomial time algorithm for £ implies a polynomial time algorithm for S, because A = U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}.
However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). $$$$$ An alignment is a set of links.
However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). $$$$$ Notably, neither of these inference approaches offers any test to know if the optimal alignment is ever found.
However, this has been shown to be infeasible for real-world data (DeNero and Klein, 2008). $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.

It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. $$$$$ Constraint equation 1 ensures that the English phrases form a partition of e – each word in e appears in exactly one phrase – as does equation 2 for f. Constraint equation 3 ensures that each phrase in the chosen partition of e appears in exactly one link, and that phrases not in the partition are not aligned (and likewise constraint 4 for f).
It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. $$$$$ By condition 4, each fassign(v) aligns to all v� or all v in e. Then, assign each v to true if fassign(v) aligns to all v, and false otherwise.
It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. $$$$$ Given a weighted sentence pair, we will consider the space of bijective phrase alignments A: those a C {eij} x {fkl} that use each word token in exactly one link.

A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. $$$$$ CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard.
A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. $$$$$ Marcu and Wong (2002) describes an approximation to O.
A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. $$$$$ Let a weighted sentence pair additionally include a real-valued function 0 : {eij}x{fkl} —* R, which scores links.
A similar approach is employed by DeNero and Klein (2008) for finding optimal phrase-based alignments for MT. $$$$$ Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard.

However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). $$$$$ SUM, S: Given (e, f, φ), compute EaEA φ(a).
However, these models are unsuccessful largely due to intractable estimation (DeNero and Klein, 2008). $$$$$ The existence of a polynomial time algorithm for £ implies a polynomial time algorithm for S, because A = U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}.
