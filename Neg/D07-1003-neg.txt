More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ All results are produced using the standard trec eval program.
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ Our model learns soft alignments as a hidden variable in discriminative training.
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust non lexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ The first step, retrieval, narrows down the search spacefrom a corpus of millions of documents to a focused set of maybe a few hundred using an IR engine, where efficiency and recall are the main focus.

Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ Punyakanok et al used answer-typing in computing edit distance; this is not available in our dataset (and our method does not explicitly carry out answer-typing).
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ in the first sentence is modifying ?com munity?, and therefore ?Henri Hadjenberg?
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ This paper presents a syntax-driven ap proach to question answering, specifically the answer-sentence selection problem forshort-answer questions.

Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ Boldface marks the best score in a column and any scores in that column not significantly worse under a a two-tailed paired t-test (p < 0.03).
Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ For a formal description of QG, we recommendSmith and Eisner (2006).
Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ Augmented with the q.-side dependency la bel.child-parent Question parent-child pair align respectively to answer child-parent pair.

Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Third, we apply a more discriminative training method (condi tional maximum likelihood estimation, Section 4.5).
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Unlike most previous work, our model does not try to find a single correspondence between words in the question and words in the answer, during training or during testing.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Using a mixture gives the advantage of smoothing (in the base model) without hav ing to normalize the log-linear model by summing over large sets.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ The total number of questions in TREC 13 is 230.

 $$$$$ be a candidate an swer sentence.
 $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.
 $$$$$ The idea is that, rather than a synchronous structure over the source and target sentences, a tree over the target sentence is modeled by a source-sentence-specific grammar that is inspired by the source sentence?s tree.1 This is implemented by a ?sense??really just a subsetof nodes in the source tree?attached to each gram mar node in the target tree.
 $$$$$ The player knows the answer (or at least thinks he knows the answer) and must quickly turn it into a question.2 The question-answer pairs used on Jeopardy!

We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ and used intraining.
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ This work is supported in part by ARDA/DTO Advanced Question Answering for Intelligence (AQUAINT) program award number NBCHC040164.
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.

Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ To model the syntactic transformation process, re searchers in these fields?especially in machine translation?have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al, 2005;Galley et al, 2006; Smith and Eisner, 2006, in ter alia).
Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ QG arose out of the empirical observation that translated sentences often have some isomorphic syntactic structure, but not usually in en tirety, and the strictness of the isomorphism may vary across words or syntactic rules.
Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ is the ?leader of ... community?

Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ Our scoring model was found to greatly out perform two state-of-the-art baselines on an answer selection task using the TREC dataset.
Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ Our model learns soft alignments as a hidden variable in discriminative training.
Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ After these data preparation steps, we have 348 positive Q-A pairs for training, 1,415 Q-A pairs in the development set, and 1,703 Q-A pairs in the test set.
Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ be a question sentence (each qi is aword), and let a = ?a1, ..., am?

 $$$$$ config maps question-word i, its parent, and their alignees to a QG configuration as described in Table 1; notethat some configurations are extended with addi tional tree information.
 $$$$$ The second step, selection, assesses each can didate answer string proposed by the first step, and finds the one that is most likely to be an answerto the given question.
 $$$$$ To model the syntactic transformation process, re searchers in these fields?especially in machine translation?have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al, 2005;Galley et al, 2006; Smith and Eisner, 2006, in ter alia).
 $$$$$ Our model learns soft alignments as a hidden variable in discriminative training.

Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (Giampiccolo et al, 2007), paraphrase identification (Dolan et al, 2004), and an answer selection task (Wang et al, 2007) for question answering (Voorhees, 2004). $$$$$ Boldface marks the best score in a column and any scores in that column not significantly worse under a a two-tailed paired t-test (p < 0.03).
Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (Giampiccolo et al, 2007), paraphrase identification (Dolan et al, 2004), and an answer selection task (Wang et al, 2007) for question answering (Voorhees, 2004). $$$$$ We can also observe a trend in recent work in textual entailment that more emphasis is put onexplicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (Mac Cartney et al, 2006).
Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (Giampiccolo et al, 2007), paraphrase identification (Dolan et al, 2004), and an answer selection task (Wang et al, 2007) for question answering (Voorhees, 2004). $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.
Experiments were conducted to evaluate tree edit models for three tasks: recognizing textual entailment (Giampiccolo et al, 2007), paraphrase identification (Dolan et al, 2004), and an answer selection task (Wang et al, 2007) for question answering (Voorhees, 2004). $$$$$ Unlike most synchronous formalisms, QG does not posit a strict iso morphism between the two trees, and it providesan elegant description for the set of local configura tions.

 $$$$$ The top part of the table shows performance using 100 manually-annotated question examples (questions 1?100 in TREC 8?12), and the bottom part adds noisily, automatically annotated questions 101?
 $$$$$ QG arose out of the empirical observation that translated sentences often have some isomorphic syntactic structure, but not usually in en tirety, and the strictness of the isomorphism may vary across words or syntactic rules.
 $$$$$ Let VS be the set of constituent to kens in the source tree.
 $$$$$ We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust non lexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model.

For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ The first variation is thechange of the word ?leader?
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ Our model learns soft alignments as a hidden variable in discriminative training.
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ 5.2 Baseline Systems.
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ syntactic structure, QG is particularly well-suited for QA insofar as QA is like ?free?

The experimental setup is the same as in Wang et al (2007). $$$$$ First, we are not interested in thealignments per se; we will sum them out as a hid den variable when scoring a question-answer pair.
The experimental setup is the same as in Wang et al (2007). $$$$$ Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines.
The experimental setup is the same as in Wang et al (2007). $$$$$ to ?French president.?
The experimental setup is the same as in Wang et al (2007). $$$$$ We might envision transformations that are performed together to form ques tions from answers (or vice versa) and to translate?

We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ We described a statistical syntax-based model that softly aligns a question sentence with a candidateanswer sentence and returns a score.
We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ We described a statistical syntax-based model that softly aligns a question sentence with a candidateanswer sentence and returns a score.
We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ The first is to apply Bayes?

The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ QCFG rules would take the augmented form ?X, S1?
The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ by constraining the cardinality of the Si (weforce all |Si| = 1), and by constraining the relation ships among the Si mentioned in a single rule.

This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ 5.2 Baseline Systems.
This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ This powerful combination leads us to believe that our model can be easily ported to other semantic processing tasks where modeling syntactic and semantic transformations is the key,such as textual entailment, paraphrasing, and cross lingual QA.
This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ between any word and any Wh-word.

 $$$$$ TreeMatch is our implementation of Punyakanok et al (2004); +WN modifies their edit distance function using WordNet.
 $$$$$ w where X,Y, and Z are ordinary CFG nonterminals, each Si ? 2VS (subsets of nodes in the source treeto which the nonterminals align), and w is a targetlanguage word.
 $$$$$ It is important to note that human judgement of answer sentence correctness was carried out prior to any experi ments, and therefore is unbiased.

In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ Furthermore, although the judgement data in our case are more labor-intensiveto obtain, we believe our evaluation method is a better indicator than the TREC evaluation for the qual ity of an answer selection algorithm.To illustrate the point, consider the example question, ?When did James Dean die??
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ The question is ?Who is the leader of France??, and the sentence ?Henri Hadjenberg, who is the leader of France ?s Jewish community, endorsed ...?

Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ QG can be made more or less ?lib eral?
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ The senses define an alignment between the trees.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ The key experimental result of this work is that loose syntactic transformations are an effective way to carry out statistical question answering.One unique advantage of our model is the mix ture of a factored, multinomial-based base model and a potentially very rich log-linear model.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ The QG defines a grammar for this derivation; the grammar de pends on the specific answer.

QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ QG arose out of the empirical observation that translated sentences often have some isomorphic syntactic structure, but not usually in en tirety, and the strictness of the isomorphism may vary across words or syntactic rules.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ We described a statistical syntax-based model that softly aligns a question sentence with a candidateanswer sentence and returns a score.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ The key experimental result of this work is that loose syntactic transformations are an effective way to carry out statistical question answering.One unique advantage of our model is the mix ture of a factored, multinomial-based base model and a potentially very rich log-linear model.
