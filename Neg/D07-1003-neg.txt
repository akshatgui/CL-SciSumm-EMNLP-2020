More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ The total number of questions in TREC 13 is 230.
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ q,?q,a,?a p?(q, ?q,a, ?a) log p?(q, ?q | a, ?a) ? ??
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ with maximizing (max) over alignments on the test set.
More recently, Wang et al (2007) explored the use a formalism called quasi synchronous grammar (Smith and Eisner, 2006) in order to find a more explicit model for matching the set of dependencies, and yet still allow for looseness in the matching. $$$$$ symbol at the left edge of the sentence; it has a single child (|{i : ?(i) = 0}| = 1).

Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ Morerecent attempts have tried to augment the bag-of words representation?which, after all, is simply a real-valued feature vector?with syntactic features.
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ We described a statistical syntax-based model that softly aligns a question sentence with a candidateanswer sentence and returns a score.
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ Our model learns soft alignments as a hidden variable in discriminative training.
Wang et al (2007) use quasi-synchronous translation to map all parent-child paths in a question to any path in an answer. $$$$$ A concrete example that is easy to understand is a binary quasi-synchronous context-free grammar 1Smith and Eisner also show how QG formalisms generalize synchronous grammar formalisms.(denoted QCFG).

Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ The question is ?Who is the leader of France??, and the sentence ?Henri Hadjenberg, who is the leader of France ?s Jewish community, endorsed ...?
Finally, in (Wang et al 2007), a quasi-synchronous grammar (Smith and Eisner,2006) is used to model relations between questions and answer sentences. $$$$$ Using a mixture gives the advantage of smoothing (in the base model) without hav ing to normalize the log-linear model by summing over large sets.

Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Thebase model gives our model robustness, and the log 29 test set training decoding MAP MRR ? ?
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ words.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ At a high level, the QA task boils down to only two essential steps (Echihabi andMarcu, 2003).

 $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.
 $$$$$ Similar stories can also be found in paraphrasing (Quirk et al, 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005).
 $$$$$ The key experimental result of this work is that loose syntactic transformations are an effective way to carry out statistical question answering.One unique advantage of our model is the mix ture of a factored, multinomial-based base model and a potentially very rich log-linear model.

We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ Acknowledgments The authors acknowledge helpful input from three anonymous reviewers, Kevin Gimpel, and David Smith.
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ This paper presents a syntax-driven ap proach to question answering, specifically the answer-sentence selection problem forshort-answer questions.
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ x(j)=0 pkid (qj , ?q(j).lab | qi, ?q(i), x(i), x(j), ?a) ?p(?
We followed the same experimental setup as Wang et al (2007) and Heilman and Smith (2010). $$$$$ Boldface marks the best score in a column and any scores in that columnnot significantly worse under a a two-tailed paired t test (p < 0.03).linear model allows us to throw in task- or domainspecific features.

Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ This involves a slight change to Equation 3, replacing the summation with a maximization.
Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ Hence we treat p(a) as uniform over A.3 The second adjustment adds a labeled, directed dependency tree to the question and the answer.The tree is produced by a state-of-the-art depen dency parser (McDonald et al, 2005) trained on the Wall Street Journal Penn Treebank (Marcus et al., 1993).
Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ pkid defines a distribution over syntactic children of qi and their labels, given (1) the word qi, (2) the parent of qi, (3) the dependency relation between qi and its parent, (4) the answer-word qi is aligned to, (5) the answer-word the child being predicted is aligned to, and (6) the remainder of the answer tree.
Results of replicated systems for the last two were reported by Wang et al (2007), with lexical-semantic augmentation from WordNet. $$$$$ This involves a slight change to Equation 3, replacing the summation with a maximization.

Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ An example of a strict configuration is that a target parent-childpair must align (respectively) to a source parent child pair.
Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ We might envision transformations that are performed together to form ques tions from answers (or vice versa) and to translate?
Results in Table 3 show that our model gives the same level of performance as Wang et al (2007), with no statistically significant difference (p $$$$$ This brings usto the question that drives this work: is there a statistical translation-like model that is natural and accu rate for question answering?

 $$$$$ with maximizing (max) over alignments on the test set.
 $$$$$ We can also observe a trend in recent work in textual entailment that more emphasis is put onexplicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (Mac Cartney et al, 2006).
 $$$$$ We might envision transformations that are performed together to form ques tions from answers (or vice versa) and to translate?
 $$$$$ to ?French president.?

Experiments were conducted to evaluate tree edit models for three tasks $$$$$ The Jeopardy base model and mixture with the lexical-semantics log-linear model perform best; both are trained using conditional maximum likelihood estimation.
Experiments were conducted to evaluate tree edit models for three tasks $$$$$ 0.6029 0.6852 ? max 0.5822 0.6489 max ? 0.5559 0.6250 max max 0.5571 0.6365Table 3: Experimental results on comparing sum ming over alignments (?)
Experiments were conducted to evaluate tree edit models for three tasks $$$$$ Boldface marks the best score in a column and any scores in that columnnot significantly worse under a a two-tailed paired t test (p < 0.03).linear model allows us to throw in task- or domainspecific features.
Experiments were conducted to evaluate tree edit models for three tasks $$$$$ Using a mixture gives the advantage of smoothing (in the base model) without hav ing to normalize the log-linear model by summing over large sets.

 $$$$$ to ?French president.?
 $$$$$ Our QG doesnot model the real-world knowledge required to fill in an an swer; its job is to know what answers are likely to look like, syntactically.
 $$$$$ which simply encodes the identity relation.
 $$$$$ Notice that the QG formalism that we have employed in this work was originally proposed formachine translation.

For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ Notice that the QG formalism that we have employed in this work was originally proposed formachine translation.
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ swer as appeared in the sentence ?In 1955, actor James Dean was killed in a two-car collision nearCholame, Calif.?
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ A concrete example that is easy to understand is a binary quasi-synchronous context-free grammar 1Smith and Eisner also show how QG formalisms generalize synchronous grammar formalisms.(denoted QCFG).
For a given set of questions, the task here is to rank candidate answers (Wang et al, 2007). $$$$$ An example of a strict configuration is that a target parent-childpair must align (respectively) to a source parent child pair.

The experimental setup is the same as in Wang et al (2007). $$$$$ Discriminative training and a relatively straightforward, barelyengineered feature set were used in the implementation.
The experimental setup is the same as in Wang et al (2007). $$$$$ We follow a similar setup to Shen and Klakow (2006) by automatically selecting answer candidate sentences and then comparing against a human-judged gold standard.
The experimental setup is the same as in Wang et al (2007). $$$$$ We propose Smith andEisner?s (2006) quasi-synchronous grammar (Sec tion 3) as a general solution and the Jeopardy model (Section 4) as a specific instance.

We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ The granularity of the tar get answer string varies depending on the type ofthe question.
We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ Rather than using syntactic features to augment exist ing statistical classifiers (as in previouswork), we build on the idea that ques tions and their (correct) answers relate toeach other via loose but predictable syntactic transformations.
We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ Our version of QG, called the Jeopardy model, and our parameter estimation method are described inSection 4.
We compare our tree edit model to three other systems as they are reported by Wang et al (2007). $$$$$ We also report our implementation of Cui et al (2005), along with our WordNet expansion (+WN).

The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ In this section we formally define 2A round of Jeopardy!
The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ It is important to note that human judgement of answer sentence correctness was carried out prior to any experi ments, and therefore is unbiased.
The results for the tree edit model are statistically significantly different (sign test, p < 0.01) from the results for all except the Wang et al (2007) system with WordNet (p $$$$$ They used heuristic functions similar to mutual information to 23 assign scores to matched pairs of dependency links.

This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ 7 Stan-.
This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ (7) 4.4 Lexical-Semantics Log-Linear Model.
This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ Shen and Klakow (2006) extend the idea furtherthrough the use of log-linear models to learn a scor ing function for relation pairs.Echihabi and Marcu (2003) presented a noisy channel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al, 1990; Brown et al, 1993) and applied it to QA.Similarly, Murdock and Croft (2005) adopted a sim ple translation model from IBM model 1 (Brown et al., 1990; Brown et al, 1993) and applied it to QA.Porting the translation model to QA is not straight forward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi andMarcu, 2003) and also replacing the lexical trans lation table with a monolingual ?dictionary?
This includes work on question answering (Wang et al, 2007), sentiment analysis (Nakagawa et al, 2010), MT reordering (Xu et al, 2009), and many other tasks. $$$$$ The usual similarity measures can then be used onthe new feature representation.

 $$$$$ Acknowledgments The authors acknowledge helpful input from three anonymous reviewers, Kevin Gimpel, and David Smith.
 $$$$$ Acknowledgments The authors acknowledge helpful input from three anonymous reviewers, Kevin Gimpel, and David Smith.
 $$$$$ However, relatively fewer attempts have been made in the QA community.
 $$$$$ (It is for computational rea sons that we assume |x(i)| = 1; in general x couldrange over subsets of {1, ...,m}.)

In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ QCFG rules would take the augmented form ?X, S1?
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ Multiple relations may fire, motivating the log-linear model, which permits ?overlapping?
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ Discriminative training and a relatively straightforward, barelyengineered feature set were used in the implementation.
In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al, 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009). $$$$$ They usually take the form of a pipeline architecture, chaining together modules that perform tasks such as answer type analysis (identifying whether the correct answer will be a person, location, date,etc.), document retrieval, answer candidate extrac tion, and answer reranking.

Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ Experimental results comparing our ap proach to two state-of-the-art baselines are presented in Section 5.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ to its semantically re lated term ?president?.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ However, relatively fewer attempts have been made in the QA community.
Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al, 2007). $$$$$ Indeed, in this work, we imagine a generative story for QA in which the question is generatedfrom the answer sentence through a series of syn tactic and semantic transformations.

QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ Discriminative training and a relatively straightforward, barelyengineered feature set were used in the implementation.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ 2,393.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ Using a mixture gives the advantage of smoothing (in the base model) without hav ing to normalize the log-linear model by summing over large sets.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ 0.6029 0.6852 ? max 0.5822 0.6489 max ? 0.5559 0.6250 max max 0.5571 0.6365Table 3: Experimental results on comparing sum ming over alignments (?)
