To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007. $$$$$ This paper presents the coarse-grained En glish all-words task at SemEval-2007.
To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007. $$$$$ We describe our experience in producing acoarse version of the WordNet sense inven tory and preparing the sense-tagged corpusfor the task.
To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007. $$$$$ To overcome this problem, different strategies can be adopted.
To address this issue, a coarse-grained English all-words task (Navigli et al, 2007) was conducted during SemEval-2007. $$$$$ We would like to thank Martha Palmer for providing us the first three texts of the test corpus.

For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%. $$$$$ It is commonly agreed that Word Sense Disambiguation needs emerge and show its usefulness in end to-end applications: after decades of research in the field it is still unclear whether WSD can provide a relevant contribution to real-world applications, such as Information Retrieval, Question Answering,etc. In previous Senseval evaluation exercises, state of-the-art systems achieved performance far below70% and even the agreement between human annotators was discouraging.
For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%. $$$$$ Second, we still need to show that coarse senses can be useful in real applications.
For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%. $$$$$ In order to allow for a critical and comparative inspection of the system results, we asked the partici pants to answer some questions about their systems.
For example, in the English coarse grained all words task (Navigli et al,2007) at the recent SemEval Workshop the base line of choosing the most frequent sense using the first WordNet sense attained precision and recall of 78.9% which is only a few percent lower than the top scoring system which obtained 82.5%. $$$$$ The test data set consisted of 5,377 words of run ning text from five different articles: the first three (in common with Task 17) were obtained from the WSJ corpus, the fourth was the Wikipedia entry for computer programming1, the fifth was an excerpt of Amy Steedman?s Knights of the Art, biographies of Italian painters2.

The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007). $$$$$ the system used semantically-annotated and unannotated resources; 2.
The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007). $$$$$ In order to allow for a critical and comparative inspection of the system results, we asked the partici pants to answer some questions about their systems.
The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007). $$$$$ In Table 4 we report the F1 mea sures for all systems where we used the MFS as abackoff strategy when no sense assignment was at tempted (this possibly reranked 6 systems - marked in bold in the table - which did not assign a sense to all word instances in the test set).
The problem of how to cluster fine-grained senses into coarse senses is hard, especially if consensus is required (Navigli et al 2007). $$$$$ Specifically, the MFS baseline performs more poorly on documents d004 and d005, from the COMPUTER SCIENCE and BIOGRAPHY domains respectively.

Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007). $$$$$ The pairwise agreement between the two authors was 93.80%.
Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007). $$$$$ We describe our experience in producing acoarse version of the WordNet sense inven tory and preparing the sense-tagged corpusfor the task.
Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007). $$$$$ i=1 1 |CoarseSenses(wi)| where T is our test corpus, wi is the i-th word instance in T , and CoarseSenses(wi) is the set ofcoarse senses for wi according to the sense cluster ing we produced as described in Section 2.2.
Similarly, the performance of WSD systems clearly indicates that WSD is not easy unless one adopts a coarse-grained approach, and then systems tagging all words at best perform a few percentage points above the most frequent sense heuristic (Navigli et al, 2007). $$$$$ LCC-WSD ? ?

Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. $$$$$ This paper presents the coarse-grained En glish all-words task at SemEval-2007.
Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. $$$$$ The accuracy of the MFS baseline was calculated as: BLMFS = 1|T | |T |?
Finally, results are presented from the SemEval-2007 coarse grained all-words task (Navigli et al, 2007), and we explore the influence of various types of selectors on the algorithm in order to draw insight for future improvement of Web-based methods. $$$$$ The remaining 2,269 content words thusconstituted the test data set.

The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). $$$$$ We describe our experience in producing acoarse version of the WordNet sense inven tory and preparing the sense-tagged corpusfor the task.
The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). $$$$$ These included information about whether: 1.
The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). $$$$$ Formally, the accuracy of the random baseline was calculated as follows: BLRand = 1|T | |T |?
The sense inventory was created by mapping senses in WordNet 2.1 to the Oxford Dictionary of English (Navigli et al., 2007). $$$$$ RACAI-SYNWSD ? ?

For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1. $$$$$ Wesummarize the participants?
For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1. $$$$$ These included information about whether: 1.
For SemEval 2007, all systems performed better than the random base line of 53.43%, but only 4 of 13 systems achieved an F1 score higher than the MFS baseline of 78.89% (Navigli et al, 2007). Table 2 lists the results of applying the generalized Web selector algorithm described in this paper in a straight-forward manner, such that all scale (T) are set to 1. $$$$$ The accuracy of the MFS baseline was calculated as: BLMFS = 1|T | |T |?

This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure accuracy, the Senseval 2007 coarse WSD dataset (Navigli et al, 2007) has been employed. $$$$$ The accuracy of the MFS baseline was calculated as: BLMFS = 1|T | |T |?
This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure accuracy, the Senseval 2007 coarse WSD dataset (Navigli et al, 2007) has been employed. $$$$$ We present the results of participating systems and discuss future direc tions.
This will allow to asses its applicability to realistic tasks, such as query processing or document indexing. Experimental Set-up In order to measure accuracy, the Senseval 2007 coarse WSD dataset (Navigli et al, 2007) has been employed. $$$$$ 2.4 Inter-Annotator Agreement.

We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007). $$$$$ In the hope of over coming the current performance upper bounds, we 34 System SC DSO SE OMWE XWN WN WND OTHER UC TR MFS CS GPLSI ? ?
We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007). $$$$$ the system was trained on some corpus.
We report our results in terms of precision, recall and F1-measure on the Semeval-2007 coarse-grained all-words dataset (Navigli et al, 2007). $$$$$ MakingWSD an enabling technique for end-to-end applications clearly depends on the ability to deal with rea sonable sense distinctions.

 $$$$$ This paper presents the coarse-grained En glish all-words task at SemEval-2007.
 $$$$$ SUSSX-FR ? ?
 $$$$$ To overcome this problem, different strategies can be adopted.

For the SemEval workshop, only 6 of 15 systems performed better than this baseline on the nouns (Navigli et al, 2007), all of which used MFS as a back off strategy and an external sense tagged data set. $$$$$ i=1 ?(wi, 1) where ?(wi, k) equals 1 when the k-th sense ofword wi belongs to the cluster(s) manually associ ated by the lexicographer to word wi (0 otherwise).
For the SemEval workshop, only 6 of 15 systems performed better than this baseline on the nouns (Navigli et al, 2007), all of which used MFS as a back off strategy and an external sense tagged data set. $$$$$ Table 7: Information about participating systems (SC = SemCor, DSO = Defence Science Organisation Corpus, SE = Senseval corpora, OMWE = Open Mind Word Expert, XWN = eXtended WordNet, WN = WordNet glosses and/or relations, WND = WordNet Domains, UC = use of unannotated corpora, TR = use of training, MFS = most frequent sense backoff strategy, CS = use of coarse senses from the organizers, ?: system from one of the task organizers).proposed the adoption of a coarse-grained sense inventory.

All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007). $$$$$ USYD ? ?
All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007). $$$$$ We repeated the same agreement evaluation onthe sense annotation task of the test corpus.
All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007). $$$$$ It is also interesting to observe that different systemsperform differently on the five documents (we high light in bold the best performing systems on each article).
All systems performing better than the MFS used the heuristic as a back off strategy when unable to output a sense (Navigli et al, 2007). $$$$$ performance by part of speech.

Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007)). $$$$$ NUS-PT ? ?
Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007)). $$$$$ The accuracy of the MFS baseline was calculated as: BLMFS = 1|T | |T |?
Currently supervised methods achieve the best disambiguation quality (about 80% precision and recall for coarse-grained WSD in the most recent WSD evaluation conference SemEval 2007 (Navigli et al, 2007)). $$$$$ These included information about whether: 1.

In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. $$$$$ The remaining 2,269 content words thusconstituted the test data set.
In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. $$$$$ answers to the questionnaires in Table 7.
In the most recent SemEval 2007 (Navigli et al, 2007), the best unsupervised systems only achieved about 70% precision and 50% recall. $$$$$ The test data set consisted of 5,377 words of run ning text from five different articles: the first three (in common with Task 17) were obtained from the WSJ corpus, the fourth was the Wikipedia entry for computer programming1, the fifth was an excerpt of Amy Steedman?s Knights of the Art, biographies of Italian painters2.

We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007). $$$$$ This paper presents the coarse-grained En glish all-words task at SemEval-2007.
We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007). $$$$$ We calculated two baselines for the test corpus: a random baseline, in which senses are chosen at random, and the most frequent baseline (MFS), in which we assign the first WordNet sense to each word in the dataset.
We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007). $$$$$ Specifically, the MFS baseline performs more poorly on documents d004 and d005, from the COMPUTER SCIENCE and BIOGRAPHY domains respectively.
We have evaluated our method using SemEval-2007 Task 07 (Coarse-grained English All-words Task) test set (Navigli et al, 2007). $$$$$ the system used the coarse senses provided by the organizers; 4.

Two authors of (Navigli et al, 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%. $$$$$ For instance, in the OntoNotes project (Hovy et al, 2006) senses are grouped until a 90% inter-annotator agreement is achieved.
Two authors of (Navigli et al, 2007) independently and manually annotated part of the test set (710 word instances), and the pairwise agreement was 93.80%. $$$$$ The main issue is certainly the subjectivity of sense clusters.

We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. $$$$$ i=1 ?(wi, 1) where ?(wi, k) equals 1 when the k-th sense ofword wi belongs to the cluster(s) manually associ ated by the lexicographer to word wi (0 otherwise).
We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. $$$$$ Inventory To tackle the granularity issue, we produced acoarser-grained version of the WordNet sense inven tory3 based on the procedure described by Navigli(2006).
We benchmark our API by performing knowledge based WSD with BabelNet on standard SemEval datasets, namely the SemEval-2007 coarse-grained all-words (Navigli et al, 2007, Coarse-WSD, hence forth) and the SemEval-2010 cross-lingual (Lefever and Hoste, 2010, CL-WSD) WSD tasks. $$$$$ We report about the use of semantic resources as well as semantically annotated corpora (SC = SemCor, DSO = Defence Science Organ isation Corpus, SE = Senseval corpora, OMWE =Open Mind Word Expert, XWN = eXtended Word Net, WN = WordNet glosses and/or relations, WND = WordNet Domains), as well as information about the use of unannotated corpora (UC), training (TR), MFS (based on the SemCor sense frequencies), and the coarse senses provided by the organizers (CS).

Table 1 $$$$$ UPV-WSD ? ?
Table 1 $$$$$ the system used semantically-annotated and unannotated resources; 2.
Table 1 $$$$$ SUSSX-C-WD ? ?
Table 1 $$$$$ This figure, compared to those in the literature for fine-grained human annotations, gives us a clear indication that the agreement of human annotators strictly depends on the granularity of the adopted sense inventory.
