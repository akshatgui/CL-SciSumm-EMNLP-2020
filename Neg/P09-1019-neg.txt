We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ At the beginning, the leftmost line segment of each envelope is inserted into a priority queue pq.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ We first review Minimum Bayes-Risk (MBR) decoding for statistical MT.

Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ Both the translations and their corresponding line segments can efficiently be computed without incorporating any error criterion.
Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ Because the right-hand side of re has n nonterminals, the arity of e is |e |= n. Let T(e) = {v1, ..., vn} denote the tail nodes of e. We now assume that each tail node vi E T(e) is associated with the upper envelope over all candidate translations that are induced by derivations of the corresponding nonterminal symbol Xi.
Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ The complete algorithm then works as follows: Traversing all nodes in x bottom-up in topological order, we proceed for each node v E V over its incoming hyperedges and combine in each such hyperedge e the envelopes associated with the tail nodes T(e) by computing their sum according to Algorithm 1 (n-operation).
Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.

For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008).
For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.
For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ Under such a linear decomposition, the MBR decoder (Equation 1) can be written as Tromble et al. (2008) implement the MBR decoder using Weighted Finite State Automata (WFSA) operations.
For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ This may not be optimal in practice for unseen test sets and language pairs, and the resulting linear loss may be quite different from the corpus level BLEU.

Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ We next extend the Lattice MBR decoding algorithm (Algorithm 3) to rescore hypergraphs produced by a SCFG based MT system.
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ If the arity of a hyperedge is zero, h(e) is called a source vertex.
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ We now describe how MERT can be used to estimate these factors to achieve a better approximation to the corpus BLEU.
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ We recall that MERT selects weights in a linear model to optimize an error criterion (e.g. corpus BLEU) on a training set.

We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ (2008).
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ This linear function contains n + 1 parameters B0, B1, ..., BN, where N is the maximum order of the n-grams involved.
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ Conceptually, the algorithm works by propagating (initially empty) envelopes from the source nodes bottom-up to its unique root node, thereby expanding the envelopes by applying SCFG rules to the partial candidate translations that are associated with the constituent line segments.

The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems.
The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system.
The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ Nodes of Type “∧”: For a type n node, the resulting envelope is the Minkowski sum over the envelopes of the incoming edges (Berg et al., 2008).

These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ The above steps are carried out one n-gram at a time.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ Tromble et al. (2008) obtained these factors as a function of n-gram precisions derived from multiple training runs.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ In this paper, we extend MERT and MBR decoding to work on hypergraphs produced by SCFG-based MT systems.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ The arity of a hypergraph is the maximum arity of its hyperedges.

MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems.
MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1, r2, ..., rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E.
MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008).

While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive.
While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ Translation lattices contain a significantly higher number of translation alternatives relative to Nbest lists.
While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ However, this does not guarantee that the resulting linear score (Equation 2) is close to the corpus BLEU.

Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ A new automaton is then created by intersecting each ngram with weight (from Equation 2) to an unweighted lattice.
Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.
Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system.

Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ The arity of a hypergraph is the maximum arity of its hyperedges.
Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems.
Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ Recently, Tromble et al. (2008) extended MBR decoding to translation lattices under an approximate BLEU score.
Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004).

All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ The algorithm is based on the insight that, under a loglinear model, the cost function of any candidate translation can be represented as a line in the plane if the initial parameter set λM is shifted along a direction dM .
All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive.
All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ Note that N-best MBR uses a sentence BLEU loss function.
All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ We show how MERT can be employed to optimize parameters for MBR decoding.

We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ On an experiment with 40 language pairs, we obtain improvements on 26 pairs, no difference on 8 pairs and drops on 5 pairs.
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ This is necessary because unlike a lattice, new ngrams may be created at subsequent nodes by concatenating words both to the left and the right side of the n-gram.
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ In other cases, Hypergraph MBR performs at least as well as N-best MBR.
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ By doing so, the system can be optimized for the translation task instead of a criterion such as likelihood that is unrelated to the evaluation metric.

(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ The new Lattice MBR decoder achieves a 20X speedup relative to either FSAMBR implementation described in Tromble et al. (2008) or MBR on 1000-best lists.
(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.
(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008).
(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ We hypothesize that the default MBR parameters (Tromble et al., 2008) are well tuned.

However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008).
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ In this paper, we have described how MERT can be employed to estimate the weights for the linear loss function to maximize BLEU on a development set.
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ We now describe how MERT can be used to estimate these factors to achieve a better approximation to the corpus BLEU.
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ These algorithms are more efficient than the lattice-based versions presented earlier.

The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ (2008).
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ The complete algorithm then works as follows: Traversing all nodes in x bottom-up in topological order, we proceed for each node v E V over its incoming hyperedges and combine in each such hyperedge e the envelopes associated with the tail nodes T(e) by computing their sum according to Algorithm 1 (n-operation).
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ (2008).
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ On lattices, it achieves similar run-times as the implementation System BLEU (%) MAP MBR default mert-b mert+b aren.pb 54.2 54.8 54.8 54.9 aren.hier 52.8 53.3 53.5 53.7 aren.samt 53.4 54.0 54.4 54.0 zhen.pb 40.1 40.7 40.7 40.9 zhen.hier 41.0 41.0 41.0 41.0 zhen.samt 41.3 41.8 41.6 41.7 described in Macherey et al. (2008).

These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ These algorithms are more efficient than the lattice-based versions presented earlier.
These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ The last two columns show the average amount of time that is required to compute the upper envelope on hypergraphs.
These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ We therefore approximate the quantity f(e, w, E) with f*(e, w,G) that counts the edge e with n-gram w that has the highest arc posterior probability relative to predecessors in the entire lattice G. f*(e, w,G) can be computed locally, and the n-gram posterior probability based on f* can be determined as follows: Algorithm 3 MBR Decoding on Lattices (Algorithm 3).
These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ Tromble et al. (2008) obtained these factors as a function of n-gram precisions derived from multiple training runs.

The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ Once the envelope has been determined, the translation candidates of its constituent line segments are projected onto their corresponding error counts, thus yielding the exact and unsmoothed error surface for all candidate translations encoded in C. The error surface can now easily be traversed in order to find that under which the new paramM minimizes the global error.
The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ For a moderately large lattice, there can be several thousands of n-grams and the procedure becomes expensive.
The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ Minimum Error Rate Training (MERT) and Minimum Bayes-Risk (MBR) decoding are used in most current state-of-theart Statistical Machine Translation (SMT) systems.
The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ To do that, we introduce an additional feature function gN+1(E, F) equal to the original decoder cost for this sentence.

The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ Tromble et al. (2008) obtained these factors as a function of n-gram precisions derived from multiple training runs.
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists.
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation.
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ In practice, such a cross-product is not prowhere P(e|G) is the posterior probability of a lattice edge.

More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ These envelopes shall be deAlgorithm 2 V-operation (Max) input: array L[0..K-1] containing line objects output: upper envelope of L noted by Env(vi).
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ We believe that our efficient algorithms will make them more widely applicable in both SCFG-based and phrase-based MT systems.
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ A weight assignment of 1.0 for this feature function and zeros for the other feature functions would imply that the MAP translation is chosen.
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ We here extend lattice-based MERT and MBR algorithms to work with hypergraphs that encode a vast number of translations produced by MT systems based on Synchronous Context Free Grammars.
