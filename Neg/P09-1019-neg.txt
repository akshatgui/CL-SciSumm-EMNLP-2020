We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ A weight assignment of 1.0 for this feature function and zeros for the other feature functions would imply that the MAP translation is chosen.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ This was achieved without any need for manual tuning for each language pair.
We learn the weights of this consensus model using hyper graph-based minimum-error-rate training (Kumar et al, 2009). $$$$$ Lattice MBR decoding is obtained under a linear approximation to BLEU, where the weights are obtained using n-gram precisions derived from development data.

Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ These algorithms are more efficient than the lattice-based versions presented earlier.
Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ This linear function contains n + 1 parameters B0, B1, ..., BN, where N is the maximum order of the n-grams involved.
Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ Nodes of Type “∧”: For a type n node, the resulting envelope is the Minkowski sum over the envelopes of the incoming edges (Berg et al., 2008).
Kumar et al (2009) describes an efficient approximate algorithm for computing n-gram posterior probabilities. $$$$$ In this section, we present an extension of the algorithm described in Macherey et al. (2008) that allows us to efficiently compute and represent upper envelopes over all candidate translations encoded in hypergraphs.

For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ For each node t in the lattice, we maintain a quantity Score(w, t) for each n-gram w that lies on a path from the source node to t. Score(w, t) is the highest posterior probability among all edges on the paths that terminate on t and contain n-gram w. The forward pass requires computing the n-grams introduced by each edge; to do this, we propagate n-grams (up to maximum order −1) terminating on each node.
For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ The key idea behind this new algorithm is to rewrite the n-gram posterior probability (Equation 4) as follows: where f(e, w, E) is a score assigned to edge e on path E containing n-gram w: { 1 w ∈ e,p(e|G) > p(e'|G), e' precedes e on E 0 otherwise In other words, for each path E, we count the edge that contributes n-gram w and has the highest edge posterior probability relative to its predecessors on the path E; there is exactly one such edge on each lattice path E. We note that f(e, w, E) relies on the full path E which means that it cannot be computed based on local statistics.
For each system, we report the performance of max-derivation decoding (MAX), hyper graph-based MBR (Kumar et al, 2009), and a linear version of forest-based consensus decoding (CON) (DeNero et al., 2009). $$$$$ The second task consists of systems for 39 language-pairs with English as the target language and trained on at most 300M word tokens mined from the web and other published sources.

Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ On lattices, it achieves similar run-times as the implementation System BLEU (%) MAP MBR default mert-b mert+b aren.pb 54.2 54.8 54.8 54.9 aren.hier 52.8 53.3 53.5 53.7 aren.samt 53.4 54.0 54.4 54.0 zhen.pb 40.1 40.7 40.7 40.9 zhen.hier 41.0 41.0 41.0 41.0 zhen.samt 41.3 41.8 41.6 41.7 described in Macherey et al. (2008).
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ This shows that this feature can allow MBR decoding to backoff to the MAP translation.
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ We employ MERT to select these weights by optimizing BLEU score on a development set.
Table 6 compares exact n-gram posterior computation (Algorithm 1) to the approximation described by Kumar et al (2009). $$$$$ In the latter case, we will have a small set of unique prefixes and suffixes on the tail nodes.

We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ This linear function contains n + 1 parameters B0, B1, ..., BN, where N is the maximum order of the n-grams involved.
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ We show how MERT can be employed to optimize parameters for MBR decoding.
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ This may not be optimal in practice for unseen test sets and language pairs, and the resulting linear loss may be quite different from the corpus level BLEU.
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ A translation lattice compactly encodes a large number of hypotheses produced by a phrase-based SMT system.

The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ To derive the algorithm, we consider the general case of a hyperedge e with rule re : X —* w1X1w2...wnXnwn+1.
The cdec MERT implementation performs inference over the decoder search space which is structured as a hyper graph (Kumar et al, 2009). $$$$$ This increase is mainly due to the representation of line segments; while the phraselattice implementation stores a single backpointer, the hypergraph version stores a vector of backpointers.

These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ Because the right-hand side of re has n nonterminals, the arity of e is |e |= n. Let T(e) = {v1, ..., vn} denote the tail nodes of e. We now assume that each tail node vi E T(e) is associated with the upper envelope over all candidate translations that are induced by derivations of the corresponding nonterminal symbol Xi.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ Therefore, we would like to allow the decoder to backoff to the MAP translation in such cases.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1, r2, ..., rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E.
These experiments demonstrate the efficiency of our algorithm which is shown empirically to be two orders of magnitude faster than Tromble et al (2008) and more than 3 times faster than even an approximation algorithm specifically designed for this problem (Kumar et al, 2009). $$$$$ : Algorithm 1 shows the pseudo code for computing the Minkowski sum over multiple envelopes.

MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ The arity of a hypergraph is the maximum arity of its hyperedges.
MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ For multi-language systems, we only show the # of language-pairs with gains/no-changes/drops for each MBR variant with respect to the MAP translation.
MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ The number of tail vertices is called the arity (|e|) of the hyperedge.
MERT learns parameters from forests (Kumar et al, 2009) with 4 restarts and 8 random directions in each iteration. $$$$$ A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1, r2, ..., rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E.

While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ Suppose we have a hyperedge e with rule re : X —* aX1bX2c and T(e) = {v1, v2}.
While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ In the NIST systems, MERT yields small improvements on top of MBR with default parameters.
While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ In this paper, we have described how MERT can be employed to estimate the weights for the linear loss function to maximize BLEU on a development set.
While other fast MBR approximations are possible (Kumar et al, 2009), we show how the exact path posterior probabilities can be calculated and applied in the implementation of Equation (1) for efficient MBR decoding over lattices. $$$$$ If the arity of a hyperedge is zero, h(e) is called a source vertex.

Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ The algorithms were originally to work with lists of translations, and recently extended to lattices that encode many more hypotheses typical lists.
Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ We recall that MERT selects weights in a linear model to optimize an error criterion (e.g. corpus BLEU) on a training set.
Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ Tromble et al. (2008) obtained these factors as a function of n-gram precisions derived from multiple training runs.
Rather than computing an error surface using k best approximations of the decoder search space, cdec's implementation performs inference over the full hyper graph structure (Kumar et al, 2009). $$$$$ Chiang (2007), Zollmann and Venugopal (2006), Mi et al. (2008)) is a directed hypergraph or a packed forest (Huang, 2008).

Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ The rules associated with every hyperedge specify how line segments in the envelopes of a hyperedge’s tail nodes can be combined.
Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ Our experiments show speedups from MERT and MBR as well as performance improvements from MBR decoding on several language pairs.
Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer. This algorithm is equivalent to the hyper graph MERT algorithm described by Kumar et al (2009). $$$$$ The algorithm gives comparable results relative to FSAMBR.

All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ For any particuthe decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment.
All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ These algorithms are more efficient than the lattice-based versions presented earlier.
All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al (2009). $$$$$ When the arity of the edge is 2, a rule has the general form aX1bX2c, where X1 and X2 are sequences from tail nodes.

We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ Algorithm 4 is an extension to the MBR decoder on lattices Algorithm 4 MBR Decoding on Hypergraphs hibitive when the maximum n-gram order in MBR does not exceed the order of the n-gram language model used in creating the hypergraph.
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ Then we substitute X1 and X2 in the rule with candidate translations associated with line segments in envelopes Env(v1) and Env(v2) respectively.
We report results using the Moses implementation of Viterbi, nbest MBR and lattice MBR decoding (Kumar et al, 2009). $$$$$ For any particuthe decoder seeks that translation which yields the largest score and therefore corresponds to the topmost line segment.

(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ Lattice MBR decoding is obtained under a linear approximation to BLEU, where the weights are obtained using n-gram precisions derived from development data.
(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ A hyperedge of arity 1 is a regular edge, and a hypergraph of arity 1 is a regular graph (lattice).
(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ If we think of statistical MT as a classifier that maps a source sentence F to a target sentence E, the MBR decoder can be expressed as follows: where L(E, E') is the loss between any two hypotheses E and E', P(E|F) is the probability model, and 9 is the space of translations (N-best list, lattice, or a hypergraph).
(Kumar et al, 2009) mention that the linear approximation to BLEU used in their lattice MBR algorithm is not guaranteed to match corpus BLEU, especially on unseen test sets. $$$$$ A path in a translation hypergraph induces a translation hypothesis E along with its sequence of SCFG rules D = r1, r2, ..., rK which, if applied to the start symbol, derives E. The sequence of SCFG rules induced by a path is also called a derivation tree for E.

However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ Then we substitute X1 and X2 in the rule with candidate translations associated with line segments in envelopes Env(v1) and Env(v2) respectively.
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ In this work, hypergraphs were rescored to maximize the expected count of synchronous constituents in the translation.
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ The above steps are carried out one n-gram at a time.
However, due to the unified nature of the training and decoding criterion in our approach, the minimum risk trained weights can be plugged directly into the sampler MBR decoder, whereas lattice MBR requires an additional expensive step of tuning the model hyper-parameters (Kumar et al, 2009). $$$$$ A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008).

The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice.
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ For these systems, a hypergraph or packed forest provides a compact representation for encoding a huge number of translation hypotheses (Huang, 2008).
The feature weight vector w in Equation 1 is tuned by MERT over hyper graphs (Kumar et al, 2009). $$$$$ A related MBR-inspired approach for hypergraphs was developed by Zhang and Gildea (2008).

These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ If E and E' are the reference and the candidate translations respectively, this linear function is given by: where w is an n-gram present in either E or E', and θ0,θ1,..., θN are weights which are determined empirically, where N is the maximum ngram order.
These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ SMT systems based on synchronous context free grammars (SCFG) (Chiang, 2007; Zollmann and Venugopal, 2006; Galley et al., 2006) have recently been shown to give competitive performance relative to phrase-based SMT.
These approaches include the work of Kumar and Byrne (2004), which re-ranks the n best output of a MT decoder, and the work of Tromble et al (2008) and Kumar et al (2009), which does MBR decoding for lattices and hyper graphs. $$$$$ Lattice MBR decoding uses a linear approximation to the BLEU score (Papineni et al., 2001); the weights in this linear loss are set heuristically by assuming that n-gram precisions decay exponentially with n. However, this may not be optimal in practice.

The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ This may not be optimal in practice for unseen test sets and language pairs, and the resulting linear loss may be quite different from the corpus level BLEU.
The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ In this section, we present an extension of the algorithm described in Macherey et al. (2008) that allows us to efficiently compute and represent upper envelopes over all candidate translations encoded in hypergraphs.
The line optimisation procedure can also be applied to a hyper graph representation of the hypotheses (Kumar et al, 2009). $$$$$ We show how MERT can be employed to optimize parameters for MBR decoding.

The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ These two techniques were originally developed for N-best lists of translation hypotheses and recently extended to translation lattices (Macherey et al., 2008; Tromble et al., 2008) generated by a phrase-based SMT system (Och and Ney, 2004).
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ Therefore, we would like to allow the decoder to backoff to the MAP translation in such cases.
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ Therefore there is little gain by additional tuning using MERT.
The LMERT and TGMERT optimisation algorithms are particularly suitable for this realisation of hiero in that the lattice representation avoids the need to use the hyper graph formulation of MERT given by Kumar et al (2009). $$$$$ Algorithm 2 contains the pseudo code.

More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ We present algorithms that are more efficient relative to the lattice algorithms presented in Macherey et al. (2008; Tromble et al.
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ The arity of a hypergraph is the maximum arity of its hyperedges.
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ This shows that this feature can allow MBR decoding to backoff to the MAP translation.
More recently, this algorithm was extended to work with hyper graphs encoding a huge number of translations produced by MT systems based on Synchronous Context Free Grammars (Kumar et al, 2009). $$$$$ On an experiment with 40 language pairs, we obtain improvements on 26 pairs, no difference on 8 pairs and drops on 5 pairs.
