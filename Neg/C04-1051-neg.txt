We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ These sentence pairs were checked by an independent human evaluator to ensure that they contained paraphrases before they were tagged for alignments.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ Of course a broad-domain SMT-influenced paraphrase solution will require very large corpora of sentential paraphrases.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.

The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ In our experience, many alignment errors are present in one side but not the other, hence this recombination also serves to filter noise from the process.
The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.

The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ One approach1 1 An alternative approach involves identifying anchor points--pairs of words linked in a known way--and collecting the strings that intervene.
The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.
The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ Pairings that were identical or differing only by punctuation were rejected, as were those where the shorter sentence in the pair was less than two thirds the length of the longer, this latter constraint in effect placing an upper bound on edit distance relative to the length of the sentence.

For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ The first used string edit distance, counting the number of lexical deletions and insertions needed to transform one string into another.
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och.
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text.
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ Discrete events like disasters, business announcements, and deaths tend to yield tightly focused clusters, while ongoing stories like the SARS crisis tend to produce less focused clusters.

We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och.
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ Given a large dataset and a well-motivated clustering of documents, useful datasets can be gleaned even without resorting to more sophisticated techniques Figure 2.
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ Our method is believed to be independent of the specific clustering technology used.

For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ To quantify the differences between L12 and F2, we randomly chose 100 sentence pairs from each dataset and counted the number of times each phenomenon was encountered.
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ It is common, for example, to find instances of clausal Reordering combined with Synonymy.
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words).
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.

Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ The field of SMT, long focused on closely aligned data, is only now beginning to address the kinds of problems immediately encountered in monolingual paraphrase (including phrasal translations and large scale reorderings).

Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ While several different learning methods have been applied to this problem, all share a need for large amounts of data in the form of pairs or sets of strings that are likely to exhibit lexical and/or structural paraphrase alternations.
Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ Lee & Barzilay (2003), for example, use Multi Sequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts.
Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ Some of these articles represent minor rewrites of an original AP or Reuters story, while others represent truly distinct descriptions of the same basic facts.

 $$$$$ Because the AER is asymmetric (though each direction differs by less than 5%), we have presented the average of the directional AERs.
 $$$$$ In this paper we have described just one example of a class of data extraction techniques that we hope will scale to this task.
 $$$$$ Lee & Barzilay (2003), for example, use Multi Sequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts.

 $$$$$ Features might include edit distance, temporal/topical clustering information, information about cross-document discourse structure, relative sentence length, and synonymy information.
 $$$$$ Reordering: Words, phrases, or entire constituents occur in different order in two related sentences, either because of major syntactic differences (e.g. topicalization, voice alternations) or more local pragmatic choices (e.g. adverb or prepositional phrase placement).
 $$$$$ On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively.
 $$$$$ We feel that this is a natural extension of the body of recent developments in SMT; perhaps explorations in monolingual data may have a reciprocal impact.

 $$$$$ A leading brain surgeon has been suspended from work following a dispute over a bowl of soup.
 $$$$$ The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
 $$$$$ Reordering: Words, phrases, or entire constituents occur in different order in two related sentences, either because of major syntactic differences (e.g. topicalization, voice alternations) or more local pragmatic choices (e.g. adverb or prepositional phrase placement).
 $$$$$ Our method is believed to be independent of the specific clustering technology used.

Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ A lawyer for Skakel says there is a claim that the murder was carried out by two friends of one of Skakel's school classmates, Tony Bryan.
Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ To isolate just those sentence pairs that represent likely paraphrases without requiring significant string similarity, we exploited a common journalistic convention: the first sentence or two of 3A maximum Levenshtein distance of 12 was selected for the purposes of this paper on the basis of experiments with corpora extracted at various edit distances.
Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ Finally we combined the two annotations into a single gold standard in the following manner: if both annotators agreed that an alignment should be SURE, then the alignment was marked as sure in the gold-standard; otherwise the alignment was marked as POSSIBLE.

the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ The story text is isolated from a sea of advertisements and other miscellaneous text through use of a supervised HMM.
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ Hand evaluation, though, indicates that many of the phenomena that we are interested in learning may be absent from this L12 data.
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ Some of these articles represent minor rewrites of an original AP or Reuters story, while others represent truly distinct descriptions of the same basic facts.

 $$$$$ We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques.
 $$$$$ We conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques.
 $$$$$ To quantify the differences between L12 and F2, we randomly chose 100 sentence pairs from each dataset and counted the number of times each phenomenon was encountered.
 $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.

In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ Thus while string difference methods may produce relatively clean training data, this is achieved at the cost of filtering out common (and interesting) paraphrase relationships.
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ To explore some of the differences between the training sets, we hand-examined a random sample of sentence pairs from each corpus type.
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ To explore some of the differences between the training sets, we hand-examined a random sample of sentence pairs from each corpus type.
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ The Chosun Ilbo said development of the new missile, with a range of up to %%number%% kilometres (%%number%% miles), had been completed and deployment was imminent.

A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ 2 Barzilay & McKeown (2001) consider the possibility of using SMT machinery, but reject the idea because of the noisy, comparable nature of their dataset.

More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ Sometimes, however, the strategy of pairing sentences based on their cluster and position goes astray.
More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ Techniques like our F2 extraction strategies appear to extract a more diverse variety of data, but yield more noise.
More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ Table 1 shows the results of training translation models on data extracted by both methods and then tested on the blind data.

Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ Are the types of data significantly different in character or utility?
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data.
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ This pair displays one Spelling alternation (defence / defense), one Reordering (position of the ?since?
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ Several major differences stand out between the two data sets.

 $$$$$ Sometimes, however, the strategy of pairing sentences based on their cluster and position goes astray.
 $$$$$ The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words).
 $$$$$ We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och.
 $$$$$ We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och.

We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details. $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details. $$$$$ These categories do not cover all possible alternations between pairs of paraphrased sentences; moreover, categories often overlap in the same sequence of words.
We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details. $$$$$ We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och.
We evaluated the systems performance across two datasets: (Dolan et al, 2004) dataset and the Extended dataset, see the text for details. $$$$$ We begin with sets of pre-clustered URLs which point to news articles on the Web, representing thousands of different news sources.
