We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ This is particularly striking given the noise inherent in the F2 training data.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ Two separate techniques were employed to extract likely pairs of sentential paraphrases from these clusters.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ In this case all instances were counted.
We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al 2004). $$$$$ We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text.

The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ The importance of learning to manipulate monolingual paraphrase relationships for applications like summarization, search, and dialog has been highlighted by a number of recent efforts (Barzilay & McKeown 2001; Shinyama et al 2002; Lee & Barzilay 2003; Lin & Pantel 2001).
The dataset is 5,801 pairs of sentences collected from news sources (Dolan et al, 2004). $$$$$ Lower-frequency changes that fell outside of the above categories were not tallied: for example, the presence or absence of a definite article (had authority / had the authority) in Figure 2 was ignored.

The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ Such data would be amenable to conventional statistical machine translation (SMT) techniques (e.g., those discussed in Och & Ney 2003).2 In what follows we compare two strategies for unsupervised construction of such a corpus, one employing string similarity and the other associating sentences that may overlap very little at the string level.
The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.
The F2 dataset was constructed from the first two sentences of the corpus on the same assumptions as those used in Dolan et al (2004). $$$$$ A corpus was produced by extracting the first two sentences of each article, then pairing these across documents within each cluster.

For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text.
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words).
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ AER measures how accurately an automatic algorithm can align words in corpus of parallel sentence pairs, with a human 4 This contrasts with 16.7% pairs assessed as unrelated in a 10,000 pair sampling of the L12 data.
For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al2004). $$$$$ The clusters are generally coherent in topic and focus.

We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ Are the types of data significantly different in character or utility?
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase.
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ This is particularly striking given the noise inherent in the F2 training data.
We employ 8 different MT metrics for identifying paraphrases across two different datasets the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al, 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al, 2010). $$$$$ The story text is isolated from a sea of advertisements and other miscellaneous text through use of a supervised HMM.

For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ The topical clustering is sufficiently precise to ensure that, in general, articles in the same cluster overlap significantly in overall semantic content.
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ Sometimes, however, the strategy of pairing sentences based on their cluster and position goes astray.
For instance, with the advent of news aggregator services such as GoogleNews, one can readily collect multiple news stories covering the same news item (Dolan et al,2004). $$$$$ However, there is a disparity between the kinds of paraphrase alternations that we need to be able to align and those that we can already align well using current SMT techniques.

Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation.
Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation.
Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ A given sentence pair might exhibit multiple instances of a single phenomenon, such as two phrasal paraphrase changes or two synonym replacements.
Dolan et al (2004) used Web-aggregated news stories to learn both sentence-level and word-level alignments. $$$$$ The combination of the first-two sentences heuristic plus topical article clusters allows us to take advantage of meta-information implicit in our corpus, since clustering exploits lexical information from the entire document, not just the few sentences that are our focus.

Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ An additional filter ensured that the word count of the shorter sentence is at least one-half that of the longer sentence.
Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ The best overall performance, irrespective of test data type, is achieved by the L12 training set, with an 11.58% overall AER on the 250 sentence pair edit distance test set (20.88% AER for non-identical words).
Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ The clustering algorithm takes into account the full text of each news article, in addition to temporal cues, to produce a set of topically and temporally related articles.
Levenshtein distance has been used in natural language processing field as a component in the variety of tasks, including semantic role labeling (Tjong Kim Sang et al, 2005), construction of the paraphrase corpora (Dolan et al, 2004), evaluation of machine translation output (Leusch et al, 2003), and others. $$$$$ An artificial bacteria-eating virus has been made from synthetic genes in the record time of just two weeks.

 $$$$$ The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data.
 $$$$$ While exact duplicate articles are filtered out of the clusters, many slightly-rewritten variants remain.
 $$$$$ We have also benefited from discussions with Ken Church, Mark Johnson, Daniel Marcu and Franz Och.

 $$$$$ Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase.
 $$$$$ We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text.
 $$$$$ Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set.

 $$$$$ Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster.
 $$$$$ The Hartford Courant reported %%day%% that Tony Bryant said two friends were the killers.
 $$$$$ Our method is believed to be independent of the specific clustering technology used.

Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ phrase), and one example of Elaboration (terror attacks occurs in only one sentence).
Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ This is particularly striking given the noise inherent in the F2 training data.
Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
Although the F2 heuristic proposed by Dolan et al (2004), which takes the first two sentences of each document pair, obtains higher relatedness score (we evaluated F2 sentences as 50% paraphrases, 37% related, and 13% unrelated), our n-gram overlap method extracted much more sentence pairs per document pair. $$$$$ In the near term, however, the relatively similar performances of F2 and L12-trained models on the F2 test data suggest that with further refinements, this more complex type of data can achieve good results.

the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ Lee & Barzilay (2003), for example, use Multi Sequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts.
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ To compute Precision, Recall, and Alignment Error Rate (AER) for the twin datasets, we used exactly the formulae listed in Och & Ney (2003).
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ An additional filter ensured that the word count of the shorter sentence is at least one-half that of the longer sentence.
the Microsoft Research Paraphrase Corpus (Dolan et al, 2004) [MSR04]. $$$$$ Some are non-compositional idioms (has pulled the plug on / is dropping plans for); others involve different phrasing (electronically / in electronic form, more than a million people / a massive crowd).

 $$$$$ The F2 training data is probably too sparse and, with 40% unrelated sentence pairs, too noisy to achieve equally good results; nevertheless the gap between the results for the two training data types is dramatically narrower on the F2 test data.
 $$$$$ Such data would be amenable to conventional statistical machine translation (SMT) techniques (e.g., those discussed in Och & Ney 2003).2 In what follows we compare two strategies for unsupervised construction of such a corpus, one employing string similarity and the other associating sentences that may overlap very little at the string level.
 $$$$$ Phrasal: An entire group of words in one sentence alternates with one word or a phrase in the other.
 $$$$$ More data will surely help.

In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ We remain, however, responsible for all content.
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation.
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ Data collection unrelated.
In contrast, traditional paraphrase detection (Dolan et al, 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. $$$$$ We show that although the edit distance corpus is well-suited as training data for the alignment algorithms currently used in SMT, it is an incomplete source of information about paraphrase relations, which exhibit many of the characteristics of comparable bilingual corpora or free translations.

A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ Discrete events like disasters, business announcements, and deaths tend to yield tightly focused clusters, while ongoing stories like the SARS crisis tend to produce less focused clusters.
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ Edit distance identifies sentence pairs that exhibit lexical and short phrasal alternations that can be aligned with considerable success.
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ The clusters are generally coherent in topic and focus.
A few unsupervised metrics have been applied to automatic paraphrase identification and extraction (Barzilay and McKeown, 2001) and (Dolan et al,2004). $$$$$ However, when comparing one human against another, both comparison and reference distinguish between SURE and POSSIBLE links.

More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ Accordingly, the dataset encompasses many paraphrases that would have been excluded under a more stringent edit-distance threshold, for example, the following non-paraphrase pair that contain an element of paraphrase: A staggering %%number%% million Americans have been victims of identity theft in the last five years , according to federal trade commission survey out this week.
More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ We begin with sets of pre-clustered URLs which point to news articles on the Web, representing thousands of different news sources.
More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ The field of SMT, long focused on closely aligned data, is only now beginning to address the kinds of problems immediately encountered in monolingual paraphrase (including phrasal translations and large scale reorderings).
More recently, (Cordeiro et al, 2007a) proposed the sumo metric specially designed for asymmetrical entailed pair identification in corpora which obtained better performance than previously established metrics, even in corpora with exclusively symmetrical entailed paraphrases as in the Microsoft Paraphrase Research Corpus (Dolan et al., 2004). $$$$$ Their goal is to extract sentential templates that can be used in high-precision generation of paraphrase alter nations within a limited domain.

Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ More data will surely help.
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ We measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text.
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ Lee & Barzilay (2003), for example, use Multi Sequence Alignment (MSA) to build a corpus of paraphrases involving terrorist acts.
Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al, 2006) and paraphrase identification (Dolan et al, 2004) are a few prototypical examples. $$$$$ In this case all instances were counted.

 $$$$$ Given a large dataset and a well-motivated clustering of documents, useful datasets can be gleaned even without resorting to more sophisticated techniques Figure 2.
 $$$$$ A given sentence pair might exhibit multiple instances of a single phenomenon, such as two phrasal paraphrase changes or two synonym replacements.
 $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.
 $$$$$ We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources.

We evaluated the systems performance across two datasets $$$$$ Our a priori assumption was that the lower the AER for a corpus, the more likely it would be to yield learnable information about paraphrase alternations.
We evaluated the systems performance across two datasets $$$$$ The nearly comparable numbers for the two training data sets, at 13.2% and 14.7% respectively, suggest that the L12 training corpus provides no substantive advantage over the F2 data when tested on the more complex test data.
We evaluated the systems performance across two datasets $$$$$ such as union to maximize recall or intersection to maximize precision.
We evaluated the systems performance across two datasets $$$$$ Table 1 shows the results of training translation models on data extracted by both methods and then tested on the blind data.
