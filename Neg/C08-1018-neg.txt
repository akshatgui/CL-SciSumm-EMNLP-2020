Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. $$$$$ Most prior work has focused on a specific instantiation of sentence compression, namely word deletion.
Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. $$$$$ We present a noveltree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including a ngram language model as a feature.
Different from prior research, Cohn and Lapata (2008) achieved sentence compression using a combination of several operations including word deletion, substitution, insertion, and reordering based on a statistical model, which is similar to our paraphrase generation process. $$$$$ In this paper we generalise the sentence compression task.

 $$$$$ Note that the prenominal modifiers US Navy Sea Stallion and the verb used have been removed.
 $$$$$ bad weather and the infinitive clause to transport to the present participle transporting.
 $$$$$ In this section we present our experimental set up for assessing the performance of our model.
 $$$$$ Pre-terminal categories are not shown for the output trees for the sake of brevity.

For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). $$$$$ ??(y ? )??(y), ??
For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). $$$$$ From this we extracted grammar rules following the technique described in Cohn and Lapata (2007).
For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). $$$$$ Specifi cally, we generalise the model of Cohn and Lapata (2007) to our abstractive task.
For example, one may want a text to be shorter (Cohn and Lapata, 2008), tailored to some reader profile (Zhu et al, 2010), compliant with some specific norms (Max, 2004), or more adapted for subsequent machine processing tasks (Chandrasekar et al., 1996). $$$$$ 5a.

Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ We collected ratings from 22 unpaid volunteers, all self reported native English speakers.
Notable exceptions are Cohn and Lapata (2008) and Zhao et al (2009) who present a model that can both compress and paraphrase individual sentences without however generating document-level summaries. $$$$$ The model incorporates a novelgrammar extraction method, uses a lan guage model for coherent output, and canbe easily tuned to a wide range of compres sion specific loss functions.

Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ The latter was trained on an extractive compression corpus drawn from the BNC (Clarke, 2008) and tunedto provide a similar compression rate to our sys tem.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ with compression in mind.
Moreover, models developed for sentence compression have been mostly designed with one rewrite operation in mind, namely word deletion, and are thus unable to model consistent syntactic effects such as reordering, sentence splitting, changes in non-terminal categories, and lexical substitution (but see Cohn and Lapata 2008 and Zhao et al 2009 for notable exceptions). $$$$$ From this we extracted grammar rules following the technique described in Cohn and Lapata (2007).

Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). $$$$$ Our paraphrase grammar extraction method uses bilingual pivoting to learn paraphrases over syntax tree fragments, i.e., STSG rules.
Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). $$$$$ These 142 Models Grammaticality Importance CompR Extract 3.10 ? 2.43 ? 82.5 Abstract 3.38 ? 2.85 ? ?
Although abstractive methods have also been proposed (Cohn and Lapata, 2008), and they may shed more light on how people compress sentences, they do not always manage to outperform extractive methods (Nomoto, 2009). $$$$$ In this paper we generalise the sentence compression task.

Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. $$$$$ Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.
Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. $$$$$ The Ham ming loss over tokens performed best with a meanrating of 3.18, closely followed by the edit dis tance (3.17).
Cohn and Lapata (2008) have also developed an abstractive version of T3, which was reported to outperform the original, extractive T3 in meaning preservation; there was no statistically significant difference in grammaticality. Finally, Nomoto (2009) presented a two-stage extractive method. $$$$$ The decoder maximises over this space: y ? =argmax y:S(y)=x ?(y) (1) where ?(y) = ? r?y ??(r, S(y)), ??

Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. $$$$$ ?a song, a song?
Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. $$$$$ This grammar will contain many rules encoding deletions and structural transformations but there will be many unobserved paraphrases, no matter how good the extraction method (recall that our corpus consistssolely of 565 sentences).
Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. $$$$$ Note that the English rules and for eign strings shown include variable indices where they have been generalised.
Our approach follows Cohn and Lapata (2008), who expand the task to include substitutions, insertions and reorderings that are automatically learned from parallel texts. $$$$$ Examples include the generation of sub titles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003).

We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). $$$$$ We present a new corpus that is suitedto our task and a discriminative tree-to tree transduction model that can naturallyaccount for structural and lexical mis matches.
We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). $$$$$ 2b.
We compared the Gibbs sampling compressor (GS) against a version of maximum a posteriori EM (with Dirichlet parameter greater than 1) and a discriminative STSG based on SVM training (Cohn and Lapata, 2008) (SVM). $$$$$ Pivoting treats the paraphrasing problem as a two-stage translation process.

The comparison system described by Cohn and Lapata (2008) attempts to solve a more general problem than ours, abstractive sentence compression. $$$$$ Acknowledgements The authors acknowledge the support of EPSRC (grants GR/T04540/01 and GR/T04557/01).

In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). $$$$$ In this paper we generalise the sentence compression task.
In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). $$$$$ E: Kurtz came from Missouri, and at 14, hitch-hiked to Los Angeles seeking top diving coaches.
In our experiments with the publicly available SVM system we used all except para phrasal rules extracted from bilingual corpora (Cohn and Lapata, 2008). $$$$$ The Ham ming loss over tokens performed best with a meanrating of 3.18, closely followed by the edit dis tance (3.17).

For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). $$$$$ < 0.01).
For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). $$$$$ < 0.01).
For example, a different compression model could incorporate rewriting rules to enable compressions that go beyond word deletion, as in Cohn and Lapata (2008). $$$$$ Here, deletions arealso the most common rewrite operation (69%) fol lowed by substitutions (24%), and insertions (7%).

Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008). $$$$$ For example, with precision loss the time complexity is O(S 3 R) as each step must consider O(S 2) pos sible loss parameter values.
Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008). $$$$$ Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.
Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008). $$$$$ Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.
Another future direction is to extend our ILP formulations to more sophisticated models that go beyond word deletion, like the ones proposed by Cohn and Lapata (2008). $$$$$ Examples include the generation of sub titles from spoken transcripts (Vandeghinste and Pan, 2004), the display of text on small screens such as mobile phones or PDAs (Corston-Oliver, 2001), and, notably, summarisation (Jing, 2000; Lin, 2003).

 $$$$$ Our loss functions are tailored to the task anddraw inspiration from metrics developed for ex tractive compression but also for summarisation and machine translation.
 $$$$$ Rules were drawn from the training set, bilingual pivoting and directly from the source trees.
 $$$$$ More sophisticated features could allow the system to narrow this gap.
 $$$$$ Rather than sim ply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion.

Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). $$$$$ Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)),a formalism that can account for structural mismatches, and is trained discriminatively.
Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). $$$$$ Our materials thus con sisted of 90 (30 ? 3) source-target sentences.
Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). $$$$$ Indeed, a variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (Knight and Marcu, 2002; Galley and McKeown, 2007;Turner and Charniak, 2005), to large-margin learn ing (McDonald, 2006; Cohn and Lapata, 2007), and Integer Linear Programming (Clarke, 2008).
Abstractive techniques in text summarization include sentence compression (Cohn and Lapata, 2008), headline generation (Soricut and Marcu, 2007), and canned-based generation (Oakes and Paice, 2001). $$$$$ Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).

The first abstractive compression method was proposed by Cohn and Lapata (2008). $$$$$ The scope for errors is greatly reduced when performing solely deletions.Finally, both the abstractive and extractive out puts are perceived as significantly worse than the gold standard both in terms of grammaticalityand importance (?
The first abstractive compression method was proposed by Cohn and Lapata (2008). $$$$$ For the pivot grammarwe use the French-English Europarl v2 which con tains approximately 688K sentences.
The first abstractive compression method was proposed by Cohn and Lapata (2008). $$$$$ In this paper we generalise the sentence compression task.

The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. $$$$$ Here, the conjunc tion high winds and snowfalls is abbreviated to 2 The term ?compression rate?
The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. $$$$$ to training is the search for a derivation which is both high scoring and has high loss compared to the gold standard.
The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. $$$$$ Our materials thus con sisted of 90 (30 ? 3) source-target sentences.
The dataset that Cohn and Lapata (2008) used to learn transduction rules consists of 570 pairs of source sentences and abstractive compressions. $$$$$ This training method allows the use of a configurable loss function, ?(y ? ,y), whichmeasures the extent to which the model?s predic tion, y, differs from the reference, y ? .

We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. $$$$$ In contrast to the previous loss func tions, this requires the true positive counts to be clipped to the number of occurrences of each type in the reference.
We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. $$$$$ The numbered boxes in the rules denote linked variables.
We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. $$$$$ Recall that training the basic model of Cohn and Lapata (2007) requires finding the maximiserof H(y) in (3).
We did not use source sentences from the other 30 documents, because they were used by Cohn and Lapata (2008) to build their abstractive dataset (Section 2), from which we drew source sentences for our dataset. $$$$$ Grammar It is relatively straightforward to extract a grammar from our corpus.

This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). $$$$$ At the same time it deleted the verb and its adjunct from the first conjunct (camefrom Missouri ) as well as the temporal modi fier at the age of 14 from the second conjunct.
This work introduces a model free approach to sentence compression, which grew out of ideas from Nomoto (2008), and examines how it com pares to a state-of-art model intensive approach known as Tree-to-Tree Transducer, or T3 (Cohn and Lapata, 2008). $$$$$ The annotators weregiven instructions that explained the task and defined sentence compression with the aid of examples.

 $$$$$ We illustrate example output of our system in Table 5.
 $$$$$ In this section we present our experimental set up for assessing the performance of our model.
 $$$$$ We used a variety of syntax-based, lexical and compression-specific 6 The software and corpus can be downloaded from http://homepages.inf.ed.ac.uk/tcohn/paraphrase.
 $$$$$ < 0.01).
