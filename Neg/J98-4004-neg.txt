Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. $$$$$ N'-V' produces trees that represent PP modification of NPs and VPs with a Chomsky adjunction representation that uses an intermediate level of X' structure.
Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. $$$$$ The &quot;parent annotation&quot; transform discussed below, which appends the category of a parent node onto the label of all of its nonterminal children as sketched in Figure 2, has just this effect.
Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. $$$$$ This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.
Structural contexts like those captured by parent annotation (Johnson, 1998) are more subtle. $$$$$ The relationship betweenf and1.2 is also plotted in Figure 4.

Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). $$$$$ On reflection, perhaps this should not be surprising.
Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). $$$$$ The previous section presented theoretical evidence that varying the tree representations used to estimate a PCFG language model can have a noticeable impact on that model's performance.
Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). $$$$$ PCFG parsing systems often perform as well as other simple broad-coverage parsing system for predicting tree structure from part-of-speech (POS) tag sequences (Chamiak 1996).
Grandparent annotation was used previously by Charniak and Carroll (1994) and Johnson (1998). $$$$$ If these likelihoods differ, then a maximum-likelihood parser will always return the same maximum-likelihood tree structure each time it is presented with its yield, and will never return the tree structure with lower likelihood, even though the PCFG assigns it a nonzero likelihood.

See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. $$$$$ SBR-9720368 and SBR-9812169.
See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. $$$$$ A PCFG defines a probability distribution over the (finite) parse trees generated by the grammar, where the probability of a tree T is given by where Cr (A --> a) is the number of times the production A a is used in the derivation T. The PCFG that assigns maximum likelihood to the sequence i= of trees in a treebank corpus is given by the relative frequency estimator.
See Johnson (1998) for a presentation of the transform/de transform paradigm in parsing. $$$$$ In general, in selecting a tree structure one faces a bias/variance trade-off, in that tree structures with fewer nodes and/or richer node labels reduce bias, but possibly at the expense of an increase in variance.

First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998). $$$$$ Whether either of these two PCFG models outperforms a PCFG induced from the original treebank is a separate question.
First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998). $$$$$ Table 1 presents an analysis of the sequences of trees produced via this detransformation process applied to the maximum-likelihood-parse trees.
First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998). $$$$$ This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.
First, it captures both P and its parent P, which predicts the distribution over child symbols far better than just P (Johnson, 1998). $$$$$ I would like to thank Dick Oehrle and Chris Manning, Eugene Charniak and my other colleagues at Brown, and the CL reviewers for their excellent advice in this research.

 $$$$$ A PCFG is a CFG in which each production A -4 a in the grammar's set of productions P is associated with an emission probability P(A a) that satisfies a normalization constraint and a consistency or tightness constraint not discussed here, that PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998).
 $$$$$ But the statistical independence assumptions embodied in a particular PCFG description of a particular natural language construction are in general much stronger than the requirement that the construction be generated by a CFG.
 $$$$$ Randomization tests for paired sample data were performed to assess the significance of the difference between the labeled precision and recall scores for the output of the Id PCFG and the other PCFGs (Cohen 1995).

These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. $$$$$ The theory of PCFGs is described elsewhere (e.g., Charniak [1993]), so it is only summarized here.
These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. $$$$$ This material is based on work supported by the National Science Foundation under Grants No.
These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. $$$$$ Points labeled with a single category A correspond to PCFGs induced after applying the Parent transform to just those nodes labeled A, while points labeled with a pair of categories KB correspond to PCFGs induced applying the Parent transform to nodes labeled A with parents labeled B.
These two requirements are related to grammar refinement by annotation (Johnson, 1998), where annotations must be bounded and monotonic. $$$$$ Here C,-- (A a) is the number of times the production A a is used in derivations of the trees in f. This estimation procedure can be used in a broad-coverage parsing procedure as follows: A PCFG G is estimated from a treebank corpus :1- of training data.

Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. $$$$$ Indeed, even though the PCFG estimated from the trees obtained by applying the &quot;parent annotation&quot; transformation to sections 2-21 of the WSJ corpus contains 22,773 productions (i.e., 7,811 more than the PCFG estimated from the untransformed corpus), only 965 of them, or just over 4%, are subsumed by two or more other productions.
Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. $$$$$ Generally, one might expect that the fewer the nodes in the training corpus trees, the weaker the independence assumptions in the induced language model.
Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
Johnson (1998) annotates each node by its parent category in a tree, and gets significant improvements compared with the original PCFGs on the Penn Treebank. $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.

It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). $$$$$ Both the Flatten and Parent transforms induced PCFGs that have substantially more productions than the original treebank grammar, perhaps reflecting the fact that they encode more contextual information than the original treebank grammar, albeit in different ways.
It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). $$$$$ In the work presented here the actual lexical items (words) are ignored, and the terminals of the trees are taken to be the part-of-speech (POS) tags assigned to the lexical items.
It is well known that richer context representation gives rise to better parsing performance (Johnson, 1998). $$$$$ The first and fourth transforms turn NP and VP nodes whose rightmost child is a PP into Chomsky adjunction structures, and the second and third transforms adjoin final PPs with a following comma punctuation into Chomsky adjunction structures.

Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars. $$$$$ Given a sequence of POS tags to be analyzed, a dynamic programming method based on the CKY algorithm (Aho and Ullman 1972) is used to search for a maximum-likelihood parse using this PCFG.
Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars. $$$$$ The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.
Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars. $$$$$ The theory of PCFGs is described elsewhere (e.g., Charniak [1993]), so it is only summarized here.
Johnson (1998) showed that refining tree bank categories with parent information leads to more accurate grammars. $$$$$ The effect of adding a particularly simple kind of contextual information—the category of the node's parent—is also studied in this paper.

 $$$$$ This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.
 $$$$$ This material is based on work supported by the National Science Foundation under Grants No.
 $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
 $$$$$ More specifically, this paper studies the effect of varying the tree structure representation of PP modification from both a theoretical and an empirical point of view.

Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998). $$$$$ This material is based on work supported by the National Science Foundation under Grants No.
Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998). $$$$$ N'-V' produces trees that represent PP modification of NPs and VPs with a Chomsky adjunction representation that uses an intermediate level of X' structure.
Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998). $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
Coming to this problem from the standpoint of tree transformation, we naturally view our work as a descendent of Johnson (1998). $$$$$ This two-free corpus, which uses Chomsky adjunction tree representations, consists of the trees (A2) with relative frequency f and the trees (B2) with relative frequency 1 —f.

 $$$$$ This material is based on work supported by the National Science Foundation under Grants No.
 $$$$$ The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.
 $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
 $$$$$ This material is based on work supported by the National Science Foundation under Grants No.

The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). $$$$$ SBR-9720368 and SBR-9812169.
The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). $$$$$ The kinds of tree representations used in a treebank corpus can have a dramatic effect on performance of a parser based on the PCFG estimated from that corpus, causing the estimated likelihood of a tree to differ substantially from its frequency in the training corpus.
The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). $$$$$ Indeed, even though the PCFG estimated from the trees obtained by applying the &quot;parent annotation&quot; transformation to sections 2-21 of the WSJ corpus contains 22,773 productions (i.e., 7,811 more than the PCFG estimated from the untransformed corpus), only 965 of them, or just over 4%, are subsumed by two or more other productions.
The implementation we use was created by Mark Johnson and used for the research in (Johnson, 1998). $$$$$ Here C,-- (A a) is the number of times the production A a is used in derivations of the trees in f. This estimation procedure can be used in a broad-coverage parsing procedure as follows: A PCFG G is estimated from a treebank corpus :1- of training data.

In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). $$$$$ The results presented earlier show that the tree representations that incorporated weaker independence assumptions performed signficantly better in the empirical studies than the more linguistically motivated Chomsky adjunction structures.
In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). $$$$$ I would like to thank Dick Oehrle and Chris Manning, Eugene Charniak and my other colleagues at Brown, and the CL reviewers for their excellent advice in this research.
In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). $$$$$ A PCFG is a CFG in which each production A -4 a in the grammar's set of productions P is associated with an emission probability P(A a) that satisfies a normalization constraint and a consistency or tightness constraint not discussed here, that PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998).
In addition we also have some features that do not have a linguistic interpretation, but result in a good PCFG, such as a parent feature on some categories, following Johnson (1998). $$$$$ Empirical studies using actual corpus data are presented in Section 5.

Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. $$$$$ This material is based on work supported by the National Science Foundation under Grants No.
Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. $$$$$ The x-axis shows the difference in number of productions in the PCFG after selective parent transform and the untransformed treebank PCFG, and the y-axis shows the difference in the average of the precision and recall scores. is usually influenced by considerations of parsimony; thus the Chomsky adjunction representation of PP modification may be preferred because it requires only a single context-free rule, rather than a rule schema abbreviating a potentially unbounded number of rules that would be required in flat tree representations of adjunction.
Parent Annotation Another common relabeling method in parsing is parent annotation (Johnson, 1998), in which a node is annotated with its parent's label. $$$$$ This material is based on work supported by the National Science Foundation under Grants No.

To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). $$$$$ This paper has presented theoretical and empirical evidence that the choice of tree representation can make a significant difference to the performance of a PCFG-based parsing system.
To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). $$$$$ In all of these cases, the corpora contained two tree structures, and the induced PCFG associates each with an estimated likelihood.
To deal with such problems ,weadopt a two-steps approach, the first being realized with Conditional Random Fields (CRF) (Lafferty et al 2001), the second with a Probabilistic Context-Free Grammar (PCFG) (Johnson, 1998). $$$$$ A PCFG is a CFG in which each production A -4 a in the grammar's set of productions P is associated with an emission probability P(A a) that satisfies a normalization constraint and a consistency or tightness constraint not discussed here, that PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998).

Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). $$$$$ This paper studies the effects of these differing tree representations of PP modification theoretically by considering their effect on very simple corpora, and empirically by means of a tree transformation/detransformation methodology introduced below.
Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). $$$$$ Here C,-- (A a) is the number of times the production A a is used in derivations of the trees in f. This estimation procedure can be used in a broad-coverage parsing procedure as follows: A PCFG G is estimated from a treebank corpus :1- of training data.
Once word shave been annotated with basic entity constituents, the tree structure of named entities is simple enough to be reconstructed with relatively simple model like PCFG (Johnson, 1998). $$$$$ This material is based on work supported by the National Science Foundation under Grants No.

A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. $$$$$ But the statistical independence assumptions embodied in a particular PCFG description of a particular natural language construction are in general much stronger than the requirement that the construction be generated by a CFG.
A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. $$$$$ The effect of adding a particularly simple kind of contextual information—the category of the node's parent—is also studied in this paper.
A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. $$$$$ The training corpus 77'4.
A PCFG model (Johnson, 1998) is used to parse complete entity trees upon components. $$$$$ The point labeled All corresponds to the PCFG induced after the Parent transform to all nonroot nonterminal nodes, as before.

As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. $$$$$ This condition studies the behavior of the Penn II tree representation used in the WSJ corpus.
As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. $$$$$ The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.
As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. $$$$$ The particular independence assumptions implicit in a tree representation can be studied theoretically and investigated empirically by means of a tree transformation / detransformation process.
As discussed in (Johnson, 1998), an important point for a parsing algorithm is the representation of trees being parsed. $$$$$ SBR-9720368 and SBR-9812169.

Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998). $$$$$ This paper points out that the Penn II treebank representations are of the kind predicted to have such an effect, and describes a simple node relabeling transformation that improves a treebank PCFG-based parser's average precision and recall by around 8%, or approximately half of the performance difference between a simple PCFG model and the best broad-coverage parsers available today.
Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998). $$$$$ This performance variation comes about because any PCFG, and hence the corpus of trees from which the PCFG is induced, embodies independence assumptions about the distribution of words and phrases.
Concerning the PCFG model, grammars, tree binarization and the different tree representations are created with our own scripts, while entity tree parsing is performed with the chart parsing algorithm described in (Johnson, 1998). $$$$$ This material is based on work supported by the National Science Foundation under Grants No.
