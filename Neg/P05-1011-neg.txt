Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ In our evaluation, one error source may cause multiple errors of dependencies.
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HPSG parsing.
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997).
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ The problem with this method was in the approximation of exponentially many parse trees by a polynomial-size sample.

The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ This study follows previous studies on the probabilistic models for HPSG.
The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ We used an HPSG grammar derived from Penn Treebank (Marcus et al., 1994) Section 02-21 (39,832 sentences) by our method of grammar development (Miyao et al., 2004).
The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ A set of parse trees is then represented as a set of relations among equivalence classes.
The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ The following combinations are used for representing the characteristics of the binary/unary schema applications.

Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ Malouf and van Noord (2004) successfully applied this method to German HPSG.
Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ We apply two techniques for reducing the training cost.
Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ Previous studies assumed T(s) could be enumerated; however, the assumption is impractical because the size of T(s) is exponentially related to the length of s. The problem of exponential explosion is inevitable in the wide-coverage parsing of real-world texts because many parse candidates are produced to support various constructions in long sentences.
Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ However, they also applied the same supertagger in a parsing stage, and this seemed to be crucial for high accuracy.

These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy. $$$$$ The other is the filtering of parse candidates according to a preliminary probability distribution (Section 4).
These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy. $$$$$ Previous studies assumed T(s) could be enumerated; however, the assumption is impractical because the size of T(s) is exponentially related to the length of s. The problem of exponential explosion is inevitable in the wide-coverage parsing of real-world texts because many parse candidates are produced to support various constructions in long sentences.
These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy. $$$$$ In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time.
These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy. $$$$$ Model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method (Malouf, 2002).

 $$$$$ It is apparent from this figure that the accuracy was significantly higher for shorter sentences (< 10 words).
 $$$$$ Table 4 revealed that our simple method of filtering caused a fatal bias in training data when a preliminary distribution was used only for filtering.
 $$$$$ Actually, in our previous study (Miyao et al., 2003), we successfully developed a probabilistic model including features on nonlocal predicate-argument dependencies.

In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). $$$$$ Since the signs of the top-most nodes are equivalent, they are packed into an equivalence class.
In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). $$$$$ Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997).
In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). $$$$$ The following combinations are used for representing the characteristics of the binary/unary schema applications.

In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ To our knowledge, this work provides the first results of extensive experiments of parsing Penn Treebank with a probabilistic HPSG.
In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ The estimation of log-linear models requires high computational cost, especially with widecoverage grammars.
In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ A feature forest is formally defined as a tuple, (C, D, R, y, b), where C is a set of conjunctive nodes, D is a set of disjunctive nodes, R C_ C is a set of root nodes2, y : D 2c is a conjunctive daughter function, and b : C 2D is a disjunctive The simplest way to map a chart of HPSG parse trees into a feature forest is to map each equivalence class e E E to a conjunctive node c E C. However, in HPSG parsing, important features for disambiguation are combinations of a mother and its daughters, i.e., (m, 1, r).
In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ This is because a feature forest model does not assume probabilistic independence of conjunctive nodes.

 $$$$$ Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997).
 $$$$$ A possible direction is to encode larger contexts of parse trees, which were shown to improve the accuracy (Toutanova and Manning, 2002; Toutanova et al., 2004).
 $$$$$ The ambiguity is represented as two pairs of arrows that come out of the node.
 $$$$$ In all of the following experiments, we show the accuracy for the test set (< 40 words) only.

By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ A possible direction is to encode larger contexts of parse trees, which were shown to improve the accuracy (Toutanova and Manning, 2002; Toutanova et al., 2004).
By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ Discriminative log-linear models are now becoming a de facto standard for probabilistic disambiguation models for deep parsing (Johnson et al., 1999; Riezler et al., 2002; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004b; Kaplan et al., 2004).
By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ To avoid exponential explosion, we represent T(s) in a packed form of HPSG parse trees.

 $$$$$ Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.

We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ The estimation of log-linear models requires high computational cost, especially with widecoverage grammars.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ Although it is true to some extent, this does not necessarily mean the impossibility of incorporating features on nonlocal dependencies into the model.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ Major causes were classified into three types: argument/modifier distinction, attachment ambiguity, and lexical ambiguity.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ Malouf and van Noord (2004) successfully applied this method to German HPSG.

We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ A check means the atomic feature is incorporated while a hyphen means the feature is ignored.
We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ Therefore, we now have the basis for the development and the evaluation of statistical disambiguation models for wide-coverage HPSG parsing.
We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ Although HPSG exploits further complicated feature constraints and requires high computational cost, our work has proved that log-linear models can be applied to HPSG parsing and attain accurate and wide-coverage parsing.
We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ The estimation of log-linear models requires high computational cost, especially with widecoverage grammars.

In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ Another approach to estimating log-linear models for HPSG is to extract a small informative sample from the original set T (s) (Osborne, 2000).
In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ Product The probability is defined as the product of p and the estimated model p. Reference distribution p is used as a reference distribution of p. Feature function log is used as a feature function of p. This method was shown to be a generalization of the reference distribution method (Johnson and Riezler, 2000).
In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ A check means the atomic feature is incorporated while a hyphen means the feature is ignored.
In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ We used an HPSG grammar derived from Penn Treebank (Marcus et al., 1994) Section 02-21 (39,832 sentences) by our method of grammar development (Miyao et al., 2004).

We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ Model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method (Malouf, 2002).
We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ HPSG exploits feature structures to represent linguistic constraints.
We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ Scaling up HPSG parsing to assess real-world texts is an emerging research field with both theoretical and practical applications.
We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.

 $$$$$ Figure 3 shows an example of filtering lexical entries assigned to “saw”.
 $$$$$ In an actual implementation, some of the atomic features are abstracted (i.e., ignored) for smoothing.
 $$$$$ Actually, in our previous study (Miyao et al., 2003), we successfully developed a probabilistic model including features on nonlocal predicate-argument dependencies.

The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005). $$$$$ In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time.
The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005). $$$$$ Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.
The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005). $$$$$ The trade-off between approximation and locality of features is an outstanding problem.

 $$$$$ Product The probability is defined as the product of p and the estimated model p. Reference distribution p is used as a reference distribution of p. Feature function log is used as a feature function of p. This method was shown to be a generalization of the reference distribution method (Johnson and Riezler, 2000).
 $$$$$ They developed log-linear models on a packed representation of parse forests, which is similar to our representation.
 $$$$$ The estimation of log-linear models requires high computational cost, especially with widecoverage grammars.
 $$$$$ A possible reason is that a hyper-parameter of the prior was set to the same value for all the features including the feature of the preliminary distribution.

 $$$$$ Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.
 $$$$$ Restricting the domain of feature functions to (cm, Cl, Cr) seems to limit the flexibility of feature design.
 $$$$$ Restricting the domain of feature functions to (cm, Cl, Cr) seems to limit the flexibility of feature design.

In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ The reference distribution method achieved higher accuracy and lower cost.
In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ While attachment/lexical ambiguities are well-known causes, the other is peculiar to deep parsing.
In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ The other is the filtering of parse candidates according to a preliminary probability distribution (Section 4).

In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ A feature forest is formally defined as a tuple, (C, D, R, y, b), where C is a set of conjunctive nodes, D is a set of disjunctive nodes, R C_ C is a set of root nodes2, y : D 2c is a conjunctive daughter function, and b : C 2D is a disjunctive The simplest way to map a chart of HPSG parse trees into a feature forest is to map each equivalence class e E E to a conjunctive node c E C. However, in HPSG parsing, important features for disambiguation are combinations of a mother and its daughters, i.e., (m, 1, r).
In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ This paper reports the development of loglinear models for the disambiguation in wide-coverage HPSG parsing.
In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ We apply two techniques for reducing the training cost.
In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ Since the signs of the top-most nodes are equivalent, they are packed into an equivalence class.
