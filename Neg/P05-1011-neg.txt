Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ For example, if a wrong lexical entry was assigned to a verb, all the argument dependencies of the verb are counted as errors.
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ The trade-off between the parsing cost and the accuracy will be examined experimentally.
Comparing the latter half of the experimental results with those on parsing (Miyao and Tsujii, 2005), we investigated similarities and differences between probabilistic models for parsing and generation. $$$$$ Previous studies assumed T(s) could be enumerated; however, the assumption is impractical because the size of T(s) is exponentially related to the length of s. The problem of exponential explosion is inevitable in the wide-coverage parsing of real-world texts because many parse candidates are produced to support various constructions in long sentences.

The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ Since they did not mention the probabilities of supertags, their method corresponds to our “filtering only” method.
The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ This is because a feature forest model does not assume probabilistic independence of conjunctive nodes.
The atomic features and their combinations are imported from the previous work on HPSG parsing (Miyao and Tsujii, 2005). $$$$$ They first assigned to each word a small number of supertags, which correspond to lexical entries in our case, and parsed supertagged sentences.

Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ Estimation of the above model requires a set of pairs (ts, T (s)), where ts is the correct parse for sentence s. While ts is provided by a treebank, T(s) is computed by parsing each s in the treebank.
Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ For the training, we eliminated sentences with no less than 40 words and for which the parser could not produce the correct parse.
Similar results are reported in parsing (Miyao and Tsujii, 2005) while the accuracy saturated around 16,000 sentences. $$$$$ However, despite research on HPSG processing efficiency (Oepen et al., 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al., 2002b; Toutanova and Manning, 2002).

These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy. $$$$$ The problem of exponential explosion is also inevitable for their methods.
These results are different from those in parsing reported by Miyao and Tsujii (2005) where COMMA and SPAN especially contributed to the accuracy. $$$$$ Figure 4 shows examples: (root is for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are “saw”, VBD, and a transitive verb, respectively. fbinary is for the binary rule application to “saw a girl” and “with a telescope”, in which the applied schema is the Head-Modifier Schema, the left daughter is VP headed by “saw”, and the right daughter is PP headed by “with”, whose part-of-speech is IN and the lexical entry is a VP-modifying preposition.

 $$$$$ The optimization algorithm was the limited-memory BFGS method (Nocedal and Wright, 1999).
 $$$$$ Product The probability is defined as the product of p and the estimated model p. Reference distribution p is used as a reference distribution of p. Feature function log is used as a feature function of p. This method was shown to be a generalization of the reference distribution method (Johnson and Riezler, 2000).
 $$$$$ The problem of exponential explosion is also inevitable for their methods.
 $$$$$ The exploration of new types of features is necessary for higher accuracy.

In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). $$$$$ However, despite the development of methods to improve HPSG parsing efficiency (Oepen et al., 2002a), the exhaustive parsing of all sentences in a treebank is still expensive.
In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). $$$$$ They first assigned to each word a small number of supertags, which correspond to lexical entries in our case, and parsed supertagged sentences.
In this paper, we use an HPSG parser developed by Miyao and Tsujii (2005). $$$$$ Features occurring more than twice were included in the model (598,326 features).

In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ Our idea is that we can omit the computation of parse trees with low probabilities in the estimation stage because T(s) can be approximated with parse trees with high probabilities.
In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ NP, VP) WORD the surface form of the head word POS the part-of-speech of the head word LE the lexical entry assigned to the head word parsing a treebank, while this approximation causes bias in the training data and results in lower accuracy.
In addition, the HPSG grammar is extracted from the HPSG Treebank using a corpus based procedure, and it does not necessarily cover all possible grammatical phenomena in unseen text (Miyao and Tsujii, 2005). $$$$$ Future work includes the investigation of such features, as well as the abstraction of lexical dependencies like semantic classes.

 $$$$$ Previous studies assumed T(s) could be enumerated; however, the assumption is impractical because the size of T(s) is exponentially related to the length of s. The problem of exponential explosion is inevitable in the wide-coverage parsing of real-world texts because many parse candidates are produced to support various constructions in long sentences.
 $$$$$ In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time.
 $$$$$ A parse tree of HPSG is represented as a set of tuples (m, 1, r), where m, 1, and r are the signs of mother, left daughter, and right daughter, respectively1.

By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ Feature functions in the log-linear models are designed to capture the characteristics of (er,,,, el, C,).
By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences.
By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ Figure 4 shows examples: (root is for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are “saw”, VBD, and a transitive verb, respectively. fbinary is for the binary rule application to “saw a girl” and “with a telescope”, in which the applied schema is the Head-Modifier Schema, the left daughter is VP headed by “saw”, and the right daughter is PP headed by “with”, whose part-of-speech is IN and the lexical entry is a VP-modifying preposition.
By running the HPSG parser described in section 2.2 on the development data without dependency constraints, we obtain similar values of LP (86.8%) and LR (85.6%) as those reported by Miyao and Tsujii (Miyao and Tsujii, 2005). $$$$$ Model parameters that maximize the loglikelihood of the training data are computed using a numerical optimization method (Malouf, 2002).

 $$$$$ Features occurring more than twice were included in the model (598,326 features).
 $$$$$ This means that we can unpack a part of the forest without changing the model.
 $$$$$ A large treebank can be used as training and test data for statistical models.

We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ However, their method has the advantage that any features on a parse tree can be incorporated into the model.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ Future work includes the investigation of such features, as well as the abstraction of lexical dependencies like semantic classes.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ In this paper, we investigate combinations of the atomic features listed in Table 1.
We examined 100 sentences using a phrase structure parser (Charniak and Johnson, 2005) and an HPSG parser (Miyao and Tsujii, 2005). $$$$$ The probability, p(tIs), of producing the parse result t from a given sentence s is defined as where po(tIs) is a reference distribution (usually assumed to be a uniform distribution), and T(s) is a set of parse candidates assigned to s. The feature function fi(t, s) represents the characteristics of t and s, while the corresponding model parameter Ai(t, s) is its weight.

We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ Figure 4 shows examples: (root is for the root node, in which the phrase symbol is S and the surface form, part-of-speech, and lexical entry of the lexical head are “saw”, VBD, and a transitive verb, respectively. fbinary is for the binary rule application to “saw a girl” and “with a telescope”, in which the applied schema is the Head-Modifier Schema, the left daughter is VP headed by “saw”, and the right daughter is PP headed by “with”, whose part-of-speech is IN and the lexical entry is a VP-modifying preposition.
We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ Log-linear models are required for credible probabilistic models and are also beneficial for incorporating various overlapping features.
We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. $$$$$ Since the signs of the top-most nodes are equivalent, they are packed into an equivalence class.

In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ The preliminary model was used to reduce the search space for parsing a training treebank.
In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ HPSG exploits feature structures to represent linguistic constraints.
In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for the processing of real-world sentences (Miyao and Tsujii, 2005). $$$$$ Recently, a wide-coverage grammar and a large treebank have become available for English HPSG (Miyao et al., 2004).

We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ The aim of this paper is to report the development of log-linear models for the disambiguation in widecoverage HPSG parsing, and their empirical evaluation through the parsing of the Wall Street Journal of Penn Treebank II (Marcus et al., 1994).
We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ Log-linear models are required for credible probabilistic models and are also beneficial for incorporating various overlapping features.
We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. $$$$$ Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency (Abney, 1997).

 $$$$$ Estimation of the above model requires a set of pairs (ts, T (s)), where ts is the correct parse for sentence s. While ts is provided by a treebank, T(s) is computed by parsing each s in the treebank.
 $$$$$ The other is the filtering of parse candidates according to a preliminary probability distribution (Section 4).
 $$$$$ One is the estimation on a packed representation of HPSG parse trees (Section 3).
 $$$$$ Figure 5 shows the accuracy for each sentence length.

The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005). $$$$$ With C = 0.95, four lexical entries are assigned.
The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005). $$$$$ However, they also applied the same supertagger in a parsing stage, and this seemed to be crucial for high accuracy.
The HPSG parser used in this study is Ninomiya et al (2006), which is based on Enju (Miyao and Tsujii, 2005). $$$$$ If T(s) is represented in a feature forest, p(tIT(s)) can be estimated using dynamic programming without unpacking the chart.

 $$$$$ Each row represents a template of a feature function.
 $$$$$ A series of experiments empirically evaluated the estimation techniques, and also examined the performance of the disambiguation models on the parsing of real-world sentences.
 $$$$$ However, their method has the advantage that any features on a parse tree can be incorporated into the model.
 $$$$$ They developed log-linear models on a packed representation of parse forests, which is similar to our representation.

 $$$$$ However, despite research on HPSG processing efficiency (Oepen et al., 2002a), the application of HPSG parsing is still limited to specific domains and short sentences (Oepen et al., 2002b; Toutanova and Manning, 2002).
 $$$$$ NP, VP) WORD the surface form of the head word POS the part-of-speech of the head word LE the lexical entry assigned to the head word parsing a treebank, while this approximation causes bias in the training data and results in lower accuracy.
 $$$$$ They first assigned to each word a small number of supertags, which correspond to lexical entries in our case, and parsed supertagged sentences.
 $$$$$ A feature forest is formally defined as a tuple, (C, D, R, y, b), where C is a set of conjunctive nodes, D is a set of disjunctive nodes, R C_ C is a set of root nodes2, y : D 2c is a conjunctive daughter function, and b : C 2D is a disjunctive The simplest way to map a chart of HPSG parse trees into a feature forest is to map each equivalence class e E E to a conjunctive node c E C. However, in HPSG parsing, important features for disambiguation are combinations of a mother and its daughters, i.e., (m, 1, r).

In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ In an actual implementation, some of the atomic features are abstracted (i.e., ignored) for smoothing.
In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ Estimation of the above model requires a set of pairs (ts, T (s)), where ts is the correct parse for sentence s. While ts is provided by a treebank, T(s) is computed by parsing each s in the treebank.
In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ Previous studies on probabilistic models for HPSG (Toutanova and Manning, 2002; Baldridge and Osborne, 2003; Malouf and van Noord, 2004) also adopted log-linear models.
In the hybrid model, the probabilities of the previous model are multiplied by the super tagging probabilities instead of a preliminary probabilistic model, which is introduced to help the process of estimation by filtering unlikely lexical entries (Miyao and Tsujii, 2005). $$$$$ The optimization algorithm was the limited-memory BFGS method (Nocedal and Wright, 1999).

In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ Using techniques to reduce the estimation cost, we trained the models using 20 sections of Penn Treebank.
In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ A feature forest is an “and/or” graph to represent exponentiallymany tree structures in a packed form.
In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ The estimation of log-linear models requires high computational cost, especially with widecoverage grammars.
In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). $$$$$ Clark and Curran (2004a) described a method of reducing the cost of parsing a training treebank in the context of CCG parsing.
