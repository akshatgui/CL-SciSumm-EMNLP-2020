It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ The use of transition systems to study computation is a standard technique in theoretical computer science, which is here combined with the notion of oracles in order to characterize parsing algorithms with deterministic search.
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ Consequently, these deterministic parsing techniques have been much less popular for natural language parsing, except as a way of modeling human sentence processing, which appears to be at least partly deterministic in nature (Marcus 1980; Shieber 1983).
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ The transitions used by stack-based parsers are essentially composed of two types of actions: adding (labeled) arcs to A and manipulating the stack σ and input buffer β.
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ It is noteworthy that there are no consistent differences in learning time between the strictly projective parsers and their pseudo-projective counterparts, despite the fact that the pseudo-projective technique increases the number of distinct classes (because of its augmented arc labels), which in turn increases the number of binary classifiers that need to be trained in order to perform multi-class classification with the one-versus-one method.

If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars.
If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ The main reason for introducing this framework is to allow us to characterize algorithms that have previously been described in different traditions and to compare their formal properties within a single unified framework.
If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ Because the node set V is given by the input sentence itself, the set Acm of dependency arcs in the terminal configuration will determine the output dependency graph Gcm = (V, Acm ).
If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ The dominant learning method in this tradition is support vector machines (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Nivre et al. 2006) but memory-based learning has also been used (Nivre, Hall, and Nilsson 2004; Nivre and Scholz 2004; Attardi 2006).

The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ Any dependency graph satisfying these conditions is a dependency forest; if it is also connected, it is a dependency tree, that is, a directed tree rooted at the node 0.
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ A dependency graph G = (V, A) is well-formed if and only if: We will refer to conditions 1–3 as ROOT, SINGLE-HEAD, and ACYCLICITY, respectively.
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm.

For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ 2007).
For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ Any dependency graph satisfying these conditions is a dependency forest; if it is also connected, it is a dependency tree, that is, a directed tree rooted at the node 0.
For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ The use of transition systems to study computation is a standard technique in theoretical computer science, which is here combined with the notion of oracles in order to characterize parsing algorithms with deterministic search.

Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ In data-driven dependency parsing, oracles normally take the form of classifiers, trained on treebank data, but they can also be defined in terms of grammars and heuristic disambiguation rules (Nivre 2003).
Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ When discussing PROJECTIVITY, we will often use the notation i →* j to mean that j is reachable from i in the reflexive and transitive closure of the arc relation.
Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ Note that this is only one of several possible notions of incrementality in parsing.
Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ The first family uses a stack to store partially processed tokens and is restricted to the derivation of projective dependency structures.

Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ However, in the formal characterization of different parsing algorithms in Sections 4 and 5, we will concentrate on properties of the underlying transition systems.
Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ In order for this to be the case, there must first of all be a significant proportion of left-headed structures in the data.
Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ It is worth noting that any dependency forest can be turned into a dependency tree by adding arcs from the node 0 to all other roots.

We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ Given a set L = 1l1, ... ,l|L|} of dependency labels, a dependency graph for a sentence x = (w0, w1, ... , wn) is a labeled directed graph G = (V, A), where The set V of nodes (or vertices) is the set of non-negative integers up to and including n, each corresponding to the linear position of a word in the sentence (including ROOT).
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing.
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ Moreover, despite its quadratic time complexity, the non-projective parser is often as efficient as the pseudo-projective parsers in practice, because the extended set of dependency labels used in pseudo-projective parsing slows down classification.
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ I want to thank my students Johan Hall and Jens Nilsson for fruitful collaboration and for their contributions to the MaltParser system, which was used for all experiments.

Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ In data-driven dependency parsing, oracles normally take the form of classifiers, trained on treebank data, but they can also be defined in terms of grammars and heuristic disambiguation rules (Nivre 2003).
Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ The first two conditions state that the buffer R never grows in size and that parsing terminates as soon as it becomes empty; the third condition states that arcs added to A can never be removed.
Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ One potential advantage of such models is that they are easily ported to any domain or language in which annotated resources exist.
Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ In contrast to spanning tree parsing, this can be characterized as a greedy inference strategy, trying to construct a globally optimal dependency graph by making a sequence of locally optimal decisions.

The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ Thus, we find that in all the data sets for which the arc-standard parsers do badly, the percentage of left-headed dependencies is in the 50–75% range.
The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label.
The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ Any dependency graph satisfying these conditions is a dependency forest; if it is also connected, it is a dependency tree, that is, a directed tree rooted at the node 0.
The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ The use of transition systems to study computation is a standard technique in theoretical computer science, which is here combined with the notion of oracles in order to characterize parsing algorithms with deterministic search.

Transition-based $$$$$ Of the algorithms described in this article, the arc-eager stack-based algorithm is essentially the algorithm proposed for unlabeled dependency parsing in Nivre (2003), extended to labeled dependency parsing in Nivre, Hall, and Nilsson (2004), and most fully described in Nivre (2006b).
Transition-based $$$$$ In particular, whereas this type of framework has previously been used to characterize algorithms in the stackbased family (Nivre 2003, 2006b; Attardi 2006), it is here being used also for the listbased algorithms first discussed by Covington (2001).
Transition-based $$$$$ In this kind of framework the syntactic structure of a sentence is modeled by a dependency graph, which represents each word and its syntactic dependents through labeled directed arcs.
Transition-based $$$$$ Given a transition system S = (C, T, cs, Ct) and an oracle o, deterministic parsing can be achieved by the following simple algorithm: It is easy to see that, provided that there is at least one transition sequence in S for every sentence, such a parser constructs exactly one transition sequence C0,m for a sentence x and returns the parse defined by the terminal configuration cm, that is, Gcm = ({0, 1, ... , n}, Acm ).

Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ Given a set L = 1l1, ... ,l|L|} of dependency labels, a dependency graph for a sentence x = (w0, w1, ... , wn) is a labeled directed graph G = (V, A), where The set V of nodes (or vertices) is the set of non-negative integers up to and including n, each corresponding to the linear position of a word in the sentence (including ROOT).
Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ Proof 2 Assuming that the oracle and transition functions can be computed in some constant time, the worst-case running time is bounded by the maximum number of transitions in a transition sequence C0,m for a sentence x = (w0, w1, ... , wn).
Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice.
Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ The use of transition systems to study computation is a standard technique in theoretical computer science, which is here combined with the notion of oracles in order to characterize parsing algorithms with deterministic search.

To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ In particular, whereas this type of framework has previously been used to characterize algorithms in the stackbased family (Nivre 2003, 2006b; Attardi 2006), it is here being used also for the listbased algorithms first discussed by Covington (2001).
To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ Let S = (C, T, cs, Ct) be a transition system for dependency parsing.
To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ I also want to thank Sabine Buchholz, Matthias Buch-Kromann, Walter Daelemans, G¨uls¸en Eryi˘git, Jason Eisner, Jan Hajiˇc, Sandra K¨ubler, Marco Kuhlmann, Yuji Matsumoto, Ryan McDonald, Kemal Oflazer, Kenji Sagae, Noah A. Smith, and Deniz Yuret for useful discussions on topics relevant to this article.

Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t $$$$$ Spanning tree algorithms have so far primarily been combined with online learning methods such as MIRA (McDonald, Crammer, and Pereira 2005).
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t $$$$$ One potential advantage of such models is that they are easily ported to any domain or language in which annotated resources exist.
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t $$$$$ The worst-case space complexity of the projective, list-based algorithm is O(n), where n is the length of the input sentence.
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t $$$$$ The approach of deterministic classifier-based parsing was first proposed for Japanese by Kudo and Matsumoto (2002) and for English by Yamada and Matsumoto (2003).

This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ It is worth noting that any dependency forest can be turned into a dependency tree by adding arcs from the node 0 to all other roots.
This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ I want to thank my students Johan Hall and Jens Nilsson for fruitful collaboration and for their contributions to the MaltParser system, which was used for all experiments.
This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ Basis: If JxJ = 1, then the only projective dependency forest for x is G = ({0},0) and Gcm = Gx for C0,m = (cs(x)).
This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ Example 1 For the graphs depicted in Figures 1 and 2, we have: Both G1 and G2 are well-formed dependency forests (dependency trees, to be specific), but only G2 is projective.

In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ In data-driven dependency parsing, oracles normally take the form of classifiers, trained on treebank data, but they can also be defined in terms of grammars and heuristic disambiguation rules (Nivre 2003).
In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ Dependency-based syntactic theories are based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric dependency relations holding between the words of a sentence.
In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ In this article, we have introduced a formal framework for deterministic incremental dependency parsing, where parsing algorithms can be defined in terms of transition systems that are deterministic only together with an oracle for predicting the next transition.

studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ .
studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ The notable exceptions are Cheng, Asahara, and Matsumoto (2005), who compare two different algorithms and two types of classifier for parsing Chinese, and Hall, Nivre, and Nilsson (2006), who compare two types of classifiers and several types of feature models for parsing Chinese, English, and Swedish.
studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ The main reason for introducing this framework is to allow us to characterize algorithms that have previously been described in different traditions and to compare their formal properties within a single unified framework.
studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ Because there are no non-projective dependencies in this data set, the projectivized training data set will be identical to the original one, which means that the pseudo-projective parser will behave exactly as the projective one.

All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ 2003), and in Figure 2, for an English sentence taken from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994).1 An artificial word ROOT has been inserted at the beginning of each sentence, serving as the unique root of the graph.
All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ I also want to thank Sabine Buchholz, Matthias Buch-Kromann, Walter Daelemans, G¨uls¸en Eryi˘git, Jason Eisner, Jan Hajiˇc, Sandra K¨ubler, Marco Kuhlmann, Yuji Matsumoto, Ryan McDonald, Kemal Oflazer, Kenji Sagae, Noah A. Smith, and Deniz Yuret for useful discussions on topics relevant to this article.
All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations.
All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ A stack-based transition system is a quadruple S = (C, T, cs, Ct), where Nivre Deterministic Incremental Dependency Parsing Transitions for the arc-standard, stack-based parsing algorithm.

The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ For the other two languages, the low number of distinct dependency labels for Japanese and the low percentage of non-projective dependencies for Bulgarian are factors that mitigate the effect of enlarging the set of dependency labels in pseudo-projective parsing.
The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ This corresponds to the ban on discontinuous constituents in orthodox phrase structure representations.
The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ I also want to thank Sabine Buchholz, Matthias Buch-Kromann, Walter Daelemans, G¨uls¸en Eryi˘git, Jason Eisner, Jan Hajiˇc, Sandra K¨ubler, Marco Kuhlmann, Yuji Matsumoto, Ryan McDonald, Kemal Oflazer, Kenji Sagae, Noah A. Smith, and Deniz Yuret for useful discussions on topics relevant to this article.
The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label.

There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ In such cases, using the arc-standard parsing strategy, with or without pseudo-projective parsing, may lead to significantly higher accuracy.
There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ Let S = (C, T, cs, Ct) be a transition system.
There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ However, in the formal characterization of different parsing algorithms in Sections 4 and 5, we will concentrate on properties of the underlying transition systems.
There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ However, in the formal characterization of different parsing algorithms in Sections 4 and 5, we will concentrate on properties of the underlying transition systems.

Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ The reason for this formulation is to facilitate comparison with the arc-eager parser described in the next section and to simplify the definition of terminal configurations.
Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label.
Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ We have used this framework to analyze four different algorithms, proving the correctness of each algorithm relative to a relevant class of dependency graphs, and giving complexity results for each algorithm.
