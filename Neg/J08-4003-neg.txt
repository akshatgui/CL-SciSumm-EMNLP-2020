It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ Given a set L = 1l1, ... ,l|L|} of dependency labels, a dependency graph for a sentence x = (w0, w1, ... , wn) is a labeled directed graph G = (V, A), where The set V of nodes (or vertices) is the set of non-negative integers up to and including n, each corresponding to the linear position of a word in the sentence (including ROOT).
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ The dominant learning method in this tradition is support vector machines (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Nivre et al. 2006) but memory-based learning has also been used (Nivre, Hall, and Nilsson 2004; Nivre and Scholz 2004; Attardi 2006).
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ A dependency graph G = (V, A) is projective if and only if, for every arc (i, l,j) E A and node k E V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1), (i1, l2,i2), ... (ik−1, lk, ik)} E A such that ik = k. In a projective dependency graph, every node has a continuous projection, where the projection of a node i is the set of nodes reachable from i in the reflexive and transitive closure of the arc relation.
It is based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ We then describe and analyze two families of such algorithms: stack-based and list-based algorithms.

If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ A more orthodox implementation of Covington’s algorithms for data-driven dependency parsing is found in Marinov (2007).
If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ Training data for the classifiers were generated by parsing each sentence in the training set using the goldstandard dependency graph as an oracle.
If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ These results indicate that, at least for dependency parsing, deterministic parsing is possible without a drastic loss in accuracy.
If we consider transition-based dependency parsing (Nivre, 2008), the purely bottom-up strategy is implemented by the arc-standard model of Nivre (2004). $$$$$ We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions.

The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ One potential advantage of such models is that they are easily ported to any domain or language in which annotated resources exist.
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ A dependency graph G = (V, A) is projective if and only if, for every arc (i, l,j) E A and node k E V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1), (i1, l2,i2), ... (ik−1, lk, ik)} E A such that ik = k. In a projective dependency graph, every node has a continuous projection, where the projection of a node i is the set of nodes reachable from i in the reflexive and transitive closure of the arc relation.
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ When discussing PROJECTIVITY, we will often use the notation i →* j to mean that j is reachable from i in the reflexive and transitive closure of the arc relation.
The above work on transition-based parsing has focused on greedy algorithms set in a statistical framework (Nivre, 2008). $$$$$ The strictly deterministic parsing strategy has been relaxed in favor of n-best parsing by Johansson and Nugues (2006), among others.

For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label.
For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ The work has been partially supported by the Swedish Research Council.
For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ Moreover, despite its quadratic time complexity, the non-projective parser is often as efficient as the pseudo-projective parsers in practice, because the extended set of dependency labels used in pseudo-projective parsing slows down classification.
For a more detailed presentation of this subject, we refer the reader to Nivre (2008). $$$$$ We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions.

Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ More research is needed to determine exactly which properties of linguistic structures and their syntactic analysis give rise to these effects.
Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ In particular, whereas this type of framework has previously been used to characterize algorithms in the stackbased family (Nivre 2003, 2006b; Attardi 2006), it is here being used also for the listbased algorithms first discussed by Covington (2001).
Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ The notable exceptions are Cheng, Asahara, and Matsumoto (2005), who compare two different algorithms and two types of classifier for parsing Chinese, and Hall, Nivre, and Nilsson (2006), who compare two types of classifiers and several types of feature models for parsing Chinese, English, and Swedish.
Quite impressively, models based on deterministic shift-reduce parsing algorithms are able to rival the other computationally more expensive models (see Nivre (2008) and references therein for more details). $$$$$ Dependency-based syntactic theories are based on the idea that syntactic structure can be analyzed in terms of binary, asymmetric dependency relations holding between the words of a sentence.

Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ Starting from the initial configuration for the sentence to be parsed, transitions will manipulate β and A (and other available data structures) until a terminal configuration is reached.
Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ In data-driven dependency parsing, oracles normally take the form of classifiers, trained on treebank data, but they can also be defined in terms of grammars and heuristic disambiguation rules (Nivre 2003).
Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ A weaker notion would be to only require that the set of arcs is built monotonically (the third condition); a stronger notion would be to require also that nodes in R are processed strictly left to right.
Nivre (2008) reports experiments on Arabic parsing using his MaltParser (Nivre et al, 2007), trained on the PADT. $$$$$ It is worth noting that any dependency forest can be turned into a dependency tree by adding arcs from the node 0 to all other roots.

We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ For each of the four algorithms, we give proofs of correctness and complexity.
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ We call this condition PROJECTIVITY.
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice.
We use McNemar's statistical significance test as implemented by Nilsson and Nivre (2008), and denote p < 0.05 and p < 0.01 with+ and ++, respectively. $$$$$ Because ∆(p − 1) < q, we can use the inductive hypothesis to infer Π(p − 1, k, j) and, from this, Π(p, k, j).

Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label.
Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ Thus, the mainstream approach to natural language parsing uses algorithms that efficiently derive a potentially very large set of analyses in parallel, typically making use of dynamic programming and well-formed substring tables or charts.
Nivre (2008) reports that non-projective and pseudo projective algorithms outperform the eager projective algorithm in MaltParser, but our training data did not contain any non-projective dependencies. $$$$$ It is worth noting that any dependency forest can be turned into a dependency tree by adding arcs from the node 0 to all other roots.

The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ A dependency graph G = (V, A) is well-formed if and only if: We will refer to conditions 1–3 as ROOT, SINGLE-HEAD, and ACYCLICITY, respectively.
The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ The reason for separating the oracle o, which maps a configuration c to a transition t, from the transition t itself, which maps a configuration c to a new configuration c', is to have a clear separation between the abstract machine defined by the transition system, which determines formal properties such as correctness and complexity, and the search mechanism used when executing the machine.
The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ Having compared the non-projective, list-based parser to the strictly projective parsers, we will now scrutinize the results obtained when coupling the projective parsers with the pseudo-projective parsing technique, as an alternative method for capturing non-projective dependencies.
The Nivre standard algorithm is also reported to do better on Arabic, but in a preliminary experimentation, it did similarly or slightly worse than the eager one, perhaps due to high percentage of right branching (left headed structures) in our Arabic training set, an observation already noted in Nivre (2008). $$$$$ In particular, assuming that both o(c) and t(c) can be performed in constant time (for every o, t and c), which is reasonable in most cases, the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S. And the space complexity is given by an upper bound on the size of a configuration c E C, because only one configuration needs to be stored at any given time in a deterministic parser.

Transition-based: An implementation of the transition-based dependency parsing framework (Nivre, 2008) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. $$$$$ Basis: If ∆(p) = 0, then i and j are adjacent and Π(p, i, j) holds vacuously.
Transition-based: An implementation of the transition-based dependency parsing framework (Nivre, 2008) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. $$$$$ Given a set L = 1l1, ... ,l|L|} of dependency labels, a dependency graph for a sentence x = (w0, w1, ... , wn) is a labeled directed graph G = (V, A), where The set V of nodes (or vertices) is the set of non-negative integers up to and including n, each corresponding to the linear position of a word in the sentence (including ROOT).
Transition-based: An implementation of the transition-based dependency parsing framework (Nivre, 2008) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. $$$$$ A transition sequence for a sentence x = (w0, w1, ... , wn) in S is a sequence C0,m = (c0, c1, ... , cm) of configurations, such that The parse assigned to x by C0,m is the dependency graph Gcm = (10,1, ... , n}, Acm ), where Acm is the set of dependency arcs in cm.
Transition-based: An implementation of the transition-based dependency parsing framework (Nivre, 2008) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. $$$$$ I also want to thank Sabine Buchholz, Matthias Buch-Kromann, Walter Daelemans, G¨uls¸en Eryi˘git, Jason Eisner, Jan Hajiˇc, Sandra K¨ubler, Marco Kuhlmann, Yuji Matsumoto, Ryan McDonald, Kemal Oflazer, Kenji Sagae, Noah A. Smith, and Deniz Yuret for useful discussions on topics relevant to this article.

Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.
Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ More recently, algorithms for non-projective classifier-based parsing have been proposed by Attardi (2006) and Nivre (2006a).
Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ The algorithms of Kudo and Matsumoto (2002), Yamada and Matsumoto (2003), and Nivre (2003, 2006b) all belong to this family.
Deterministic algorithms for dependency parsing exist that can extract syntactic dependency structure very quickly (Nivre, 2008), but this approach is often undesirable as constituent parsers are more accurate and more adaptable to new domains (Petrov et al, 2010). $$$$$ A dependency graph G = (V, A) is well-formed if and only if: We will refer to conditions 1–3 as ROOT, SINGLE-HEAD, and ACYCLICITY, respectively.

To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ More research is needed to determine exactly which properties of linguistic structures and their syntactic analysis give rise to these effects.
To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ The evaluation also shows that using the nonprojective, list-based parsing algorithm gives a more stable improvement in this respect than applying the pseudo-projective parsing technique to a strictly projective parsing algorithm.
To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ The first two conditions state that the buffer R never grows in size and that parsing terminates as soon as it becomes empty; the third condition states that arcs added to A can never be removed.
To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). $$$$$ We call this condition PROJECTIVITY.

Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. $$$$$ Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations.
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. $$$$$ The notions of soundness and completeness, as defined here, can be seen as corresponding to the notions of soundness and completeness for grammar parsing algorithms, according to which an algorithm is sound if it only derives parses licensed by the grammar and complete if it derives all such parses (Shieber, Schabes, and Pereira 1995).
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. $$$$$ The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice.
Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S= (C, T, cs, Ct), where 1. C is a set of configurations, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration c ∈ C, 4. Ct ⊆ C is a set of terminal configurations. $$$$$ More research is needed to determine exactly which properties of linguistic structures and their syntactic analysis give rise to these effects.

This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ Moreover, the non-projective parser tends to outperform the best pseudo-projective parsers, both on average and for individual languages.
This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ However, because support vector machines constitute the state of the art in classifierbased parsing, it is still worth examining how learning and parsing times vary with the parsing algorithm while parameters of learning and classification are kept constant.
This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ The algorithm was generalized to allow both head-final and head-initial dependencies by Yamada and Matsumoto (2003), who reported very good parsing accuracy for English, using dependency structures extracted from the Penn Treebank for training and testing.
This definition of transition sequence differs from that of Nivre (2008) but is equivalent and suits our presentation better. $$$$$ Let S = (C, T, cs, Ct) be a transition system for dependency parsing.

In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ In particular, whereas this type of framework has previously been used to characterize algorithms in the stackbased family (Nivre 2003, 2006b; Attardi 2006), it is here being used also for the listbased algorithms first discussed by Covington (2001).
In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems.
In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ A weaker notion would be to only require that the set of arcs is built monotonically (the third condition); a stronger notion would be to require also that nodes in R are processed strictly left to right.
In this paper, we assume that the resulting dependency graph for a sentence is well-formed and projective (Nivre, 2008). $$$$$ 2007).

studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ In this section, we introduce a formal framework for the specification of deterministic dependency parsing algorithms in terms of two components: a transition system, which is nondeterministic in the general case, and an oracle, which always picks a single transition out of every parser configuration.
studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ Remark 1).
studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ When discussing PROJECTIVITY, we will often use the notation i →* j to mean that j is reachable from i in the reflexive and transitive closure of the arc relation.
studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto,2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). $$$$$ Because the node set V is given by the input sentence itself, the set Acm of dependency arcs in the terminal configuration will determine the output dependency graph Gcm = (V, Acm ).

All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ Because the node set V is given by the input sentence itself, the set Acm of dependency arcs in the terminal configuration will determine the output dependency graph Gcm = (V, Acm ).
All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ Let S = (C, T, cs, Ct) be a transition system.
All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ In this kind of framework the syntactic structure of a sentence is modeled by a dependency graph, which represents each word and its syntactic dependents through labeled directed arcs.
All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). $$$$$ A dependency graph G = (V, A) is projective if and only if, for every arc (i, l,j) E A and node k E V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1), (i1, l2,i2), ... (ik−1, lk, ik)} E A such that ik = k. In a projective dependency graph, every node has a continuous projection, where the projection of a node i is the set of nodes reachable from i in the reflexive and transitive closure of the arc relation.

The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ In this section, we introduce a formal framework for the specification of deterministic dependency parsing algorithms in terms of two components: a transition system, which is nondeterministic in the general case, and an oracle, which always picks a single transition out of every parser configuration.
The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ Moreover, if p has a head k in Gx, then k must be the topmost node in σcq that is not a dependent of p (anything else would again be inconsistent with the assumption that Gx is projective).
The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ I want to thank my students Johan Hall and Jens Nilsson for fruitful collaboration and for their contributions to the MaltParser system, which was used for all experiments.
The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). $$$$$ A parser configuration is therefore defined as a quadruple, consisting of two lists, an input buffer, and a set of dependency arcs.

There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ A dependency graph G = (V, A) is projective if and only if, for every arc (i, l,j) E A and node k E V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1), (i1, l2,i2), ... (ik−1, lk, ik)} E A such that ik = k. In a projective dependency graph, every node has a continuous projection, where the projection of a node i is the set of nodes reachable from i in the reflexive and transitive closure of the arc relation.
There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ The set A of arcs (or directed edges) is a set of ordered triples (i, l, j), where i and j are nodes and l is a dependency label.
There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ I want to thank my students Johan Hall and Jens Nilsson for fruitful collaboration and for their contributions to the MaltParser system, which was used for all experiments.
There are two dominant transition based dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). $$$$$ We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions.

Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ The remainder of the article is structured as follows.
Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems.
Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.
Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. $$$$$ In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant.
