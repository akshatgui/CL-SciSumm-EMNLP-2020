Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ Data preparation followed the annotation guidelines created to deal with the specificities of event and timex expressions in Spanish (Saur´ı et al., 2009a; Sauriet al., 2009b).
Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ For the extents of events and time expressions (tasks A and B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the response.
Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ Not all corpora contained data for all six tasks.
Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ The results for task A, recognition and normalization of time expressions, are given in tables 2 and 3.

Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps $$$$$ (8) The President spokee, to the nation on Tuesday on the financial crisis.
Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps $$$$$ For attributes of events and time expressions (the second part of tasks A and B) and for relation types (tasks C through F) we use an even simpler metric: the number of correct answers divided by the number of answers.
Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps $$$$$ The TempEval annotation language is a simplified version of TimeML.2 using three TimeML tags: TIMEX3, EVENT and TLINK.

We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.
We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ Determine the extent of the events in a text as defined by the TimeML EVENT tag.
We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ Irina Prodanof.
We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ In addition, participants could choose one or more of the six languages for which we provided data: Chinese, English, French, Italian, Korean, and Spanish.

Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ Table 1 gives the size of the training set and the relation tasks that were included.
Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ Times can be expressed syntactically by adverbial or prepositional phrases, as shown in the following example.
Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ The data for the five languages were prepared independently of each other and do not comprise a parallel corpus.
Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ The data for the five languages were prepared independently of each other and do not comprise a parallel corpus.

We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ In addition, participants could choose one or more of the six languages for which we provided data: Chinese, English, French, Italian, Korean, and Spanish.
We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ Finally, thanks to all the participants, for sticking with a task that was not always as flawless and timely as it could have been in a perfect world.
We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ For Spanish, the f-measure for TIMEX3 extents ranges from 0.88 through 0.91 with an average of 0.89; for English the f-measure ranges from 0.26 through 0.86, for an average of 0.78.
We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ For attributes of events and time expressions (the second part of tasks A and B) and for relation types (tasks C through F) we use an even simpler metric: the number of correct answers divided by the number of answers.

We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ In addition, participants could choose one or more of the six languages for which we provided data: Chinese, English, French, Italian, Korean, and Spanish.
We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful.
We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ Data preparation followed the annotation guidelines created to deal with the specificities of event and timex expressions in Spanish (Saur´ı et al., 2009a; Sauriet al., 2009b).

This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ With the task decomposition allowed by BAT, it is possible to structure the complex task of temporal annotation by splitting it up in as many sub tasks as seems useful.
This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ The EVENT tag is used to annotate those elements in a text that describe what is conventionally referred to as an eventuality.
This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ In the rest of this paper, we first introduce the data that we are dealing with.

There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ The distribution over the six languages was very uneven: sixteen systems for English, two for Spanish and one for English and Spanish.
There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ Work on the English corpus was supported under the NSF-CRI grant 0551615, ”Towards a Comprehensive Linguistic Annotation of Language” and the NSFINT-0753069 project ”Sustainable Interoperability for Language Technology (SILT)”, funded by the National Science Foundation.
There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ The data released for the TempEval-2 Spanish edition is a fragment of the Spanish TimeBank, currently under development.

In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ We proceed by shortly describing the data resources and their creation, followed by the performance of the systems that participated in the tasks.
In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ In the rest of this paper, we first introduce the data that we are dealing with.
In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text.

TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ Manually annotated data were
TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ The ultimate aim of temporal processing is the automatic identification of all temporal referring expressions, events and temporal relations within a text.
TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ Examples of some of these features are shown below: Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 57–62, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics The relation types for the TimeML TLINK tag form a fine-grained set based on James Allen’s interval logic (Allen, 1983).

The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ We proceed by shortly describing the data resources and their creation, followed by the performance of the systems that participated in the tasks.
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ Finally, thanks to all the participants, for sticking with a task that was not always as flawless and timely as it could have been in a perfect world.
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ AFTER(e1,e2)
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ Table 8 lists the average scores and the standard deviations for all the tasks (on the English data) that Tempeval-1 and Tempeval-2 have in common.
