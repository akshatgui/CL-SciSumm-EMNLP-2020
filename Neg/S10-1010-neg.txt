Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ The work on the Spanish corpus was supported by a EU Marie Curie International Reintegration Grant (PIRG04-GA-2008-239414).
Besides the increasing availability of an notation standards (e.g., TIMEML (Pustejovsky et al., 2003a)) and corpora (e.g., TIDES (Ferro et al., 2000), TimeBank (Pustejovsky et al, 2003b)), the community has also organized three successful evaluation workshops TempEval1 (Verhagen et al, 2009), -2 (Verhagen et al, 2010), and-3 (Uzzaman et al, 2013). $$$$$ Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.

Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). $$$$$ Manually annotated data were
Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). $$$$$ Finally, thanks to all the participants, for sticking with a task that was not always as flawless and timely as it could have been in a perfect world.
Following the "divide-and-conquer" approach described in Verhagen et al (2010), results from the three temporal processing steps: 1) timex normalization, 2) event-timex temporal relationship classification, and 3) event-event temporal relationship classification, are merged to obtain time lines (top half of Figure 3). $$$$$ This task involves identifying the temporal relations between events and temporal expressions in text.

We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ The results for task B, event recognition, are given in tables 4 and 5.
We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ In addition, determine the value of the features CLASS, TENSE, ASPECT, POLARITY, and MODALITY.
We evaluate our model on all six languages in the TempEval2 Task A dataset (Verhagen et al, 2010), comparing against state-of-the-art systems for English and Spanish. $$$$$ In addition, participants could choose one or more of the six languages for which we provided data: Chinese, English, French, Italian, Korean, and Spanish.

Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ In the rest of this paper, we first introduce the data that we are dealing with.
Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ In addition, determine the value of the features CLASS, TENSE, ASPECT, POLARITY, and MODALITY.
Part of our task is similar to task C of TempEval-2 (Verhagen et al 2010), determining the temporal relation between an event and a time expression in the same sentence. $$$$$ Splitting the task into substask reduces the error rate in the manual annotation, and that merging the different sub-task into a unique layer as a postprocessing operation (see figure 1) provides better and more reliable results (annotated data) than doing a complex task all at once.

We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ Using a subset of TimeML temporal relations, we show how temporal relations and anchorings can be annotated and identified in six different languages.
We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ Irina Prodanof.
We also evaluated our system on TempEval 2 (Verhagen et al 2010) for better comparison to the state-of-the-art. $$$$$ The most salient event attributes encode tense, aspect, modality and polarity information.

We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ The French corpus contained a subcorpus with temporal relations but these relations were not split into the four tasks C through F. Annotation proceeded in two phases: a dual annotation phase where two annotators annotate each document and an adjudication phase where a judge resolves disagreements between the annotators.
We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ Work on the English corpus was supported under the NSF-CRI grant 0551615, ”Towards a Comprehensive Linguistic Annotation of Language” and the NSFINT-0753069 project ”Sustainable Interoperability for Language Technology (SILT)”, funded by the National Science Foundation.
We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ Table 1 gives the size of the training set and the relation tasks that were included.
We evaluate our model against current state-of-the art systems for temporal resolution on the English portion of the TempEval-2 Task A dataset (Verhagen et al, 2010). $$$$$ However, annotation specifications and guidelines for the five languages were developed in conjunction with one other, in many cases based on version 1.2.1 of the TimeML annotation guidelines for English3.

This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ Finally, thanks to all the participants, for sticking with a task that was not always as flawless and timely as it could have been in a perfect world.
This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ The work on the Spanish corpus was supported by a EU Marie Curie International Reintegration Grant (PIRG04-GA-2008-239414).
This task is based on task A in the TempEval-2 challenge (Verhagen et al, 2010). $$$$$ One thing that would need to be addressed in a follow-up task is what the optimal number of tasks is.

There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ This work paves the way towards establishing a broad and open standard metadata markup language for natural language texts, examining events, temporal expressions, and their orderings.
There has been much work addressing the problems of temporal expression extraction and normalization, i.e. the systems developed in TempEval-2 challenge (Verhagen et al, 2010). $$$$$ F. Determine the temporal relation between two events where one event syntactically dominates the other event.

In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ Some of these systems only participated in one or two tasks while others participated in all tasks.
In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ For Spanish, the f-measure for TIMEX3 extents ranges from 0.88 through 0.91 with an average of 0.89; for English the f-measure ranges from 0.26 through 0.86, for an average of 0.78.
In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ Not all corpora contained data for all six tasks.
In recent years a renewed interest in temporal processing has spread in the NLP community, thanks to the success of the TimeML annotation scheme (Pustejovsky et al, 2003a) and to the availability of annotated resources, such as the English and French TimeBanks (Pustejovsky et al., 2003b; Bittar, 2010) and the TempEval corpora (Verhagen et al, 2010). $$$$$ For the extents of events and time expressions (tasks A and B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the response.

TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ The two main attributes of the TIMEX3 tag are TYPE and VAL, both shown in the example (2). type=&quot;DATE&quot; val=&quot;2004-11-22&quot; For TempEval-2, we distinguish four temporal types: TIME (at 2:45 p.m.), DATE (January 27, 1920, yesterday), DURATION (two weeks) and SET (every Monday morning).
TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ Manually annotated data were
TempEval (Verhagen et al 2007), in 2007, and more recently TempEval-2 (Verhagen et al 2010), in 2010, were concerned with this problem. $$$$$ In addition, tasks can be ranked, allowing systems to feed the results of one (more precise) task as a feature into another task.

The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ Manually annotated data were
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ Temporal relations come in two broad flavours: anchorings of events to time expressions and orderings of events.
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ For TempEval, the set of labels was simplified to aid data preparation and to reduce the complexity of the task.
The results of TempEval-2 are fairly similar (Verhagen et al 2010), but the data used are similar but not identical. $$$$$ For the extents of events and time expressions (tasks A and B), precision, recall and the f1-measure are used as evaluation metrics, using the following formulas: Where tp is the number of tokens that are part of an extent in both key and response, fp is the number of tokens that are part of an extent in the response but not in the key, and fn is the number of tokens that are part of an extent in the key but not in the response.
