 $$$$$ Once the reordered target (here English) sentences are generated, we measure the distortion between the language pair by computing the BLEU3 score between the original target and reordered target, treating the original target as the reference.
 $$$$$ It contains 663 segments (i.e., sentences).

Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Word Alignment a can be rewritten as a1 = 1 and a2 = 3 (i.e., the second target word is aligned to the third source word).
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Only phrases that conform to the so-called consistent alignment restrictions (Och et al., 1999) are extracted.
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ The Chinese side is word segmented and consists of 4,319 tokens and the English consists of 5,525 tokens.
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Word Alignment a can be rewritten as a1 = 1 and a2 = 3 (i.e., the second target word is aligned to the third source word).

Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ There is a fundamental difference between decoding for machine translation and decoding for speech recognition.
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ Phrase-based monotone decoding does not directly address word order issues.

This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.
This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ The ArabicEnglish test set is the NIST MT Evaluation 2003 test set.
This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.

 $$$$$ Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score.
 $$$$$ It contains 663 segments (i.e., sentences).
 $$$$$ We presented a new distortion model that can be integrated with existing phrase-based SMT decoders.
 $$$$$ We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.

Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ The latter models define the distortion parameters for a cept (one or more words).
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ Which word position to translate next?
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.

As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Therefore, we would like to incorporate syntactic or part-of-speech information in our distortion model.
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ The language model we use for the experiments reported here is the same as the one used for other experiments reported in this paper.
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Our results on the 2004 and 2005 NIST MT Evaluation test sets using the distortion model are 0.4497 and 0.464610, respectively.

Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ This new model leads to significant improvements in MT quality as measured by BLEU (Papineni et al., 2002).
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ Our proposed distortion model relies solely on word alignments and is conditioned on the source words.
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion.
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.

As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ This method is based on word-alignments and the BLEU metric.
As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ Section 2 presents a review of related work.

As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ The latter models define the distortion parameters for a cept (one or more words).
As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ The distortion is conditioned on classes of the aligned source and target words.
As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ A different approach to allow for a limited reordering is to reorder the input sentence such that the source and the target sentences have similar word order and then proceed to monotonically decode the reordered source sentence.

We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ The distortion cost is added to the log-linear mixture of the hypothesis extension in a fashion similar to the language model cost.
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ Distortion probability is also conditioned on the source and target sentence lengths.

Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ Our proposed distortion model relies solely on word alignments and is conditioned on the source words.
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ Therefore, when translating between Spanish and English, words must usually be reordered.

The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ For instance, the outbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated.
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ From this partial alignment we increase the counts for the following outbound, inbound, and pair distortions: Po(δ = +2|f10), Pi(δ = +2|f300). and Pp(δ = +2|f10, f300).
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ In Section 3 we propose a method for measuring the distortion between any given pair of languages.
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ From this partial alignment we increase the counts for the following outbound, inbound, and pair distortions: Po(δ = +2|f10), Pi(δ = +2|f300). and Pp(δ = +2|f10, f300).

This assumption is realistic $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
This assumption is realistic $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
This assumption is realistic $$$$$ Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score.
This assumption is realistic $$$$$ The word alignments we use are both annotated manually by human annotators.

The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ However, when the distortion model is used, we see statistically significant increases in the BLEU score as we consider more word reorderings.
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ The search is done in n time steps.
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ These rewrite patterns are automatically extracted by parsing the source and target sides of the training parallel corpus.

To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ For example, a phrase-based decoder might translate the Arabic phrase AlwlAyAt AlmtHdp2 correctly into English as the United States if it was seen in its training data, was aligned correctly, and was added to the phrase dictionary.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.

 $$$$$ In Section 5, we present some empirical results that show the utility of our distortion model for statistical machine translation systems.
 $$$$$ However, that is not necessarily the case in MT due to the fact that different languages have different word order requirements.
 $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
 $$$$$ We propose a new distortion model that can be used as an additional component in SMT decoders.

The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ Then, we conclude this paper with a discussion in Section 6.
The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ The experimental results we report in this paper are for Arabic-English machine translation of news stories.
The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ As suggested by the BLEU scores reported in Table 2, Arabic-English has more word order differences than Chinese-English.
The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ As suggested by the BLEU scores reported in Table 2, Arabic-English has more word order differences than Chinese-English.

 $$$$$ The distortion cost is added to the log-linear mixture of the hypothesis extension in a fashion similar to the language model cost.
 $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
 $$$$$ We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.

In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ The word reorderings that are explored by the search algorithm are controlled by two parameters s and w as described in (Tillmann and Ney, 2003).
In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ Therefore, we would like to incorporate syntactic or part-of-speech information in our distortion model.
