 $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
 $$$$$ We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
 $$$$$ The majority of word movement in translation is mainly due to syntactic differences between the source and target language.
 $$$$$ They play an important role in selecting among several candidate word realization of a given acoustic signal.

Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ We illustrate this with an example: Suppose the last position translated in the source sentence so far is n and we are to cover a source phrase p=wlAyp wA$nTn that begins at position m in the source sentence.
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ For instance, the outbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated.
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Chinese-English.
Lexicalized distortion models predict the jump from the last translated word to the next one, with a class for each possible jump length (Al-Onaizan and Papineni, 2006), or bin of lengths (Green et al, 2010). $$$$$ Then, we conclude this paper with a discussion in Section 6.

Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ We also propose a novel metric to measure word order similarity (or differences) between any pair of languages based on word alignments.
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ The difference in n-gPrec is bigger for smaller values of n, which suggests that ArabicEnglish has more local word order differences than in Chinese-English.
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ The Chinese side is word segmented and consists of 4,319 tokens and the English consists of 5,525 tokens.
Demonstrating the inadequacy of such approaches, Al-Onaizan and Papineni (2006) showed that even given the words in the reference translation, and their alignment to the source words, a decoder of this sort charged with merely rearranging them into the correct target-language order could achieve a BLEU score (Papineni et al., 2002) of at best 69% and that only when restricted to keep most words very close to their source positions. $$$$$ The latter models define the distortion parameters for a cept (one or more words).

This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ We presented a new distortion model that can be integrated with existing phrase-based SMT decoders.
This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ We illustrate this with an example: Suppose the last position translated in the source sentence so far is n and we are to cover a source phrase p=wlAyp wA$nTn that begins at position m in the source sentence.
This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ Also, suppose that our phrase dictionary provided the translation Washington State, with internal word alignment a = (a1 = 2, a2 = 1) (i.e., a=(<Washington,wA$nTn>, <State,wlAyp>), then the outbound phrase cost is defined as: where l is the length of the target phrase, a is the internal word alignment, fn is source word at position n (in the sentence), and fa, is the source word that is aligned to the i-th word in the target side of the phrase (not the sentence).
This is similar to the oracle ordering used by Al-Onaizan and Papineni (2006), but differs in the handling of unaligned words. $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.

 $$$$$ For instance, the outbound distortion component attempts to capture what is typically translated immediately after the word that has just been translated.
 $$$$$ It does not conillustrate word positions in the sentence.
 $$$$$ Also, suppose that our phrase dictionary provided the translation Washington State, with internal word alignment a = (a1 = 2, a2 = 1) (i.e., a=(<Washington,wA$nTn>, <State,wlAyp>), then the outbound phrase cost is defined as: where l is the length of the target phrase, a is the internal word alignment, fn is source word at position n (in the sentence), and fa, is the source word that is aligned to the i-th word in the target side of the phrase (not the sentence).

Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ The Chinese side is word segmented and consists of 4,319 tokens and the English consists of 5,525 tokens.
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.
Since we are using the distortion model in (Al-Onaizan and Papineni, 2006) the entire last source phrase interval needs to be stored. $$$$$ Chinese-English.

As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ For the results reported in this paper word alignments were obtained using a maximum-posterior word aligner4 described in (Ge, 2004).
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ Distortion probability is also conditioned on the source and target sentence lengths.
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
As mentioned by (Al-Onaizan and Papineni, 2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ The distortion cost is added to the log-linear mixture of the hypothesis extension in a fashion similar to the language model cost.

Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ The outbound distortion cost is defined as: where P3(δ) is a smoothing distribution 7 and α is a linear-mixture parameter 8.
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ Rewriting the input sentence whether using syntactic rules or heuristics makes hard decisions that can not be undone by the decoder.
Our baseline MT decoder is a phrase-based decoder as described in (Al-Onaizan and Papineni 2006). $$$$$ We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.

As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ Therefore, SMT decoders typically limit the number of permutations considered for efficiency reasons by placing reordering restrictions.
As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ IBM Models 2 and 3 define the distortion parameters in terms of the word positions in the sentence pair, not the actual words at those positions.
As pointed out in (Al-Onaizan and Papineni, 2006), these strategies make hard decisions in reordering which cannot be undone during decoding. $$$$$ The majority of word movement in translation is mainly due to syntactic differences between the source and target language.

As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
As mentioned by (Al-Onaizan and Papineni,2006), it can be problematic that these deterministic choices are beyond the scope of optimization and cannot be undone by the decoder. $$$$$ The run-time search procedure used to find the most likely translation (or transcription in the case of Speech Recognition) is typically referred to as decoding.

We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score.
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ We also showed that n-gram language modlations.
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ Our distortion parameters are directly estimated from word alignments by simple counting over alignment links in the training data.
We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). $$$$$ In this section, we propose a simple, novel method for measuring word order similarity (or differences) between any given language pair.

Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ The language model, phrase dictionary, and other decoder tuning parameters remain the same in all experiments reported in this table.
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.
Other further generalizations of orientation include the global prediction model (Nagata et al, 2006) and distortion model (Al-Onaizan and Papineni, 2006). $$$$$ When decoding a speech signal, words are generated in the same order in which their corresponding acoustic signal is consumed.

The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ The distortion model we propose assigns a probability distribution over possible relative jumps conditioned on source words.
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ We illustrate this with an example: Suppose the last position translated in the source sentence so far is n and we are to cover a source phrase p=wlAyp wA$nTn that begins at position m in the source sentence.
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ The word alignments we use are both annotated manually by human annotators.
The MT system is a phrase based SMT system as described in (Al-Onaizanand Papineni, 2006). $$$$$ We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.

This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). $$$$$ The phrases in the phrase dictionary we use in the experiments reported here are a combination of phrases automatically extracted from maximumposterior alignments and maximum entropy alignments.
This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). $$$$$ We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
This assumption is realistic: while truly parallel data (humanly created) might be in short supply or harder to acquire, adapting statistical machine translation (SMT) systems from one language-pair to another is not as challenging as it used to be (Al-Onaizan and Papineni, 2006). $$$$$ (Och et al., 2004; Tillman, 2004) propose orientation-based distortion models lexicalized on the phrase level.

The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ The value for δ, in theory, ranges from −max to +max (where max is the maximum source sentence length observed), but in practice only a small number of those step sizes are observed in the training data, and hence, have non-zero value).
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ Section 2 presents a review of related work.
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ Table 2 shows these scores for Arabic-English and 3the BLEU scores reported throughout this paper are for case-sensitive BLEU.
The Chinese to English SMT system has similar architecture to the one described in (Al-Onaizan and Papineni, 2006). $$$$$ A hypothesis h is extended by matching the phrase dictionary against source word sequences in the input sentence that are not covered in h. The cost of the new hypothesis C(hn w) = C(h) + C(e), where C(e) is the cost of this extension.

To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ It was also partially supported by DARPA TIDES program monitored by SPAWAR under contract number N66001-99-2-8916.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ The Chinese side is word segmented and consists of 4,319 tokens and the English consists of 5,525 tokens.
To remedy these deficiencies, Al-Onaizan and Papineni (2006) proposed a lexicalized, generative distortion model. $$$$$ The Chinese side is word segmented and consists of 4,319 tokens and the English consists of 5,525 tokens.

 $$$$$ (Berger et al., 1996) allow only reordering of at most n words at any given time.
 $$$$$ Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU score.
 $$$$$ In this section, we propose a simple, novel method for measuring word order similarity (or differences) between any given language pair.
 $$$$$ IBM Models 4 and 5 alleviate this limitation by replacing absolute word positions with relative positions.

The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ The ultimate word order choice made is influenced by both the language model cost as well as the distortion cost.
The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ We illustrate this with an example: Suppose the last position translated in the source sentence so far is n and we are to cover a source phrase p=wlAyp wA$nTn that begins at position m in the source sentence.
The lexicalized distortion model was used as described in (Al-Onaizan and Papineni, 2006) with a window width of up to 5 and a maximum number of skipped (not covered) words during decoding of 2. $$$$$ From this partial alignment we increase the counts for the following outbound, inbound, and pair distortions: Po(δ = +2|f10), Pi(δ = +2|f300). and Pp(δ = +2|f10, f300).

 $$$$$ The language model used is an interpolated trigram model described in (Bahl et al., 1983).
 $$$$$ The value for δ, in theory, ranges from −max to +max (where max is the maximum source sentence length observed), but in practice only a small number of those step sizes are observed in the training data, and hence, have non-zero value).
 $$$$$ This work was partially supported by DARPA GALE program under contract number HR0011-06-2-0001.
 $$$$$ Table 4 shows some examples of original English, English in Arabic order, and the decoder output for two different sets of reordering parameters.

In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ The inbound and pair distortion costs (i..e, Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined in a similar fashion.
In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ The number of references used is also reported (e.g., BLEUr1n4c: r1 means 1 reference, n4 means upto 4-gram are considred, c means case sensitive).
In future work, we plan to extend the parameterization of our models to not only predict phrase orientation, but also the length of each displacement as in (Al-Onaizan and Papineni, 2006). $$$$$ If a target word is not aligned, then, we assume that it is aligned to the same source word that the preceding aligned target word is aligned to.
