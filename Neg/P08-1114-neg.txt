In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. $$$$$ Consistent patterns reappeared: improvements over the baseline up to 1.69 BLEU (p < .01), with AdvP2 again in the lead (also outperforming the undifferentiated constituency feature, p < .05).
In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. $$$$$ We also investigated whether non-canonical linguistic constituency labels such as PRN, FRAG, UCP and VSB introduce “noise”, by means of the XP features — the XP= feature is, in fact, simply the undifferentiated constituency feature, but sensitive only to “standard” XPs.
In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. $$$$$ A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment.
In Marton and Resnik (2008), hiero variables were disambiguated with additional binary feature functions, with their weights optimized in standard MER training. $$$$$ This work was supported in part by DARPA prime agreement HR0011-06-2-0001.

Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. $$$$$ This work was supported in part by DARPA prime agreement HR0011-06-2-0001.
Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. $$$$$ For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003).
Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. $$$$$ This work was supported in part by DARPA prime agreement HR0011-06-2-0001.
Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), here after MR08. $$$$$ Formally, Hiero’s translation model is a weighted synchronous contextfree grammar.

Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. $$$$$ Formally, Hiero’s translation model is a weighted synchronous contextfree grammar.
Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. $$$$$ We obtain substantial improvements in performance for translation from Chinese and Arabic to English.
Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. $$$$$ Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion.
Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. $$$$$ (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others).

Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. $$$$$ Within language, we also see considerable consistency across multiple test sets, in terms of which constraints tend to help most.
Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. $$$$$ **: Better than baseline (p < .01).
Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. $$$$$ This work was supported in part by DARPA prime agreement HR0011-06-2-0001.
Marton and Resnik (2008) utilize the language linguistic analysis that is derived from parse tree to constrain the translation in a soft way. $$$$$ If NP= matches a rule, the model score is incremented by ANP_, and if NP+ matches, the model score is decremented by the same quantity.

Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. $$$$$ Since some combinations seem to cancel individual contributions, we can conclude that the higher the number of participant features (of the kinds described here), the more likely a cancellation effect is; therefore, a “double-feature” combination is more likely to yield higher gains than a combination containing more features.
Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. $$$$$ We found it more surprising that no NP variant yielded much gain in Arabic; this question will be taken up in future work.
Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. $$$$$ Formally, Hiero’s translation model is a weighted synchronous contextfree grammar.
Chiang (2005), Marton and Resnik (2008) explored the constituent match/violation in hiero; Xiong (2009 a) added constituent parse tree based linguistic analysis into BTG model; Xiong (2009 b) added source dependency structure to BTG; Zhang (2009) added tree-kernel to BTG model. $$$$$ Although performance of XP=, XP2 and all-labels+ were similar to that of the undifferentiated constituency feature, XP+ achieved the highest gain.

We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). $$$$$ We first evaluated translation performance using the NIST MT06 (nisttext) set.
We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). $$$$$ When looking at Hiero rules, which are acquired automatically by the model from parallel text, it is easy to find many cases that seem to respect linguistically motivated boundaries.
We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). $$$$$ Since some combinations seem to cancel individual contributions, we can conclude that the higher the number of participant features (of the kinds described here), the more likely a cancellation effect is; therefore, a “double-feature” combination is more likely to yield higher gains than a combination containing more features.
We believe a greater improvement can be expected if we apply our idea to finer-grained approaches that use constraints softly (Marton and Resnik (2008) and Cherry (2008)). $$$$$ A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment.

By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). $$$$$ Finding the right way to balance linguistic analysis with unconstrained data-driven modeling is clearly a key challenge.
By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). $$$$$ In this paper, we returned to the idea of linguistically motivated soft constraints, and we demonstrated that they can, in fact, lead to substantial improvements in translation performance when integrated into the Hiero framework.
By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). $$$$$ Hiero (Chiang, 2005; Chiang, 2007) is a hierarchical phrase-based statistical MT framework that generalizes phrase-based models by permitting phrases with gaps.
By relaxing the distortion limit, we have left room for more sophisticated re-ordering models in conventional phrase-based decoders while maintaining a significant performance advantage over hierarchical systems (Marton and Resnik, 2008). $$$$$ On the face of it, there are any number of possible reasons Chiang’s (2005) soft constraint did not work – including, for example, practical issues like the quality of the Chinese parses.5 However, we focus here on two conceptual issues underlying his use of source language syntactic constituents.

Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. $$$$$ In Section 2, we briefly review the Hiero statistical MT framework (Chiang, 2005, 2007), upon which this work builds, and we discuss Chiang’s initial effort to incorporate soft source-language constituency constraints for Chinese-English translation.
Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. $$$$$ An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results.
Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. $$$$$ This work was supported in part by DARPA prime agreement HR0011-06-2-0001.
Marton and Resnik (2008) revised this method by distinguishing different constituent syntactic types, and defined features for each type to count whether a phrase matches or crosses the syntactic boundary. $$$$$ We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language.

Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). $$$$$ The idea then seems essentially to have been abandoned; it does not appear in later discussions (Chiang, 2007).
Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). $$$$$ Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g.
Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). $$$$$ Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgments for in-coverage cases.
Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). $$$$$ We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language.

Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. $$$$$ Section 5 discusses the results, and Section 6 considers related work.
Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. $$$$$ In adding syntax to statistical MT, there is a tradeoff between taking advantage of linguistic analysis, versus allowing the model to exploit linguistically unmotivated mappings learned from parallel training data.
Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. $$$$$ An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results.
Marton and Resnik (2008) add feature functions to penalize or reward non-terminals which cross constituent boundaries of the source sentence. $$$$$ In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data.

Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. $$$$$ The authors would like to thank David Chiang and Adam Lopez for making their source code available; the Stanford Parser team and Mary Harper for making their parsers available; David Chiang, Amy Weinberg, and CLIP Laboratory colleagues, particularly Chris Dyer, Adam Lopez, and Smaranda Muresan, for discussion and invaluable assistance.
Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. $$$$$ Since component features in those combinations were informed by individual-feature performance on the test set, we tested the best performing conditions from MT06 on a new test set, NIST MT08.
Marton and Resnik (2008) and Cherry (2008) imposed syntactic constraints on the PBMT system by making use of prior linguistic knowledge in the form of syntax analysis. $$$$$ We demonstrated improvements for ChineseEnglish translation, and succeed in obtaining substantial gains for Arabic-English translation, as well.

Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). $$$$$ We therefore define a space of new features as the cross product {CP, IP, NP, VP, ...} x {_, +}. where = and + signify matching and crossing boundaries, respectively.
Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). $$$$$ 8We map SBAR and S labels in Arabic parses to CP and IP, respectively, consistent with the Chinese parses.
Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). $$$$$ We first tested on on the NIST MT03 and MT06 (nist-text) sets.
Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005). $$$$$ • For each constituent type, e.g.

We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). $$$$$ An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results.
We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). $$$$$ Furthermore, our results provide some insight into why the original approach may have failed to yield a positive outcome.
We compare our method with the baseline and some typical approaches listed in Table 1 where XP+ denotes the approach in (Marton and Resnik, 2008) and TOFW (topological ordering of function words) stands for the method in (Setiawan et al., 2009). $$$$$ An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results.

Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). $$$$$ In this example, a, would be added to the hypothesis score for any rule used in the hypothesis whose source side spanned the minister, a speech, yesterday, gave a speech yesterday, or the minister gave a speech yesterday.
Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). $$$$$ Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs.
Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). $$$$$ (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal coverage.
Early works reward/penalize spans that respect the syntactic parse constituents of an input sentence (Chiang, 2005), and (Marton and Resnik, 2008). $$$$$ He tested this conjecture by adding a soft constraint in the form of a “constituency feature”: if a synchronous rule X —* (e, f) is used in a derivation, and the span of f is a constituent in the sourcelanguage parse, then a term a, is added to the model score in expression (1).4 Unlike a hard constraint, which would simply prevent the application of rules violating syntactic boundaries, using the feature to introduce a soft constraint allows the model to boost the “goodness” for a rule if it is constitent with the source language constituency analysis, and to leave its score unchanged otherwise.

On the contrary, Marton and Resnik (2008) and Cherry (2008) accumulate a count whenever hypotheses violate constituent boundaries. $$$$$ An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results.

Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. $$$$$ In Section 2, we briefly review the Hiero statistical MT framework (Chiang, 2005, 2007), upon which this work builds, and we discuss Chiang’s initial effort to incorporate soft source-language constituency constraints for Chinese-English translation.
Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. $$$$$ Space limitations preclude a thorough review of work attempting to navigate the tradeoff between using language analyzers and exploiting unconstrained data-driven modeling, although the recent literature is full of variety and promising approaches.
Marton and Resnik (2008) find that some constituency types favor matching the source parse while others encourage violations. $$$$$ We limit ourselves here to several approaches that seem most closely related.

To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. $$$$$ They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks.
To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. $$$$$ Finally we conclude in Section 7 with a summary and potential directions for future work.
To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. $$$$$ For example, seems to capture the use of jingtian/this year as a temporal modifier when building linguistic constituents such as noun phrases (the election this year) or verb phrases (voted in the primary this year).
To compare with the CMVC, we also conduct experiments using (Marton and Resnik, 2008)'s XP+. $$$$$ We present an approach that explores the tradeoff from the other direction, starting with a context-free translation model learned directly from aligned parallel text, and then adding soft constituent-level constraints based on parses of the source language.

Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. $$$$$ This makes sense, since — at least for these language pairs and perhaps more generally — clauses and verb phrases seem to correspond often on the source and target side.
Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. $$$$$ An approach to incorporating parser-based constituents in the model was explored briefly, treating syntactic constituency as a soft constraint, with negative results.
Like (Marton and Resnik, 2008), we find that the XP+feature obtains a significant improvement of 1.08 BLEU over the baseline. $$$$$ The authors would like to thank David Chiang and Adam Lopez for making their source code available; the Stanford Parser team and Mary Harper for making their parsers available; David Chiang, Amy Weinberg, and CLIP Laboratory colleagues, particularly Chris Dyer, Adam Lopez, and Smaranda Muresan, for discussion and invaluable assistance.

Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. $$$$$ The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g.
Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. $$$$$ Since some combinations seem to cancel individual contributions, we can conclude that the higher the number of participant features (of the kinds described here), the more likely a cancellation effect is; therefore, a “double-feature” combination is more likely to yield higher gains than a combination containing more features.
Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. $$$$$ (Chiang, 2005; Chiang, 2007; Wu, 1997)).
Experiments show that our model achieves substantial improvements over baseline and significantly outperforms (Marton and Resnik, 2008)'s XP+. $$$$$ Our results contribute to a growing body of work on combining monolingually based, linguistically motivated syntactic analysis with translation models that are closely tied to observable parallel training data.

Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs. $$$$$ A number of previous efforts have tackled this tradeoff by starting with a commitment to linguistically motivated analyses and then finding appropriate ways to soften that commitment.
Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs. $$$$$ This work was supported in part by DARPA prime agreement HR0011-06-2-0001.
Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs. $$$$$ By combining multiple finer-grained syntactic features, we obtain significant improvements of up to 1.65 BLEU points (NP_, VP2, IP2, all-labels_, and XP+).
Marton and Resnik (2008) find that their constituent constraints are sensitive to language pairs. $$$$$ (Chiang, 2005; Chiang, 2007; Wu, 1997)).
