We follow the recent work of (Klementiev and Roth 2006) who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for transliteration. $$$$$ One of the objectives for this work was to use as little of the knowledge of both languages as possible.
We follow the recent work of (Klementiev and Roth 2006) who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for transliteration. $$$$$ (Shinyama and Sekine, 2004) used the idea to discover NEs, but in a single language, English, across two news sources.
We follow the recent work of (Klementiev and Roth 2006) who addressed the problem of discovery of transliterated named entities from comparable corpora and suggested that alignment may not be necessary for transliteration. $$$$$ A large amount of previous work exists on transliteration models.

 $$$$$ Michael Collins and Yoram Singer.
 $$$$$ Named Entity recognition (NER) is an important part of many natural language processing tasks.
 $$$$$ For a given NE in one language, the transliteration model chooses a top ranked list of candidates in another language.
 $$$$$ A cumulative distribution was then collected for such equivalence classes.

 $$$$$ The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
 $$$$$ Pairwise time sequence scoring and transliteration models should give better confidence in NE matches.
 $$$$$ 1999.
 $$$$$ For example, Mount in Mount Rainier will probably be translated, and Rainier - transliterated.

 $$$$$ This approach is specific to Russian morphology, and would have to be altered for other languages.
 $$$$$ If its similarity score surpassed a set threshold, it was added to the list of positive examples for the next round of training.
 $$$$$ Identification of the entity’s equivalence class of transliterations is important for obtaining its accurate time sequence.
 $$$$$ Named Entity recognition (NER) is an important part of many natural language processing tasks.

 $$$$$ While generative models are often robust, they tend to make independence assumptions that do not hold in data.
 $$$$$ .
 $$$$$ In order to generate time sequence for a word, we divide the corpus into a sequence of temporal bins, and count the number of occurrences of the word in each bin.
 $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.

 $$$$$ For a given NE in one language, the transliteration model chooses a top ranked list of candidates in another language.
 $$$$$ The site provides loose translations of (and pointers to) the original English texts.
 $$$$$ We make use of it here too, to learn a discriminative transliteration model that requires little knowledge of the target language.
 $$$$$ We ran a series of experiments to see how the size of the initial training set affects the accuracy of the model as training progresses (Figure 5).

We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal cooccurrence similarity when ranking the transliteration candidates. $$$$$ However, for many languages, these resources do not exist.
We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal cooccurrence similarity when ranking the transliteration candidates. $$$$$ Couplings of the substrings from both sets produce features we use for training.
We compared our algorithm to two models described in (Klementiev and Roth, 2006b) one uses only phonetic similarity and the second also considers temporal cooccurrence similarity when ranking the transliteration candidates. $$$$$ For time sequence matching, we used a scoring metric novel in this domain.

This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ In keeping with our objective to provide as little language knowledge as possible, we introduced a simplistic approach to identifying transliteration equivalence classes, which sometimes produced erroneous groupings (e.g. an equivalence class for NE lincoln in Russian included both lincoln and lincolnshire on Figure 6).
This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.
This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new.
This configuration is equivalent to the model used in (Klementiev and Roth, 2006b). $$$$$ In this work, we only consider single word Named Entities.

We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. $$$$$ In this work, we only consider single word Named Entities.
We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. $$$$$ As supported by our own experiments, this method outperforms other scoring metrics traditionally used (such as cosine (Salton and McGill, 1986)) when corpora are not well temporally aligned.
We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. $$$$$ Pairwise time sequence scoring and transliteration models should give better confidence in NE matches.
We adopt a methodology parallel to that of [Klementiev and Roth, 2006], but we focus instead on mining parallel named entity transliteration pairs, using a well-trained linear classifier to identify transliteration pairs. $$$$$ We use the perceptron (Rosenblatt, 1958) algorithm to train the model.

Recently, [Klementiev and Roth, 2006] outlined an approach by leveraging the availability of article aligned news corpora between English and Russian, and tools in English, for discovering transliteration pairs between the two languages, and progressively refining the discovery process. $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
Recently, [Klementiev and Roth, 2006] outlined an approach by leveraging the availability of article aligned news corpora between English and Russian, and tools in English, for discovering transliteration pairs between the two languages, and progressively refining the discovery process. $$$$$ We developed a linear discriminative transliteration model, and presented a method to automatically generate features.

As in [Klementiev and Roth, 2006] no language specific knowledge was used to refine our mining process, making the approach broadly applicable. $$$$$ To this end, we would like to compare the performance of an NER system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.
As in [Klementiev and Roth, 2006] no language specific knowledge was used to refine our mining process, making the approach broadly applicable. $$$$$ This research is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program and a DOI grant under the Reflex program.
As in [Klementiev and Roth, 2006] no language specific knowledge was used to refine our mining process, making the approach broadly applicable. $$$$$ To this end, we would like to compare the performance of an NER system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.

In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details. $$$$$ Most current approaches employ machine learning techniques and require supervised data.
In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details. $$$$$ A subject of future work is to extend the algorithm to the multi-word setting.
In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details. $$$$$ On the other hand, comparable multilingual data (such as multilingual news streams) are increasingly available (see section 4).
In this section, we outline briefly the methodology presented in [Klementiev and Roth, 2006], and refer interested readers to the source for details. $$$$$ To this end, we would like to compare the performance of an NER system trained on a corpus tagged using this approach to one trained on a hand-tagged corpus.

We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool. $$$$$ We train a linear model to decide whether a word is a transliteration of an NE .
We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool. $$$$$ (Hetland, 2004) surveys recent methods for scoring time sequences for similarity.
We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool. $$$$$ We observe that NEs have similar time distributions across such corpora, and that they are often transliterated, and develop an algorithm that exploits both iteratively.
We start with comparable corpora in English and Tamil, similar in size to that used in [Klementiev and Roth, 2006], and using the English side of this corpora, first, we extract all the NEs that occur more than a given threshold parameter, FE, using a standard NER tool. $$$$$ The ultimate goal of this work is to automatically tag NEs so that they can be used for training of an NER system for a new language.

While we adopted a methodology similar to that in [Klementiev and Roth, 2006], our focus was on mining parallel NE transliteration pairs, leveraging the availability of comparable corpora and a well-trained linear classifier to identify transliteration pairs. $$$$$ However, they focus on the classification stage of already segmented entities, and make use of contextual and morphological clues that require knowledge of the language beyond the level we want to assume with respect to the target language.
While we adopted a methodology similar to that in [Klementiev and Roth, 2006], our focus was on mining parallel NE transliteration pairs, leveraging the availability of comparable corpora and a well-trained linear classifier to identify transliteration pairs. $$$$$ The discriminative learning framework argued for in (Roth, 1998; Roth, 1999) as an alternative to generative models is now used widely in NLP, even in the context of word alignment (Taskar et al., 2005; Moore, 2005).

However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al, 2009b). $$$$$ These numbers suggest that we only need a few initial positive examples to bootstrap the transliteration model.
However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al, 2009b). $$$$$ The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new.
However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al, 2009b). $$$$$ Figure 4 shows parts of transliteration lists for NE forsyth for two iterations of the algorithm.
However, several techniques for mining name transliterations from monolingual and comparable corpora have been studied (Pasternack and Roth, 2009), (Goldwasser and Roth, 2008), (Klementiev and Roth, 2006), (Sproat et al, 2006), (Udupa et al, 2009b). $$$$$ We would like to verify if this is indeed the case.

Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). $$$$$ Another reason is the lack of transliteration standards.
Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). $$$$$ We developed a linear discriminative transliteration model, and presented a method to automatically generate features.
Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
Character unigrams and bigrams were used as features to learn a discriminative transliteration model and time series similarity was combined with the transliteration similarity model (Klementiev and Roth, 2006). $$$$$ The algorithm can be naturally extended to comparable corpora of more than two languages.

Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al, 2008). $$$$$ The algorithm makes use of a new, frequency based, metric for time distributions and a resource free discriminative approach to transliteration.
Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al, 2008). $$$$$ The algorithm can be naturally extended to comparable corpora of more than two languages.
Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al, 2008). $$$$$ Note that although we expect that better use of language specific knowledge would improve the results, it would defeat one of the goals of this work.
Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al, 2008). $$$$$ As supported by our own experiments, this method outperforms other scoring metrics traditionally used (such as cosine (Salton and McGill, 1986)) when corpora are not well temporally aligned.

The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ The weak transliteration model selects the correct transliteration (italicized) as the 24th best transliteration in the first iteration.
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ The use of similarity of time distributions for information extraction, in general, and NE extraction, in particular, is not new.
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ On each iteration, it selects a set of transliteration candidates for each NE according to the current model (line 6).
The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al, 2006) and train a discriminative classifier. $$$$$ Named Entity recognition (NER) is an important part of many natural language processing tasks.

We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. $$$$$ Michael Collins and Yoram Singer.
We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. $$$$$ Unique words were grouped into 15,594 equivalence classes, and 1,605 of those classes were discarded using this method.
We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. $$$$$ A large amount of previous work exists on transliteration models.

Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We evaluate the algorithm on an English-Russian corpus, and show high level of NEs discovery in Russian.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ We thank Richard Sproat, ChengXiang Zhai, and Kevin Small for their useful feedback during this work, and the anonymous referees for their helpful comments.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ The ultimate goal of this work is to automatically tag NEs so that they can be used for training of an NER system for a new language.
Our initial feature extraction method follows the one presented in (Klementiev and Roth, 2006a), in which the feature space consists of n-gram pairs from the two languages. $$$$$ There has been other work to automatically discover NE with minimal supervision.
