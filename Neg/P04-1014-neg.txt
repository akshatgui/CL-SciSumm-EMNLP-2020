The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ The packed charts perform a number of roles: they are a compact representation of a very large number of CCG derivations; they allow recovery of the highest scoring parse or dependency structure without enumerating all derivations; and they represent an instance of what Miyao and Tsujii (2002) call a feature forest, which is used to efficiently estimate a log-linear model.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ We compare models use all including nonstandard derivations, with normal-form models.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ We also develop a new efficient parsing for maximises expected recall of dependencies.

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ Non-standard derivations are an integral part of the CCG formalism, and it is an interesting question whether efficient estimation and parsing algorithms can be defined for models which use all derivations.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ This paper significantly extends our earlier work in a number of ways.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ Figure 1 gives an algorithm for finding nodes in a packed chart which appear in correct derivations. cdeps(c) is the number of correct dependencies on conjunctive node c, and takes the value −1 if there are any incorrect dependencies on c. dmax(c) is the maximum number of correct dependencies produced by any sub-derivation headed by c, and takes the value −1 if there are no sub-derivations producing only correct dependencies. dmax(d) is the same value but for disjunctive node d. Recursive definitions for calculating these values are given in Figure 1; the base case occurs when conjunctive nodes have no disjunctive daughters.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The development of parsing and estimation algorithms for models which use all derivations extends existing CCG parsing techniques, and allows us to test whether there is useful information in the additional derivations.

 $$$$$ We use a technique from the numerical optimisation literature, the L-BFGS algorithm (Nocedal and Wright, 1999), to optimise the objective function.
 $$$$$ However, one of the disadvantages of the dependency model is that the estimation process is already using a large proportion of our existing resources, and extending the feature set will further increase the execution time and memory requirement of the estimation algorithm.
 $$$$$ Clark and Curran (2004) shows that the normal-form constraints significantly increase parsing speed and, in combination with adaptive supertagging, result in a highly efficient wide-coverage parser.
 $$$$$ The entries within spanning derivations form a feature forest (Miyao and Tsujii, 2002).

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ One motivation for using a log-linear model is that long-range dependencies which CCG was designed to handle can easily be encoded as features.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ We have answered this question, and in doing so developed a new parsing algorithm for CCG which maximises expected recall of dependencies.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ The results given so far have all used gold standard POS tags from CCGbank.

The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ Log-linear models have been applied to the parsing problem across a range of grammar formalisms, e.g.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The use of adaptive supertagging and the normal-form constraints result in a very efficient wide-coverage parser.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ There are a number of advantages to the normal-form model: it requires less space and time resources for estimation and it produces a faster parser.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ We would like to extend the dependency model, by including the local-rule dependencies which are used by the normal-form model, for example.

However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ The packed charts perform a number of roles: they are a compact representation of a very large number of CCG derivations; they allow recovery of the highest scoring parse or dependency structure without enumerating all derivations; and they represent an instance of what Miyao and Tsujii (2002) call a feature forest, which is used to efficiently estimate a log-linear model.
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ Our algorithm is based on Goodman’s (1996) labelled recall algorithm for the phrase-structure PARSEVAL measures.
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ For the dependency model we returned the dependency structure with the highest expected labelled recall score.

 $$$$$ There are a number of advantages to the normal-form model: it requires less space and time resources for estimation and it produces a faster parser.
 $$$$$ This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.
 $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
 $$$$$ We also develop a new efficient parsing for maximises expected recall of dependencies.

In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ Let E fi be the expected value of fi over the forest (D for model A; then the values in (5) can be obtained by calculating Ej  fi for the complete forest (Dj for each sentence Sj in the training data (the second sum in (5)), and also Ej fi for each forest Tj of correct derivations (the first sum in (5)): where log Z is the normalisation constant for (D.
In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ A feature forest (D is a tuple (C, D, R, y, 6) where: The individual entries in a cell are conjunctive nodes, and the equivalence classes of entries are dis2By rule instantiation we mean the local tree arising from the application of a CCG combinatory rule. junctive nodes.
In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ The performances of the two models are comparable and the results are competitive with ex

 $$$$$ The development of parsing and estimation algorithms for models which use all derivations extends existing CCG parsing techniques, and allows us to test whether there is useful information in the additional derivations.
 $$$$$ The dependency structures considered in this paper are described in detail in Clark et al. (2002) and Clark and Curran (2003).
 $$$$$ And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996).
 $$$$$ Non-standard derivations are an integral part of the CCG formalism, and it is an interesting question whether efficient estimation and parsing algorithms can be defined for models which use all derivations.

Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ Jason Baldridge, Frank Keller, Yuval Krymolowski and Miles Osborne provided useful feedback.
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ For the dependency model, we define the probability of a dependency structure as follows: where  is a dependency structure, S is a sentence and A() is the set of derivations which lead to .
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ Let E fi be the expected value of fi over the forest (D for model A; then the values in (5) can be obtained by calculating Ej  fi for the complete forest (Dj for each sentence Sj in the training data (the second sum in (5)), and also Ej fi for each forest Tj of correct derivations (the first sum in (5)): where log Z is the normalisation constant for (D.
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ For a labelled dependency to be correct, the first 4 elements of the dependency tuple must match exactly.

Clark and Curran (2004b) describes the CCG parser. $$$$$ There are a number of advantages to the normal-form model: it requires less space and time resources for estimation and it produces a faster parser.
Clark and Curran (2004b) describes the CCG parser. $$$$$ Our system demonstrates that accurate and efficient wide-coverage CCG parsing is feasible.

 $$$$$ And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996).
 $$$$$ Setting the gradient to zero yields the usual maximum entropy constraints (Berger et al., 1996), except that in this case the empirical values are themselves expectations (over all derivations leading to each gold standard dependency structure).
 $$$$$ For the normal-form model, the Viterbi algorithm is used to find the most probable derivation.
 $$$$$ Let L, be the number of correct dependencies in 7r with respect to a gold standard dependency structure G; then the dependency structure, 7rmax, which maximises the expected recall rate is: LP LR UP UR cat where S is the sentence for gold standard dependency structure G and i ranges over the dependency structures for S. This expression can be expanded further: The final score for a dependency structure  is a sum of the scores for each dependency  in ; and the score for a dependency  is the sum of the probabilities of those derivations producing .

 $$$$$ Finally, the oracle results suggest that further experimentation with the supertagger will significantly improve parsing accuracy, efficiency and robustness.
 $$$$$ Our algorithm is based on Goodman’s (1996) labelled recall algorithm for the phrase-structure PARSEVAL measures.
 $$$$$ The function fi is a feature of the parse which can be any real-valued function over the space of parses SZ.

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ We have also shown that a normal-form model performs as well as the dependency model.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ Section 6 explains how the gold standard structures are obtained.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.

We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ The packed charts perform a number of roles: they are a compact representation of a very large number of CCG derivations; they allow recovery of the highest scoring parse or dependency structure without enumerating all derivations; and they represent an instance of what Miyao and Tsujii (2002) call a feature forest, which is used to efficiently estimate a log-linear model.
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ We also develop a new efficient parsing for maximises expected recall of dependencies.
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ Our algorithm is based on Goodman’s (1996) labelled recall algorithm for the phrase-structure PARSEVAL measures.

 $$$$$ Additional generalised features for each feature type are formed by replacing words with their POS tags.
 $$$$$ Let E fi be the expected value of fi over the forest (D for model A; then the values in (5) can be obtained by calculating Ej  fi for the complete forest (Dj for each sentence Sj in the training data (the second sum in (5)), and also Ej fi for each forest Tj of correct derivations (the first sum in (5)): where log Z is the normalisation constant for (D.
 $$$$$ A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b).

We applied the same normal-form restrictions used in Clark and Curran (2004b) $$$$$ Table 2 gives the results for the normal-form model for various feature sets.
We applied the same normal-form restrictions used in Clark and Curran (2004b) $$$$$ We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling (GIS) to perform the estimation.
We applied the same normal-form restrictions used in Clark and Curran (2004b) $$$$$ Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C&C POS tagger (Curran and Clark, 2003).
We applied the same normal-form restrictions used in Clark and Curran (2004b) $$$$$ Let L, be the number of correct dependencies in 7r with respect to a gold standard dependency structure G; then the dependency structure, 7rmax, which maximises the expected recall rate is: LP LR UP UR cat where S is the sentence for gold standard dependency structure G and i ranges over the dependency structures for S. This expression can be expanded further: The final score for a dependency structure  is a sum of the scores for each dependency  in ; and the score for a dependency  is the sum of the probabilities of those derivations producing .

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ For the first set of experiments, we used a setting which assigns 1.7 categories on average per word.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ And finally, we show that the parsing algorithm described in Clark and Curran (2003) is extremely slow in some cases, and suggest an efficient alternative based on Goodman (1996).
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ They use a conditional model, based on Collins (1996), which, as the authors acknowledge, has a number of theoretical deficiencies; thus the results of Clark et al. provide a useful baseline for the new models presented here.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ Lexical category features are word–category pairs at the leaf nodes, and root features are headword–category pairs at the root nodes.

Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ We have answered this question, and in doing so developed a new parsing algorithm for CCG which maximises expected recall of dependencies.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ We compare models use all including nonstandard derivations, with normal-form models.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ Jason Baldridge, Frank Keller, Yuval Krymolowski and Miles Osborne provided useful feedback.

 $$$$$ The entries within spanning derivations form a feature forest (Miyao and Tsujii, 2002).
 $$$$$ We also compare log-linear models which use all CCG derivations, including non-standard derivations, with normal-form models.
 $$$$$ The parser uses a maximum entropy supertagger (Clark and Curran, 2004) to assign lexical categories to the words in a sentence, and applies the CKY chart parsing algorithm described in Steedman (2000).
