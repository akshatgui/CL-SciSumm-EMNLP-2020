The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ However, the introduction of extra derivations increases the complexity of the modelling and parsing problem.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Finally, the oracle results suggest that further experimentation with the supertagger will significantly improve parsing accuracy, efficiency and robustness.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ This extends the approach of Clark et al. (2002) who modelled the dependency structures directly, not using any information from the derivations.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Clark and Curran (2003) outlines an algorithm for finding the most probable dependency structure, which keeps track of the highest scoring set of dependencies for each node in the chart.

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ Setting the gradient to zero yields the usual maximum entropy constraints (Berger et al., 1996), except that in this case the empirical values are themselves expectations (over all derivations leading to each gold standard dependency structure).
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The idea behind a packed chart is simple: equivalent chart entries of the same type, in the same cell, are grouped together, and back pointers to the daughters indicate how an individual entry was created.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ L-BFGS over forests can be parallelised, using the method described in Clark and Curran (2003) to calculate the feature expectations.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ Let E fi be the expected value of fi over the forest (D for model A; then the values in (5) can be obtained by calculating Ej  fi for the complete forest (Dj for each sentence Sj in the training data (the second sum in (5)), and also Ej fi for each forest Tj of correct derivations (the first sum in (5)): where log Z is the normalisation constant for (D.

 $$$$$ We follow Riezler et al. (2002) in using a discriminative estimation method by maximising the conditional likelihood of the model given the data.
 $$$$$ The results show that our parser is performing significantly better than that of Clark et al., demonstrating the benefit of derivation features and the use of a sound statistical model.
 $$$$$ A dependency structure is a multiset of these dependencies.

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ The gold standard dependency structures are produced by running our CCG parser over the normal-form derivations in CCGbank (Hockenmaier, 2003a).
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ We compare models use all including nonstandard derivations, with normal-form models.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ Figure 1 gives an algorithm for finding nodes in a packed chart which appear in correct derivations. cdeps(c) is the number of correct dependencies on conjunctive node c, and takes the value −1 if there are any incorrect dependencies on c. dmax(c) is the maximum number of correct dependencies produced by any sub-derivation headed by c, and takes the value −1 if there are no sub-derivations producing only correct dependencies. dmax(d) is the same value but for disjunctive node d. Recursive definitions for calculating these values are given in Figure 1; the base case occurs when conjunctive nodes have no disjunctive daughters.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ As an alternative to GIS, we have implemented a parallel version of our L-BFGS code using the Message Passing Interface (MPI) standard.

The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ Riezler et al. (2002) and Toutanova et al.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ Dependency features are the 5-tuples defined in Section 1.
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The performances of the two models are comparable and the results are competitive with ex
The parsing results in Clark and Curran (2004b) rely on a super tagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ This very high score demonstrates the large amount of information in lexical categories.

However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ In this paper we use features from the dependency structure, and features defined on the local rule instantiations.2 Hence, any two entries with identical category type, identical head, and identical unfilled dependencies are equivalent.
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ In practice large numbers of such conjunctive nodes lead to very long parse times.
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
However, the scores in Clark and Curran (2004b) give an indication of how super tagging accuracy corresponds to overall dependency recovery. $$$$$ This paper describes and evaluates log-linear parsing models for Combinatory Categorial A parallel implementation of algorithm is described, which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation.

 $$$$$ Log-linear models have been applied to the parsing problem across a range of grammar formalisms, e.g.
 $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
 $$$$$ Future work will investigate extending the feature sets used by the log-linear models with the aim of further increasing parsing accuracy.

In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ There are a number of advantages to the normal-form model: it requires less space and time resources for estimation and it produces a faster parser.
In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
In order to access categorial and structural information, we used the C&C toolkit (Clark and Curran, 2004). $$$$$ The objective function and gradient vector for the normal-form model are as follows: where dj is the the gold standard derivation for sentence Sj and B(Sj) is the set of possible derivations for Sj.

 $$$$$ Our algorithm is based on Goodman’s (1996) labelled recall algorithm for the phrase-structure PARSEVAL measures.
 $$$$$ ZS is a normalising constant which ensures that P(|S) is a probability distribution: where (S) is the set of possible parses for S. For the dependency model a parse, , is a (d, ) pair (as given in (1)).

Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ More investigation is needed to find the optimal chart size for estimation, but the results show a gain in accuracy.
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ For local dependencies l is assigned a null value.
Previous discriminative models for CCG (Clark and Curran, 2004b) required cluster computing resources to train. $$$$$ We also compare log-linear models which use all CCG derivations, including non-standard derivations, with normal-form models.

Clark and Curran (2004b) describes the CCG parser. $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
Clark and Curran (2004b) describes the CCG parser. $$$$$ A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and used in parsers applied to the WSJ Penn Treebank (Clark et al., 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b).
Clark and Curran (2004b) describes the CCG parser. $$$$$ The results show that each additional feature type increases performance.
Clark and Curran (2004b) describes the CCG parser. $$$$$ The performances of the two models are comparable and the results are competitive with ex

 $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
 $$$$$ However, the introduction of extra derivations increases the complexity of the modelling and parsing problem.
 $$$$$ Following Clark et al. (2002), evaluation is by precision and recall over dependencies.

 $$$$$ Clark et al. (2002) handle the additional derivations by modelling the derived structure, in their case dependency structures.
 $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
 $$$$$ Dependency features are the 5-tuples defined in Section 1.
 $$$$$ Second, we find that GIS is unsuitable for estimating a model of the size being considered, and develop a parallel version of the L-BFGS algorithm (Nocedal and Wright, 1999).

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ A dependency structure is a multiset of these dependencies.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ Second, we find that GIS is unsuitable for estimating a model of the size being considered, and develop a parallel version of the L-BFGS algorithm (Nocedal and Wright, 1999).
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ For a set of equivalent entries in the chart (a disjunctive node), this involves summing over all conjunctive node daughters which head subderivations leading to the same set of high scoring dependencies.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of the BFGS training algorithm, to solve this problem. $$$$$ The first stage in parsing the test data is to apply the supertagger.

We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ The roots of the CCG derivations represent the root disjunctive nodes.3
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ More investigation is needed to properly compare our parser and Hockenmaier’s, since there are a number of differences in addition to the models used: Hockenmaier effectively reads a lexicalised PCFG off CCGbank, and is able to use all of the available training data; Hockenmaier does not use a supertagger, but does use a beam search.
We use the same feature representation (x, y) as in Clark and Curran (2004b), to allow comparison with the log-linear model. $$$$$ We have also shown that a normal-form model performs as well as the dependency model.

 $$$$$ More investigation is needed to properly compare our parser and Hockenmaier’s, since there are a number of differences in addition to the models used: Hockenmaier effectively reads a lexicalised PCFG off CCGbank, and is able to use all of the available training data; Hockenmaier does not use a supertagger, but does use a beam search.
 $$$$$ Clark and Curran (2003) outlines an algorithm for finding the most probable dependency structure, which keeps track of the highest scoring set of dependencies for each node in the chart.
 $$$$$ Future work will investigate extending the feature sets used by the log-linear models with the aim of further increasing parsing accuracy.
 $$$$$ However, estimating a log-linear model for a widecoverage CCG grammar is very computationally expensive.

We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. $$$$$ There is an additional rule feature type which also encodes the lexical head of the resulting category.
We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. $$$$$ Section 5 shows how the same technique can also be applied to all derivations leading to a gold standard dependency structure.
We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. $$$$$ The roots of the CCG derivations represent the root disjunctive nodes.3
We applied the same normal-form restrictions used in Clark and Curran (2004b): categories can only combine if they have been seen to combine in Sections 2-21 of CCGbank, and only if they do not violate the Eisner (1996a) normal-form constraints. $$$$$ In practice large numbers of such conjunctive nodes lead to very long parse times.

In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ We were able to parse 98.9% of section 23 using this strategy.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments, and Mark Steedman for guidance and advice.
In Clark and Curran (2004b) we use a cluster of 45 machines, together with a parallel implementation of BFGS, to solve this problem, but need up to 20 GB of RAM. $$$$$ These entries can be easily found by traversing the chart top-down, starting with the entries which span the sentence.

Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ A feature forest (D is a tuple (C, D, R, y, 6) where: The individual entries in a cell are conjunctive nodes, and the equivalence classes of entries are dis2By rule instantiation we mean the local tree arising from the application of a CCG combinatory rule. junctive nodes.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ Clark and Curran (2003) show how the sum over the complete derivation space can be performed efficiently using a packed chart and a variant of the inside-outside algorithm.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ The normal-form model requires only 5 machines for estimation, with an average memory usage of 730 MB for each machine.
Following Clark and Curran (2004b), accuracy is measured using F-score over the gold standard predicate-argument dependencies in CCG bank. $$$$$ A feature forest (D is a tuple (C, D, R, y, 6) where: The individual entries in a cell are conjunctive nodes, and the equivalence classes of entries are dis2By rule instantiation we mean the local tree arising from the application of a CCG combinatory rule. junctive nodes.

 $$$$$ For local dependencies l is assigned a null value.
 $$$$$ However, we found that GIS converged extremely slowly; this is in line with other recent results in the literature applying GIS to globally optimised models such as conditional random fields, e.g.
 $$$$$ This work was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
 $$$$$ Again there are 3 additional distance feature types, as above, which only include the head of the resulting category.
