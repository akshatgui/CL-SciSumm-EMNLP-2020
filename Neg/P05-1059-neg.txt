Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ The numbers of iterations for the training of the IBM models were chosen to be the turning points of AER changing on the cross-validation data.
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ The performance of the full model of unlexicalized ITG is compared with the pruned model of lexicalized ITG using more training data and evaluation data.
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ The comparison of results for ITG and LITG in Table 2 and the fact that AER began to rise after only one iteration of training seem to indicate that keeping few distinct lexical heads caused convergence on a suboptimal set of parameters, leading to a form of overfitting.

In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ In the Model 1 estimate of the outside probability, source and target words can align using any combination of points from the four outside corners of the tic-tac-toe pattern.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ Acknowledgments We are very grateful to Rebecca Hwa for assistance with the Chinese-English data, to Kevin Knight and Daniel Marcu for their feedback, and to the authors of GIZA.

The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ Now there are 4 forms of binary rules: determined by the four possible combinations of head selections (Y or Z) and orientation selections (straight or inverted).
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.

 $$$$$ We also want to point out that for a pair of long sentences, it would be hard to reflect the inherent bilingual syntactic structure using the lexicalized binary bracketing parse tree.
 $$$$$ The pruning and the limitations of the bracketing grammar may be the reasons that the result on sentences of up to 25 words on both sides is not better than that of the unlexicalized ITG.
 $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
 $$$$$ We presented the formal description of a Stochastic Lexicalized Inversion Transduction Grammar with its EM training procedure, and proposed specially designed pruning and smoothing techniques.

Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ The results from the second experiment are shown in Table 2.
Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ The space of alignments that is to be considered by LITG is exactly the space considered by ITG since the structural rules shared by them define the alignment space.
Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.

We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ Our pruned version of LITG controlled the running time for one iteration to be less than 1200 CPU hours, despite the fact that both the number of sentences and the average length of sentences were more than doubled.
We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ The experiments on a parallel corpus of Chinese and English showed that lexicalization helped for aligning sentences of up to 15 words on both sides.
We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.

In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages.
In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ We think the limitations of the bracketing grammar are another reason for not being able to improve the AER of longer sentence pairs after lexicalization.

Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ Mathematically, our figure of merit for the cell (l, m, i, j) is a product of the inside Model 1 probability and the outside Model 1 probability: alignments included in the figure of merit for bitext cell (l, m, i, j) (Equation 1); solid black cells show the Model 1 Viterbi alignment within the shaded area.
Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ The first step of lexicalization is to associate a lexical pair with each nonterminal.

Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages.
Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ However, whether or not the top-k lexical head pruning technique is equally safe remains a question.

For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ The experiments on a parallel corpus of Chinese and English showed that lexicalization helped for aligning sentences of up to 15 words on both sides.
For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ In our Chinese-English data, only one type of alignment was marked, meaning that GP = GS.

Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ However, whether or not the top-k lexical head pruning technique is equally safe remains a question.
Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ The number of CPU hours would increase to a point that is unacceptable if we doubled the average sentence length.
Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ The number of CPU hours would increase to a point that is unacceptable if we doubled the average sentence length.
Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ The experiments on a parallel corpus of Chinese and English showed that lexicalization helped for aligning sentences of up to 15 words on both sides.

Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ This probabilistic, syntax-based approach has inspired much subsequent reasearch.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ The head word pair generation rules are designed for this purpose: The word pair e/f is representative of the lexical content of X in the two languages.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ The algorithm has a complexity of O(N4s N4t ), where Ns and Nt are the lengths of source and target sentences respectively.

Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ To verify the safety of the tic-tac-toe pruning technique, we applied it to the unlexicalized ITG using the same beam ratio (10−5) and found that the AER on the test data was not changed.
Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ The comparison of results for ITG and LITG in Table 2 and the fact that AER began to rise after only one iteration of training seem to indicate that keeping few distinct lexical heads caused convergence on a suboptimal set of parameters, leading to a form of overfitting.
Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.
Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ Lexicalization introduces an additional factor of O(NsNt), caused by the choice of headwords e and f in the pseudocode.

Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ The experiments on a parallel corpus of Chinese and English showed that lexicalization helped for aligning sentences of up to 15 words on both sides.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.

The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ One noticeable implication of this technique for training is the reliance on initial probabilities of lexical pairs that are discriminative enough.
The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ The performance of the full model of unlexicalized ITG is compared with the pruned model of lexicalized ITG using more training data and evaluation data.
The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ The subset of up to 25 words in both languages was used for the same purpose in the second experiment.
The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ The pruning and the limitations of the bracketing grammar may be the reasons that the result on sentences of up to 25 words on both sides is not better than that of the unlexicalized ITG.

Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ Then, we recursively expand the lexicalized head constituents using the lexicalized structural rules.
