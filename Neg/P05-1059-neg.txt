Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ Acknowledgments We are very grateful to Rebecca Hwa for assistance with the Chinese-English data, to Kevin Knight and Daniel Marcu for their feedback, and to the authors of GIZA.
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ For scoring the Viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: where A is the set of word pairs aligned by the automatic system, GS is the set marked in the gold standard as “sure”, and GP is the set marked as “possible” (including the “sure” pairs).
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ We replaced words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words.
Zhang and Gildea (2005) [6] show that lexicalized ITGs can further improve alignment accuracy. $$$$$ The numbers of iterations for the training of the IBM models were chosen to be the turning points of AER changing on the cross-validation data.

In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ The Chinese data were automatically segmented into tokens, and English capitalization was retained.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ The resulting training corpus had 18,773 sentence pairs with a total of 276,113 Chinese words and 315,415 English words.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ In this experiment, we didn’t apply the pruning techniques for the lexicalized ITG.
In order to take into account competing hypotheses, we can use for our queue discipline not only the inside probability I (ak), but also the outside probability O (ak), the probability of generating all spans other than ak, as in A* search for CFGs (Klein and Manning, 2003), and tic-tac-toe pruning for word based ITGs (Zhang and Gildea, 2005). $$$$$ For scoring the Viterbi alignments of each system against goldstandard annotated alignments, we use the alignment error rate (AER) of Och and Ney (2000), which measures agreement at the level of pairs of words: where A is the set of word pairs aligned by the automatic system, GS is the set marked in the gold standard as “sure”, and GP is the set marked as “possible” (including the “sure” pairs).

The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ In our Chinese-English data, only one type of alignment was marked, meaning that GP = GS.
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ We need to further restrict the space of alignments spanned by the source and target strings to make the algorithm feasible.
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al, 1994) and the HMM based alignment model (Vogel et al, 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax-based approaches (Zhang and Gildea, 2005) for word alignment are also studied. $$$$$ Each of the O(n4) inductive steps takes one additive or multiplicative computation.

 $$$$$ The pruning and the limitations of the bracketing grammar may be the reasons that the result on sentences of up to 25 words on both sides is not better than that of the unlexicalized ITG.
 $$$$$ The space of alignments that is to be considered by LITG is exactly the space considered by ITG since the structural rules shared by them define the alignment space.
 $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.
 $$$$$ Let INS(l, m, i, j) denote the major factor of our Model 1 estimate of a cell’s inside probability, Ht∈(i,j) Es∈{0,(l,m)} t(ft  |es).

Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ The lexicalized ITG is designed to be more sensitive to the lexical influence on the choices of inversions so that it can find better alignments.
Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ The subset of up to 25 words in both languages was used for the same purpose in the second experiment.
Zhang and Gildea (2005) described a model in which the nonterminals are lexicalized by Englishand foreign language word pairs so that the inversions are dependent on lexical information on the left hand side of synchronous rules. $$$$$ Hence, it’s reasonable to see a better chance of improving the alignment result for sentences less than 16 words.

We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
We apply one of the pruning techniques used in Zhang and Gildea (2005). $$$$$ Most work on ITG has focused on the 2-normal form, which consists of unary production rules that are responsible for generating word pairs: and binary production rules in two forms that are responsible for generating syntactic subtree pairs: The rules with square brackets enclosing the right hand side expand the left hand side symbol into the two symbols on the right hand side in the same order in the two languages, whereas the rules with pointed brackets expand the left hand side symbol into the two right hand side symbols in reverse order in the two languages.

In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ For our results, we used the max version. with a cell of width one: Figure 3(b) and (c) illustrate the inductive computation indicated by the two equations.
In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ To verify the safety of the tic-tac-toe pruning technique, we applied it to the unlexicalized ITG using the same beam ratio (10−5) and found that the AER on the test data was not changed.
In the first comparison, we measured the performance of five word aligners, including IBM models, ITG, the lexical ITG (LITG) of Zhang and Gildea (2005), and our bi lexical ITG (BLITG), on a hand-aligned bilingual corpus. $$$$$ The comparison of results for ITG and LITG in Table 2 and the fact that AER began to rise after only one iteration of training seem to indicate that keeping few distinct lexical heads caused convergence on a suboptimal set of parameters, leading to a form of overfitting.

Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ In this experiment, we didn’t apply the pruning techniques for the lexicalized ITG.
Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.
Past approaches have pruned spans using IBM Model 1 probability estimates (Zhang and Gildea, 2005) or using agreement with an existing parse tree (Cherry and Lin, 2006). $$$$$ For instance, the probability of the parse in Figure 1 is: It is important to note that besides the bottomlevel word-pairing rules, the other rules are all nonlexical, which means the structural alignment component of the model is not sensitive to the lexical contents of subtrees.

Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ Some type of pruning is a must-have.
Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ We presented the formal description of a Stochastic Lexicalized Inversion Transduction Grammar with its EM training procedure, and proposed specially designed pruning and smoothing techniques.
Fortunately, exploiting the recursive nature of the cells, we can compute values for the inside and outside components of each cell using dynamic programming in O (n4) time (Zhang and Gildea, 2005). $$$$$ Acknowledgments We are very grateful to Rebecca Hwa for assistance with the Chinese-English data, to Kevin Knight and Daniel Marcu for their feedback, and to the authors of GIZA.

For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ We presented the formal description of a Stochastic Lexicalized Inversion Transduction Grammar with its EM training procedure, and proposed specially designed pruning and smoothing techniques.
For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.
For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ The pruning and the limitations of the bracketing grammar may be the reasons that the result on sentences of up to 25 words on both sides is not better than that of the unlexicalized ITG.
For example, Haghighi et al. (2009) do pruning based on the probabilities of links from a simpler alignment model (viz. HMM); Zhang and Gildea (2005) propose Tic-tac-toe pruning, which is based on the Model 1 probabilities of word pairs inside and outside a pair of spans. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.

Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ Each of the O(n4) inductive steps takes one additive or multiplicative computation.
Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
Zhang and Gildea (2005) show that Model 1 (Brown et al, 1993) probabilities of the word pairs inside and outside a span pair are useful. $$$$$ In our Chinese-English data, only one type of alignment was marked, meaning that GP = GS.

Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ One noticeable implication of this technique for training is the reliance on initial probabilities of lexical pairs that are discriminative enough.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ We replaced words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words.
Tic-tac-toe pruning algorithm (Zhang and Gildea, 2005) uses dynamic programming to compute inside and outside scores for a span pair in O (n4). $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.

Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ Then, we recursively expand the lexicalized head constituents using the lexicalized structural rules.
Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ In the Model 1 estimate of the outside probability, source and target words can align using any combination of points from the four outside corners of the tic-tac-toe pattern.
Like Zhang and Gildea (2005), it is used to prune bi text cells rather than score phrases. $$$$$ The comparison of results for ITG and LITG in Table 2 and the fact that AER began to rise after only one iteration of training seem to indicate that keeping few distinct lexical heads caused convergence on a suboptimal set of parameters, leading to a form of overfitting.

Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ Thus in Figure 3(a), there is one solid cell (corresponding to the Model 1 Viterbi alignment) in each column, falling either in the upper or lower outside shaded corner.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ This work was partially supported by NSF ITR IIS-09325646 and NSF ITR IIS-0428020.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ Acknowledgments We are very grateful to Rebecca Hwa for assistance with the Chinese-English data, to Kevin Knight and Daniel Marcu for their feedback, and to the authors of GIZA.
Our pruning differs from Zhang and Gildea (2005) in two major ways. $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.

The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ Acknowledgments We are very grateful to Rebecca Hwa for assistance with the Chinese-English data, to Kevin Knight and Daniel Marcu for their feedback, and to the authors of GIZA.
The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ We ran the experiments on sentences up to 25 words long in both languages.
The tic-tac-toe pruning algorithm (ZhangandGildea, 2005) uses dynamic programming to compute the product of inside and outside scores for all cells in O (n4) time. $$$$$ We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.

Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ Our pruned version of LITG controlled the running time for one iteration to be less than 1200 CPU hours, despite the fact that both the number of sentences and the average length of sentences were more than doubled.
Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ We presented the formal description of a Stochastic Lexicalized Inversion Transduction Grammar with its EM training procedure, and proposed specially designed pruning and smoothing techniques.
Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea (2005). $$$$$ We ran the experiments on sentences up to 25 words long in both languages.
