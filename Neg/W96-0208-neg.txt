This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). $$$$$ The ordering of rules employed in a decision list in order to simplify the representation and perform conflict resolution apparently gives it an advantage over other symbolic methods on this task.
This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). $$$$$ This research was partially supported by the National Science Foundation through grant IRI9310819.
This includes basic linguistic problems such as morphological analysis (van den Bosch et al., 1996), parsing (Zelle and Mooney, 1996), word sense disambiguation (Mooney, 1996), and anaphora resolution (Aone and Bennett, 1996). $$$$$ I would also like to thank Goeff Towell for providing access to the &quot;line&quot; corpus.

Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. $$$$$ The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this obdifference.
Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. $$$$$ Nearest neighbor bases its classifications on all features; however, it weights them all equally.
Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. $$$$$ The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.
Mooney (1996) argues that Naive Bayes classification and perceptron classifiers are particularly fit for lexical sample word sense disambiguation problems, because they combine weighted evidence from all features rather than select a subset of features for early discrimination. $$$$$ The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.

While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. $$$$$ The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. $$$$$ The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.
While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. $$$$$ A recent special issue of the Machine Learning journal on &quot;Bias Evaluation and Selection&quot; introduced by Gordon and desJardins (1995) presents current research in this general area.

Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). $$$$$ The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques.
Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). $$$$$ We also tested C4.5-RULES, a variant of C4.5 in which decision trees are translated into rules and pruned; however, its performance was slightly inferior to the base C4.5 system on the &quot;line&quot; corpus; therefore, its results are not included.
Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). $$$$$ Therefore, our results indicate that lexical disambiguation is perhaps best performed using methods that combine weighted evidence from all of the features rather tures actually present in the examples.
Decision trees have been used in supervised learning approaches to word sense disambiguation, and have fared well in a number of comparative studies (e.g., (Mooney, 1996), (Pedersen and Bruce, 1997)). $$$$$ This requires actually knowing the conditional probability of each category given each of the exponentially large number of possible instance descriptions. curves generally follow a power law where predictive accuracy climbs fairly rapidly and then levels off at an asymptotic level.

Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. $$$$$ We also discuss the role of in machine learning and its importance in explaining performance differences observed on specific problems.
Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. $$$$$ The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.
Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. $$$$$ The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.
Bag of words feature sets made up of unigrams have had a long history of success in text classification and word sense disambiguation (Mooney, 1996), and we believe that despite creating quite a bit of noise can provide useful information for discrimination. $$$$$ I would also like to thank Goeff Towell for providing access to the &quot;line&quot; corpus.

Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. $$$$$ The worst in this regard are PFOIL-DNF and PFoiLCNF which have a worst-case complexity of 0(n2) (Mooney, 1995).
Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. $$$$$ This research was partially supported by the National Science Foundation through grant IRI9310819.
Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. $$$$$ A simple approach is to automate the selection of a method using internal cross-validation (Schaffer, 1993).
Mooney (Mooney, 1996) has discussed the effect of bias on inductive learning methods. $$$$$ This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.

Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. $$$$$ Not surprisingly, there is a trade-off between training time and testing time, the symbolic methods spend more effort during training compressing the representation of the learned concept resulting in a simpler description that is quicker to test.
Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. $$$$$ Among the other methods tested, decision lists seem to perform the best.
Naive Bayes models (e.g., Mooney (1996), Chodorow et al (1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks. $$$$$ In addition to the results reported by Yarowsky (1994) and Mooney and Calif (1995), it provides evidence for the utility of this representation for natural-language problems.

Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. $$$$$ This research was partially supported by the National Science Foundation through grant IRI9310819.
Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. $$$$$ This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.
Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. $$$$$ The specific problem tested involves disambiguating six senses of the word &quot;line&quot; using the words in the current and proceeding sentence as context.

They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). $$$$$ This research was partially supported by the National Science Foundation through grant IRI9310819.
They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). $$$$$ Unfortunately, there have been very few direct comparisons of alternative methods on identical test data.
They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). $$$$$ Alternative encodings which exploit positional information, syntactic word tags, syntactic parse trees, semantic information, etc. should be tested to determine the utility of more sophisticated representations.
They have a long history of use in word sense disambiguation, dating back to early work by (Black, 1988), and have fared well in comparative studies such as (Mooney,1996) and (Pedersen and Bruce, 1997). $$$$$ This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context.

Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. $$$$$ We also discuss the role of in machine learning and its importance in explaining performance differences observed on specific problems.
Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. $$$$$ In addition to measuring generalization accuracy, we also collected data on the CPU time taken to train and test each method for each trainingset size measured on the learning curve.
Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by naive Bayes models (e.g., Mooney (1996), Chodorowetal. $$$$$ I would also like to thank Goeff Towell for providing access to the &quot;line&quot; corpus.

Some clusters of studies have used common test suites, most notably the 2094-word Hne data of Leacock et al (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). $$$$$ Except for C4.5, which uses the C code provided by QuinIan (1993), all of these methods are implemented in Common Lisp and available on-line at http://www.cs.utexas.edu/users/ml/ml-progs.html.

Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996). $$$$$ After training on 1,200 examples, the symbolic structures learned for the line corpus are relatively large.
Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996). $$$$$ The worst in this regard are PFOIL-DNF and PFoiLCNF which have a worst-case complexity of 0(n2) (Mooney, 1995).
Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al, 1985, Gallant, 1991, Leacock et al, 1993, and Mooney, 1996). $$$$$ I would also like to thank Goeff Towell for providing access to the &quot;line&quot; corpus.
