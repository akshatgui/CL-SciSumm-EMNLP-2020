Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). $$$$$ In this work we explore the use of Amazon Mechanical Turk1 (AMT) to determine whether nonexpert labelers can provide reliable natural language annotations.
Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.

Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. $$$$$ We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.
Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. $$$$$ The lowest line is for the naive 50% majority voting rule.

Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web.
Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ AMT also allows a requester to restrict which workers are allowed to annotate a task by requiring that all workers have a particular set of qualifications, such as sufficient accuracy on a small test set or a minimum percentage of previously accepted submissions.
Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.

Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ While an expert ITA of 0.77 was reported for the more general task involving all fourteen labels on both noun and verb events, no expert ITA numbers have been reported for this simplified temporal ordering task.
Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ As in Table 2 we calculate the minimum number of non-expert annotations per example k required on average to achieve similar performance to the expert annotations; surprisingly we find that for five of the seven tasks, the average system trained with a single set of non-expert annotations outperforms the average system trained with the labels from a single expert.
Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ Here Time is given as the total amount of time in hours elapsed from submitting the group of HITs to AMT until the last assignment is submitted by the last worker.
Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ Intuitively, workers who are more than 50% accurate have positive votes; workers whose judgments are pure noise have zero votes; and anticorrelated workers have negative votes.

 $$$$$ Numerous expert and non-expert studies have shown that this task typically yields very high interannotator agreement as measured by Pearson correlation; (Miller and Charles, 1991) found a 0.97 correlation of the annotations of 38 subjects with the annotations given by 51 subjects in (Rubenstein and Goodenough, 1965), and a following study (Resnik, 1999) with 10 subjects found a 0.958 correlation with (Miller and Charles, 1991).
 $$$$$ One possible hypothesis for the cause of this non-intuitive result is that individual labelers (including experts) tend to have a strong bias, and since multiple non-expert labelers may contribute to a single set of non-expert annotations, the annotator diversity within the single set of labels may have the effect of reducing annotator bias and thus increasing system performance.
 $$$$$ The tasks are: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.
 $$$$$ Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations.

Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ In our next comparison we ask how many averaged non-experts it would take to rival the performance of a single expert.
Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ In fact, further analysis reveals that there was only a single disagreement between the averaged non-expert vote and the gold standard; on inspection it was observed that the annotators voted strongly against the original gold label (9-to-1 against), and that it was in fact found to be an error in the original gold standard annotation.6 After correcting this error, the non-expert accuracy rate is 100% on the 177 examples in this task.
Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ For each task, we used AMT to annotate data and measured the quality of the annotations by comparing them with the gold standard (expert) labels on the same data.
Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful discussions, and for the generous support of Dolores Labs.

Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful discussions, and for the generous support of Dolores Labs.
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data.
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ Another method is to use Amazon’s compensation mechanisms to give monetary bonuses to highlyperforming workers and deny payments to unreliable ones; this is useful, but beyond the scope of this paper.

MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful discussions, and for the generous support of Dolores Labs.
MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ Here we take an approach based on gold standard labels, using a small amount of expert-labeled training data in order to correct for the individual biases of different non-expert annotators.
MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ In this section we describe Amazon Mechanical Turk and the general design of our experiments.

 $$$$$ Our evaluation of non-expert labeler data vs. expert annotations for five tasks found that for many tasks only a small number of nonexpert annotations per item are necessary to equal the performance of an expert annotator.
 $$$$$ Sorokin and Forsyth (2008) collect data for machine vision tasks and report speed and costs similar to our findings; their summaries of worker behavior also corroborate with what we have found.
 $$$$$ This experiment is based on the affective text annotation task proposed in Strapparava and Mihalcea (2007), wherein each annotator is presented with a list of short headlines, and is asked to give numeric judgments in the interval [0,100] rating the headline for six emotions: anger, disgust, fear, joy, sadness, and surprise, and a single numeric rating in the interval [-100,100] to denote the overall positive or negative valence of the emotional content of the headline, as in this sample headline-annotation pair: For our experiment we select a 100-headline sample from the original SemEval test set, and collect 10 affect annotations for each of the seven label types, for a total of 7000 affect labels.
 $$$$$ Kaisser et al. (2008) examines the task of customizing the summary length of QA output; non-experts from AMT chose a summary length that suited their information needs for varying query types.

This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ In fact, further analysis reveals that there was only a single disagreement between the averaged non-expert vote and the gold standard; on inspection it was observed that the annotators voted strongly against the original gold label (9-to-1 against), and that it was in fact found to be an error in the original gold standard annotation.6 After correcting this error, the non-expert accuracy rate is 100% on the 177 examples in this task.
This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.
This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.
This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ These word pairs range from highly similar (e.g., {boy, lad}), to unrelated (e.g., {noon, string}).

In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ We collect 10 annotations for each of 100 RTE sentence pairs; as displayed in Figure 3, we achieve a maximum accuracy of 89.7%, averaging over the annotations of 10 workers5.
In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ : To infer the posterior probability of the true label for a new example, worker judgments are integrated via Bayes rule, yielding the posteri The worker response likelihoods P(ywJx = Y ) and P(ywJx = N) can be directly estimated from frequencies of worker performance on gold standard examples.
In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ They introduce an EM algorithm to simultaneously estimate annotator biases and latent label classes.
In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ The most surprising aspect of this study was the speed with which it was completed; the task of 300 annotations was completed by 10 annotators in less than 11 min4(Miller and Charles, 1991) and others originally used a numerical score of [0,4]. utes from the time of submission of our task to AMT, at the rate of 1724 annotations / hour.

Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ A Requester can create a group of Human Intelligence Tasks (or HITs), each of which is a form composed of an arbitrary number of questions.
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ A Requester can create a group of Human Intelligence Tasks (or HITs), each of which is a form composed of an arbitrary number of questions.
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ In addition to the single meta-labeler, we ask: what is the minimum number of non-expert annotations k from which we can create a meta-labeler that has equal or better ITA than an expert annotator?

Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.
Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ These Amazon accounts are anonymous, but are referenced by a unique Amazon ID.
Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ Wiebe et al. (1999) analyze linguistic annotator agreement statistics to find bias, and use a similar model to correct labels.
Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ Every example i has a true label xi.

Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. $$$$$ For all five, we show high agreement between Mechanical Turk non-expert annotations and existing gold standard labels provided by expert labelers.
Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. $$$$$ The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost.
Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. $$$$$ 2 4 6 8 10 annotations This task is inspired by the TimeBank corpus (Pustejovsky et al., 2003), which includes among its annotations a label for event-pairs that represents the temporal relation between them, from a set of fourteen relations (before, after, during, includes, etc.).
Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. $$$$$ (The other categorical task, word sense disambiguation, could not be improved because it already had maximum accuracy.)

It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ As shown in Figure 5, performing simple majority voting (with random tie-breaking) over annotators results in a rapid accuracy plateau at a very high rate of 0.994 accuracy.
It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ Our first comparison showed that individual experts were better than individual non-experts.
It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful discussions, and for the generous support of Dolores Labs.
It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ Here Time is given as the total amount of time in hours elapsed from submitting the group of HITs to AMT until the last assignment is submitted by the last worker.

Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ Finally, we demonstrate significant improvement by controlling for labeler bias.
Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ The reliability of individual workers varies.
Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ Sheng et al. (2008) explore several methods for using many noisy labels to create labeled data, how to choose which examples should get more labels, and how to include labels’ uncertainty information when training classifiers.
Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web.

Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.
Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful discussions, and for the generous support of Dolores Labs.

We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. $$$$$ Our evaluation of non-expert labeler data vs. expert annotations for five tasks found that for many tasks only a small number of nonexpert annotations per item are necessary to equal the performance of an expert annotator.
We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. $$$$$ The results in Table 1 conform to the expectation that experts are better labelers: experts agree with experts more than non-experts agree with experts, although the ITAs are in many cases quite close.
We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. $$$$$ The design of the system is as follows: one is required to have an Amazon account to either submit tasks for annotations or to annotate submitted tasks.
We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. $$$$$ Since expert ITA was not reported per word on this dataset, we compare instead to the performance of the best automatic system performance for disambiguating “president” in SemEval Task 17 (Cai et al., 2007), with an accuracy of 0.98.

We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. $$$$$ RTE has an average +4.0% acWhere JHJ is simply the number of tokens in headline H, ignoring tokens not observed in the training set.
We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. $$$$$ We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.
We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. $$$$$ For every task we collect ten independent annotations for each unique item; this redundancy allows us to perform an in-depth study of how data quality improves with the number of independent annotations.

Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%. $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.
Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%. $$$$$ Since we are fortunate to have the six separate expert annotations in this task, we can perform an extended systematic comparison of the performance of the classifier trained with expert vs. non-expert data.
