Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). $$$$$ Albert and Dodd (2004) review several related models, but argue they have various shortcomings and emphasize instead the importance of having a gold standard.
Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). $$$$$ We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.
Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). $$$$$ : annotators annotators numbers of annotators.
Previous work has shown that data collected through the Mechanical Turk service is reliable and comparable in quality with trusted sources (Snow et al, 2008). $$$$$ For example, in a dialogue about a plane explosion, we have the utterance: “It just blew up in the air, and then we saw two fireballs go down to the, to the water, and there was a big small, ah, smoke, from ah, coming up from that”.

Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.
Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. $$$$$ We propose a technique for bias correction that significantly improves annotation quality on two tasks.
Previous studies using this on line task marketplace have shown that the collective judgments of many workers are comparable to those of trained annotators on labeling tasks (Snow et al, 2008) although these judgments can be obtained at a fraction of the cost and effort. $$$$$ We investigate five tasks: affect recognition, word similarity, recognizing textual entailment, event temporal ordering, and word sense disambiguation.

Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ The user requesting annotations for the group of HITs can specify the number of unique annotations per HIT they are willing to pay for, as well as the reward payment for each individual HIT.
Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ Furthermore, rather than marking both nouns and verbs in the text as possible events, we only consider possible verb events.
Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ RTE has an average +4.0% acWhere JHJ is simply the number of tokens in headline H, ignoring tokens not observed in the training set.
Snow et al (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. $$$$$ For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.

Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ Since the performance of many natural language processing tasks is limited by the amount and quality of data available to them (Banko and Brill, 2001), one promising alternative for some tasks is the collection of non-expert annotations.
Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.
Human judgments for the triples were collected through the Amazon Mechanical Turk (AMT) crowd sourcing platform (Snow et al, 2008). $$$$$ A large literature in biostatistics addresses this same problem for medical diagnosis.

 $$$$$ For each expert annotator we train a system using only the judgments provided by that annotator, and then create a gold standard test set using the average of the responses of the remaining five labelers on that set.
 $$$$$ We implement temporal ordering as a simplified version of the TimeBank event temporal annotation task: rather than annotating all fourteen event types, we restrict our consideration to the two simplest labels: “strictly before” and “strictly after”.
 $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.
 $$$$$ Here Time is given as the total amount of time in hours elapsed from submitting the group of HITs to AMT until the last assignment is submitted by the last worker.

Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ Finally, we demonstrate significant improvement by controlling for labeler bias.
Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.
Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ We find a +3.4% gain on event annotation.
Mechanical Turk has also been used to create labeled data sets for word sense disambiguation (Snow et al, 2008) and even to modify sense inventories. $$$$$ Here Time is given as the total amount of time in hours elapsed from submitting the group of HITs to AMT until the last assignment is submitted by the last worker.

Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ Figure 6 shows accuracy rates for individual workers on one task.
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ Intuitively, workers who are more than 50% accurate have positive votes; workers whose judgments are pure noise have zero votes; and anticorrelated workers have negative votes.
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.
Along this spirit, Snow et al (2008) show that obtaining multiple low quality labels (through Mechanical Turk) can approach high-quality editorial labels. $$$$$ We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.

MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ First we took a sample of annotations giving k responses per example.
MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ In Figure 1 we plot the expert ITA correlation as the horizontal dashed line.
MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ Luis von Ahn pioneered the collection of data via online annotation tasks in the form of games, including the ESPGame for labeling images (von Ahn and Dabbish, 2004) and Verbosity for annotating word relations (von Ahn et al., 2006).
MTurk is becoming a popular means of eliciting and collecting linguistic intuitions for NLP research; see Snow et al (2008) for an overview and a further discussion. $$$$$ We chose five natural language understanding tasks that we felt would be sufficiently natural and learnable for non-experts, and for which we had gold standard labels from expert labelers, as well as (in some cases) expert labeler agreement information.

 $$$$$ In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data.
 $$$$$ RTE has an average +4.0% acWhere JHJ is simply the number of tokens in headline H, ignoring tokens not observed in the training set.
 $$$$$ These Amazon accounts are anonymous, but are referenced by a unique Amazon ID.

This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ 2 4 6 8 10 annotators In Table 3 we give a summary of the costs associated with obtaining the non-expert annotations for each of our 5 tasks.
This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ In (Su et al., 2007) workers provided annotations for the tasks of hotel name entity resolution and attribute extraction of age, product brand, and product model, and were found to have high accuracy compared to gold-standard labels.
This might suggest that a further increase in the number of HIT assignments would outperform expert ITA scores, as was previously reported in (Snow et al, 2008). $$$$$ We conclude that many large labeling tasks can be effectively designed and carried out in this method at a fraction of the usual expense.

In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ We explore the use of Amazon’s Mechanical Turk system, a significantly cheaper and faster method for collecting annotations from a broad base of paid non-expert contributors over the Web.
In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ Our evaluation of non-expert labeler data vs. expert annotations for five tasks found that for many tasks only a small number of nonexpert annotations per item are necessary to equal the performance of an expert annotator.
In this paper, we address these issues by using Amazon's Mechanical Turk (MTurk), online non-expert annotators (Snow et al, 2008). $$$$$ (This is equivalent to the model under uniform priors and equal accuracies across workers and labels.)

Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful discussions, and for the generous support of Dolores Labs.
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ For each expert labeler, we computed this ITA score of the expert against the other five; we then average these ITA scores across all expert annotators to compute the average expert ITA (reported in Table 1 as “E vs. E”.
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ The lowest line is for the naive 50% majority voting rule.
Recently, however, attempts have been made to leverage non-expert annotations provided by Amazon's Mechanical Turk (MTurk) service to create large training corpora at a fraction of the usual costs (Snow et al 2008). $$$$$ Finally, after each HIT has been annotated, the Requester has the option of approving the work and optionally giving a bonus to individual workers.

Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.
Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ Figure 6 shows accuracy rates for individual workers on one task.
Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ As in Table 2 we calculate the minimum number of non-expert annotations per example k required on average to achieve similar performance to the expert annotations; surprisingly we find that for five of the seven tasks, the average system trained with a single set of non-expert annotations outperforms the average system trained with the labels from a single expert.
Commonly, MTurk has been used for the classification task (Snow et al. 2008) or for straightforward data entry. $$$$$ AMT also allows a requester to restrict which workers are allowed to annotate a task by requiring that all workers have a particular set of qualifications, such as sufficient accuracy on a small test set or a minimum percentage of previously accepted submissions.

Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. $$$$$ The construction of these datasets, however, is extremely expensive in both annotator-hours and financial cost.
Snow et al (2008) have validated AMT as a valid data source by comparing non-expert with gold-standard expert judgments. $$$$$ Here Time is given as the total amount of time in hours elapsed from submitting the group of HITs to AMT until the last assignment is submitted by the last worker.

It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ In addition to the single meta-labeler, we ask: what is the minimum number of non-expert annotations k from which we can create a meta-labeler that has equal or better ITA than an expert annotator?
It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ 2 4 6 8 10 annotators In Table 3 we give a summary of the costs associated with obtaining the non-expert annotations for each of our 5 tasks.
It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ In (Nakov, 2008) workers generated paraphrases of 250 noun-noun compounds which were then used as the gold standard dataset for evaluating an automatic method of noun compound paraphrasing.
It is common to do such filtering when using crowd sourced data by using the majority or median vote as the final judgment or to calibrate judges using expert judgments (Snow et al 2008). $$$$$ In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.

Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ While an expert ITA of 0.77 was reported for the more general task involving all fourteen labels on both noun and verb events, no expert ITA numbers have been reported for this simplified temporal ordering task.
Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ While an expert ITA of 0.77 was reported for the more general task involving all fourteen labels on both noun and verb events, no expert ITA numbers have been reported for this simplified temporal ordering task.
Most of our measures were scalar; we chose to do this because previous work on estimating the relationship between MTurk annotations and expert an notations suggest that taking the means of scalar annotations could be a good way to reduce noise in MTurk annotations (Snow et al, 2008). $$$$$ First we took a sample of annotations giving k responses per example.

Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ In this way we create six independent expert-trained systems and compute the average across their performance, calculated as Pearson correlation to the gold standard; this is reported in the “1-Expert” column of Table 4.
Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ Dawid and Skene (1979) are the first to consider the case of having multiple annotators per example but unknown true labels.
Recently, AMT has been shown to be an effective tool for annotation and evalatuation in NLP tasks ranging from word similarity detection and emotion detection (Snow et al, 2008) to Machine Translation quality evaluation (Callison-Burch, 2009). $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.

We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. $$$$$ 2 4 6 8 10 annotations This task is inspired by the TimeBank corpus (Pustejovsky et al., 2003), which includes among its annotations a label for event-pairs that represents the temporal relation between them, from a set of fourteen relations (before, after, during, includes, etc.).
We intend to experiment with different guidelines and instructions, and to screen (Callison Burch, 2009) and weight Turkers' responses (Snow et al, 2008), in order to lower the number of Turkers required for this task. $$$$$ For the task of affect recognition, we also show that using non-expert labels for training machine learning algorithms can be as effective as using gold standard annotations from experts.

We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. $$$$$ We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.
We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. $$$$$ We then performed two comparisons to evaluate the quality of the AMT annotations.
We have used the dataset created by Snow et al (2008) for the task of recognising textual entailment, originally proposed by Dagan et al (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. $$$$$ The idea of collecting annotations from volunteer contributors has been used for a variety of tasks.

Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%. $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.
Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%. $$$$$ We use 100 headlines as a training set (examples 500-599 from the test set of SemEval Task 14), and we use the remaining 900 headlines as our test set.
Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%. $$$$$ : To infer the posterior probability of the true label for a new example, worker judgments are integrated via Bayes rule, yielding the posteri The worker response likelihoods P(ywJx = Y ) and P(ywJx = N) can be directly estimated from frequencies of worker performance on gold standard examples.
Snow et al (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89.7%. $$$$$ This work was supported in part by the Disruptive Technology Office (DTO)’s Advanced Question Answering for Intelligence (AQUAINT) Phase III Program.
