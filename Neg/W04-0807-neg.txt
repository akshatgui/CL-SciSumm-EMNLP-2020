This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.
This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.
This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.

In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ Without them, all these comparative analyses would not be possible.
In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.
In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.
In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ With an average of five senses per word, the average value for the agreement by chance is measured at 0.20, resulting in a micro- statistic of 0.58.

The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ The main changes in this year’s evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth).
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ The selection is made using checkboxes, which list all possible senses of the current target word, plus two additional choices, “unclear” and “none of the above.” Although users are encouraged to select only one meaning per word, the selection of two or more senses is also possible.
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.

At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ The precision/recall baseline obtained for this task under the “most frequent sense” heuristic is 55.2% (fine grained) and 64.5% (coarse grained).
At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise.
At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ The inter-tagger agreement obtained so far is closely comparable to the agreement figures previously reported in the literature.
At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.

We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.
We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ 27 teams participated in this word sense disambiguation task.

 $$$$$ For macro- estimations, we assume that word senses follow the distribution observed in the OMWE annotated data, and under this assumption, the macro- is 0.35.
 $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.

S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ The English lexical sample task in SENSEVAL3 featured English ambiguous words that were to be tagged with their most appropriate WordNet or Wordsmyth sense.
S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ Kilgarriff (2002) mentions that for the SENSEVAL-2 nouns and adjectives there was a 66.5% agreement between the first two taggings (taken in order of submission) entered for each item.
S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ Tables 2 and 3 show all the submissions for each team, gives a brief description of their approaches, and lists the precision and recall obtained by each system under fine and coarse grained evaluations.

Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ The precision/recall baseline obtained for this task under the “most frequent sense” heuristic is 55.2% (fine grained) and 64.5% (coarse grained).
Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.

In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.
In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.

We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.
We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ The data set used for the SENSEVAL-3 English lexical sample task consists of examples extracted from the British National Corpus (BNC).
We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ This results in an overall agreement of about 67.3% which is reasonable and closely comparable with previous figures.
We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.

Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ The tagging exercise proceeds as follows.

These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ We are particularly grateful to the National Science Foundation for their support under research grant IIS-0336793, and to the University of North Texas for a research grant that provided funding for contributor prizes.
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ For each target word the system extracts a set of sentences from a large textual corpus.
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.

WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ For macro- estimations, we assume that word senses follow the distribution observed in the OMWE annotated data, and under this assumption, the macro- is 0.35.
WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ Without them, all these comparative analyses would not be possible.
WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ 27 teams participated in this word sense disambiguation task.

The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.
The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers.
The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.
The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.

Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise.
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ The inter-tagger agreement obtained so far is closely comparable to the agreement figures previously reported in the literature.
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ The main reason motivating selection of a different sense inventory is the weak verb performance of systems participating in the English lexical sample in SENSEVAL-2, which may be due to the high number of senses defined for verbs in the WordNet sense inventory.
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers.

Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ This task is a follow-up to similar tasks organized during the SENSEVAL-1 (Kilgarriff and Palmer, 2000) and SENSEVAL-2 (Preiss and Yarowsky, 2001) evaluations.
Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ With an average of five senses per word, the average value for the agreement by chance is measured at 0.20, resulting in a micro- statistic of 0.58.
Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.

In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). $$$$$ In particular, we are grateful to Gwen Lenker – our most productive contributor.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). $$$$$ There were no restrictions placed on the number of submissions each team could make.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets: the NLM-WSD corpus (Weeber et al, 2001) which Stevenson et al (2008a) used for their experiments, the Senseval-3 lexical sample task (Mihalcea et al, 2004) and the coarse grained version of the SemEval English lexical sample task (Pradhan et al, 2007). $$$$$ The precision/recall baseline obtained for this task under the “most frequent sense” heuristic is 55.2% (fine grained) and 64.5% (coarse grained).

In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ In particular, we are grateful to Gwen Lenker – our most productive contributor.
In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.
In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ There were no restrictions placed on the number of submissions each team could make.

For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.

Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers.
Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ The objective of this task was to: (1) Determine feasibility of reliably finding the English lexical sample Word Sense Disambiguation task.
Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.
Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.
