This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ We are indebted to the Princeton WordNet team, for making WordNet available free of charge, and to Robert Parks from Wordsmyth, for making available the verb entries used in this evaluation.
This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ In addition to raw inter-tagger agreement, the kappa statistic, which removes from the agreement rate the amount of agreement that is expected by chance(Carletta, 1996), was also determined.
This was mainly because of their attested strength at earlier Senseval evaluations (Edmonds et al 2002, Mihalcea et al 2004) and mutual complementarity discovered by us (Saarikoski et al., 2007). $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.

In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ The results of 47 systems that participated in this event tentatively suggest that supervised machine learning techniques can significantly improve over the most frequent sense baseline, and also that it is possible to design unsupervised techniques for reliable word sense disambiguation.
In addition, it has been Senseval practice (Edmonds et al 2002, Mihalcea et al 2004) that words with great number of test instances tend to have an equally great number of training instances. $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.

The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ Without them, all these comparative analyses would not be possible.
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ The objective of this task was to: (1) Determine feasibility of reliably finding the English lexical sample Word Sense Disambiguation task.
The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al, 2004) (hereafter SE-3) and SemCor (Miller et al, 1993). $$$$$ The precision/recall baseline obtained for this task under the “most frequent sense” heuristic is 55.2% (fine grained) and 64.5% (coarse grained).

At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.
At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ The main reason motivating selection of a different sense inventory is the weak verb performance of systems participating in the English lexical sample in SENSEVAL-2, which may be due to the high number of senses defined for verbs in the WordNet sense inventory.
At Senseval-3 (Mihalcea et al, 2004) the top systems were considered to have reached a ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained. $$$$$ A total number of 47 submissions were received for this task.

We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ The tagging exercise proceeds as follows.
We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise.
We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
We employ supervised WSD systems ,since Senseval results have amply demonstrated that supervised models significantly outperform unsupervised approaches (see for instance the English lexical sample tasks results described by Mihalcea et al (2004)). $$$$$ By choosing a different set of senses, we hope to gain insight into the dependence of difficulty of the sense disambiguation task on sense inventories.

 $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.
 $$$$$ Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.
 $$$$$ The precision/recall baseline obtained for this task under the “most frequent sense” heuristic is 55.2% (fine grained) and 64.5% (coarse grained).
 $$$$$ Without them, all these comparative analyses would not be possible.

S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ With an average of five senses per word, the average value for the agreement by chance is measured at 0.20, resulting in a micro- statistic of 0.58.
S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise.
S3LS-best stands for the the winner of S3LS (Mihalcea et al, 2004), which is 8.3 points over our method. $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.

Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ Corresponding team and reference to system description (in this volume) are indicated for the first system for each team. appropriate sense for words with various degrees of polysemy, using different sense inventories; and (2) Determine the usefulness of sense annotated data collected over the Web (as opposed to other traditional approaches for building semantically annotated corpora).
Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers.
Results from the last edition of the Senseval competition (Mihalcea et al, 2004) have shown that, for supervised learning, the best accuracies are obtained with a combination of various types of features, together with traditional machine learning algorithms based on feature-value vectors, such as Support Vector Machines (SVMs) and Naive Bayes. $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.

In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ The tagging exercise proceeds as follows.
In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ In particular, we are grateful to Gwen Lenker – our most productive contributor.
In this paper we use an automatic method to map the induced senses to WordNet using hand-tagged corpora, enabling the automatic evaluation against available gold standards (Senseval 3 English Lexical Sample S3LS (Mihalcea et al, 2004)) and the automatic optimization of the free parameters of the method. $$$$$ We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.

We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.
We include three supervised systems, the winner of S3LS (Mihalcea et al, 2004), an in-house system (kn N-all, CITATION OMITTED) which uses optimized kn N, and the same in-house system restricted to bag-of-words features only (kn N-bow) ,i.e. discarding other local features like bi grams or trigrams (which is what most unsupervised systems do). $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.

Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Tables 2 and 3 show all the submissions for each team, gives a brief description of their approaches, and lists the precision and recall obtained by each system under fine and coarse grained evaluations.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Without them, all these comparative analyses would not be possible.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.

These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ We are particularly grateful to the National Science Foundation for their support under research grant IIS-0336793, and to the University of North Texas for a research grant that provided funding for contributor prizes.
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ The objective of this task was to: (1) Determine feasibility of reliably finding the English lexical sample Word Sense Disambiguation task.
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ Corresponding team and reference to system description (in this volume) are indicated for the first system for each team. appropriate sense for words with various degrees of polysemy, using different sense inventories; and (2) Determine the usefulness of sense annotated data collected over the Web (as opposed to other traditional approaches for building semantically annotated corpora).
These approaches have shown good results; particularly those using supervised learning (see Mihalcea et al, 2004 for an overview of state-of the-art systems). $$$$$ We are also grateful to all the participants in this task, for their hard work and involvement in this evaluation exercise.

WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories (Mihalcea et al, 2004). $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.

The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ The training and test data sets used in this exercise are available online from http://www.senseval.org and http://teach-computers.org.
The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.
The experiments are performed on the set of ambiguous nouns from the SENSEVAL-3 English lexical sample evaluation (Mihalcea et al, 2004). $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.

Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ The inter-tagger agreement obtained so far is closely comparable to the agreement figures previously reported in the literature.
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the OMWE system enables the collection of semantically annotated corpora over the Web.
Since the test data for the nouns of SENSEVAL-3 English lexical sample task (Mihalcea et al, 2004) were also drawn from BNC and represented a difference in domain from the parallel texts we used, we also expanded our evaluation to these SENSEVAL-3 nouns. $$$$$ We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.

Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ Many thanks to all those who contributed to the Open Mind Word Expert project, making this task possible.
Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.
Extracting Senses Preliminary experiments on 10 nouns of SensEval-3 English lexical-sample task (Mihalcea et al, 2004) (S3LS), suggested that our hyper graphs 415 are small-world networks, since they exhibited a high clustering coefficient and a small average path length. $$$$$ Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.

In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ Precision and recall figures are provided for both fine grained and coarse grained scoring.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ Tables 2 and 3 list the names of the participating systems, the corresponding institutions, and the name of the first author – which can be used as reference to a paper in this volume, with more detailed descriptions of the systems and additional analysis of the results.

In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ Not surprisingly, several of the top performing systems are based on combinations of multiple classifiers, which shows once again that voting schemes that combine several learning algorithms outperform the accuracy of individual classifiers.
In detail, we first mapped senses of ambiguous words, as defined in the gold-standard TWA (Mihalcea, 2003) and Senseval-3 lexical sample (Mihalcea et al, 2004) datasets (which we use for evaluation) onto their corresponding Chinese translations. $$$$$ The selection is made using checkboxes, which list all possible senses of the current target word, plus two additional choices, “unclear” and “none of the above.” Although users are encouraged to select only one meaning per word, the selection of two or more senses is also possible.

For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ This paper presents the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was orgaas part of the evaluation exercise.
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ Additionally, this task has highlighted creation of testing and training data by leveraging the knowledge of Web volunteers.
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ In particular, we are grateful to Gwen Lenker – our most productive contributor.
For fine grained evaluation, we used Senseval-3 English lexical sample dataset (Mihalcea et al, 2004), which comprises 7,860 sense-tagged instances for training and 3,944 for testing, on 57 words (nouns, verbs and adjectives). $$$$$ The task drew the participation of 27 teams from around the world, with a total of 47 systems.

Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ The performance of most systems (including several unsupervised systems, as listed in Table 3) is significantly higher than the baseline, with the best system performing at 72.9% (79.3%) for fine grained (coarse grained) scoring.
Still, the performance is significantly lower than the score achieved by supervised systems, which can reach above 72% recall (Mihalcea et al, 2004). $$$$$ We describe in this paper the task definition, resources, participating systems, and comparative results for the English lexical sample task, which was organized as part of the SENSEVAL-3 evaluation exercise.
