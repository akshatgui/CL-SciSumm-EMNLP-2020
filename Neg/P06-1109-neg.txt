While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ Manning and Schütze 1999: 401).
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus,  |t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as o. Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree).

Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ This has roughly the same effect as the correction factor used in Bod (2003, 2006).
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ The resulting subtrees are then given to MLDOP's reestimation procedure.
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.

While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.

We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ Prescher et al. 2004).

This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).
This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ As it happens, in Bod (2000) we already developed such a DOP model, termed ML-DOP, which reestimates the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.
This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.

Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ Prescher et al. 2004).
Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ The key idea of DOP is this: given an annotated corpus, use all subtrees, regardless of size, to parse new sentences.
Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ Yet we should neither rule out the possibility that entirely unsupervised methods will in fact surpass semi-supervised methods.

To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn's WSJ10 which contains 7422 sentences ? 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ? 10 words after removing punctuation. $$$$$ We were also interested in testing UML-DOP on longer sentences.
To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn's WSJ10 which contains 7422 sentences ? 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ? 10 words after removing punctuation. $$$$$ Again, t' is defined to be t − tb.
To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006): Penn's WSJ10 which contains 7422 sentences ? 10 words after removing empty elements and punctuation, the German NEGRA10 corpus and the Chinese Treebank CTB10 both containing 2200+ sentences ? 10 words after removing punctuation. $$$$$ Bod (2006) reports that U-DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ.

All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ Nevertheless we will see that our reestimation procedure leads to significantly better accuracy compared to U-DOP (the latter would be equal to UML-DOP under 0 iterations).
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ The DOP1 model in Bod (1998) computes the probabilities of parse trees and sentences from the relative frequencies of the subtrees.
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ Moreover, Collins and Duffy (2002) show how a tree kernel can be applied to DOP1's all-subtrees representation.

Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ But the most surprising result is that UML-DOP's fscore is higher than the supervised binarized treebank PCFG (ML-PCFG) for both WSJ10 and WSJ40.
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ The key idea of DOP is this: given an annotated corpus, use all subtrees, regardless of size, to parse new sentences.
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ ML-DOP reestimates DOP's subtree probabilities in an iterative way until the changes become negligible.

Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ The updated probabilities are iteratively reestimated until the decrease in cross-entropy becomes negligible.
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ In order to check whether this difference is statistically significant, we additionally tested on 10 different 90/10 divisions of the WSJ40 (which were the same splits as in Bod 2006).

While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. $$$$$ To derive the reestimation formula, it is useful to consider the state space of all possible derivations of a tree.
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006): 1.8%, while the difference in table 5 is 7.2%, corresponding to 19.7% error reduction. $$$$$ Prescher et al. 2004).

DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ Schütze 1995) which can be directly combined with unsupervised parsers.
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ The updated probabilities are iteratively reestimated until the decrease in cross-entropy becomes negligible.
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ This has roughly the same effect as the correction factor used in Bod (2003, 2006).

Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ Fortunately there exists a compact isomorphic PCFG-reduction of DOP1 whose size is linear rather than exponential in the size of the training set (Goodman 2003).
Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ To give an example of the dependencies that U-DOP can take into account, consider the following subtrees in figure 5 from the trees in probabilities of its distinct derivations.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ To this end, we used a random 90%/10% division of WSJ40 into a training set and a test set.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ And the hybrid approach of Klein and Manning (2004), which combines constituency and dependency models, yields 77.6% f-score.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ The currently most successful version of DOP1 uses a PCFG-reduction of the model with an n-best parsing algorithm (Bod 2003).

Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ It is also noteworthy that UMLDOP, like U-DOP, does not employ a separate class for non-constituents, so-called distituents, while CCM and CCM+DMV do.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ To be sure, the unbinarized version of the treebank PCFG obtains 89.0% average f-score on WSJ10 and 72.3% average f-score on WSJ40.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ It then reestimates the initial probabilities of these subtrees by ML-DOP on the sentences from a held-out part of the tree set.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.

Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.
Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ For our experiments in section 6, we repeated this procedure 200,000 times for each depth.
Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ Although it is now known that DOP1's relative frequency estimator is statistically inconsistent (Johnson 2002), the model yields excellent empirical results and has been used in state-of-the-art systems.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ The resulting set of trees is referred to as the binary tree set.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ We were also interested in testing UML-DOP on longer sentences.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ Fortunately there exists a compact isomorphic PCFG-reduction of DOP1 whose size is linear rather than exponential in the size of the training set (Goodman 2003).

Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ This unsupervised DOP model, coined U-DOP, first assigns all possible unlabeled binary trees to a set of sentences and next uses all subtrees from (a large subset of) these trees to compute the most probable parse trees.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ In this paper we will show that an unsupervised version of ML-DOP can be constructed along the lines of U-DOP.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ Finally, the reestimated subtrees are used to compute the most probable parse trees for all sentences using Viterbi n-best, as described in section 3, where the most probable parse is estimated from the 100 most probable derivations.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus,  |t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as o. Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree).

For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ We were also interested in testing UML-DOP on longer sentences.
For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ We therefore included all WSJ sentences up to 40 words after removing empty elements and punctuation (WSJ40) and again sampled 200,000 subtrees for each depth, using the same method as before.

Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ Furthermore, we compared UML-DOP against a supervised binarized PCFG, i.e. a treebank PCFG whose simple relative frequency estimator corresponds to maximum likelihood (Chi and Geman 1998), and which we shall refer to as &quot;ML-PCFG&quot;.
Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ Manning and Schütze 1999: 401).
Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
