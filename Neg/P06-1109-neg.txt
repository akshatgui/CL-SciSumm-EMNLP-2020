While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ Moreover, in contrast to U-DOP, UML-DOP can be theoretically motivated: it maximizes the likelihood of the data using the statistically consistent EM algorithm.
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing.
While Klein and Manning's approach may be described as an "all-substrings" approach to unsupervised parsing, an even richer model consists of an "all-subtrees" approach to unsupervised parsing, called U-DOP (Bod 2006). $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.

Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ Klein's definitions differ slightly from the standard PARSEVAL metrics: multiplicity of brackets is ignored, brackets of span one are ignored and the bracket labels are ignored.
Bod (2006) reports 82.9% unlabeled f-score on the same WSJ10 as used by Klein and Manning (2002, 2004). $$$$$ A disadvantage of the approach may be that an extremely large number of subtrees (and derivations) must be considered.

While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). $$$$$ Previous models like Klein and Manning's (2002, 2005) CCM model limit the dependencies to &quot;contiguous subsequences of a sentence&quot;.
While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). $$$$$ DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus,  |t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as o. Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree).
While we do not achieve as high an f-score as the UML-DOP model in Bod (2006), we will show that U-DOP* can operate without subtree sampling, and that the model can be trained on corpora that are two orders of magnitude larger than in Bod (2006). $$$$$ The derivations of a parse tree T can be viewed as a state trellis, where each state contains a partially constructed tree in the course of a leftmost derivation of T. st denotes a state containing the tree t which is a subtree of T. The state trellis is defined as follows.

We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ U-DOP starts by While we can efficiently represent the set of all binary trees of a string by means of a chart, we need to unpack the chart if we want to extract subtrees from this set of binary trees.
We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ The DOP1 model in Bod (1998) computes the probabilities of parse trees and sentences from the relative frequencies of the subtrees.
We will use the same all subtrees methodology as in Bod (2006), but now by applying the efficient and consistent DOP* based estimator. $$$$$ Moreover, in contrast to U-DOP, UML-DOP can be theoretically motivated: it maximizes the likelihood of the data using the statistically consistent EM algorithm.

This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ But the most surprising result is that UML-DOP's fscore is higher than the supervised binarized treebank PCFG (ML-PCFG) for both WSJ10 and WSJ40.
This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ That is, for each particular depth we sample subtrees by first randomly selecting a node in a random tree from the binary tree set after which we select random expansions from that node until a subtree of the particular depth is obtained.
This is a huge reduction compared to Bod (2006) where the number of subtrees of all trees increases with the Catalan number, and only ad hoc sampling could make the method work. $$$$$ For our experiments in section 6, we repeated this procedure 200,000 times for each depth.

Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ To be sure, the unbinarized version of the treebank PCFG obtains 89.0% average f-score on WSJ10 and 72.3% average f-score on WSJ40.
Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ To derive the reestimation formula, it is useful to consider the state space of all possible derivations of a tree.
Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ These subtrees show that U-DOP takes into account both contiguous and non-contiguous substrings.
Note that the direct conversion of parse forests into a PCFG reduction also allows us to efficiently implement the maximum likelihood extension of U-DOP known as UML-DOP (Bod 2006). $$$$$ This resulted in a total set of roughly 1.7 * 106 subtrees that were reestimated by our maximum-likelihood procedure.

To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006) $$$$$ Although it is now known that DOP1's relative frequency estimator is statistically inconsistent (Johnson 2002), the model yields excellent empirical results and has been used in state-of-the-art systems.
To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006) $$$$$ We investigate generalizations of the allsubtrees &quot;DOP&quot; approach to unsupervised parsing.
To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006) $$$$$ We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.
To evaluate U-DOP* against UML-DOP and other unsupervised parsing models, we started out with three corpora that are also used in Klein and Manning (2002, 2004) and Bod (2006) $$$$$ DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus,  |t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as o. Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree).

All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.
All trees in the test set were binarized beforehand, in the same way as in Bod (2006). $$$$$ We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.

Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ Remember that the Penn Treebank annotations are often exceedingly flat, and many branches have arity larger than two.
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus,  |t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as o. Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree).
Table 1 shows the f-scores for U-DOP* and UML-DOP against the f-scores for U-DOP reported in Bod (2006), the CCM model in Klein and Manning (2002), the DMV dependency model in Klein and Manning (2004) and their combined model DMV+CCM. $$$$$ But the most surprising result is that UML-DOP's fscore is higher than the supervised binarized treebank PCFG (ML-PCFG) for both WSJ10 and WSJ40.

Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ Nevertheless we will see that our reestimation procedure leads to significantly better accuracy compared to U-DOP (the latter would be equal to UML-DOP under 0 iterations).
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ This crosstraining is important since otherwise UML-DOP would assign the training set trees their empirical frequencies and assign zero weight to all other subtrees (cf.
Bod (2006) reports that an unbinarized treebank grammar achieves an average 72.3% f-score on WSJ sentences ? 40 words, while the binarized version achieves only 64.6% f-score. $$$$$ Moreover, Collins and Duffy (2002) show how a tree kernel can be applied to DOP1's all-subtrees representation.

While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006) $$$$$ We will raise the question whether the end of supervised parsing is in sight.
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006) $$$$$ Bod (2006) reports that U-DOP not only outperforms previous unsupervised parsers but that its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ.
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006) $$$$$ Thus a new sentence such as Mary likes Susan can be derived by The probability of a derivation t1o...otn is computed by the product of the probabilities of its subtrees ti: As we have seen, there may be several distinct derivations that generate the same parse tree.
While a similar result was obtained in Bod (2006), the absolute difference between unsupervised parsing and the treebank grammar was extremely small in Bod (2006) $$$$$ The currently most successful version of DOP1 uses a PCFG-reduction of the model with an n-best parsing algorithm (Bod 2003).

DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ This has roughly the same effect as the correction factor used in Bod (2003, 2006).
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ The initial state, s0, is a tree with depth zero, consisting of simply a root node labeled with S. The final state, sT, is the given parse tree T. A state st is connected forward to all states stf such that tf = t ° t', for some t'.
DOP maximizes what has been called the 'structural analogy' between a sentence and a corpus of previous sentence-structures (Bod 2006b). $$$$$ We will start out by summarizing DOP, U-DOP and ML-DOP, and next create a new unsupervised model called UML-DOP.

Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ The EM algorithm for this ML-DOP model is related to the Inside-Outside algorithm for context-free grammars, but the reestimation formula is complicated by the presence of subtrees of depth greater than 1.
Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ The key idea of DOP is this: given an annotated corpus, use all subtrees, regardless of size, to parse new sentences.
Although several alternative versions of U DOP have been proposed (e.g. Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper. $$$$$ DOP1 computes the probability of a subtree t as the probability of selecting t among all corpus subtrees that can be substituted on the same node as t. This probability is computed as the number of occurrences of t in the corpus,  |t |, divided by the total number of occurrences of all subtrees t' with the same root label as t.1 Let r(t) return the root label of t. Then we may write: New sentences may be derived by combining fragments, i.e. subtrees, from this corpus, by means of a node-substitution operation indicated as o. Node-substitution identifies the leftmost nonterminal frontier node of one subtree with the root node of a second subtree (i.e., the second subtree is substituted on the leftmost nonterminal frontier node of the first subtree).

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ As a consequence, parsing with ML-DOP is very costly and the model has hitherto never been tested on corpora larger than OVIS (Bonnema et al. 1997).
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ The construction of the state lattice and assignment of transition probabilities according to the ML-DOP model is called the forward pass.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ This resulted in a total set of roughly 1.7 * 106 subtrees that were reestimated by our maximum-likelihood procedure.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ Unfortunately, no compact PCFG-reduction of MLDOP is known.

Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ The initial state, s0, is a tree with depth zero, consisting of simply a root node labeled with S. The final state, sT, is the given parse tree T. A state st is connected forward to all states stf such that tf = t ° t', for some t'.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ We will start out by summarizing DOP, U-DOP and ML-DOP, and next create a new unsupervised model called UML-DOP.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
Still, Klein and Manning (2002) and Bod (2006) stick to tag-based models. $$$$$ (Zollmann and Sima'an 2005 propose a different consistent estimator for DOP, which we cannot go into here).

Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ Again, t' is defined to be t − tb.
Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ The difference in accuracy between UMLDOP and ML-PCFG was statistically significant according to paired t-testing (p≤0.05).
Bod (2006) describes an unsupervised system within the Data-Oriented-Parsing frame work. $$$$$ Let's illustrate DOP1 with a simple example.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ A substantial part of this research was carried out in the context of the NWO Exact project &quot;Unsupervised Stochastic Grammar Induction from Unlabeled Data&quot;, project number 612.066.405.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ The training is carried out by dividing the tree set into two equal parts, and reestimating the subtrees from one part on the other.

Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ Of course, if we only had the sentence Investors suffered heavy losses in our corpus, there would be no difference in probability between the five parse trees in figure 4.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ Table 1 shows that UML-DOP scores better than U-DOP and Klein and Manning's models in all cases.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ Instead, UDOP's all-subtrees approach captures both contiguous and non-contiguous lexical dependencies.
Interestingly, the results reported for other constituency models (the CCM model (Klein and Manning, 2002) and the U-DOP model (Bod, 2006a; Bod, 2006b)) are reported when the parser is trained on its test corpus even if the sentences is that corpus are of bounded length (e.g. WSJ10). $$$$$ To avoid overtraining, ML-DOP uses the subtrees from one half of the training set to be trained on the other half, and vice versa.

For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).
For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ It would thus be interesting to see how close UML-DOP can get to ML-DOP's performance if we enlarge the amount of training data.
For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ This raises the question whether we can create an unsupervised DOP model which is also statistically consistent.
For some time, multipoint performance degradations caused by switching to automatically induced word categories have been interpreted as indications that "good enough" parts-of-speech induction methods exist, justifying the focus on grammar induction with supervised part-of-speech tags (Bod, 2006), pace (Cramer, 2007). $$$$$ The derivations of a parse tree T can be viewed as a state trellis, where each state contains a partially constructed tree in the course of a leftmost derivation of T. st denotes a state containing the tree t which is a subtree of T. The state trellis is defined as follows.

Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ Thanks to Willem Zuidema, David Tugwell and especially to three anonymous reviewers whose unanymous suggestions on DOP and EM considerably improved the original paper.
Finally, Seginer (2007) and Bod (2006) approach unsupervised parsing by constructing novel syntactic models. $$$$$ We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
