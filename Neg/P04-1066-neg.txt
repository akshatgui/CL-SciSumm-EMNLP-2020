 $$$$$ We could take |V  |simply to be the total number of distinct words observed in the target language training, but we know that the target language will have many words that we have never observed.
 $$$$$ A target sentence may contain many words that ideally should be aligned to null, plus some other instances of the same word that should be aligned to an actual source language word.
 $$$$$ This is exactly the property needed to prevent rare source words from becoming garbage collectors.
 $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.

 $$$$$ Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a “truly significant improvement” was the Model 1 score (Och et al., 2004).
 $$$$$ However, in virtually every application of statistical techniques in natural-language processing, maximizing the likelihood of the training data causes overfitting, resulting in lower task performance than some other estimates for the model parameters.
 $$$$$ Even if the rare word also needs to be used to generate its actual translation in the sentence pair, a relatively high joint probability will be obtained by giving the rare word a probability of 0.5 of generating its true translation and 0.5 of spuriously generating the translation of the frequent source word.
 $$$$$ Even though EM will head towards those values from any initial position in the parameter space, there may be some starting points we can systematically find that will take us closer to the optimal parameter values for alignment accuracy along the way.

Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ The structure of Model 1 again suggests why we should not be surprised by this problem.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.

Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ Add-n smoothing is a way of smoothing with a uniform distribution, so it is not surprising that it performs poorly in language modeling when it is compared to smoothing with higher order models; e.g, smoothing trigrams with bigrams or smoothing bigrams with unigrams.
Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ If AER does not reflect the optimal balance between precision and recall for a particular application, then optimizing AER may not produce the best task-based performance for that application.
Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ This makes these probabilities look like they are not normalized, but Model 1 can be applied in such a way that the translation probabilities for the null word are only ever used when multiplied by the number of null words in the sentence, so we are simply using the null word translation parameters to keep track of this product pre-computed.

 $$$$$ In those studies, however, add-n smoothing was used to smooth bigram or trigram models.
 $$$$$ This happens because there are are two instances of of in the source sentence and only one hypothetical null word, and Model 1 gives equal weight to each occurrence of each source word.
 $$$$$ To limit the number of translation probabilities that we had to store, we first computed LLR association scores for all bilingual word pairs with a positive association (p(t, s) > p(t)·p(s)), and discarded from further consideration those with an LLR score of less that 0.9, which was chosen to be just low enough to retain all the “sure” word alignments in the trial data.
 $$$$$ To limit the number of translation probabilities that we had to store, we first computed LLR association scores for all bilingual word pairs with a positive association (p(t, s) > p(t)·p(s)), and discarded from further consideration those with an LLR score of less that 0.9, which was chosen to be just low enough to retain all the “sure” word alignments in the trial data.

Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ The training is normally initialized by setting all translation probability distributions to the uniform distribution over the target language vocabulary.
Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.

this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ We have identified at least two ways in which the standard EM training method for Model 1 leads to suboptimal performance in terms of wordalignment accuracy.
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ The ablation experiments show how important the different modifications are to the various models.
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ We have identified at least two ways in which the standard EM training method for Model 1 leads to suboptimal performance in terms of wordalignment accuracy.

Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ We could leave the distribution for the null word as the uniform distribution, but we know that a high proportion of the words that should align to the null word are frequently occuring function words.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ This makes these probabilities look like they are not normalized, but Model 1 can be applied in such a way that the translation probabilities for the null word are only ever used when multiplied by the number of null words in the sentence, so we are simply using the null word translation parameters to keep track of this product pre-computed.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ First, to address the problem of rare words aligning to too many words, at each interation of EM we smooth all the translation probability estimates by adding virtual counts according to a uniform probability distribution over all target words.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ This is exactly the property needed to prevent rare source words from becoming garbage collectors.

Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ The other parameters of the various versions of Model 1 described in Sections 4–6 were optimized with respect to alignment error rate on the trial data using simple hill climbing.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ On the other hand, the re-estimation null-word weight is crucial to the combined model.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ This multiplication is performed during every iteration of EM, as the translation probabilities for the null word are re-estimated from the corresponding expected counts.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ Och and Ney find that the standard version of Model 1 produces more accurate alignments after only one iteration of EM than either of the heuristic models they consider, while we find that our heuristic model outperforms the standard version of Model 1, even with an optimal number of iterations of EM.

This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ We address the nonstructural problems of Model 1 discussed above by three methods.
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ The 2-tailed paired t test comparing this model to the standard model showed significance at a level of better than 10−10.
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ Och and Ney find that the standard version of Model 1 produces more accurate alignments after only one iteration of EM than either of the heuristic models they consider, while we find that our heuristic model outperforms the standard version of Model 1, even with an optimal number of iterations of EM.
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1.

 $$$$$ |V  |and n are both free parameters in this equation.
 $$$$$ Both of these are far short of convergence to the maximum likelihood estimates for the model parameters.
 $$$$$ This problem is not unique to Model 1, but anecdotal examination of Model 1 alignments suggests that it may be worse for Model 1, perhaps because Model 1 lacks the fertility and distortion parameters that may tend to mitigate the problem in more complex models.

(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ To estimate the smoothed probabilties we use the following formula: where C(t, s) is the expected count of s generating t, C(s) is the corresponding marginal count for s, |V  |is the hypothesized size of the target vocabulary V , and n is the added count for each target word in V .
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ It should certainly be better than smoothing with a unigram distribution, since we especially want to benefit from smoothing the translation probabilities for the rarest words, and smoothing with a unigram distribution would assume that rare words are more likely to translate to frequent words than to other rare words, which seems counterintuitive.
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ First, to address the problem of rare words aligning to too many words, at each interation of EM we smooth all the translation probability estimates by adding virtual counts according to a uniform probability distribution over all target words.
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ A target sentence may contain many words that ideally should be aligned to null, plus some other instances of the same word that should be aligned to an actual source language word.

For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ This makes these probabilities look like they are not normalized, but Model 1 can be applied in such a way that the translation probabilities for the null word are only ever used when multiplied by the number of null words in the sentence, so we are simply using the null word translation parameters to keep track of this product pre-computed.
For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ This happens because there are are two instances of of in the source sentence and only one hypothetical null word, and Model 1 gives equal weight to each occurrence of each source word.
For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ The other parameters of the various versions of Model 1 described in Sections 4–6 were optimized with respect to alignment error rate on the trial data using simple hill climbing.
For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ We may also be interested in the question of what is the most likely alignment of a source sentence and a target sentence, given an instance of Model 1; where, by an alignment, we mean a specification of which source words generated which target words according to the generative model.

 $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
 $$$$$ The results of our evaluation are presented in Table 1.
 $$$$$ Furthermore, we would argue that the word translation probabilities of Model 1 are a case where there is no clearly better alternative to a uniform distribution as the smoothing distribution.

 $$$$$ Normally, the translation probabilities of Model 1 are initialized to a uniform distribution over the target language vocabulary to start iterating EM.
 $$$$$ Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003).
 $$$$$ This prevents the model from becoming too confident about the translation probabilities for rare source words on the basis of very little evidence.
 $$$$$ We may also be interested in the question of what is the most likely alignment of a source sentence and a target sentence, given an instance of Model 1; where, by an alignment, we mean a specification of which source words generated which target words according to the generative model.

 $$$$$ We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.
 $$$$$ The columns of the table present (in order) a description of the model being tested, the AER on the trial data, the AER on the test data, test data recall, and test data precision, followed by the optimal values on the trial data for the LLR exponent, the initial (heuristic model) null-word weight, the nullword weight used in EM re-estimation, the add-n parameter value used in EM re-estimation, and the number of iterations of EM.
 $$$$$ The training is normally initialized by setting all translation probability distributions to the uniform distribution over the target language vocabulary.

 $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
 $$$$$ If AER does not reflect the optimal balance between precision and recall for a particular application, then optimizing AER may not produce the best task-based performance for that application.
 $$$$$ Our concern in this paper is with two other problems with Model 1 that are not deeply structural, and can be addressed merely by changing how the parameters of Model 1 are estimated.
 $$$$$ It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models, but it has subsequently found a host of additional uses.

Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures.
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ It should certainly be better than smoothing with a unigram distribution, since we especially want to benefit from smoothing the translation probabilities for the rarest words, and smoothing with a unigram distribution would assume that rare words are more likely to translate to frequent words than to other rare words, which seems counterintuitive.

 $$$$$ All the results we report for the 447 sentence pairs of test data use the parameter values set to their optimal values for the trial data.
 $$$$$ The results of our evaluation are presented in Table 1.
 $$$$$ The parameters of Model 1 for a given pair of languages are normally estimated using EM, taking as training data a corpus of paired sentences of the two languages, such that each pair consists of sentence in one language and a possible translation in the other language.
 $$$$$ We cannot make use of LLR scores because the null word occurs in every source sentence, and any word occuring in every source sentence will have an LLR score of 0 with every target word, since p(t|s) = p(t) in that case.

Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ To estimate the smoothed probabilties we use the following formula: where C(t, s) is the expected count of s generating t, C(s) is the corresponding marginal count for s, |V  |is the hypothesized size of the target vocabulary V , and n is the added count for each target word in V .
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ We cannot make use of LLR scores because the null word occurs in every source sentence, and any word occuring in every source sentence will have an LLR score of 0 with every target word, since p(t|s) = p(t) in that case.
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ This happens because there are are two instances of of in the source sentence and only one hypothetical null word, and Model 1 gives equal weight to each occurrence of each source word.
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000; Moore, 2001).
