 $$$$$ This may be due in part to the fact that Brown et al. (1993a) proved that the log-likelihood objective function for Model 1 is a strictly concave function of the model parameters, so that it has a unique local maximum.
 $$$$$ In training a version of Model 1 with only one null word per sentence, the parameters have their normal interpretation, since we are multiplying the standard probability estimates by 1.
 $$$$$ We address the lack of sufficient alignments of target words to the null source word by adding extra null words to each source sentence.
 $$$$$ This, in turn, means that EM training will converge to that maximum from any starting point in which none of the initial parameter values is zero.

 $$$$$ Even if the rare word also needs to be used to generate its actual translation in the sentence pair, a relatively high joint probability will be obtained by giving the rare word a probability of 0.5 of generating its true translation and 0.5 of spuriously generating the translation of the frequent source word.
 $$$$$ For example, we may have an English/French sentence pair that contains two instances of of in the English sentence, and five instances of de in the French sentence.
 $$$$$ Results for the four principal versions of Model 1 are presented in bold.
 $$$$$ This resulted in 13,285,942 possible word-to-word translation pairs (plus 66,406 possible null-word-to-word pairs).

Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ We address the nonstructural problems of Model 1 discussed above by three methods.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ The smoothed model with only the nullword weight and no add-n smoothing has essentially the same error as the standard model; and the smoothed model with add-n smoothing alone has essentially the same error as the smoothed model with both the null-word weight and add-n smoothing.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ The major difference between our model and theirs is that they base theirs on the Dice coefficient, which is computed by the formula4 while we use the log-likelihood-ratio statistic defined in Section 6.
Problems with the standard EM estimation of IBM Model 1 was pointed out by Moore (2004) and a number of heuristic changes to the estimation procedure, such as smoothing the parameter estimates, were shown to reduce the alignment error rate, but the effects on translation performance was not reported. $$$$$ This problem is not unique to Model 1, but anecdotal examination of Model 1 alignments suggests that it may be worse for Model 1, perhaps because Model 1 lacks the fertility and distortion parameters that may tend to mitigate the problem in more complex models.

Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ Model 1 is a probabilistic generative model within a framework that assumes a source sentence 5 of length l translates as a target sentence T, according to the following stochastic process: – A generating word si in 5 (including a null word so) is selected, and – The target word tj at position j is generated depending on si.
Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ It is easy to show that for Model 1, the most likely alignment aˆ of S and T is given by this equation: Since in applying Model 1, there are no dependencies between any of the ajs, we can find the most likely aligment simply by choosing, for each j, the value for aj that leads to the highest value for tr(tj|sa;).
Finally, the IBM models (Moore, 2004) impose the limitation that each word in the target sentence can be generated by at most one word in the source sentence. $$$$$ We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30%, simply by changing the way its parameters are estimated.

 $$$$$ The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values, so the intial values should not matter.
 $$$$$ For example, English phrases of the form (nouns)(noun�) are often expressed in French by a phrase of the form (noun�) de (nouns), which may also be expressed in English (but less often) by a phrase of the form (noun�) of (nouns).
 $$$$$ In training a version of Model 1 with only one null word per sentence, the parameters have their normal interpretation, since we are multiplying the standard probability estimates by 1.
 $$$$$ Model 1 is defined as a particularly simple instance of this framework, by assuming all possible lengths for T (less than some arbitrary upper bound) have a uniform probability E, all possible choices of source sentence generating words are equally likely, and the translation probability tr(tj|si) of the generated target language word depends only on the generating source language word—which Brown et al. (1993a) show yields the following equation: Equation 1 gives the Model 1 estimate for the probability of a target sentence, given a source sentence.

Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1.
Moore (2004) also suggested adding multiple empty words to the target sentence for IBM Model 1. $$$$$ Model 1 is defined as a particularly simple instance of this framework, by assuming all possible lengths for T (less than some arbitrary upper bound) have a uniform probability E, all possible choices of source sentence generating words are equally likely, and the translation probability tr(tj|si) of the generated target language word depends only on the generating source language word—which Brown et al. (1993a) show yields the following equation: Equation 1 gives the Model 1 estimate for the probability of a target sentence, given a source sentence.

this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003).
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ Add-n smoothing is a way of smoothing with a uniform distribution, so it is not surprising that it performs poorly in language modeling when it is compared to smoothing with higher order models; e.g, smoothing trigrams with bigrams or smoothing bigrams with unigrams.
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1.
this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. $$$$$ We address the lack of sufficient alignments of target words to the null source word by adding extra null words to each source sentence.

Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ The differences between all these models were significant at a level of 10−7 or better, except for the difference between the standard model and the smoothed model, which was “significant” at the 0.61 level—that is, not at all significant.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ Hence we initialize the distribution for the null word to be the unigram distribution of target words, so that frequent function words will receive a higher probability of aligning to the null word than rare words, which tend to be content words that do have a translation.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ This happens because there are are two instances of of in the source sentence and only one hypothetical null word, and Model 1 gives equal weight to each occurrence of each source word.
Similar to Berger and Lafferty (1999), the probability distribution of terms given a category is estimated using a normalized log-likelihood ratio (Moore, 2004), and query terms are sampled randomly from this distribution. $$$$$ We still have to define an initialization of the translation probabilities for the null word.

Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ For example, we may have an English/French sentence pair that contains two instances of of in the English sentence, and five instances of de in the French sentence.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ Results for the four principal versions of Model 1 are presented in bold.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ There is no guarantee, of course, that this is the optimal way of discounting the probabilities assigned to less frequent words.
Following Moore (2004a) rather than Munteanu and Marcu, our current notion of co-occurrence is that a data field and word co-occur if they are present in the same pair of data fields and sentence (as identified by the method described in Section 4.1 above). $$$$$ Our concern in this paper is with two other problems with Model 1 that are not deeply structural, and can be addressed merely by changing how the parameters of Model 1 are estimated.

This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ If AER does not reflect the optimal balance between precision and recall for a particular application, then optimizing AER may not produce the best task-based performance for that application.
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ Even if the rare word also needs to be used to generate its actual translation in the sentence pair, a relatively high joint probability will be obtained by giving the rare word a probability of 0.5 of generating its true translation and 0.5 of spuriously generating the translation of the frequent source word.
This allows us to compute the G2 score, for which we use the formulation from Moore (2004b) shown in Figure 2. $$$$$ Finally, we also effectively add extra null words to every sentence in this heuristic model, by multiplying the null word probabilities by a constant, as described in Section 5.

 $$$$$ Even if the rare word also needs to be used to generate its actual translation in the sentence pair, a relatively high joint probability will be obtained by giving the rare word a probability of 0.5 of generating its true translation and 0.5 of spuriously generating the translation of the frequent source word.
 $$$$$ Since Model 1, like many other word-alignment models, requires each target word to be generated by exactly one source word (including the null word), an alignment a can be represented by a vector a1,... , am, where each aj is the sentence position of the source word generating tj according to the alignment.
 $$$$$ While the Dice coefficient is simple and intuitive—the value is 0 for words never found together, and 1 for words always found together—it lacks the important property of the LLR statistic that scores for rare words are discounted; thus it does not address the over-fitting problem for rare words.
 $$$$$ A similar analysis shows that add-n smoothing is much less important in the combined model than 3Modificiations are “omitted” by setting the corresponding parameter to a value that is equivalent to removing the modification from the model. the smoothed model.

(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ Since the rare source word has no other occurrences in the data, EM training is free to assign whatever probability distribution is required to maximize the joint probability of this sentence pair.
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ To estimate the smoothed probabilties we use the following formula: where C(t, s) is the expected count of s generating t, C(s) is the corresponding marginal count for s, |V  |is the hypothesized size of the target vocabulary V , and n is the added count for each target word in V .
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ A similar analysis shows that add-n smoothing is much less important in the combined model than 3Modificiations are “omitted” by setting the corresponding parameter to a value that is equivalent to removing the modification from the model. the smoothed model.
(Moore, 2004) translation limitation in the IBM model, due to which each word in the target document can be generated by at most one word in the question. $$$$$ Suppose the frequent source word has the translation present in the target sentence only 10% of the time in our training data, and thus has an estimated translation probability of around 0.1 for this target word.

For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., Melamed, 2000; Moore, 2001).
For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ Furthermore, we would argue that the word translation probabilities of Model 1 are a case where there is no clearly better alternative to a uniform distribution as the smoothing distribution.
For the same reasons, it also alleviates another related limitation by enabling translation between contiguous words across the query and documents (Moore, 2004). $$$$$ We report results for four principal versions of Model 1, trained using English as the source language and French as the target language: We also performed ablation experiments in which we ommitted each applicable modification in turn from each principal version of Model 1, to observe the effect on alignment error.

 $$$$$ However, in virtually every application of statistical techniques in natural-language processing, maximizing the likelihood of the training data causes overfitting, resulting in lower task performance than some other estimates for the model parameters.
 $$$$$ Add-n smoothing is a way of smoothing with a uniform distribution, so it is not surprising that it performs poorly in language modeling when it is compared to smoothing with higher order models; e.g, smoothing trigrams with bigrams or smoothing bigrams with unigrams.
 $$$$$ This problem is not unique to Model 1, but anecdotal examination of Model 1 alignments suggests that it may be worse for Model 1, perhaps because Model 1 lacks the fertility and distortion parameters that may tend to mitigate the problem in more complex models.
 $$$$$ If AER does not reflect the optimal balance between precision and recall for a particular application, then optimizing AER may not produce the best task-based performance for that application.

 $$$$$ We trained and evaluated our various modifications to Model 1 on data from the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003).
 $$$$$ While the Dice coefficient is simple and intuitive—the value is 0 for words never found together, and 1 for words always found together—it lacks the important property of the LLR statistic that scores for rare words are discounted; thus it does not address the over-fitting problem for rare words.
 $$$$$ In this paper we show that by addressing these issues, substantial improvements in word-alignment accuracy can be achieved.
 $$$$$ To limit the number of translation probabilities that we had to store, we first computed LLR association scores for all bilingual word pairs with a positive association (p(t, s) > p(t)·p(s)), and discarded from further consideration those with an LLR score of less that 0.9, which was chosen to be just low enough to retain all the “sure” word alignments in the trial data.

 $$$$$ The smoothed model with only the nullword weight and no add-n smoothing has essentially the same error as the standard model; and the smoothed model with add-n smoothing alone has essentially the same error as the smoothed model with both the null-word weight and add-n smoothing.
 $$$$$ The probable explanation for this is that add-n smoothing is designed to address over-fitting from many iterations of EM.
 $$$$$ Even though EM will head towards those values from any initial position in the parameter space, there may be some starting points we can systematically find that will take us closer to the optimal parameter values for alignment accuracy along the way.
 $$$$$ We have identified at least two ways in which the standard EM training method for Model 1 leads to suboptimal performance in terms of wordalignment accuracy.

 $$$$$ This makes these probabilities look like they are not normalized, but Model 1 can be applied in such a way that the translation probabilities for the null word are only ever used when multiplied by the number of null words in the sentence, so we are simply using the null word translation parameters to keep track of this product pre-computed.
 $$$$$ In this paper we show that by addressing these issues, substantial improvements in word-alignment accuracy can be achieved.

Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment (Nevado et al., 2003), extracting parallel sentences from comparable corpora (Munteanu et al., 2004), bilingual sentence alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003).
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ We arbitrarily chose |V  |to be 100,000, which is somewhat more than the total number of distinct words in our target language training data.
Moore (2004) has found that smoothing to correct overestimated IBM1 lexical probabilities for rare words can improve word-alignment performance. $$$$$ One can make arguments in favor of adding the same number of null words to every sentence, or in favor of letting the number of null words be proportional to the length of the sentence.

 $$$$$ This multiplication is performed during every iteration of EM, as the translation probabilities for the null word are re-estimated from the corresponding expected counts.
 $$$$$ However, in virtually every application of statistical techniques in natural-language processing, maximizing the likelihood of the training data causes overfitting, resulting in lower task performance than some other estimates for the model parameters.
 $$$$$ We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30%, simply by changing the way its parameters are estimated.
 $$$$$ We may also be interested in the question of what is the most likely alignment of a source sentence and a target sentence, given an instance of Model 1; where, by an alignment, we mean a specification of which source words generated which target words according to the generative model.

Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters.
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ We address the lack of sufficient alignments of target words to the null source word by adding extra null words to each source sentence.
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ Despite the fact that IBM Model 1 is so widely used, essentially no attention seems to have been paid to whether it is possible to improve on the standard Expectation-Maximization (EM) procedure for estimating its parameters.
Previous attempted remedies include early stopping, smoothing (Moore, 2004), and posterior regularization (Graca et al, 2010). $$$$$ First, to address the problem of rare words aligning to too many words, at each interation of EM we smooth all the translation probability estimates by adding virtual counts according to a uniform probability distribution over all target words.
