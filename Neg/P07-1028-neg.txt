We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ Acknowledgements Many thanks to Jason Baldridge, Razvan Bunescu, Stefan Evert, Ray Mooney, Ulrike and Sebastian Pad6, and Sabine Schulte im Walde for helpful discussions.
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ Line (c) looks at the size of like(S).
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ In similarity-based models, on the other hand, two words that have never been seen in the same argument slot in the generalization corpus will have zero similarity.

Our model builds on the architecture of Erk (2007). $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
Our model builds on the architecture of Erk (2007). $$$$$ The similarity-based model is particularly simple and easy to compute, and seems not very sensitive to parameters.
Our model builds on the architecture of Erk (2007). $$$$$ Since the flexibility of similarity-based models extends to the vector space for computing similarities, one obvious remedy to the coverage problem would be the use of a less sparse vector space.

Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ The generalization corpus is used to compute a corpus-based semantic similarity metric.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ Concerning (2), the corpusbased similarity metrics that we use for selectional preference induction open up interesting possibilities of mixing domains.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ Section 4 describes the data used for the experiments reported in Section 5, and Section 6 concludes.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ In the CoNLL-05 shared task, participating systems showed about 10 points F-score difference between in-domain and out-of-domain test data.

In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ The induction of selectional preferences from corpus data was pioneered by Resnik (1996).
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 5001000, and more than 1000 occurrences.
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.

We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ In addition, we test a frequency-based weight, i.e. wtrp(w) = f(w, rp), and inverse document frequency, which weighs a word according to its discriminativity: num. words This similarity-based model of selectional preferences is a straightforward implementation of the idea of generalization from seen headwords to other, similar words.
We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ If only one is assigned a preference, that word is counted as chosen.

Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Plan of the paper.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ (Katz and Fodor, 1963; Wilks, 1975)).

Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ In this paper we have used the model to compute sense-specific selectional preferences for semantic roles.
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ Focusing on the task of semantic role labeling, we compute selectional preferences for semantic roles.
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ The results confirm that the Lin and Cosine metrics tend to propose less frequent words as similar.
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ Noun headwords are paired with noun confounders in order not to disadvantage Resnik's model, which only works with nouns.

 $$$$$ Resnik's model was trained on the primary corpus (59,608 sentences).
 $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
 $$$$$ The argument positions for which we compute selectional preferences will be semantic roles in the FrameNet (Baker et al., 1998) paradigm, and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word).
 $$$$$ Like the clustering-based model, it is not tied to the availability of WordNet or any other manually created resource.

Selectional preferences are computed as in Erk (2007). $$$$$ So we choose a particular instantiation of the similarity-based model that makes use of the fact that the two-corpora approach allows us to use different notions of “predicate” and “argument” in the primary and generalization corpus.
Selectional preferences are computed as in Erk (2007). $$$$$ (Katz and Fodor, 1963; Wilks, 1975)).
Selectional preferences are computed as in Erk (2007). $$$$$ In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource.
Selectional preferences are computed as in Erk (2007). $$$$$ This gives the model flexibility to influence the similarity metric through the choice of text domain of the generalization corpus.

In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ Our generalization corpus is the BNC.

Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ The corpus-based induction of selectional preferences was first proposed by Resnik (1996).
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ We use FrameNet (Baker et al., 1998), a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame.
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ Acknowledgements Many thanks to Jason Baldridge, Razvan Bunescu, Stefan Evert, Ray Mooney, Ulrike and Sebastian Pad6, and Sabine Schulte im Walde for helpful discussions.
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ Noun headwords are paired with noun confounders in order not to disadvantage Resnik's model, which only works with nouns.

We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ The name in the first column is the name of the similarity metric used.
We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ Acknowledgements Many thanks to Jason Baldridge, Razvan Bunescu, Stefan Evert, Ray Mooney, Ulrike and Sebastian Pad6, and Sabine Schulte im Walde for helpful discussions.
We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ The generalization corpus is used to compute a corpus-based semantic similarity metric.
We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ Instantiation used in this paper.

The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005).
The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ Why this is so is not clear.
The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ Semantic similarity, on the other hand, will be computed on automatically syntactically parsed corpus, where the predicates are words and the arguments are syntactic dependents.
The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories, (see e.g.

These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ The similarity-based model is particularly simple and easy to compute, and seems not very sensitive to parameters.
These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ However its coverage is considerably lower than that of EMbased clustering, comparable to Resnik's model.
These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ For the similarity-based model we test the five similarity metrics and three weighting schemes listed in section 3.
These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ This is especially relevant in view of the domain-dependence problem that SRL faces.

Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words.
Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ We use FrameNet (Baker et al., 1998), a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame.
Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ For other similarity metrics the results were similar.
Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.

Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ We will be using the similarity metrics shown in Table 1: Cosine, the Dice and Jaccard coefficients, and Hindle's (1990) and Lin's (1998) mutual information-based metrics.
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ His information-theoretic approach models the selectional preference strength of an argument positions rp of a predicate p as where the c are WordNet synsets.
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ Like Rooth et al. (1999) we evaluate selectional preference induction approaches in a pseudodisambiguation task.

In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ In addition, we test a frequency-based weight, i.e. wtrp(w) = f(w, rp), and inverse document frequency, which weighs a word according to its discriminativity: num. words This similarity-based model of selectional preferences is a straightforward implementation of the idea of generalization from seen headwords to other, similar words.
In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ Resnik uses the WordNet noun hierarchy for generalization.

 $$$$$ Noun headwords are paired with noun confounders in order not to disadvantage Resnik's model, which only works with nouns.
 $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
 $$$$$ In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin's (1998) mutual informationbased metric, for the generalization step.

 $$$$$ They model the probability of a word w occurring as the argument rp of a predicate p as being independently conditioned on a set of classes C: 'We write rp to indicate predicate-specific roles, like “the direct object of catch”, rather than just “obj&quot;.
 $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
 $$$$$ For our experiments, we chose 100 frame-specific semantic roles at random, 20 each from five frequency bands: 50-100 annotated occurrences of the role, 100-200 occurrences, 200-500, 5001000, and more than 1000 occurrences.

The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ For other similarity metrics the result is similar.
The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.
The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ The error rates sus error rate by frequency band, Jaccard, uniform weights of Resnik's model are considerably higher than both the EM-based and the similarity-based models, which is unexpected.
