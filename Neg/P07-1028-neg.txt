We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ The parameters P(c), P(rp|c) and P(w|c) are estimated using the EM algorithm.
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ Resnik's model was trained on the primary corpus (59,608 sentences).
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ (Katz and Fodor, 1963; Wilks, 1975)).
We build on a recent selectional preference model (Erk, 2007) that bases its generalisations on word similarity in a vector space. $$$$$ Resnik's model was trained on the primary corpus (59,608 sentences).

Our model builds on the architecture of Erk (2007). $$$$$ It was parsed using Minipar (Lin, 1993), which is considerably faster than the Collins parser but failed to parse about a third of all sentences.
Our model builds on the architecture of Erk (2007). $$$$$ The choice of similarity metric is critical in similarity-based models, with Jaccard and Lin achieving the best performance, and Cosine surprisingly bringing up the rear.
Our model builds on the architecture of Erk (2007). $$$$$ It is, however, more flexible in that it induces similarities from a separate generalization corpus, which allows us to control the similarities we compute by the choice of text domain for the generalization corpus.
Our model builds on the architecture of Erk (2007). $$$$$ In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005).

Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ We obtained parses for 5,941,811 sentences of the generalization corpus.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ We obtained parses for 5,941,811 sentences of the generalization corpus.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ In this section we describe experiments comparing the similarity-based model for selectional preferences to Resnik's WordNet-based model and to an EM-based clustering model3.
Erk (2007) extracted the set of seen head words from corpora with semantic role annotation, and used only a single vector space representation. $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.

In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ Parameters of similarity-based models.
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ To determine headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.
In addition, we discuss in detail which properties of the vector space are crucial for the prediction of plausibility ratings, a much more fine-grained task than the pseudo-word disambiguation task presented in Erk (2007) that is more closely related to semantic role labelling. $$$$$ In similarity-based models, on the other hand, two words that have never been seen in the same argument slot in the generalization corpus will have zero similarity.

We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ It is, however, more flexible in that it induces similarities from a separate generalization corpus, which allows us to control the similarities we compute by the choice of text domain for the generalization corpus.
We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
We have demonstrated that the successful evaluation of the model in Erk (2007) on the coarse-grained pseudo-word disambiguation task carries over to the prediction of human plausibility judgments which requires relatively fine-grained, relation-based distinctions. $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.

Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ In addition, we test a frequency-based weight, i.e. wtrp(w) = f(w, rp), and inverse document frequency, which weighs a word according to its discriminativity: num. words This similarity-based model of selectional preferences is a straightforward implementation of the idea of generalization from seen headwords to other, similar words.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Let Seen(rp) be the set of seen headwords for an argument rp of a predicate p. Then we model the selectional preference S of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: sim(w0, w) is the similarity between the seen and the potential headword, and wtrp(w) is the weight of seen headword w. Similarity sim(w0, w) will be computed on the generalization corpus, again on the basis of extracted tuples (p, rp, w).
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ In this section we describe experiments comparing the similarity-based model for selectional preferences to Resnik's WordNet-based model and to an EM-based clustering model3.
Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al, 2008). $$$$$ Like the EM-based clustering model, it is not dependent on lexical resources.

Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ The EM-based clustering model was computed with all of the FrameNet 1.3 data (139,439 sentences) as input.
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ After all, the generalization corpus of the similarity-based models is far larger than the corpus used for clustering.
Erk (2007) and Erk et al (2010) modeled the contexts of a word as the distribution of words that co-occur with it. $$$$$ The particular implementation of the similarity-based model we have chosen, using frames and roles as predicates and arguments in the primary corpus, should enable the model to compute preferences specific to word senses.

 $$$$$ Brockmann and Lapata (2003) perform a comparison of WordNet-based models.
 $$$$$ Our generalization corpus is the BNC.
 $$$$$ Lines (c) and (d), however, do just that.
 $$$$$ Accordingly, the arguments r extracted from the generalization corpus are Minipar dependencies, except that paths through preposition nodes were collapsed, using the preposition as the dependency relation.

Selectional preferences are computed as in Erk (2007). $$$$$ The task is to choose the more likely role headword from the pair (w, w').
Selectional preferences are computed as in Erk (2007). $$$$$ Our generalization corpus is the BNC.
Selectional preferences are computed as in Erk (2007). $$$$$ In the evaluation, the similarity-model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model.
Selectional preferences are computed as in Erk (2007). $$$$$ For the similarity-based model we test the five similarity metrics and three weighting schemes listed in section 3.

In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ In this paper we propose a new, simple model for selectional preference induction that uses corpus-based semantic similarity metrics, such as Cosine or Lin's (1998) mutual informationbased metric, for the generalization step.
In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ As the table shows, there was some variation across frequency bands, but not as much as between models.
In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
In (Erk, 2007) a distributional similarity based model for selectional preferences is introduced, reminiscent of that of Pantel and Lin (2000). $$$$$ In this section we describe experiments comparing the similarity-based model for selectional preferences to Resnik's WordNet-based model and to an EM-based clustering model3.

Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ In this paper we only study corpus-based metrics.
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ Coverage.
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ The preference that rp has for a given synset c0, the selectional association between the two, is then defined as the contribution of c0 to rp's selectional preference strength: Further WordNet-based approaches to selectional preference induction include Clark and Weir (2001), and Abe and Li (1993).
Bergsma et al. (2008) test pairs that fall below a mutual information threshold (might include some seen pairs), and Erk (2007) selects a subset of roles in FrameNet (Baker et al, 1998) to test and uses all labeled instances within this subset (unclear what portion of subset of data is seen). $$$$$ To test the performance difference between models for significance, we use Dietterich's Under the null hypothesis, the t statistic has approximately a t distribution with 5 degrees of freedom.5

We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ Note that the headword actually was more frequent than the confounder in only 36% of all pairs.
We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ We propose a new, simple model for the automatic induction of selectional preferences, using corpus-based semantic similarity metrics.
We implemented the current state-of-the-art smoothing model of Erk (2007). $$$$$ In a pseudo-disambiguation task the similarity-based model showed error rates down to 0.16, far lower than both EM-based clustering and Resnik's WordNet model.

The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ It is, however, more flexible in that it induces similarities from a separate generalization corpus, which allows us to control the similarities we compute by the choice of text domain for the generalization corpus.
The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ In SRL, the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntacticallybased systems, and (2) the problem of the domain dependence (Carreras and Marquez, 2005).
The Train size is approximately the same size used in Erk (2007), although on a different corpus. $$$$$ Examples of extracted (p, rp, w) tuples are (Morality evaluation, Evaluee, gamblers) and (Placing, Goal, briefcase).

These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ Rooth et al. (1999) generalize over seen headwords using EM-based clustering rather than WordNet.
These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ The most probable reason for this is the sparsity of the underlying vector space.
These results appear consistent with Erk (2007) because that work used the BNC corpus (the same size as one year of our data) and Erk chose confounders randomly within a broad frequency range. $$$$$ As can be seen, error rates reach a plateau at about 25 seen headwords for Jaccard.

Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ The annotated data for these 100 roles comprised 59,608 sentences, our primary corpus.
Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ This is especially relevant in view of the domain-dependence problem that SRL faces.
Similar to Erk (2007), we used an adapted version which we computed for semantic roles by means of the FN database rather than for verb argument positions. $$$$$ Examples of extracted (p, rp, w) tuples are (Morality evaluation, Evaluee, gamblers) and (Placing, Goal, briefcase).

Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ We contrast this with another condition, where we count a pair as covered if at least one of the two words w, w' is assigned a level of preference by a model (“half coverage”).
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ So the EM-based model tends to have preferences only for the “right” words.
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ The choice of similarity metric is critical in similarity-based models, with Jaccard and Lin achieving the best performance, and Cosine surprisingly bringing up the rear.
Erk et al (2010) propose the Exemplar-Based Model of Selectional Preferences, in turn based on Erk (2007). $$$$$ In a test set of pairs (rp, w), each headword w is paired with a confounder w' chosen randomly from the BNC according to its frequency4.

In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ In addition, the corpus for computing the similarity metrics can be freely chosen, allowing greater variation in the domain of generalization than a fixed lexical resource.
In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ All subsequent approaches have followed the same twostep procedure, first collecting argument headwords from a corpus, then generalizing over the seen headwords to similar words.
In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ Next steps will be to test the similarity-based model “in vivo”, in an SRL task; to test the model in a WSD task; to evaluate the model on a primary corpus that is not semantically analyzed, for greater comparability to previous approaches; to explore other vector spaces to address the coverage issue; and to experiment on domain transfer, using an appropriate generalization corpus to induce selectional preferences for a domain different from that of the primary corpus.
In (Erk, 2007) a number of SP models are tested in a pseudo-task related to SRL. $$$$$ However, the EM-based clustering model has higher coverage than both other paradigms.

 $$$$$ Lines (c) and (d), however, do just that.
 $$$$$ A cutoff was used in the similarity-based model: For each seen headword, only the 500 most similar words (according to a given similarity measure) were included in the computaof similar words; (b) % of times the more frequent word won; (c) number of distinct similar words per seen headword; (d) avg. size of intersection between roles tion; for all others, a similarity of 0 was assumed.
 $$$$$ Noun headwords are paired with noun confounders in order not to disadvantage Resnik's model, which only works with nouns.

 $$$$$ We focus on one application of selectional preferences: semantic role labeling.
 $$$$$ Let Seen(rp) be the set of seen headwords for an argument rp of a predicate p. Then we model the selectional preference S of rp for a possible headword w0 as a weighted sum of the similarities between w0 and the seen headwords: sim(w0, w) is the similarity between the seen and the potential headword, and wtrp(w) is the weight of seen headword w. Similarity sim(w0, w) will be computed on the generalization corpus, again on the basis of extracted tuples (p, rp, w).
 $$$$$ The EM-based clustering model was computed with all of the FrameNet 1.3 data (139,439 sentences) as input.

The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ For Jaccard similarity, the model had an error rate of 0.1858 for uniform weights, 0.1874 for frequency weighting, and 0.1806 for discriminativity.
The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ In evaluations the similarity-based model shows lower error rates than both Resnik's WordNet-based model and the EM-based clustering model, but has coverage problems.
The notion of selectional preference is not restricted to surface-level predicates such as verbs and modifiers, but also extends to semantic frames (Erk, 2007) and inference rules (Pantel et al, 2007). $$$$$ In a pseudo-disambiguation task the similarity-based model showed error rates down to 0.16, far lower than both EM-based clustering and Resnik's WordNet model.
