Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ In Figure 3, we plot the performance of the target-language parser on held-out bitext.
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ When a small number of target-language parse trees is available, projection gives a boost equivalent to doubling the number of target trees.
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ This meant that in the phrase das Kind (“the child”) in, say, subject position, das was the child of Kind; but, in f¨ur das Kind (“for the child”), das was the child of f¨ur.

 $$$$$ The parameters of the model are thus of the form where head and child are part-of-speech tags, dir E {left, right}, and adj, stop E {true, false}.
 $$$$$ What does this noise consist of?
 $$$$$ We connect two scenarios in structured parser trained on one corpus to another annotation style, and annotations from one to another.
 $$$$$ Our experiments show that unsupervised QG projection improves on parses trained using only highprecision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, learning an unsupervised parser from raw target-language text alone.

QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ Finally, we consider the problem of parser projection when some target language trees are available.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ Suppose we are given a sentence w' in the “source” language and its translation w into the “target” language.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ However, it was only 80.3% accurate when applied to our task, the English side of the ECTB.7 After parsing the source side of the bitext, we train a parser on the annotated target side, using QG features described above (§2).
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ Of course, we needed to filter more than 1000 sentences of bitext to output 1000 training sentences in this way.

Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ We propose quasigrammar features for these structured learning tasks.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ We can thus measure the contribution of adaptation loss alone, and the combined loss of imperfect source-domain parsing with adaptation (Table 1).
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ This approach (“hard projection with EM”), however, performed worse than using only the fully projected trees.

 $$$$$ These discouragingly low numbers led them to write languagespecific transformation rules to fix up the projected trees.
 $$$$$ When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.
 $$$$$ For these experiments, we used the LDC’s English-Chinese Parallel Treebank (ECTB).
 $$$$$ On the more difficult problem of cross-lingual parser projection, we learn a dependency parser for a target language by using bilingual text, an English parser, and automatic word alignments.

Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ The methods presented here move parser projection much closer in efficiency and simplicity to monolingual parsing.
Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.

Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: � wjgj(t,t0,a,w,w0) + j The features f look only at target words and dependencies.
Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ Finally, naturally occurring bitexts contain some number of free or erroneous translations.

Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ Rather than trying to adjudicate which dependencies are “mere” annotation conventions, it would be useful to test learned dependency models on some extrinsic task such as relation extraction or machine translation.
Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ First, we consider the problem of parser projection when there are zero target-language trees available.

 $$$$$ We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
 $$$$$ Similar features fire when x0 and y0 are in grandparent-grandchild, sibling, c-command, or none-of-the above relationships, or when y aligns to NULL.
 $$$$$ Say that we wish to output parses in the Prague style and so have annotated a small target corpus—e.g., 100 sentences—with those conventions.
 $$$$$ We trained an edge-factored dependency parser (McDonald et al., 2005) on “source” domain data that followed one set of dependency conventions.

Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.
Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ Since manual word alignments also exist for a part of this corpus, we were able to measure the loss in accuracy (if any) from the use of an automatic English parser and word aligner.
Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ To focus EM on modeling the syntactic relationships, they conditioned the prediction of w on almost complete knowledge of the lexical items.

Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ In order to benefit from the source language, we also need to include bilingual features g. When scoring a candidate target dependency link from word x → y, these features consider the relationship of their corresponding source words x0 and y0.
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ Finally, we consider the problem of parser projection when some target language trees are available.
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ We showed that augmenting a target parser with quasi-synchronous features can lead to significant improvements—first in experiments with adapting to different dependency representations in English, and then in cross-language parser projection.

 $$$$$ We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
 $$$$$ The score s of a given tuple of trees, words, and alignment can thus be written as a dot product of weights w with features f and g: � wjgj(t,t0,a,w,w0) + j The features f look only at target words and dependencies.
 $$$$$ One example would be adapting a constituency parser to produce dependency parses.
 $$$$$ If x0 = y0, i.e. x and y align to the same word, the same-word feature fires.
