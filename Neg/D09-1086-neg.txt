Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ There is clearly more information in these direct links than one would think from the F-scores.
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ In the German-English translation pair, with co-indexed words aligned, [an [den Libanon1]] denken2 H remember2 Lebanon1 we would prefer that the preposition an attach to denken, even though the preposition’s object Libanon aligns to a direct child of remember.
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ Our experiments show that unsupervised QG projection improves on parsers trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, unsupervised EM.
Following this idea, Ganchev et al (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results. $$$$$ To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.

 $$$$$ So far, this is very similar to the monolingual parser adaptation scenario, but there are a few key differences.
 $$$$$ Of course, we needed to filter more than 1000 sentences of bitext to output 1000 training sentences in this way.
 $$$$$ Since manual word alignments also exist for a part of this corpus, we were able to measure the loss in accuracy (if any) from the use of an automatic English parser and word aligner.
 $$$$$ At each step of generation, the DMV stochastically chooses whether to stop generating, conditioned on the currently generating head; whether it is generating to the right or left; and whether it has yet generated any children on that side.

QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ However, it was only 80.3% accurate when applied to our task, the English side of the ECTB.7 After parsing the source side of the bitext, we train a parser on the annotated target side, using QG features described above (§2).
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ It should be noted that this process introduced empty words into the projected target language tree and left words that are unaligned to English detached from the tree; as a result, they measured performance in dependency Fscore rather than accuracy.
QG has been applied to some NLP tasks other than MT, including answer selection for question-answering (Wang et al, 2007), paraphrase identification (Das and Smith, 2009), and parser adaptation and projection (Smith and Eisner, 2009). $$$$$ For these experiments, we used the LDC’s English-Chinese Parallel Treebank (ECTB).

Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ The source parser outputs were produced for all target data, both training and test, so that features for the target parser could refer to them.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Similar features fire when x0 and y0 are in grandparent-grandchild, sibling, c-command, or none-of-the above relationships, or when y aligns to NULL.
Indeed, the QG formalism has been previously applied to parser adaptation and projection (Smith and Eisner, 2009), paraphrase identification (Das and Smith, 2009), question answering (Wang et al, 2007), and title generation (Woodsend et al, 2010). $$$$$ Both the monolingual target-language parser and the projected parsers are trained to optimize conditional likelihood of the target trees t' with ten iterations of stochastic gradient ascent.

 $$$$$ They also used human-produced word alignments.
 $$$$$ We would also like to thank Noah Smith and Rebecca Hwa for helpful discussions and the anonymous reviewers for their suggestions for improving the paper.
 $$$$$ (For example, two nodes can be aligned only if their respective parents are also aligned.)
 $$$$$ What should our model of source and target trees look like?

Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ We showed that augmenting a target parser with quasi-synchronous features can lead to significant improvements—first in experiments with adapting to different dependency representations in English, and then in cross-language parser projection.
Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ That is, we score a aligned pair of source and target trees based on local features of the trees and the alignment.
Another projection based system is that of Smith and Eisner (2009), who report results for German (68.5%) and Spanish (64.8%) on sentences of length 15 and less inclusive of punctuation. $$$$$ They also used human-produced word alignments.

Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ When a few target-language parse trees are available, projection gives a boost equivalent to doubling the number of target-language trees. first author would like to thank the Center for Intelligent Information Retrieval at UMass Amherst.
Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ We will then compare three approaches (§5.3): §5.3.2 a straight EM baseline (which does not condition on t', w' at all) §5.3.3 a “hard” projection baseline (which naively projects t', w' to derive direct supervision in the target language) §5.3.4 our conditional EM approach above (which makes t', w' available to the learner for “soft” indirect supervision via QG) Our base models of target-language syntax are generative dependency models that have achieved state-of-the art results in unsupervised dependency structure induction.
Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ As in the adaptation case (§3), we train a conditional model (not a generative DMV) of the target tree given the target sentence, using the monolingual and bilingual QG features, including configurations conjoined with tags, outlined above (§2).
Smith and Eisner (2009) and Li et al (2012) generated rich quasi synchronous grammar features to improve parsing performance. $$$$$ In that paper, QG was applied to word alignment and has since found applications in question answering (Wang et al., 2007), paraphrase detection (Das and Smith, 2009), and machine translation (Gimpel and Smith, 2009).

Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ If, on the other hand, y0 → x0 E t0, a head-swapping feature fires.
Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ We have concentrated here on adapting between two dependency parse styles, in order to line up with the cross-lingual tasks to which we now turn.
Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ In German coordinations, the coordinands all attach to the first, but in English, they all attach to the last.
Smith and Eisner (2009) think of cross-language adaptation as unsupervised projection using word aligned parallel text to construct training material for the target language. $$$$$ Although projection performance is, not surprisingly, better if we know the true source trees at training and test time, even with the 1-best output of the source parser, QG features help produce a parser as accurate asq one trained on twice the amount of monolingual data.

 $$$$$ Although we could also try many random starting points, the initializer in Klein and Manning (2004) performs quite well.
 $$$$$ Our experiments show that unsupervised QG projection improves on parsers trained using only high-precision projected annotations and far outperforms, by more than 35% absolute dependency accuracy, unsupervised EM.
 $$$$$ When training is conditioned on the target words (see §3 and §6 below), we conjoin these configuration features with the part of speech and coarse part of speech of one or both of the source and target words, i.e. the feature template has from one to four tags.

Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ Training a target-domain parser on as few as 10 sentences shows substantial improvements in accuracy.
Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ To test our handling of QG features, we performed experiments in which training saw the correct parse trees in both source and target domains, and the mapping between them was simple and regular.
Smith and Eisner (2009) propose effective QG features for parser adaptation and projection. $$$$$ In our QG experiments, therefore, we started with a bias towards direct parent–child links and a very small probability for breakages of locality.

Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ The source parser outputs were produced for all target data, both training and test, so that features for the target parser could refer to them.
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ Hwa et al. (2005) have called this intuition the Direct Correspondence Assumption (DCA, with slight notational changes): Given a pair of sentences w and w' that are translations of each other with syntactic structure t and t', if nodes x' and y' of t' are aligned with nodes x and y of t, respectively, and if syntactic relationship R(x', y') holds in t', then R(x, y) holds in t. The validity of this assumption clearly depends on the node-to-node alignment of the two trees.
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ For many target languages, however, we do not have the luxury of a large parsed “source corpus” in the language, even one in a different style or domain as above.
Smith and Eisner (2009) perform dependency projection and annotation adaptation with quasi-synchronous grammar features. $$$$$ The loss in performance from conditioning only on noisy 1-best source parses points to some natural avenues for improvement.

 $$$$$ In ablation experiments, we included bilingual features only for directly projected links, with no features for head-swapping, grandparents, etc.
 $$$$$ This is the only supervised data we used in the target.
 $$$$$ Note that the penalty for using automatic alignments instead of gold alignments is negligible; in fact, using Source text alone is often higher than +Gold alignments.
 $$$$$ We trained an edge-factored dependency parser (McDonald et al., 2005) on “source” domain data that followed one set of dependency conventions.
