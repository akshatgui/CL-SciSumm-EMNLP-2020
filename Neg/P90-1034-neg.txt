Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. $$$$$ We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample.
Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. $$$$$ We use the observed frequencies to derive a cooccurrence score Cab; (an estimate of mutual information) defined as follows. where fin v) is the frequency of noun n occurring as object of verb v, fin) is the frequency of the noun n occurring as argument of any verb, f(v) is the frequency of the verb v, and N is the count of clauses in the sample.
Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. $$$$$ Sample size.
Hindle (1990) used noun-verb syntactic relations, and Hatzivassiloglou and McKeown (1993) used coordinated adjective-adjective modifier pairs. $$$$$ To capture this intuition, we turn, following Church and Hanks (1989), to &quot;mutual information&quot; (see Fano 1961).

Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. $$$$$ Of course, not all nouns fall into such neat clusters: Table 6 shows a quite heterogeneous group of nouns similar to table, though even here the most similar word (floor) is plausible.
Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. $$$$$ A number of issues will have to be confronted to further exploit these structurallymediated lexical constraints, including: Potysemy.
Hindle (1990) uses a mutual-information based metric derived from the distribution of subject, verb and object in a large corpus to classify nouns. $$$$$ For any of verb in the sample, we can ask what nouns it has as subjects or objects.

The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). $$$$$ The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). $$$$$ The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). $$$$$ For this sample, 36 had a reciprocal nearest neighbor.
The second class of algorithms uses co occurrence statistics (Hindle 1990, Lin 1998). $$$$$ Despite these errors, the analysis is succeeds in discovering a number of the correct predicate-argument relations.

NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ Table 2 shows the objects of the verb drink that occur (more than once) in the sample, in effect giving the answer to the question &quot;what can you drink?&quot; This list of drinkable things is intuitively quite good.
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
NLP researchers have developed many algorithms for mining knowledge from text and the Web, including facts (Etzioni et al 2005), semantic lexicons (Riloff and Shepherd 1997), concept lists (Lin and Pantel 2002), and word similarity lists (Hindle 1990). $$$$$ Some we recognize as synonyms in news reporting style: explosion - blast, bomb - device, tie - relation.

To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994). $$$$$ This is a rather stringent definition; under this definition, boat and ship do not qualify because, while ship is the most similar to boat, the word most similar to ship is not boat but plane (boat is second).
To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994). $$$$$ In the subject-verb-object table, the root form of the head of phrases is recorded, and the deep subject and object are used when available.
To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994). $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
To extract semantic information of words such as synonyms and antonyms from corpora, previous research used syntactic structures (Hindle 1990, Hatzivassiloglou 1993 and Tokunaga 1995), response time to associate synonyms and antonyms in psychological experiments (Gross 1989), or extracting related words automatically from corpora (Grefensette 1994). $$$$$ Further analysis.

 $$$$$ Each noun may therefore be characterized according to the verbs that it occurs with.
 $$$$$ Not all nouns are equally contentful.
 $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
 $$$$$ For any of verb in the sample, we can ask what nouns it has as subjects or objects.

 $$$$$ The parsing errors that do occur seem to result, for the current purposes, in the omission of predicate-argument relations, rather than their misidentification.
 $$$$$ (March 22 1987) The parser aims to be non-committal when it is unsure of an analysis.

Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. $$$$$ The verb drink is the key common factor.
Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. $$$$$ The reason is clear: section is semantically a rather empty word, and the selectional restrictions on its cooccurence depend primarily on its complement.
Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. $$$$$ So for example, a car is an object that you can have and sell, like wine and beer, but you do not — in this sample (confirming what we know from the meanings of the words) -typically drink a car.
Our method is similar to (Hindle, 1990), (Lin, 1998), and (Gasperin, 2001) in the use of dependency relationships as the word features. $$$$$ Nouns may then be grouped according to the extent to which they appear in similar environments.

Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ On the basis of a six million word sample of Associated Press news stories, a classification of nouns was developed according to the predicates they occur with.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ The first column lists the noun which is similar to boat.

 $$$$$ Sample size.
 $$$$$ It will be useful to extend the analysis presented here to other kinds of relationships, including more complex kinds of verb complementation, noun complementation. and modification both preceding and following the head noun.
 $$$$$ Table 5 shows the ten nouns most similar to legislator, again a fairly coherent set.

The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). $$$$$ (It may also skew the sample to the extent that the parsing errors are consistent.)
The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). $$$$$ Therefore, it is useful to have a classification of words into semantically similar sets.
The method of using distributional patterns in a large text corpus to find semantically related English nouns first appeared in Hindle (1990). $$$$$ The parsing errors that do occur seem to result, for the current purposes, in the omission of predicate-argument relations, rather than their misidentification.

Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. $$$$$ This non-committal approach simply reduces the effective size of the sample.
Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. $$$$$ This demonstration has depended on: 1) the availability of relatively large text corpora; 2) the existence of parsing technology that, despite a large error rate, allows us to find the relevant syntactic relations in unrestricted text; and 3) (most important) the fact that the lexical relations involved in the distribution of words in syntactic structures are an extremely strong linguistic constraint.
Hindle (1990) grouped nouns into thesaurus-like lists based on the similarity of their syntactic con texts. $$$$$ For example, section is a general word that can refer to sections of all sorts of things.

The only difference is that we also work on partial parsing as a task in its own right: Hindle (1990) inter alia. $$$$$ Of these, ship shares 25 common environments (ship also occurs in many other unshared environments).
The only difference is that we also work on partial parsing as a task in its own right: Hindle (1990) inter alia. $$$$$ Their predicate-argument information may be coded as a table of 5-tupks, consisting of verb, surface subject, surface object, underlying subject, underlying object, as shown in Table 1.
The only difference is that we also work on partial parsing as a task in its own right: Hindle (1990) inter alia. $$$$$ Of these, ship shares 25 common environments (ship also occurs in many other unshared environments).

The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). $$$$$ The verb drink is the key common factor.
The classifier for learning coordinate terms relies on the notion of distributional similarity, i.e., the idea that two words with similar meanings will be used in similar contexts (Hindle, 1990). $$$$$ The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.

Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. $$$$$ In sentence (1), six clauses are found.
Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. $$$$$ We need, in further work, to explore both automatic and supervised means of discriminating the semantically relevant associations from the spurious. hide beneath„ convolute, memorize_, sit at, sit across_, redo_, structure_, sit around, litter_, _carry, lie on_, go from_, _hold, wait_, come to_, return to, turn_, approach_, cover, be on_, share_, publish_, claim_, mean_, go to_, raise, leave_, -have_, do_, be litter, lie on, cover_, be on_, come to_, go to_ _carry, be on_, cover, return to_, tum_, go to_, leave_, -have_ approach_, return to_, mean_, go to_, be on_, tum_, come to_, leave, do_, be go from_, come to, return to_, claim_, go to, -have_, do_ structure, share_, claim_, publish_, be sit across_, mean_, be on, leave_ litter_, approach_, go to_, return to_, come to_, leave_ lie on_, be on, go to_, _hold, -have_, cover_, leave_, come to_ go from_, come to_, cover_, return to, go to_, leave_, -have_ return to, claim_, come to_, go to, cover, leave_ Reciprocally most similar nouns We can define &quot;reciprocally most similar&quot; nouns or &quot;reciprocal nearest neighbors&quot; (RNN) as two nouns which are each other's most similar noun.
Our syntactic-relation-based thesaurus is based on the method proposed by Hindle (1990), although Hindle did not apply it to information retrieval. $$$$$ The stumbling block to any automatic use of distributional patterns has been that no sufficiently robust syntactic analyzer has been available.

 $$$$$ But in expanding the number of different structural relations noted, it may become less useful to compute a single-dimensional similarity score of the sort proposed in Section 4.
 $$$$$ Despite these errors, the analysis is succeeds in discovering a number of the correct predicate-argument relations.
 $$$$$ The list in Table 7 shows quite a good set of substitutable words, many of which are near synonyms.
 $$$$$ Their predicate-argument information may be coded as a table of 5-tupks, consisting of verb, surface subject, surface object, underlying subject, underlying object, as shown in Table 1.

In (Hindle, 1990), a small set of sample results are presented. $$$$$ More is to be learned from the fact that you can drink wine than from the fact that you can drink it even though there are more clauses in our sample with it as an object of drink than with wine.
In (Hindle, 1990), a small set of sample results are presented. $$$$$ Rather, the various lexical relations revealed by parsing a corpus, will be available to be combined in many different ways yet to be explored.
In (Hindle, 1990), a small set of sample results are presented. $$$$$ Sparck Jones (1986) takes a similar view.

Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ A variety of linguistic relations apply to sets of semantically similar words.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ Thus, of the ten nouns most similar to boat (Table 4), nine are words for vehicles; the most similar noun is the near-synonym ship.
Hindle (1990) classified nouns on the basis of co-occurring patterns of subject verb and verb-object pairs. $$$$$ To capture this intuition, we turn, following Church and Hanks (1989), to &quot;mutual information&quot; (see Fano 1961).

For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. $$$$$ A method of determining the similarity of nouns on the basis of a metric derived from the distribution of subject, verb and object in a large text corpus is described.
For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. $$$$$ The objects in Table 2 are ranked not by raw frequency, but by a cooccurrence score listed in the last column.
For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. $$$$$ (March 22 1987) The parser aims to be non-committal when it is unsure of an analysis.
For example, Hindle (1990) used co-occurrences between verbs and their subjects and objects, and proposed a similarity metric based on mutual information, but no exploration concerning the effectiveness of other kinds of word relationship is provided, although it is extendable to any kinds of contextual information. $$$$$ This reduction is useful in that it permits, for example, a clustering analysis of the nouns in the sample, and for some purposes (such as demonstrating the plausibility of the distribution-based metric) such clustering is useful.

To date, researchers have harvested, with varying success, several resources, including concept lists (Lin and Pantel 2002), topic signatures (Lin and Hovy 2000), facts (Etzioni et al 2005), and word similarity lists (Hindle 1990). $$$$$ The list in Table 7 shows quite a good set of substitutable words, many of which are near synonyms.
