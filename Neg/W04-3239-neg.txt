They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ We think that it is more useful to propose a learning algorithm that can automatically capture relevant structural information observed in text, rather than to heuristically add this information as new features.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ Accordingly, learning algorithms must be created that can handle the structures observed in texts.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ The difference between them can be explained by sparseness.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ Definition 3 Decision Stumps for Trees Let t and x be labeled ordered trees, and y be a class label (y ∈ {±1}), a decision stump classifier for trees is given by The parameter for classification is the tuple ht, yi, hereafter referred to as the rule of the decision stumps.

The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ Figure 4 presents a pseudo code of the algorithm Find Optimal Rule.
The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.
The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ The focused problem can be formalized as a general problem, called the tree classification problem.
The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ Table 1 summarizes the results of PHS and MOD tasks.

In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ However, we argue that Boosting has the following practical advantages.
In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ However, it makes little difference since a given tree is often sparse in NLP and the cardinality of substructures will be approximated by their existence.
In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.
In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ In our experiments, n-gram features showed comparable performance to dependency features.

For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). $$$$$ Boosting, in contrast, runs faster, since the complexity depends only on the small number of decision stumps.
For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). $$$$$ The l1-norm margin allows us to realize this property.

Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ In our experiments, n-gram features showed comparable performance to dependency features.
Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ Note that word sequence, base-phrase annotation, dependency tree and an XML document can be modeled as a labeled ordered tree.
Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ Let us introduce a labeled ordered tree (or simply tree), its definition and notations, first.
Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ Examples that have non-zero coefficient are called support vectors that form the final solution.

posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ We here use Arc-GV (Breiman, 1999) instead of AdaBoost, since Arc-GV asymptotically maximizes the margin and shows faster convergence to the optimal solution than AdaBoost.
posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ Boosting repeatedly calls a given weak learner to finally produce hypothesis f, which is a linear combination of K hypotheses produced by the prior weak learners, i,e.
posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ The difference between them can be explained by sparseness.
posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ In our experiments, n-gram features showed comparable performance to dependency features.

Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ Rightmost extension defines a canonical search space in which one can enumerate all subtrees from a given set of trees.
Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ The tree classification problem is to induce a mapping f(x) : X → {±1}, from given training examples T = {hxi, yii}Li=1, where xi ∈ X is a labeled ordered tree and yi ∈ {±1} is a class label associated with each training data (we focus here on the problem of binary classification.).
Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ Boostexter (Schapire and Singer, 2000) uses word-based decision stumps for topic-based text classification.

This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ In our experiments, n-gram features showed comparable performance to dependency features.
This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ In all tasks and categories, our subtree-based Boosting algorithm (dep/ngram) performs better than the baseline method (bow).
This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ The accuracies of these two methods depends on the given training data.
This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.

To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ During the traverse of the subtree lattice built by the recursive process of rightmost extension, we always maintain the temporally suboptimal gain τ among all gains calculated previously.
To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ One natural extension is to adopt confidence rated predictions to the subtree-based weak learners.
To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002).
To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ We denote the number of nodes in t as |t|.

Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ Accordingly, learning algorithms must be created that can handle the structures observed in texts.
Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ However, it makes little difference since a given tree is often sparse in NLP and the cardinality of substructures will be approximated by their existence.
Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ To classify trees, we here extend the decision stump definition as follows.
Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ The complexity of SVMs with tree kernel is O(L'|N1||N2|), where N1 and N2 are trees, and L' is the number of support vectors, which is too heavy to realize real applications.

To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ We also discuss the relation between our algorithm and SVMs with tree kernel.
To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ Two experiments on opinion/modality classification confirm that subtree features are important.
To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ This setting yields a fair comparison in terms of feature space.
To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ The complexity of SVMs with tree kernel is O(L'|N1||N2|), where N1 and N2 are trees, and L' is the number of support vectors, which is too heavy to realize real applications.

 $$$$$ argument: T = {hx1, y1, d1i ..., hxL, yL, dLi} (xi a tree, yi ∈ {±1} is a class, and di (PLi=1 di = 1, di ≥ 0) is a weight) returns: Optimal rule hˆt, ˆyi begin We can thus see that both algorithms are essentially the same in terms of their feature space.
 $$$$$ Text classification plays an important role in organizing the online texts available on the World Wide Web, Internet news, and E-mails.
 $$$$$ The important characteristic is that the input example xi is represented not as a numerical feature vector (bagof-words) but a labeled ordered tree.

An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ : A weak learner is built at each iteration k with different distributions or weights d(k) = (d(k) The weights are calculated in such a way that hard examples are focused on more than easier examples.
An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ The two pruning are marked with (1) and (2) respectively.
An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ In our experiments, n-gram features showed comparable performance to dependency features.

To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). $$$$$ We thus adopt an alternative strategy to avoid such exhaustive enumeration.
To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). $$$$$ Accordingly, learning algorithms must be created that can handle the structures observed in texts.
To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). $$$$$ The concept behind Boosting is that only a few hypotheses are needed to express the final solution.

Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ This extension is also found in BoosTexter and shows better performance than binary-valued learners.
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ We would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications (e.g., parse re-ranking (Collins and Duffy, 2002) and information extraction).
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ In this paper, we propose a Boosting algorithm that captures sub-structures embedded in texts.
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ Accordingly, learning algorithms must be created that can handle the structures observed in texts.

These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ Two experiments on opinion/modality classification tasks confirmed that subtree features are important.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ This problem is formally defined as follows.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ Figure 1 shows an example of a labeled ordered tree and its subtree and non-subtree.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples.
