They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ In this paper, we focused on an algorithm for the classification of semi-structured text in which a sentence is represented as a labeled ordered tree7.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ The unit of classification is a sentence.
They adopted the BACT learning algorithm (Kudo and Matsumoto, 2004) to effectively learn subtrees useful for both antecedent identification and zero pronoun detection. $$$$$ Let us introduce a labeled ordered tree (or simply tree), its definition and notations, first.

The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ This is because each word occurring in the text is highly relevant to the predefined “topics” to be identified.
The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ Ignoring structural information embedded in text, we simply represent a text as a set of words.
The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). $$$$$ We would like to apply our method to other applications where instances are represented in a tree and their subtrees play an important role in classifications (e.g., parse re-ranking (Collins and Duffy, 2002) and information extraction).

In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ The accuracies of these two methods depends on the given training data.
In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ In the traditional text classification tasks, one has to identify predefined text “topics”, such as politics, finance, sports or entertainment.
In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004). $$$$$ In this section, we introduce an efficient and practical algorithm to find the optimal rule (ˆt, ˆy) from given training data.

For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). $$$$$ Table 2 shows examples of extracted support features (pairs of feature (tree) t and weight wt in (Eq.
For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). $$$$$ However, accuracies can be boosted by the Boosting algorithm (Freund and Schapire, 1996; Schapire and Singer, 2000).
For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). $$$$$ However, it makes little difference since a given tree is often sparse in NLP and the cardinality of substructures will be approximated by their existence.
For this problem, we can apply a boosting technique presented in (Kudo and Matsumoto, 2004). $$$$$ argument: T = {hx1, y1, d1i ..., hxL, yL, dLi} (xi a tree, yi ∈ {±1} is a class, and di (PLi=1 di = 1, di ≥ 0) is a weight) returns: Optimal rule hˆt, ˆyi begin We can thus see that both algorithms are essentially the same in terms of their feature space.

Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002).
Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ From these points of view, this paper proposes a classification algorithm that captures sub-structures embedded in text.
Note that for simplicity we use bag-of-functional words and their part-of-speech intervening between a zero-pronoun and its candidate antecedent as features instead of learning syntactic patterns with the Bact algorithm (Kudo and Matsumoto, 2004). $$$$$ A straightforward way to extend the traditional bag-of-words representation is to heuristically add new types of features to the original bag-of-words features, such as fixed-length n-grams (e.g., word bi-gram or tri-gram) or fixedlength syntactic relations (e.g., modifier-head relations).

posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ The decision stumps are trained to find rule hˆt, ˆyi that minimizes the error rate for the given training data T = {hxi, yii}Li=1: In this paper, we will use gain instead of error rate for clarity.
posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ The following theorem, an extension of Morhishita (Morhishita, 2002), gives a convenient way of computing a tight upper bound on gain(ht', yi) for any super-tree t' of t. We can efficiently prune the search space spanned by right most extension using the upper bound of gain u(t).
posed by Kudo and Matsumoto (2004) is designed to learn subtrees useful for classification. $$$$$ We say t' is a rightmost extension of t, if and only if t and t' satisfy the following three conditions: Consider Figure 2, which illustrates example tree t with the labels drawn from the set G = {a, b, c}.

Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ Maximizing l2norm margin gives a sparse solution in the example space, (i.e., most of Ai becomes 0).
Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ Two experiments on opinion/modality classification confirm that subtree features are important.
Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ Our proposal consists of i) decision stumps that use subtrees as features and ii) Boosting algorithm in which the subtree-based decision stumps are applied as weak learners.
Therefore, in order to identify word dependencies, we followed Kudo' s rule (Kudo and Matsumoto, 2004) the original sentence. $$$$$ Rightmost extension defines a canonical search space in which one can enumerate all subtrees from a given set of trees.

This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ Boosting thus finds a sparse solution in the feature space.
This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ In the traditional text classification tasks, one has to identify predefined text “topics”, such as politics, finance, sports or entertainment.
This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ The difference between them is the metric of margin; the margin of Boosting is measured in l1-norm, while, that of SVMs is measured in l2-norm.
This may be accomplished with SVMs 131 using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004). $$$$$ In this paper, we focused on an algorithm for the classification of semi-structured text in which a sentence is represented as a labeled ordered tree7.

To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ The tree classification problem is to induce a mapping f(x) : X → {±1}, from given training examples T = {hxi, yii}Li=1, where xi ∈ X is a labeled ordered tree and yi ∈ {±1} is a class label associated with each training data (we focus here on the problem of binary classification.).
To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ It is difficult to give such analysis with kernel methods, since they define the feature space implicitly.
To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ In this paper, we focused on an algorithm for the classification of semi-structured text in which a sentence is represented as a labeled ordered tree7.
To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004). $$$$$ We can also prune the space with respect to the expanded single node s. Even if µ(t) ≥ τ and a node s is attached to the tree t, we can ignore the space spanned from the tree t' if µ(s) < τ, since no super-tree of s can yield optimal gain.

Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ Two experiments on opinion/modality classification tasks confirmed that subtree features are important.
Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ Accordingly, learning algorithms must be created that can handle the structures observed in texts.
Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ This extension is also found in BoosTexter and shows better performance than binary-valued learners.
Even though the approach in Kudo and Matsumoto (2004) and ours are similar, there are two clear distinctions. $$$$$ The difference between them is the metric of margin; the margin of Boosting is measured in l1-norm, while, that of SVMs is measured in l2-norm.

To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ The proposal consists of i) decision stumps that use subtrees as features and ii) the Boosting algorithm which employs the subtree-based decision stumps as weak learners.
To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ In the PHS task, the speeds of Boosting and SVMs are 0.531 sec./5,741 instances and 255.42 sec./5,741 instances respectively 6.
To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ We here describe a connection between our Boosting algorithm and SVMs with tree kernel (Collins and Duffy, 2002; Kashima and Koyanagi, 2002).
To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data{ (x i, y i)}. $$$$$ Second, sparse hypotheses are useful in practice as they provide “transparent” models with which we can analyze how the model performs or what kind of features are useful.

 $$$$$ Unfortunately, the latter goals exceed the ability of the traditional bag-of-word representation approach, and a richer, more structural representation is required.
 $$$$$ Note that word sequence, base-phrase annotation, dependency tree and an XML document can be modeled as a labeled ordered tree.
 $$$$$ Since there are three nodes on the rightmost path and the size of the label set is 3 (= |G|), a total of 9 trees are enumerated from the original tree t. Note that rightmost extension preserves the prefix ordering of nodes in t (i.e., nodes at positions 1..|t |are preserved).

An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples.
An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ In our experiments, n-gram features showed comparable performance to dependency features.
An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ Two experiments on opinion/modality classification tasks confirmed that subtree features are important.
An example of a word-polarity lattice Various methods have already been proposed for sentiment polarity classification, ranging from the use of co-occurrence with typical positive and negative words (Turney, 2002) to bag of words (Pang et al, 2002) and dependency structure (Kudo and Matsumoto, 2004). $$$$$ Two experiments on opinion/modality classification tasks confirmed that subtree features are important.

To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). $$$$$ We here consider an upper bound of the gain that allows subspace pruning in this canonical search space.
To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). $$$$$ We thus adopt an alternative strategy to avoid such exhaustive enumeration.
To solve the problem efficiently, we now adopt a variant of the branch-and-bound algorithm, similar to that described in (Kudo and Matsumoto, 2004). $$$$$ Boostexter (Schapire and Singer, 2000) uses word-based decision stumps for topic-based text classification.

Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ To use the decision stumps as the weak learner of Boosting, we redefine the gain function (2) as follows: There exist many Boosting algorithm variants, however, the original and the best known algorithm is AdaBoost (Freund and Schapire, 1996).
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ We also discuss the relation between our algorithm and SVMs with tree kernel.
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ We can also prune the space with respect to the expanded single node s. Even if µ(t) ≥ τ and a node s is attached to the tree t, we can ignore the space spanned from the tree t' if µ(s) < τ, since no super-tree of s can yield optimal gain.
Among various classifier induction algorithms for tree-structured data, in our experiments, we have so far examined Kudo and Matsumoto (2004)'s algorithm, packaged as a free software named BACT. $$$$$ Tree kernel is one of the convolution kernels, and implicitly maps the example represented in a labeled ordered tree into all subtree spaces.

These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ Recent studies (Breiman, 1999; Schapire et al., 1997; R¨atsch et al., 2001) have shown that both Boosting and SVMs (Boser et al., 1992) have a similar strategy; constructing an optimal hypothesis that maximizes the smallest margin between the positive and negative examples.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ Boosting repeatedly calls a given weak learner to finally produce hypothesis f, which is a linear combination of K hypotheses produced by the prior weak learners, i,e.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ The algorithm, rightmost extension, avoids such duplicated enumerations by restricting the position of attachment.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm (Kudo and Matsumoto, 2004). $$$$$ First, the algorithm starts with a set of trees consisting of single nodes, and then expands a given tree of size (k − 1) by attaching a new node to this tree to obtain trees of size k. However, it would be inefficient to expand nodes at arbitrary positions of the tree, as duplicated enumeration is inevitable.
