Stevenson and Wilks (2001) presented a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999). $$$$$ It remains unclear exactly what it means to optimize the combination of modules within a learning system like TiMBL: we could, in further work, treat the part-of-speech tagger as a partial tagger and not a filter, and we could allow the system to learn some &quot;optimal&quot; weighting of all the partial taggers.
Stevenson and Wilks (2001) presented a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999). $$$$$ If the collocation occurs in the text being disambiguated, then it is assumed that the words it contains are being used in the same senses as were assigned manually.
Stevenson and Wilks (2001) presented a classifier combination framework where disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al, 1999). $$$$$ It also remains an interesting question whether, because of the undoubted existence of novel senses in text, a sense tagger can ever reach the level that part-of-speech tagging has.

Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. $$$$$ This paper reported a system which disambiguated all content words in a text, as defined by a standard machine readable dictionary, with a high degree of accuracy.
Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. $$$$$ Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky (1995) and Schtitze (1992) who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, namely interest.
Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. $$$$$ It seems likely that these changes are the source of the improved results.
Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. $$$$$ Gillian Callaghan was extremely helpful in the preparation of the final version of this paper.

Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. $$$$$ Our application filtered out senses with the incorrect part of speech in addition to using a different method to calculate overlap that takes account of short definitions.
Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. $$$$$ Any errors are our own.
Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. $$$$$ The average polysemy for our evaluation corpus is 14.62.
Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. $$$$$ Our evaluation shows that disambiguation can be carried out with more accurate results when several knowledge sources are combined.

However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process. $$$$$ Resnik and Yarowsky (1997) noted that, for the most part, part-of-speech tagging is tackled using the noisy channel model, although transformation rules and grammaticostatistical methods have also had some success.
However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process. $$$$$ Word sense disambiguation (WSD) is a computational linguistics task likely to benefit from the tradition of combining different knowledge sources in artificial intelligence research.
However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process. $$$$$ Both Ng and Lee (1996) and Yarowsky (1993) reported some results in the area.
However, they are quite rare, even in monolingual contexts (Stevenson and Wilks, 2001, e.g.), and they are not able to integrate and use knowledge coming from corpus and other resources during the learning process. $$$$$ For the partial tagger based on Yarowsky's subject-code algorithm, we choose the sense with the highest saliency value.

 $$$$$ The work described here was supported by the European Union Language Engineering project ECRAN â€” Extraction of Content: Research at Near-market (LE-2110).
 $$$$$ For symbolic features, the unweighted distance is 0 if they are identical, and 1 otherwise.
 $$$$$ Gillian Callaghan was extremely helpful in the preparation of the final version of this paper.
 $$$$$ In Section 3 we discuss the relationship between semantic disambiguation and part-of-speech tagging, reporting an experiment which quantifies the connection.

This is also shown by Stevenson and Wilks (2001), who used the Longman Dictionary of Contemporary English (LDOCE) as sense inventory. $$$$$ Any errors are our own.
This is also shown by Stevenson and Wilks (2001), who used the Longman Dictionary of Contemporary English (LDOCE) as sense inventory. $$$$$ It is argued that this approach is more likely to assist the creation of practical systems.
This is also shown by Stevenson and Wilks (2001), who used the Longman Dictionary of Contemporary English (LDOCE) as sense inventory. $$$$$ Sense tagger architecture.
This is also shown by Stevenson and Wilks (2001), who used the Longman Dictionary of Contemporary English (LDOCE) as sense inventory. $$$$$ Any errors are our own.

 $$$$$ Using the evaluation procedure described in the previous section, it was found that the system correctly disambiguated 90% of the ambiguous instances to the fine-grained sense level, and in excess of 94% to the homograph level.
 $$$$$ The second partial tagger based on an existing approach is the one which uses simulated annealing to optimize the overlap of words shared by the dictionary definitions for a set of senses.
 $$$$$ The authors attribute the lower performance on the Brown Corpus to the wider variety of text types it contains.

However, there has been no direct comparison of which knowledge sources are the most useful or whether combining a variety of knowledge sources, a strategy which has been shown to be successful for WSD in the general domain (Stevenson and Wilks, 2001), improves results. $$$$$ An important step in the exploration of this hypothesis is to determine which linguistic knowledge sources are most useful and whether their combination leads to improved results.
However, there has been no direct comparison of which knowledge sources are the most useful or whether combining a variety of knowledge sources, a strategy which has been shown to be successful for WSD in the general domain (Stevenson and Wilks, 2001), improves results. $$$$$ The approach was made practical by Cowie, Guthrie, and Guthrie (1992) (see also (Wilks, Slator, and Guthrie 1996)).
However, there has been no direct comparison of which knowledge sources are the most useful or whether combining a variety of knowledge sources, a strategy which has been shown to be successful for WSD in the general domain (Stevenson and Wilks, 2001), improves results. $$$$$ This system currently incorporates a single filter (part-of-speech filter), three partial taggers (simulated annealing, subject codes, selectional restrictions) and a single feature extractor (collocation extractor).
However, there has been no direct comparison of which knowledge sources are the most useful or whether combining a variety of knowledge sources, a strategy which has been shown to be successful for WSD in the general domain (Stevenson and Wilks, 2001), improves results. $$$$$ We are grateful for the feedback from many colleagues in Sheffield, especially Mark Hepple, and for the detailed comments from the anonymous reviewers of an earlier version of this paper.

 $$$$$ We have already examined the usefulness of part-of-speech tags for semantic disambiguation in Section 3.
 $$$$$ Among the researchers mentioned above, one must distinguish between, on the one hand, supervised approaches that are inherently limited in performance to the words over which they evaluate because of limited training data and, on the other hand, approaches whose unsupervised learning methodology is applied to only small numbers of words for evaluation, but which could in principle have been used to tag all content words in a text.
 $$$$$ Previously reported WSD systems that enjoyed a high level of accuracy have often operated on restricted vocabularies and employed a single WSD methodology.
 $$$$$ At the homograph level, the number correctly disambiguated to the homograph is divided by the number which are polyhomographic.

In the hybrid approaches that have been explored so far, deep knowledge, like selectional preferences, is either pre-processed into a vector representation to accommodate machine learning algorithms, or used in previous steps to filter out possible senses e.g. (Stevenson and Wilks, 2001). $$$$$ Roget is a thesaurus, so each entry in the lexicon belongs to one of the large categories; but over half (56%) of the senses in LDOCE are not assigned a primary code.
In the hybrid approaches that have been explored so far, deep knowledge, like selectional preferences, is either pre-processed into a vector representation to accommodate machine learning algorithms, or used in previous steps to filter out possible senses e.g. (Stevenson and Wilks, 2001). $$$$$ This is because the sense discrimination of adjectives is carried out after that for nouns in our algorithm (see Section 4.4), and the former is hindered by the low results of the latter.
In the hybrid approaches that have been explored so far, deep knowledge, like selectional preferences, is either pre-processed into a vector representation to accommodate machine learning algorithms, or used in previous steps to filter out possible senses e.g. (Stevenson and Wilks, 2001). $$$$$ A comprehensive review of WSD is beyond the scope of this paper but may be found in Ide and Veronis (1998).

We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). $$$$$ In our evaluation corpus, the most frequent ambiguous type is have, which appears 604 times.
We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). $$$$$ However, LDOCE has a higher level of average ambiguity and does not contain as complete a thesaural hierarchy as Roget, so we would not expect such good results when the algorithm is adapted to LDOCE.
We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). $$$$$ The translation contains only 36,869 words tagged with LDOCE senses; however, this is a reasonable size for an evaluation corpus for the task, and it is several orders of magnitude larger than those used by other researchers working in large vocabulary WSD, for example Cowie, Guthrie, and Guthrie (1992), Harley and Glennon (1997), and Mahesh et al. (1997).
We refer to this method as stacking, and it has been previously used to integrate heterogeneous knowledge sources for WSD (Stevenson and Wilks, 2001). $$$$$ For each noun, we then check whether it is modified by an adjective.

POS tags of the focus word itself are also included, to aid sense disambiguations related to syntactic differences (Stevenson and Wilks, 2001). $$$$$ In Section 5 we explain the strategy used to evaluate this system, and we report the results in Section 6.
POS tags of the focus word itself are also included, to aid sense disambiguations related to syntactic differences (Stevenson and Wilks, 2001). $$$$$ Despite this there is still a considerable diversity of methods employed by researchers, as well as differences in the definition of the problems to be tackled.
POS tags of the focus word itself are also included, to aid sense disambiguations related to syntactic differences (Stevenson and Wilks, 2001). $$$$$ This can be computed if the semantic categories are organized into a hierarchy.

Prior research (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies. $$$$$ We are grateful for the feedback from many colleagues in Sheffield, especially Mark Hepple, and for the detailed comments from the anonymous reviewers of an earlier version of this paper.
Prior research (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies. $$$$$ The sentence can be disambiguated on pragmatic grounds because it is far more likely that sports equipment will be bought in a sports shop.
Prior research (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies. $$$$$ Using the evaluation procedure described in the previous section, it was found that the system correctly disambiguated 90% of the ambiguous instances to the fine-grained sense level, and in excess of 94% to the homograph level.
Prior research (e.g., (McRoy, 1992), (Ng and Lee, 1996), (Stevenson and Wilks, 2001), (Yarowsky and Florian, 2002)) suggests that use of both syntactic and lexical features will improve disambiguation accuracies. $$$$$ Previously reported WSD systems that enjoyed a high level of accuracy have often operated on restricted vocabularies and employed a single WSD methodology.
