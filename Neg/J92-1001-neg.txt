Those relations between the sense and its defining words are reflected in semantic dusters that are termed categorical, functional, and situational clusters in McRoy (1992). $$$$$ First, we discovered that a human &quot;expert&quot; had great difficulty identifying each sense, and that this task was far more tedious than manual part-of-speech tagging or bracketing.
Those relations between the sense and its defining words are reflected in semantic dusters that are termed categorical, functional, and situational clusters in McRoy (1992). $$$$$ I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.

Moreover, those relations have been shown to be very effective knowledge sources for WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). $$$$$ There are some encouraging results from applying the system to sense tagging of arbitrary text.
Moreover, those relations have been shown to be very effective knowledge sources for WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). $$$$$ No simple recipe can resolve the general problem of lexical ambiguity.
Moreover, those relations have been shown to be very effective knowledge sources for WSD (McRoy 1992) and interpretation of noun sequences (Vanderwende 1994). $$$$$ Since the conceptual hierarchy already encodes these clusters implicitly, we need not write formal cluster definitions for them.

Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques. $$$$$ Current approaches to these applications are often word-based — that is, they treat words in the input as strings, mapping them directly to other words.
Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques. $$$$$ To discriminate senses, an understander can consider a diversity of information, including syntactic tags, word frequencies, collocations, semantic context, role-related expectations, and syntactic restrictions.
Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques. $$$$$ Here we will describe how to use the whole range of information.
Even if most of the techniques for WSD are presented as stand-alone, it is our belief, following the ideas of (McRoy, 1992), that full-fledged lexical ambiguity resolution should combine several information sources and techniques. $$$$$ For example, TRUMP's military lexicon contains a sense of engage that means 'attack.'

 $$$$$ Our goal has been a natural language system that can effectively analyze an arbitrary input at least to the level of word sense tagging.
 $$$$$ To apply this information most efficiently, the approach introduces a preprocessing phase that uses preference information available prior to parsing to eliminate some of the lexical ambiguity and establish baseline preferences.
 $$$$$ The judge had to leave town for the day.

 $$$$$ We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences.
 $$$$$ Having access to a large amount of information and being able to use it effectively are essential for understanding unrestricted texts, such as newspaper articles.
 $$$$$ We will also discuss an application of the approach in a system that computes sense tags for arbitrary texts, even when it is unable to determine a single syntactic or semantic representation for some sentences.
 $$$$$ The system's matching procedures allow for punctuation and verb-complement inversion.

(McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP. $$$$$ I am grateful to Paul Jacobs for his comments and his encouragement of my work on natural language processing at GE; to George Krupka for helping me integrate my work with TRUMP, and for continuing to improve the system; to Graeme Hirst for his many comments and suggestions on this article; and to Jan Wiebe and Evan Steeg for their comments on earlier drafts.
(McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP. $$$$$ This example also shows some of the limitations of our system in practice.
(McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP. $$$$$ The preference combination mechanism of the system uses dynamic measures of strength based on specificity, rather than relying on some fixed, ordered set of rules.
(McRoy, 1992) was one of the first to use multiple kinds of features for word sense disambiguation in the semantic interpretation system, TRUMP. $$$$$ When tested on a sample of 25,000 words of text from the Wall Street Journal, the system covered 98% of non-proper noun, non-abbreviated word occurrences (and 91% of all words).

 $$$$$ We have developed a substantial knowledge base for text processing, including a word sensebased lexicon that contains both core senses and dynamically triggered entries.
 $$$$$ The network also is changing its halftime show to include viewer participation, in an attempt to hold on to its audience through halftime and into the second halves of games.
 $$$$$ Failing to meet a very general preference is always strong information because, in practice, the purpose of such preferences is to eliminate the grossly inappropriate — such as trying to use a relation with a physical object when it should only be applied to events.
 $$$$$ I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.

 $$$$$ The approach incorporates a number of innovations, including: Although improvements to our system are ongoing, it already interprets arbitrary text and makes coarse word sense selections reasonably well.
 $$$$$ Using a lexicon of approximately 10,000 roots and 10,000 derivations, the system shows excellent lexical and morphological coverage.
 $$$$$ Dahlgren, McDowell, and Stabler 1989) or dictionary examples (cf.
 $$$$$ This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences.

 $$$$$ This paper addresses the problem of how to identify the intended meaning of individual words in unrestricted texts, without necessarily having access to complete representations of sentences.
 $$$$$ Twelve percent of the senses the system selected were derivatives.
 $$$$$ However, current approaches make use of only small subsets of this information.

 $$$$$ The approach incorporates a number of innovations, including: Although improvements to our system are ongoing, it already interprets arbitrary text and makes coarse word sense selections reasonably well.
 $$$$$ Twelve percent of the senses the system selected were derivatives.
 $$$$$ In Section 3, we discuss some sources of knowledge relevant to solving these problems, and, in Section 4, how TRUMP's semantic interpreter uses this knowledge to identify sense preferences.
 $$$$$ I am grateful to Paul Jacobs for his comments and his encouragement of my work on natural language processing at GE; to George Krupka for helping me integrate my work with TRUMP, and for continuing to improve the system; to Graeme Hirst for his many comments and suggestions on this article; and to Jan Wiebe and Evan Steeg for their comments on earlier drafts.

We propose a tagger that makes use of several types of information (dictionary definitions, parts-of-speech, domain codes, selectional preferences and collocates) in the tradition of McRoy (McRoy, 1992) although, the information sources we use are orthogonal, unlike the sources he used, making it easier to evaluate the performance of the various modules. $$$$$ For example, our system has a small number of part-whole clusters that list the parts associated with the object named by the cluster.
We propose a tagger that makes use of several types of information (dictionary definitions, parts-of-speech, domain codes, selectional preferences and collocates) in the tradition of McRoy (McRoy, 1992) although, the information sources we use are orthogonal, unlike the sources he used, making it easier to evaluate the performance of the various modules. $$$$$ I am grateful to Paul Jacobs for his comments and his encouragement of my work on natural language processing at GE; to George Krupka for helping me integrate my work with TRUMP, and for continuing to improve the system; to Graeme Hirst for his many comments and suggestions on this article; and to Jan Wiebe and Evan Steeg for their comments on earlier drafts.
We propose a tagger that makes use of several types of information (dictionary definitions, parts-of-speech, domain codes, selectional preferences and collocates) in the tradition of McRoy (McRoy, 1992) although, the information sources we use are orthogonal, unlike the sources he used, making it easier to evaluate the performance of the various modules. $$$$$ I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.
We propose a tagger that makes use of several types of information (dictionary definitions, parts-of-speech, domain codes, selectional preferences and collocates) in the tradition of McRoy (McRoy, 1992) although, the information sources we use are orthogonal, unlike the sources he used, making it easier to evaluate the performance of the various modules. $$$$$ Twelve percent of the senses the system selected were derivatives.

This approach as been recently used by (McRoy, 1992) and (Mahesh and Beale, 1996). $$$$$ I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.
This approach as been recently used by (McRoy, 1992) and (Mahesh and Beale, 1996). $$$$$ Similarly, the score for a cluster is the specificity of that cluster (as defined in Section 3.4).
This approach as been recently used by (McRoy, 1992) and (Mahesh and Beale, 1996). $$$$$ I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.
This approach as been recently used by (McRoy, 1992) and (Mahesh and Beale, 1996). $$$$$ When tested on a sample of 25,000 words of text from the Wall Street Journal, the system covered 98% of non-proper noun, non-abbreviated word occurrences (and 91% of all words).

There are a variety of methods for combining multiple knowledge sources (linguistic cues) (McRoy, 1992). $$$$$ The system's matching procedures allow for punctuation and verb-complement inversion.
There are a variety of methods for combining multiple knowledge sources (linguistic cues) (McRoy, 1992). $$$$$ Twelve percent of the senses the system selected were derivatives.
There are a variety of methods for combining multiple knowledge sources (linguistic cues) (McRoy, 1992). $$$$$ I acknowledge the financial support of the General Electric Company, the University of Toronto, and the Natural Sciences and Engineering Research Council of Canada.

McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. $$$$$ I am grateful to Paul Jacobs for his comments and his encouragement of my work on natural language processing at GE; to George Krupka for helping me integrate my work with TRUMP, and for continuing to improve the system; to Graeme Hirst for his many comments and suggestions on this article; and to Jan Wiebe and Evan Steeg for their comments on earlier drafts.
McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. $$$$$ The idea is to preserve in the core lexicon only the common, coarse distinctions among senses (cf.
McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. $$$$$ One way to gain such flexibility is give the system enough generic information about word senses and semantic relations so that it will be able to handle texts spanning more than a single domain.
McRoy (1992) describes a study of different sources useful for word sense disambiguation, including morphological information. $$$$$ Using a lexicon of approximately 10,000 roots and 10,000 derivations, the system shows excellent lexical and morphological coverage.
