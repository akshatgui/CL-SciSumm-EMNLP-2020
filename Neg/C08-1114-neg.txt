Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ The size of the corpus has a cost in terms of disk spaceand processing time.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ We believe that the difference between these two types is largely a matter of complexity.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ However, we believe that the main contribution of this paper is not PairClass itself, but the extension of supervised word pair classification beyond theclassification of noun-modifier pairs and semantic relations between nominals, to analogies, syn onyms, antonyms, and associations.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ The task is to select the choice pair that is most analogous to the stem pair.

An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ We conclude in Section 7.
An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ We apply our algorithm for recognizing analogies to multiple-choice analogy questions from the SAT college entrance test, multiple-choice synonym questions from the TOEFL (test of English as aforeign language), ESL (English as a second language) practice questions for distinguishing syn onyms and antonyms, and a set of word pairs thatare labeled similar, associated, and both, devel oped for experiments in cognitive psychology.We discuss the results of the experiments in Section 4.
An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ WordNet includes more thana dozen semantic relations (e.g., synonyms, hy ponyms, hypernyms, meronyms, holonyms, and antonyms).

Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ In effect, a single example (mason:stone) becomes a sui generis; it con stitutes a class of its own.
Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms.
Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ For ex ample, the noun-modifier expression brain biopsy was classified as Procedure.

Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ PairClass should be applicable to allof these relations.
Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ Random guessing would 5For more information, see SAT Analogy Questions (State of the art) at http://aclweb.org/aclwiki/.
Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ Experiment Accuracy Best previous Human Baseline Rank SAT Analogies 52.1% 56.1% 57.0% 20.0% 2 higher out of 12 TOEFL Synonyms 76.2% 97.5% 64.5% 25.0% 8 higher out of 15 Synonyms and Antonyms 75.0% none unknown 65.4% none Similar, Associated, and Both 77.1% none unknown 33.3% none Table 9: Summary of experimental results.

Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ As far as we know, the algorithm proposed here is the first attempt to deal with all four tasks using a uniform approach.
Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ They then trained a neu ral network to distinguish 13 classes of semantic relations, such as Cause, Location, Measure, andInstrument.
Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ The average foreign applicant to a US university achieves 64.5% correct (Landauer and Dumais, 1997).
Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual?

In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. $$$$$ Past work on these problems has required implicitly coding our knowledge ofthe nature of the task into the structure of the algo rithm.
In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. $$$$$ We can provide copies on request.

Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ In this template, X:Y consists of morphologicalvariations of the given word pair, in either order; for example, mason:stone, stone:mason, masons:stones, and so on.
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ The task attracted 14 teams who created 15 systems, all of which used supervised machine learning with features that were lexicon-based, corpus-based, or both.PairClass is most similar to the algorithm of Tur ney (2006), but it differs in the following ways:?
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ Our guess foreach TOEFL question is the choice with the high est probability of being positive, when paired with the corresponding stem.
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ In the future, as hardware im proves, this will become less of an issue, but there may be ways to improve the algorithm, so that a smaller corpus is sufficient.Another area for future work is to apply Pair Class to more tasks.

Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ A typical phrase for ma son:stone would be ?the mason cut the stone with?.
Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ The size of the corpus has a cost in terms of disk spaceand processing time.
Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ The main limitation of PairClass is the need for a large corpus.
Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con tains a theory of synonymy (Landauer and Dumais, 1997).

Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ Two words (levied and imposed) are synonymousin a context (levied a tax) when they can be interchanged (imposed a tax), they are are antony mous when they have opposite meanings (black c ? 2008, National Research Council of Canada (NRC).Licensed to the Coling 2008 Organizing Committee for pub lication in Coling 2008 and for re-publishing in any form or medium.
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ Rosario and Hearst(2001) constructed feature vectors for each nounmodifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources.
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ We then normalize all of the phrases that are found, by using morpha to remove suffixes.

Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ In the future, as hardware im proves, this will become less of an issue, but there may be ways to improve the algorithm, so that a smaller corpus is sufficient.Another area for future work is to apply Pair Class to more tasks.
Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms.
Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ The task is to select the choice word that is most similar in meaning to the stem word.
Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ We then sort the patterns in descending order of the number of word pairs that generated them.

Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ We use a standard supervised 905 machine learning approach, with feature vectorsbased on the frequencies of patterns in a large cor pus.
Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ Rosario and Hearst(2001) constructed feature vectors for each nounmodifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources.
Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ WordNet includes more thana dozen semantic relations (e.g., synonyms, hy ponyms, hypernyms, meronyms, holonyms, and antonyms).
Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ The size of the corpus has a cost in terms of disk spaceand processing time.

Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ Turney (2006) used a corpus-based algorithm.We may view Table 2 as a binary classification problem, in which mason:stone and carpen ter:wood are positive examples and the remaining word pairs are negative examples.
Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ The al gorithm takes as input a training set of word pairs with class labels and a testing set of word pairs without labels.
Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ Word pair Class label levied:imposed positive levied:believed negative levied:requested negative levied:correlated negative Table 5: How to fit a TOEFL question into the framework of supervised pair classification.
Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ The output of the algorithm is anassignment of labels to the word pairs in the test ing set.

We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ There is past work on recognizing analogies(Reitman, 1965), synonyms (Landauer and Dumais, 1997), antonyms (Lin et al, 2003), and asso ciations (Lesk, 1969), but each of these four tasks has been examined separately, in isolation from the others.
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ 3http://www.wumpus-search.org/.

The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ However, the strength of this ap proach is not its performance on any one task, but the range of tasks it can handle.As far as we know, this is the first time a standard supervised learning algorithm has been ap plied to any of these four problems.
The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ They used a nearest-neighbour learning algorithm to classify general-domain noun-modifier pairsinto 30 different classes of semantic relations.
The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ 909

This type of similarity is reminiscent of relational analogies investigated in Turney (2008). $$$$$ The four experiments are summarized in Tables 8 and 9.
This type of similarity is reminiscent of relational analogies investigated in Turney (2008). $$$$$ Veale (2004) used WordNet to answer 374 multiple-choice SAT analogy questions, achievingan accuracy of 43%, but the best corpus-based ap proach attains an accuracy of 56% (Turney, 2006).

Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. $$$$$ Some work is required to fit each probleminto the general framework of PairClass (supervised classification of word pairs) but the core al gorithm is the same in each case.
Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. $$$$$ yields the patterns ?the X cut * Y with?, ?* X * the Y *?, and so on.
