Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ In effect, a single example (mason:stone) becomes a sui generis; it con stitutes a class of its own.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ The task attracted 14 teams who created 15 systems, all of which used supervised machine learning with features that were lexicon-based, corpus-based, or both.PairClass is most similar to the algorithm of Tur ney (2006), but it differs in the following ways:?
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ 2The corpus was collected by Charles Clarke, University of Waterloo.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ In NLP, analogical algo rithms have been applied to machine translation(Lepage and Denoual, 2005), morphology (Lep age, 1998), and semantic relations (Turney and Littman, 2005).

An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ For a given word pair, such as mason:stone, the first step is to generate morphological variations,such as masons:stones.
An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.
An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ We conclude in Section 7.
An alternative embedding is that used by Turney (2008) in his PairClass system (see Section 6). $$$$$ Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms.

Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ there are no previous results for comparison.
Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ For each pattern, we count the number of input word pairs that generated the pattern.
Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual?
Turney (2008) has recently proposed a simpler SVM-based algorithm for analogical classification called PairClass. $$$$$ For example, consider the knowledge en coded in WordNet: much of the knowledge in WordNet is embedded in the graph structure that connects words.

Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.
Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ There is past work on recognizing analogies(Reitman, 1965), synonyms (Landauer and Dumais, 1997), antonyms (Lin et al, 2003), and asso ciations (Lesk, 1969), but each of these four tasks has been examined separately, in isolation from the others.
Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ To evaluate the performance, we use a set of 374 multiple-choice ques tions from the SAT college entrance exam.
Turney (2008) argues that many NLP tasks can be formulated in terms of analogical reasoning, and he applies his PairClass algorithm to a number of problems including SAT verbal analogy tests, synonym/antonym classification and distinction between semantically similar and semantically associated words. $$$$$ We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.

Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ Table 7 shows some ex amples from this collection of 144 word pairs (48 pairs in each of the three classes).
Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ We view the problem ofrecognizing analogies as the classification of se mantic relations between words.
Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual?
Finally, (Turney, 2008) proposes a supervised machine learning approach for discovering synonyms, antonyms, analogies and associations. For that purpose, feature vectors are based on frequencies of patterns and classified by a SVM. $$$$$ However, weprefer to take a corpus-based approach to seman tics.

In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. $$$$$ For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con tains a theory of synonymy (Landauer and Dumais, 1997).
In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. $$$$$ To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.
In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. $$$$$ Another reason to prefer a corpus-based approachto a lexicon-based approach is that the former re quires less human labour, and thus it is easier to extend to other languages.In Section 2, we describe our algorithm for rec ognizing analogies.
In particular, (Turney, 2008) tackled the problem of classifying different lexical information such as synonymy, antonymy, hypernymy and association by using context words. $$$$$ However, we believe that the main contribution of this paper is not PairClass itself, but the extension of supervised word pair classification beyond theclassification of noun-modifier pairs and semantic relations between nominals, to analogies, syn onyms, antonyms, and associations.

Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ We view the problem ofrecognizing analogies as the classification of se mantic relations between words.
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ PairClass should be applicable to allof these relations.
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ That is, meaning is largely about relations among words, rather thanproperties of individual words, considered in isolation.
Turney (2008) proposed a supervised method to solve word analogy questions that require identifying synonyms, antonyms, hypernyms, and other lexical-semantic relations between word pairs. $$$$$ In the past, the four tasks have been treated independently, using a widevariety of algorithms.

Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ It may be possible to apply the machinery of supervised learning toother problems that apparently call for unsupervised learning (for example, clustering or measur ing similarity), by using this sui generis device.
Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ We can provide copies on request.
Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ Phrases that contain a pair of wordstend to be more rare than phrases that contain either of the members of the pair, thus a large cor pus is needed to ensure that sufficient numbers of phrases are found for each input word pair.
Turney (2008) recently advocated the need for a uniform approach to corpus-based semantic tasks. $$$$$ The automatically generated patterns in PairClass are slightly more general than the pat terns of Turney (2006).

Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ or ?simulated?
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ In the following experi ments, we use morpha (morphological analyzer)and morphg (morphological generator) for mor phological processing (Minnen et al, 2001).1 The second step is to search in a large corpus for all phrases of the following form: ?[0 to 1 words] X [0 to 3 words] Y [0 to 1 words]?
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ However, weprefer to take a corpus-based approach to seman tics.
Such tasks will require an extension of the current framework of Turney (2008) beyond evidence from the direct co-occurrence of target word pairs. $$$$$ The advantageof being able to cast these problems in the frame work of standard supervised learning problems isthat we can now exploit the huge literature on su pervised learning.

Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ The al gorithm takes as input a training set of word pairs with class labels and a testing set of word pairs without labels.
Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual?
Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ We apply our algorithm for recognizing analogies to multiple-choice analogy questions from the SAT college entrance test, multiple-choice synonym questions from the TOEFL (test of English as aforeign language), ESL (English as a second language) practice questions for distinguishing syn onyms and antonyms, and a set of word pairs thatare labeled similar, associated, and both, devel oped for experiments in cognitive psychology.We discuss the results of the experiments in Section 4.
Turney (2008) presents a general approach for classifying word pairs into semantic relations by extracting the strings occurring between the two words of a pair (up to three words in-between, up to one word on either side) and using a frequency-based selection process to select sub-patterns where words from the extracted context pattern may have been replaced by a wild card. $$$$$ We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.

Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ We approach this as a standard classificationproblem for supervised machine learning.
Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ The accuracy of the algorithm is competitive with other systems, but the strength of the al gorithm is that it is able to handle all four tasks, with no tuning of the learning parameters to the particular task.
Building on a recent proposal in this direction by Turney (2008), we propose a generic method of this sort, and we test it on a set of unrelated tasks, reporting good performance across the board with very little task-specific tweaking. $$$$$ PairClass uses a support vector machine (SVM) instead of a nearest neighbour (NN) learning algorithm.

Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ The size of the corpus has a cost in terms of disk spaceand processing time.
Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ In the past, the four tasks have been treated independently, using a widevariety of algorithms.
Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ The next step is to generate feature vectors, one vector for each input word pair.
Turney (2008) is the first, to the best of our knowledge, to raise the issue of a unified approach. $$$$$ To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.

We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations.
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood Table 2: An example of a question from the 374 SAT analogy questions.
We adopt a similar approach to the one used in Turney (2008) and consider each question as a separate bi nary classification problem with one positive training instance and 5 unknown pairs. $$$$$ The four experiments are summarized in Tables 8 and 9.

The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ In essence, we say that X and Y are antonyms when the pair X:Y is analogous to the pair black:white, X and Y are synonyms when they are analogous to the pair levied:imposed, and X and Y are associated when they are analogous to the pair doctor:hospital.
The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ In effect, a single example (mason:stone) becomes a sui generis; it con stitutes a class of its own.
The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ See Section 3 for explanations.
The algorithm proposed by Turney (2008) is labeled as Turney-PairClass. $$$$$ We look at the normalized phrases that we collected for mason:stone and we count how many match this pattern.

This type of similarity is reminiscent of relational analogies investigated in Turney (2008). $$$$$ We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.
This type of similarity is reminiscent of relational analogies investigated in Turney (2008). $$$$$ We use a standard supervised 905 machine learning approach, with feature vectorsbased on the frequencies of patterns in a large cor pus.
This type of similarity is reminiscent of relational analogies investigated in Turney (2008). $$$$$ The accuracy of the algorithm is competitive with other systems, but the strength of the al gorithm is that it is able to handle all four tasks, with no tuning of the learning parameters to the particular task.

Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. $$$$$ For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con tains a theory of synonymy (Landauer and Dumais, 1997).
Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. $$$$$ We propose to subsume a broad range of phenom ena under analogies.
Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. $$$$$ The al gorithm takes as input a training set of word pairs with class labels and a testing set of word pairs without labels.
Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. $$$$$ This is a form of bagging (Breiman, 1996).
