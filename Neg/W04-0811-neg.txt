We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ The glosses do not themselves make the sense distinctions explicit.
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ Even though any given team may have intended their results to be interpreted one way or the other, we have included both sets of scores for comparative purposes.

The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ the glosses.
The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.
The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ The glosses do not themselves make the sense distinctions explicit.
The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).

State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ The glosses do not themselves make the sense distinctions explicit.
State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ The system names, along with email contacts are listed in table 3.

Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ the glosses.

While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ The glosses do not themselves make the sense distinctions explicit.
While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ Deniz Yuret, of Koc University, computed a baseline of 60.9% precision and recall by using the first WordNet entry for the given word and part-of-speech.
While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).

Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ the glosses.
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ Thus precision was not affected by those instances, but recall was lowered.

SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ The average precision is 57.4% and 51.9% is the average recall.
SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ The glosses do not themselves make the sense distinctions explicit.

Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ The system names, along with email contacts are listed in table 3.
Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.

Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.

Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.
Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.

As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ The average precision and recall is 52.2%.
As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ The glosses do not themselves make the sense distinctions explicit.
As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ the glosses.

Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ For the second set of scores (&quot;Without U&quot;), we simply skipped every instance where the system did not provide a sense.
Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.
Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ the glosses.

We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.

First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ the glosses.
First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.

Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.

We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ System Name Email Contact autoPS dianamÂ©sussex.ac.uk autoPSNVs dianamÂ©sussex.ac.uk clr04-aw kenÂ©clres.com DFA-Unsup-AW davidÂ©lsi.uned.es DLSI-UA-Nosu montoyoÂ©dlsi.ua.es GAMBL-AW bart.decadtÂ©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuretÂ©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parkerÂ©languagecomputer.com Meaning lluismÂ©lsi.upc.es Meaning simple lluismÂ©lsi.upc.es merl.systeml bhikshaÂ©merl.com merl.system2 bhikshaÂ©merl.com merl.system3 bhikshaÂ©merl.com R2D2: EAW montoyoÂ©dlsi.ua.es SenseLearner radaÂ©cs.unt.edu UJAEN mgarciaÂ©ujaen.es USussex-Prob3 Judita.PreissÂ©cl.cam.ac.uk USussex-Prob4 Judita.PreissÂ©cl.cam.ac.uk USussex-Prob5 Judita.PreissÂ©cl.cam.ac.uk upv-shmm-eaw amolinaÂ©dsic.upv.es upv-CIAOSENSO amolinaÂ©dsic.upv.es upv-CIAOSENS02 amolinaÂ©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001).
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ Table 1 shows the system performance under the first interpretation of the results (&quot;With U&quot;).

Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ Deniz Yuret, of Koc University, computed a baseline of 60.9% precision and recall by using the first WordNet entry for the given word and part-of-speech.
Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ The average precision and recall is 52.2%.

S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ Table 2 shows the system performance under the second interpretation of the results (&quot;Without U&quot;).
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ System Name Email Contact autoPS dianamÂ©sussex.ac.uk autoPSNVs dianamÂ©sussex.ac.uk clr04-aw kenÂ©clres.com DFA-Unsup-AW davidÂ©lsi.uned.es DLSI-UA-Nosu montoyoÂ©dlsi.ua.es GAMBL-AW bart.decadtÂ©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuretÂ©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parkerÂ©languagecomputer.com Meaning lluismÂ©lsi.upc.es Meaning simple lluismÂ©lsi.upc.es merl.systeml bhikshaÂ©merl.com merl.system2 bhikshaÂ©merl.com merl.system3 bhikshaÂ©merl.com R2D2: EAW montoyoÂ©dlsi.ua.es SenseLearner radaÂ©cs.unt.edu UJAEN mgarciaÂ©ujaen.es USussex-Prob3 Judita.PreissÂ©cl.cam.ac.uk USussex-Prob4 Judita.PreissÂ©cl.cam.ac.uk USussex-Prob5 Judita.PreissÂ©cl.cam.ac.uk upv-shmm-eaw amolinaÂ©dsic.upv.es upv-CIAOSENSO amolinaÂ©dsic.upv.es upv-CIAOSENS02 amolinaÂ©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001).

However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.
However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ Since comprehensive groupings of the WordNet senses do not yet exist, all results given are the result of fine-grained scoring.
However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ the glosses.

Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses â€” resources more suitable for the task at hand.
Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.
Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
