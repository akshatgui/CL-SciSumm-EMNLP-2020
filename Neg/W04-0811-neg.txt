We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ Deniz Yuret, of Koc University, computed a baseline of 60.9% precision and recall by using the first WordNet entry for the given word and part-of-speech.
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.
We estimate the optimal value for the threshold by maximizing F1 on a development set obtained by combining the Senseval-2 (Palmer et al., 2001) and Senseval-3 (Snyder and Palmer, 2004) English all-words datasets. $$$$$ The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline.

The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ System Name Email Contact autoPS dianam©sussex.ac.uk autoPSNVs dianam©sussex.ac.uk clr04-aw ken©clres.com DFA-Unsup-AW david©lsi.uned.es DLSI-UA-Nosu montoyo©dlsi.ua.es GAMBL-AW bart.decadt©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuret©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001).
The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ the glosses.
The F-score obtained by training on SemCor (mixed-domain corpus) and testing on the two target domains without using any injections (srcb) F-score of 61.7% on Tourism and F score of 65.5% on Health is comparable to the best result reported on the SEMEVAL datasets (65.02%, where both training and testing hap pens on a mixed-domain corpus (Snyder and Palmer, 2004)). $$$$$ The glosses do not themselves make the sense distinctions explicit.

State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline.
State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ The average precision and recall is 52.2%.
State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ Table 2 shows the system performance under the second interpretation of the results (&quot;Without U&quot;).
State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. $$$$$ The glosses do not themselves make the sense distinctions explicit.

Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.
Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer,2004) and 67.3% on the Open Mind Word Expert an notation exercise (Chklovski and Mihalcea, 2002). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.

While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ The greatest difference between these results those of the all-words task is that a greater number of systems have now achieved scores at or above the baseline.
While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.
While contextual evidence is required for accurate WSD, it is useful to look at this heuristic since it is so widely used as a back-off model by many systems and is hard to beat on an all words task (Snyder and Palmer, 2004). $$$$$ The average precision and recall is 52.2%.

Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004). $$$$$ Thus precision was not affected by those instances, but recall was lowered.

SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ For the first set of scores (&quot;With U&quot;), we assumed an answer of U (untaggable) whenever the system failed to provide a sense.
SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.
SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Corpus. $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.

Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ For the first set of scores (&quot;With U&quot;), we assumed an answer of U (untaggable) whenever the system failed to provide a sense.
Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Snyder and Palmer (2004) report 62% of all word types on the English all-words task at SENSEVAL-3 were labelled unanimously. $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).

Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ Table 2 shows the system performance under the second interpretation of the results (&quot;Without U&quot;).
Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ Table 1 shows the system performance under the first interpretation of the results (&quot;With U&quot;).
Nouns and verbal nouns (vn) have the highest agreements, similar to the results for the English all-words task at SENSEVAL-3 (Snyder and Palmer, 2004). $$$$$ For the second set of scores (&quot;Without U&quot;), we simply skipped every instance where the system did not provide a sense.

Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). $$$$$ merl.systeml .459 .447 IR,ST-DDD-09-U .729 .441 autoPS-U .490 .433 clr04-aw .506 .431 autoPSNVs-U .563 .354 merl.system2 .480 .352 DLSI-UA-all-Nosu .343 .275 Table 2: &quot;Without U&quot; scores, sorted by recall; a -S or -U suffix after the system name indicates that the system was reported as supervised or unsupervised, respectively.

As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
As a point of comparison, the Senseval 3 all-words task had a 75% agreement on nouns (Snyder and Palmer, 2004). $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.

Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 6570% range.
Existing hand-annotated corpora like SemCor (Miller et al, 1993), which is annotated with Word Netsenses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ the glosses.

We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ Table 1 shows the system performance under the first interpretation of the results (&quot;With U&quot;).
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ the glosses.

First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ The glosses do not themselves make the sense distinctions explicit.
First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
First, we use the acquired dominant senses to disambiguate the meanings of words in the Senseval-2 (Palmer et al, 2001) and Senseval-3 (Snyder and Palmer, 2004) datasets. $$$$$ The glosses do not themselves make the sense distinctions explicit.

Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ The greatest difference between these results and those of the SENSEVAL-2 English all-words task is that a greater number of systems have now achieved scores at or above the baseline.
Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ Although we did not compute a baseline score, we received several baseline figures from our participants.
Senseval 3 shared task data (Snyder and Palmer, 2004). $$$$$ Since comprehensive groupings of the WordNet senses do not yet exist, all results given are the result of fine-grained scoring.

We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ System Name Email Contact autoPS dianam©sussex.ac.uk autoPSNVs dianam©sussex.ac.uk clr04-aw ken©clres.com DFA-Unsup-AW david©lsi.uned.es DLSI-UA-Nosu montoyo©dlsi.ua.es GAMBL-AW bart.decadt©ua.ac.be IR ST-DDD-00 strappaKfitc.it IRST-DDD-09 strappaKfitc.it IRST-DDD-LSI strappaKfitc.it Koc University dyuret©ku.edu.tr KUNLP-Eng-All hcseoKM1p.korea.ac.kr LCCaw parker©languagecomputer.com Meaning lluism©lsi.upc.es Meaning simple lluism©lsi.upc.es merl.systeml bhiksha©merl.com merl.system2 bhiksha©merl.com merl.system3 bhiksha©merl.com R2D2: EAW montoyo©dlsi.ua.es SenseLearner rada©cs.unt.edu UJAEN mgarcia©ujaen.es USussex-Prob3 Judita.Preiss©cl.cam.ac.uk USussex-Prob4 Judita.Preiss©cl.cam.ac.uk USussex-Prob5 Judita.Preiss©cl.cam.ac.uk upv-shmm-eaw amolina©dsic.upv.es upv-CIAOSENSO amolina©dsic.upv.es upv-CIAOSENS02 amolina©dsic.upv.es Table 3: email contact for each system; sorted alphabetically. the unsupervised systems (Palmer et al., 2001).
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ The glosses do not themselves make the sense distinctions explicit.

Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ The average precision and recall is 52.2%.
Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Existing hand-annotated corpora like Sem Cor (Miller et al, 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). $$$$$ As with the SENSEVAL-2 English all-words task, the supervised systems fared much better than the unsupervised systems (Palmer et al., 2001).

S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ Table 2 shows the system performance under the second interpretation of the results (&quot;Without U&quot;).
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ In fact, we believe that most of the annotator disagreements were, like this example, between closely related WordNet senses with only subtle (and often inexplicit) distinctions and that more coarse-grained sense distinctions are needed (Palmer et al., 2004).
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ The glosses do not themselves make the sense distinctions explicit.
S3AW task In the Senseval-3 all-words task (Snyder and Palmer, 2004) all words in three document excerpts need to be disambiguated. $$$$$ For the first set of scores (&quot;With U&quot;), we assumed an answer of U (untaggable) whenever the system failed to provide a sense.

However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ We believe that further significant progress must await the development of resources with coarser-grained sense distinctions and with glosses that draw explicit contrasts between the senses — resources more suitable for the task at hand.
However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ Bart Decadt, of the University of Antwerp and submitter of the GAMBL-AW system, provided a baseline of 62.4% using the same method (the 1.5% difference is most likely explained by how well the baseline systems dealt with multi-word constructions and hyphenated words).
However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.
However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 65- 70% range.

Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ While this result is encouraging, it seems that the best systems have a hit a wall in the 6570% range.
Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ This is not surprising given the typical inter-annotator agreement of 70-75% for this task.
Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ In fact, all of the seven systems reported as supervised scored higher than any of the nine systems reported as unsupervised in both precision and recall (using either of the two scoring criteria).
Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall). $$$$$ The glosses do not themselves make the sense distinctions explicit.
