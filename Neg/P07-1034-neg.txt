For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ In Section 4, we present the experiment results.
For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ We call this second method the balanced bootstrapping method.
For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ Florian et al. (2004) first train a NE tagger on the source domain, and then use the tagger’s predictions as features for training and testing on the target domain.
For evaluation we selected two domain adaptation datasets: spam (Jiang and Zhai, 2007) and sentiment (Blitzer et al, 2007). $$$$$ The framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters.

In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ We thank the anonymous reviewers for their valuable comments.
In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ However, for domain adaptation, we want to focus more on the target domain instances.
In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ For example, in POS tagging, the source domain may be tagged WSJ articles, and the target domain may be scientific literature that contains scientific terminology.

 $$$$$ Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
 $$$$$ If pt(y|x) deviates from ps(y|x) to some extent, we have one of the following choices: It may be the case that if we change the representation of the instances, i.e., if we choose a feature space X0 different from X, we can bridge the gap between the two distributions ps(y|x) and pt(y|x).
 $$$$$ In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation.
 $$$$$ In another word, αsi of the top k instances for which ysi =� arg maxy p(y|xsi ; �θt,l) are set to 0, and αi of all the ces are set to 1. other source instan Accurately setting involves accurately estimating and pt(x) from the empirical distributions.

We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ All other instances in are not considered.
We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ Based on this analysis, we propose a general instance weighting method for domain adaptation, which can be regarded as a generalization of an existing approach to semi-supervised learning.
We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation.

Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ We now formally define our instance weighting framework.
Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ The second task is entity type classification.

Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ A possible reason for this is that the set of labeled target instances we use is a biased sample from the target domain, and therefore the model trained on these instances is not always a good predictor of “misleading” source instances.
Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.

 $$$$$ The framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters.
 $$$$$ Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
 $$$$$ Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.
 $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.

Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ Suppose we use a naive NE tagger that only looks at the word itself.
Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ First, with standard supervised learning, we train a model 0t,l from Dt,l.
Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ The top k instances that are incorrectly predicted by �θt,l (ranked by their prediction confidence) are discarded.
Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.

It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ In this paper, we study the domain adaptation problem from the instance weighting perspective.
It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ However, for many NLP tasks, x resides in a high dimensional space, which makes it hard to apply standard non-parametri c density estimation meth268 ods.
It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ The results are shown in the last row of Table 2.
It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ Our empirical results on three NLP tasks show that incorporating and exploiting more information from the target domain through instance weighting is effective.

This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ In contrast, we weight the instances.
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ We formally analyze and characterize the domain adaptation problem from this distributional view.
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ In the case where pt(y|x) is similar to ps(y|x), but pt(x) deviates from ps(x), we may use the (unlabeled) target domain instances to bias the estimate of ps(x) toward a better approximation of pt(x), and thus achieve domain adaptation.

(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ We consider p(y|x; �θt,l) to be a crude approximation of pt(y|x).
(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ The proposed method implements several adaptation heuristics with a unified objective function: (1) removing misleading training instances in the source domain; (2) assigning more weights to labeled target instances than labeled source instances; (3) augmenting training instances with target instances with predicted labels.

Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ However, for many NLP tasks, x resides in a high dimensional space, which makes it hard to apply standard non-parametri c density estimation meth268 ods.
Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ The source (target) domain data is generated from a mixture of the “truly source (target) domain” distribution and the “general domain” distribution.
Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ The theoretical analysis we give in Section 2 suggests that one way to solve the domain adaptation problem is through instance weighting.

This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ The first task is POS tagging, for which we used 6166 WSJ sentences from Sections 00 and 01 of Penn Treebank as the source domain data, and 2730 PubMed sentences from the Oncology section of the PennBioIE corpus as the target domain data.
This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ The framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters.
This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ We thus face the domain adaptation problem.

We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ We then propose a general instance weighting framework for domain adaptation.
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ The theoretical analysis we give in Section 2 suggests that one way to solve the domain adaptation problem is through instance weighting.
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.

For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ Finally, we introduce three global parameters As, At,l and At,u that are not instance-specific but are associated with Ds, Dt,l and Dt,u, respectively.
For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ If we place a prior on the weight for this feature so that a large weight will be penalized, then we can prevent the learned model from relying too much on this domain specific feature.
For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ In general, the domain adaptation problem arises when the source instances and the target instances are from two different, but related distributions.
For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ The framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters.

The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ Domain adaptation is a very important problem with applications to many NLP tasks.
The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ Experiment results on three NLP tasks show that while regular semi-supervised learning methods and supervised learning methods can be applied to domain adaptation without considering domain difference, they do not perform as well as our new method, which explicitly captures domain difference.

 $$$$$ Large αi means the two probabilities are close, and therefore we can trust the labeled instance (xsi , ysi ) for the purpose of learning a classifier for the target domain.
 $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.
 $$$$$ This again shows that weighting the target instances more is a right direction to go for domain adaptation.
 $$$$$ Consider again the NE tagging example.

Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ We then propose a general instance weighting framework for domain adaptation.
Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ For example, in POS tagging, the source domain may be tagged WSJ articles, and the target domain may be scientific literature that contains scientific terminology.
Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation.

 $$$$$ Domain adaptation is a very important problem with applications to many NLP tasks.
 $$$$$ In the second setting, we assume there is no labeled target instance.
 $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
 $$$$$ The proposed method implements several adaptation heuristics with a unified objective function: (1) removing misleading training instances in the source domain; (2) assigning more weights to labeled target instances than labeled source instances; (3) augmenting training instances with target instances with predicted labels.

 $$$$$ For many NLP classification tasks, we do not have a good parametric model for p(x).
 $$$$$ For example, we can set pt(y|x, B) = p(y|x; B).
 $$$$$ Finally, we compare our framework with related work in Section 5 before we conclude in Section 6.
