For evaluation we selected two domain adaptation datasets $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.
For evaluation we selected two domain adaptation datasets $$$$$ The results show that regular semi-supervised and supervised learning methods do not perform as well as our new method, which explicitly captures domain difference.
For evaluation we selected two domain adaptation datasets $$$$$ Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data.
For evaluation we selected two domain adaptation datasets $$$$$ We thank the anonymous reviewers for their valuable comments.

In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ We propose a framework that incorporates instance pruning in Section 2.2 and the three approximations in Section 2.3.
In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ Our results also show that incorporating and exploiting more information from the target domain is much more useful than excluding misleading training examples from the source domain.
In a complimentary approach, Jiang and Zhai (2007) weighed training instances based on their similarity to unlabeled target domain data. $$$$$ First, with standard supervised learning, we train a model 0t,l from Dt,l.

 $$$$$ There have been several studies in NLP that address domain adaptation, and most of them need labeled data from both the source domain and the target domain.
 $$$$$ The results are shown in the last row of Table 2.
 $$$$$ Domain adaptation is a very important problem with applications to many NLP tasks.
 $$$$$ Domain adaptation is a very important problem with applications to many NLP tasks.

We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ For these instances, we set = 1 for y = arg maxy, and = 0 for all other y.
We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ Before we show the formal framework, we first introduce some weighting parameters and explain the intuitions behind these parameters.
We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ In the rest of this section, we introduce several heuristics that we used in our experiments to set these parameters.
We show that, on the NER task, DAB outperforms supervised, transductive and standard bootstrapping algorithms, as well as a bootstrapping variant, called balanced bootstrapping (Jiang and Zhai, 2007), that has recently been proposed for domain adaptation. $$$$$ An immediate possible solution is semi-supervised learning, where we simply treat the target instances as unlabeled data but do not distinguish the two domains.

Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ And in almost all cases, weighting the target instances more than the source instances performed better than weighting them equally.
Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ In neither case are the target instances emphasize more than source instances.
Jiang and Zhai (2007) proposed an instance re-weighting framework that handles both the [S+T+] and [S+T] settings. $$$$$ Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data.

Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ In general, the domain adaptation problem arises when the source instances and the target instances are from two different, but related distributions.
Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ We then propose a general instance weighting framework for domain adaptation.
Jiang and Zhai (2007) recently proposed an instance re-weighting framework to take domain shift into account. $$$$$ The idea of using ����� ����� to weight instances has been studied in statistics (Shimodaira, 2000), but has not been applied to NLP tasks.

 $$$$$ In general, as long as we have sufficient labeled data, this approximation is fine because the unlabeled instances we want to classify are from the same p(x, y).
 $$$$$ For all the three solutions given above, we need either some prior knowledge about the target domain, or some labeled target domain instances; from only the unlabeled target domain instances, we would not know where and why pt(y|x) differs from ps(y|x).
 $$$$$ For generative syntactic parsing, Roark and Bacchiani (2003) have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain.

Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ Our goal is to recover this unknown distribution so that we can predict unlabeled instances drawn from the same distribution.
Balanced bootstrapping has been shown to be more effective for domain adaptation than standard bootstrapping (Jiang and Zhai, 2007) for named entity classification on a subset of the dataset used here. $$$$$ The source domain contains 4000 spam and ham emails from publicly available sources, and the target domains are three individual users’ inboxes, each containing 2500 emails.

It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ In this paper, we study the domain adaptation problem from the instance weighting perspective.
It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ The domain adaptation problem is commonly encountered in NLP.
It also outperforms balanced bootstrapping, an approach designed for domain adaptation (Jiang and Zhai, 2007). $$$$$ Here we highlight a few representative ones.

This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ Experiment results on three NLP tasks show that while regular semi-supervised learning methods and supervised learning methods can be applied to domain adaptation without considering domain difference, they do not perform as well as our new method, which explicitly captures domain difference.
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ So intuitively, we want to make and somehow larger relative to As we will show in Section 4, this is indeed beneficial.
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ In another word, αsi of the top k instances for which ysi =� arg maxy p(y|xsi ; �θt,l) are set to 0, and αi of all the ces are set to 1. other source instan Accurately setting involves accurately estimating and pt(x) from the empirical distributions.
This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). $$$$$ In Section 3, we then propose a general instance weighting framework for domain adaptation.

(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ In particular, it can support adaptation with some target domain labeled instances as well as that without any labeled target instances.
(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ Finally, we introduce three global parameters As, At,l and At,u that are not instance-specific but are associated with Ds, Dt,l and Dt,u, respectively.
(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ Domain adaptation is an important problem in natural language processing (NLP) due to the lack of labeled data in novel domains.
(Jiang and Zhai, 2007) used a small number of labeled data from target domain to weight source instances. $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.

Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ The second task is entity type classification.
Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ We then propose a general instance weighting framework for domain adaptation.
Among the previously mentioned work, (Jiang and Zhai, 2007) is a special case given that it discusses both aspects of adaptation algorithms. $$$$$ We thank the anonymous reviewers for their valuable comments.

This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ Table 3 shows the results.
This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.
This highly effective approach is not directly applicable to the multinomial models used for core SMT components, which have no natural method for combining split features, so we rely on an instance-weighting approach (Jiangand Zhai, 2007) to down weight domain-specific examples in OUT. $$$$$ None of the above methods would work if there were no labeled target instances.

We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ A possible reason for this is that the set of labeled target instances we use is a biased sample from the target domain, and therefore the model trained on these instances is not always a good predictor of “misleading” source instances.
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ We formally analyze and characterize the domain adaptation problem from a distributional view, and show that there are two distinct needs for adaptation, corresponding to the different distributions of instances and classification functions in the source and the target domains.
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (non discriminative) instance weighting. $$$$$ Because the need for domain adaptation arises from two different factors, we need different solutions for each factor.

For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ They assume a “truly source domain” distribution, a “truly target domain” distribution, and a “general domain” distribution.
For example, Blitzer et al (2007) learned correspondences between features across domains and Jiang and Zhai (2007) weighted source domain examples by their similarity to the target distribution. $$$$$ In general, the domain adaptation problem arises when the source instances and the target instances are from two different, but related distributions.

The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.
The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ For spam filtering, we used 200 labeled target instances and 1800 unlabeled target instances.
The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ We thank the anonymous reviewers for their valuable comments.
The maintainer of the system may be notified that performance is suffering, labels can be obtained for a sample of instances from the stream for retraining, or large volumes of unlabeled instances can be used for instance reweighting (Jiang and Zhai, 2007). $$$$$ If we consider capitalization, then the instance Bush is represented differently from the instance bush.

 $$$$$ In this paper, we formally analyze the domain adaptation problem and propose a general instance weighting framework for domain adaptation.
 $$$$$ From the approximation in Section 2.3 that uses only Ds, it is clear that such a parameter is useful.
 $$$$$ The second task is entity type classification.
 $$$$$ In this paper, we study the domain adaptation problem from the instance weighting perspective.

Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ We now formally define our instance weighting framework.
Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ The framework is flexible to support many different strategies for adaptation.
Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. $$$$$ We then propose a general instance weighting framework for domain adaptation.

 $$$$$ Small αi means these two probabilities are very different, and therefore we should probably discard the instance (xsi , ysi ) in the learning process.
 $$$$$ The framework opens up many interesting future research directions, especially those related to how to more accurately set/estimate those weighting parameters.
 $$$$$ This work was in part supported by the National Science Foundation under award numbers 0425852 and 0428472.

 $$$$$ We then propose a general instance weighting framework for domain adaptation.
 $$$$$ We thank the anonymous reviewers for their valuable comments.
 $$$$$ For example, in POS tagging, the source domain may be tagged WSJ articles, and the target domain may be scientific literature that contains scientific terminology.
