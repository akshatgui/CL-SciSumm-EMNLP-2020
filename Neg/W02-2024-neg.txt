The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002). $$$$$ Stages use the output of previous stages for obtaining an improved performance (Spanish test set: F,3=1=73.92; Dutch test set: F,3=1=71.36) Tjong Kim Sang (2002) has applied a memory-based learner to the data of the shared task.
The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002). $$$$$ Example: [PER Wolff ] , currently a journalist in [LOC Argentina ] , played with [PER Del Bosque ] in the final years of the seventies in [ORG Real Madrid ] .
The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002). $$$$$ The shared task of CoNLL-2002 concerns language-independent named entity recognition.

We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al, 2005). $$$$$ He evaluated different features in both subprocesses.
We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al, 2005). $$$$$ The data contains entities of four types: persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC).

Empirical evidence for this argument can be seen from the result of the CoNLL shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang and Meulder, 2003), where the ranking of the participating systems changes with the test corpora. $$$$$ The organizers of the shared task were especially interested in approaches that make use of additional nonannotated data for improving their performance.
Empirical evidence for this argument can be seen from the result of the CoNLL shared tasks (Tjong Kim Sang, 2002) (Tjong Kim Sang and Meulder, 2003), where the ranking of the participating systems changes with the test corpora. $$$$$ Additionally, it relies on lists of entities which have been compiled from the training data.

Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. $$$$$ The categorization process was trained separately from the extraction process but that did not seem to have harmed overall performance (Spanish test set: F,3=1=73.89; Dutch test set: F,3=1=69.68) Patrick, Whitelaw and Munro (2002) present SLINERC, a language-independent named entity recognizer.
Similarly to classical NLP tasks such as base noun phrase chunking (Ramshaw and Marcus, 1994), text chunking (Ramshaw and Marcus, 1995) or named entity recognition (Tjong Kim Sang, 2002), we formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. $$$$$ The organizers of the shared task were especially interested in approaches that make use of additional nonannotated data for improving their performance.

It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). $$$$$ Additionally the article boundaries in the text have been marked explicitly with lines containing the tag -DOCSTART-.
It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). $$$$$ The participants of the shared task have been offered training and test data for two European languages: Spanish and Dutch.
It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). $$$$$ They have used the data for developing a named-entity recognition system that includes a machine learning component.
It is especially challenging to extract the named entities from the text sources written in languages other than English which, in practice, is supported by the results of the shared tasks on the named entity recognition (Tjong Kim Sang, 2002). $$$$$ The participants of the shared task have been offered training and test data for two European languages: Spanish and Dutch.

In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). $$$$$ When the best parameters are found, the method can be trained on the training data and tested on the test data.
In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). $$$$$ The annotator has followed the MITRE and SAIC guidelines for named entity recognition (Chinchor et al., 1999) as well as possible.
In addition, several competitions have been organized, with a focus on multilingual NER (Tjong Kim Sang, 2002). $$$$$ Apart from the base classifier, his system made use of three extra techniques for boosting performance: cascading classifiers (stacking), feature selection and system combination.

Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost (Carreras et al., 2002), Hidden Markov Models (Bikel et al,), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used. $$$$$ It was produced by a system which only identified entities which had a unique class in the training data.
Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost (Carreras et al., 2002), Hidden Markov Models (Bikel et al,), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used. $$$$$ A boosted decision tree method obtained the best performance on both data sets (Carreras et al., 2002).
Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost (Carreras et al., 2002), Hidden Markov Models (Bikel et al,), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used. $$$$$ Here is a part of the example sentence: Words tagged with O are outside of named entities.

This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ The second method was a decision tree method.
This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ Recall is the percentage of named entities present in the corpus that are found by the system.
This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ We have described the CoNLL-2002 shared task: language-independent named entity recognition.
This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ The shared task of CoNLL-2002 concerns language-independent named entity recognition.

Similarly to classical NLP tasks, such as Base Phrase Chunking (Ramshaw and Marcus, 1999) (BPC) or NER (Tjong Kim Sang, 2002), we formulate the MD task as a sequence classification problem, i.e. the classifier assigns to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. $$$$$ The organizers of the shared task were especially interested in approaches that make use of additional nonannotated data for improving their performance.
Similarly to classical NLP tasks, such as Base Phrase Chunking (Ramshaw and Marcus, 1999) (BPC) or NER (Tjong Kim Sang, 2002), we formulate the MD task as a sequence classification problem, i.e. the classifier assigns to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. $$$$$ All data files contain one word per line with empty lines representing sentence boundaries.
Similarly to classical NLP tasks, such as Base Phrase Chunking (Ramshaw and Marcus, 1999) (BPC) or NER (Tjong Kim Sang, 2002), we formulate the MD task as a sequence classification problem, i.e. the classifier assigns to each token in the text a label indicating whether it starts a specific mention, is inside a specific mention, or is outside any mentions. $$$$$ Twelve different systems have been applied to data covering two Western European languages: Spanish and Dutch.

We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL 2002 shared task (Tjong Kim Sang, 2002). $$$$$ We have described the CoNLL-2002 shared task: language-independent named entity recognition.
We used three data sets: the English and German data for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003) and the Dutch data for the CoNLL 2002 shared task (Tjong Kim Sang, 2002). $$$$$ Named entities are phrases that contain the names of persons, organizations, locations, times and quantities.

We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). $$$$$ The Dutch data consist of four editions of the Belgian newspaper &quot;De Morgen&quot; of 2000 (June 2, July 1, August 1 and September 1).
We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). $$$$$ The participants of the shared task have been offered training and test data for two European languages: Spanish and Dutch.
We use the Dutch data set from the CoNLL 2002 shared task (Tjong Kim Sang, 2002). $$$$$ We have described the CoNLL-2002 shared task: language-independent named entity recognition.

The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ A baseline rate was computed for both sets.
The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ The results of the different learning methods on the test sets will be compared in the evaluation of the shared task.
