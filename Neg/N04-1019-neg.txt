It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ In addition, the DUC method is not powerful enough to distinguish between systems.
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ Evaluating content selection in summarization has proven to be a difficult problem.
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ Also, since not much agreement is expected between two summaries, many model units will have no counterpart in the peer and thus the expected scores will necessarily be rather low.

In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ SCU annotation involves two types of choices: extracting a contributor from a sentence, and assigning it to an SCU.
In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited.

To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ Evaluation data consists of human relevance judgments on a scale from 0 to 10 on for all sentences in the original documents.
To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ The work closest to ours is (Halteren and Teufel, 2003), and we profited from the lessons they derived from an annotation of 50 summaries of a single 600-word document into content units that they refer to as factoids.
To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ When applied to per set ranking of summaries, no correlation was seen with pyramid scores.
To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ There are many open questions about how to parameterize a summary for specific goals, making evaluation in itself a significant research question (Jing et al., 1998).

Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ No other evaluation method predicts sets of equally informative summaries, identifies semantic differences between more and less highly ranked summaries, or constitutes a tool that can be applied directly to further analysis of content selection.
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ Second, (Halteren and Teufel, 2003) do not make direct use of factoid frequency (our weights): to construct a model 100-word summary, they select factoids that occur in at least 30% of summaries, but within the resulting model summary, they do not differentiate between more and less highly weighted factoids.

These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ As described in Section 3, we used three of these sets, and collected six additional summaries per set, in order to study the distribution of content units across increasingly many summaries.
These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ Also, since not much agreement is expected between two summaries, many model units will have no counterpart in the peer and thus the expected scores will necessarily be rather low.

The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ In contrast, factoids are semi-formal expressions in a FOPL-style semantics, which are compositionally interpreted.
The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ There are many open questions about how to parameterize a summary for specific goals, making evaluation in itself a significant research question (Jing et al., 1998).
The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ In Section 5, we present our conclusions and point to our next step, the feasibility of automating our method.

In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ The exact formula we use is computed as follows.
In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.

the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ Figure 1 shows a scatterplot of human scores for all 30 sets, and illustrates an apparently random relation of summarizers to each other, and to document sets.
the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ The pyramid method not only assigns a score to a summary, but also allows the investigator to find what important information is missing, and thus can be directly used to target improvements of the summarizer.
the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ We hope to address two drawbacks to our method in future work.

Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). $$$$$ For multidocument summarization compression rates are high, so even sentences with the highest relevance judgments are potentially not used.
Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). $$$$$ Radev et al. (2003) also exploits relative importance of information.
Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). $$$$$ However, our SCU annotated summaries and correlated pyramids provide a valuable data resource that will allow us to investigate such questions.
Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al, 2007). $$$$$ These qualifications aside, we wanted to test whether it is possible to use their approach for assigning scores not for an entire test suite but on a per set basis.

Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ We compare our approach with previous work in Section 4.
Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ Instead of attempting to develop a method to elicit reliable judgments from humans, we chose to calibrate our method to human summarization behavior.
Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ The null hypothesis can be rejected with one-sided test with level of significance α = 0.05, given our sample size N = 5, if rs ≥ 0.85.

Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ This suggests that the DUC scores cannot be used to distinguish a good human summarizer from a bad one.
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ In 2003 they provided four human summaries for each of the 30 multi-document test sets, any one of which could serve as the model, with no criteria for choosing among possible models.
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002).

This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ Evaluation involves comparison of a peer summary (baseline, or produced by human or system) by comparing its content to a gold standard, or model.
This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ Within DUC, different types of summarization have been studied: the generation of abstracts and extracts of different lengths, single- and multi-document summaries, and summaries focused by topic or opinion.
This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ In Section 2, we describe the DUC method.
This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ Our method quantifies the relative importance of facts to be conveyed.

From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ Our method quantifies the relative importance of facts to be conveyed.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ This suggests that the DUC scores cannot be used to distinguish a good human summarizer from a bad one.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ We have started exploring the feasibility of automation and we are collecting additional data sets.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ However, our SCU annotated summaries and correlated pyramids provide a valuable data resource that will allow us to investigate such questions.

To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ The procedure used for evaluating summaries in DUC is the following: The final score is based on the content unit coverage.
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ Given a pyramid of order n, we can predict the optimal summary content–it should contain all the SCUs from the top tier, if length permits, SCUs from the next tier and so on.
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ These methodological anomalies lead to unreliable scores.
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ Evaluating content selection in summarization has proven to be a difficult problem.

They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ We present an empirically grounded method for evaluating content selection in summarization.

Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ The total probability of error p = p1 * P(|sum1 |==9 |sum2|) + (p2 + p3) * (1 − P(|sum1 |==9 |sum2|)) is also in Table 3.
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ Then we present results demonstrating the need for at least five summaries per pyramid, given this corpus of 100-word summaries.
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ Second, (Halteren and Teufel, 2003) do not make direct use of factoid frequency (our weights): to construct a model 100-word summary, they select factoids that occur in at least 30% of summaries, but within the resulting model summary, they do not differentiate between more and less highly weighted factoids.

The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ First, pyramid scores ignore interdependencies among content units, including ordering.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.

As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ In the official DUC results tables, the score for the entire summary is the average of the scores of all the content model units, thus a number between 0 and 1.
As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ For multidocument summarization compression rates are high, so even sentences with the highest relevance judgments are potentially not used.
As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ The partition is based on the weight of the SCU; each tier contains all and only the SCUs with the same weight.

Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ Our method quantifies the relative importance of facts to be conveyed.
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ First, an SCU is a set of contributors that are largely similar in meaning, thus SCUs differ from each other in both meaning and weight (number of contributors).
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ The pyramid method not only assigns a score to a summary, but also allows the investigator to find what important information is missing, and thus can be directly used to target improvements of the summarizer.
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.

In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ These methodological anomalies lead to unreliable scores.
In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).
In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ This is the most severe kind of mistake and ideally it should never happen–the two summaries appear with scores opposite to what they really are.7 The probabilities p1, p2 and p3 can be computed directly by counting how many times the particular error occurs for all possible pyramids of order n. By taking each pyramid that does not contain either of sum1 or sum2 and comparing the scores they are assigned, the probabilities in Table 3 are obtained.
In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ The pyramid method not only assigns a score to a summary, but also allows the investigator to find what important information is missing, and thus can be directly used to target improvements of the summarizer.
