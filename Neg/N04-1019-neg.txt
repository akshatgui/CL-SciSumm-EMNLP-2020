It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ These methodological anomalies lead to unreliable scores.
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003).
It also corresponds to the pyramid measure proposed by Nenkova and Passonneau (2004), which also considers an estimation of the maximum value reachable. $$$$$ Additionally, the task of determining the percentage overlap between two text units turns out to be difficult to annotate reliably – (Lin and Hovy, 2002) report that humans agreed with their own prior judgment in only 82% of the cases.

In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003).
In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ The selection of units with the same content is facilitated by the use of the Summary Evaluation Environment (SEE)2 developed at ISI, which displays the model and peer summary side by side and allows the user to make selections by using a mouse.
In evaluations of summarization algorithms, it is common practice to derive the gold standard con tent importance scores from human summaries, as done, for example, in the pyramid method, where the importance of a content element corresponds to the number of reference human summaries that make use of it (Nenkova and Passonneau, 2004). $$$$$ At order-5 pyramids, the total probability of error drops to 0.1 and is mainly due to error E2, which is the mildest one.

To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ The selection of units with the same content is facilitated by the use of the Summary Evaluation Environment (SEE)2 developed at ISI, which displays the model and peer summary side by side and allows the user to make selections by using a mouse.
To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. $$$$$ The work closest to ours is (Halteren and Teufel, 2003), and we profited from the lessons they derived from an annotation of 50 summaries of a single 600-word document into content units that they refer to as factoids.

Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ Instead of attempting to develop a method to elicit reliable judgments from humans, we chose to calibrate our method to human summarization behavior.
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. $$$$$ More important than interannotator reliability is the robustness of the pyramid metric, given different SCU annotations.

These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ However, our SCU annotated summaries and correlated pyramids provide a valuable data resource that will allow us to investigate such questions.
These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ These methodological anomalies lead to unreliable scores.
These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). $$$$$ In contrast to Haltern and Teufel, we do not believe it is possible to arrive at the correct representation for a set of summaries; they refer to the observation that the factoids arrived at depend on the summaries one starts with as a disadvantage in that adding a new summary can require adjustments to the set of factoids.

The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
The second metric we use is similar to Pyramid (Nenkova and Passonneau, 2004), which has been used for summarization evaluation. $$$$$ Second, (Halteren and Teufel, 2003) do not make direct use of factoid frequency (our weights): to construct a model 100-word summary, they select factoids that occur in at least 30% of summaries, but within the resulting model summary, they do not differentiate between more and less highly weighted factoids.

In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries.
In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ In Section 5, we present our conclusions and point to our next step, the feasibility of automating our method.
In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. $$$$$ We create a weighted inventory of Summary Content Units–a pyramid–that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same meaning.

the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ This can lead to more stable, more informative scores, and hence to a meaningful content evaluation.
the PYRAMID framework (Nenkova and Passonneau, 2004) was used for manual summary evaluations. $$$$$ This can lead to more stable, more informative scores, and hence to a meaningful content evaluation.

Pyramid evaluation $$$$$ They found a total of 256 factoids and note that the increase in factoids with the number of summaries seems to follow a Zipfian distribution.
Pyramid evaluation $$$$$ The strengths of pyramid scores are that they are reliable, predictive, and diagnostic.
Pyramid evaluation $$$$$ We compare our approach with previous work in Section 4.

Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ This can lead to more stable, more informative scores, and hence to a meaningful content evaluation.
Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ The procedure used for evaluating summaries in DUC is the following: The final score is based on the content unit coverage.
Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ However, our SCU annotated summaries and correlated pyramids provide a valuable data resource that will allow us to investigate such questions.
Paralleling work in summarization, it is hypothesized that the quality of a rewritten story can be defined by the presence or absence of 'semantic content units' that are crucial details of the text that may have a variety of syntactic forms (Nenkova and Passonneau, 2004). $$$$$ In addition, the DUC method is not powerful enough to distinguish between systems.

Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ Again, information is lost relative to the pyramid method because a unique reference summary is produced instead of using all the data.
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ Second, creating an initial pyramid is laborious so large-scale application of the method would require an automated or semi-automated approach.
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ Instead of attempting to develop a method to elicit reliable judgments from humans, we chose to calibrate our method to human summarization behavior.
Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.

This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ The strengths of pyramid scores are that they are reliable, predictive, and diagnostic.
This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ We present an empirically grounded method for evaluating content selection in summarization.
This paraphrase matching process is observed in the Pyramid annotation procedure shown in (Nenkova and Passonneau, 2004) over three summary sets (10 summaries each). $$$$$ Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.

From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ We have started exploring the feasibility of automation and we are collecting additional data sets.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ In Section 5, we present our conclusions and point to our next step, the feasibility of automating our method.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ The exact formula we use is computed as follows.
From an in-depth analysis on the manually created SCUs of the DUC2003 summary set D30042 (Nenkova and Passonneau, 2004), we find that 54.48% of 1746 cases where a non-stop word from one SCU did not match with its supposedly human-aligned pairing SCUs are in need of some level of paraphrase matching support. $$$$$ Also, since not much agreement is expected between two summaries, many model units will have no counterpart in the peer and thus the expected scores will necessarily be rather low.

To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ Again, information is lost relative to the pyramid method because a unique reference summary is produced instead of using all the data.
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ At order-5 pyramids, the total probability of error drops to 0.1 and is mainly due to error E2, which is the mildest one.
To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). $$$$$ The use of a single model summary is one of the surprises – all research in summarization evaluation has indicated that no single good model exists.

They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ We hope to address two drawbacks to our method in future work.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ Our first step in investigating score variability was to examine all pairs of summaries where the difference in scores for an order 9 pyramid was greater than 0.1; there were 68 such pairs out of 135 total.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ There are numerous problems with the DUC human evaluation method.

Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ Second, (Halteren and Teufel, 2003) do not make direct use of factoid frequency (our weights): to construct a model 100-word summary, they select factoids that occur in at least 30% of summaries, but within the resulting model summary, they do not differentiate between more and less highly weighted factoids.
Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. $$$$$ We create a weighted inventory of Summary Content Units–a pyramid–that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same meaning.

The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ We intentionally avoid creating a representation language for SCU labels; the function of an SCU label is to focus the annotator’s attention on the shared meaning of the contributors.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ It incorporates the idea that no single best model summary for a collection of documents exists.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ Second, (Halteren and Teufel, 2003) do not make direct use of factoid frequency (our weights): to construct a model 100-word summary, they select factoids that occur in at least 30% of summaries, but within the resulting model summary, they do not differentiate between more and less highly weighted factoids.
The pyramid method (Nenkova and Passonneau, 2004) addresses the problem by using multiple human summaries to create a gold-standard. $$$$$ The strengths of pyramid scores are that they are reliable, predictive, and diagnostic.

As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ Here we use three DUC 2003 summary sets for which four human summaries were written.
As an example, the Pyramid Method (Nenkova and Passonneau, 2004), represents a good first attempt at a realistic model of human variations. $$$$$ The reference summary consists of the sentences with highest relevance judgements that satisfy the compression constraints.

Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ We present an empirically grounded method for evaluating content selection in summarization.
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ The procedure used for evaluating summaries in DUC is the following: The final score is based on the content unit coverage.
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ Instead of attempting to develop a method to elicit reliable judgments from humans, we chose to calibrate our method to human summarization behavior.
Nenkova and Passonneau (2004) proposed a manual evaluation method that was based on the idea that there is no single best model summary for a collection of documents. $$$$$ For each of the 30 test sets, three of the four humanwritten summaries and the machine summaries were scored against the fourth human model summary: each human was scored on ten summaries.

In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ We create a weighted inventory of Summary Content Units–a pyramid–that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same meaning.
In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ Second, creating an initial pyramid is laborious so large-scale application of the method would require an automated or semi-automated approach.
In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. $$$$$ First, pyramid scores ignore interdependencies among content units, including ordering.
