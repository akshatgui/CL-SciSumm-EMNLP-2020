Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. $$$$$ One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent.
Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. $$$$$ In particular, TERp did significantly better than the TER metric, indicating the benefit of the enhancements made to TER.
Recently, Snover et al (2009) extended the TER algorithm in a similar fashion to produce a new evaluation metric, TER plus (TERp), which allows tuning of the edit costs in order to maximize correlation with human judgment. $$$$$ Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics.

In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). $$$$$ TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation.
In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). $$$$$ TERp was one of 39 automatic metrics evaluated in the 2008 NIST Metrics MATR Challenge.
In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). $$$$$ These constraints are intended to simulate the way in which a human editor might choose the words to shift.
In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al, 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2009), and GTM (Turianetal., 2003). $$$$$ While all three types of human judgements differ from the alignment costs used in alignment, the HTER edit costs differ most significantly.

The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. $$$$$ This work was supported, in part, by BBN Technologies under the GALE Program, DARPA/IPTO Contract No.
The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. $$$$$ HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/.
The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. $$$$$ While TER has been shown to correlate well with human judgments of translation quality, it has several flaws, including the use of only a single reference translation and the measuring of similarity only by exact word matches between the hypothesis and the reference.
The extended TER plus (Snover et al 2009) metric addresses the first problem but not the other two. $$$$$ These paraphrase pairs were extracted from Europarl data using each of 10 European languages (German, Italian, French etc.) as a pivot language separately and then combining the extracted paraphrase pairs.

Lemma is added later in the TERplus extension (Snover et al 2009). $$$$$ In several cases, TERp was not the best metric (if a metric was the best in all conditions, its average rank would be 1), although it performed well on average.
Lemma is added later in the TERplus extension (Snover et al 2009). $$$$$ This work was supported, in part, by BBN Technologies under the GALE Program, DARPA/IPTO Contract No.
Lemma is added later in the TERplus extension (Snover et al 2009). $$$$$ While HTER has been shown to be more consistent and finer grained than individual human annotators of Fluency and Adequacy, it is much more time consuming and taxing on human annotators than other types of human judgments, making it difficult and expensive to use.
Lemma is added later in the TERplus extension (Snover et al 2009). $$$$$ As part of the 2008 NIST Metrics MATR workshop (Przybocki et al., 2008), a development subset of translations from eight Arabic-to-English MT systems submitted to NIST’s MTEval 2006 was released that had been annotated for Adequacy.

 $$$$$ TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation.
 $$$$$ We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases.
 $$$$$ Correlation numbers that are statistically indistinguishable from the highest correlation, using a 95% confidence interval, are shown in bold and numbers that are actually not statistically significant correlations are marked with a †.
 $$$$$ Paraphrases have also been shown to be useful in expanding the number of references used for parameter tuning (Madnani et al., 2007; Madnani et al., 2008) although they are not used directly in this fashion within TERp.

Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. $$$$$ These new edit costs can then be optimized to allow better correlation with human judgments.
Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. $$$$$ Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance.
Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. $$$$$ It is likely that some features that make an evaluation metric good for evaluating the final output of a system would make it a poor metric for use in system tuning.
Effort in this direction brings up some advanced metrics such as METEOR (Banerjee and Lavie, 2005) and TERp (Snover et al, 2009) that seem to have already achieved considerably strong correlations with human judgments. $$$$$ We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases.

The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. $$$$$ While all edit costs in TER are constant, all edit costs in TERp are optimized to maximize correlation with human judgments.
The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. $$$$$ Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance.
The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. $$$$$ To determine the effect that the pivot language might have on the quality and utility of the extracted paraphrases in TERp, we used paraphrase pairsmade available by Callison-Burch (2008).
The extended TER plus (Snover et al, 2009) metric addresses the first problem but not the other two. $$$$$ The MT systems evaluated with HTER are all highly performing state of the art systems, while the systems used for adequacy and fluency are older MT systems.

Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009). $$$$$ TER-Plus was shown to be one of the top metrics in NIST’s Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation.
Synonym relations are defined according to WordNet (Miller et al, 1990), and paraphrase matches are given by a lookup table use din TERplus (Snover et al, 2009). $$$$$ One of the difficulties in the creation of targeted references is a further requirement that the annotator attempt to minimize the number of edits, as measured by TER, between the MT output and the targeted reference, creating the reference that is as close as possible to the MT output while still being adequate and fluent.

Lemma is added later in the TERplus extension (Snover et al, 2009). $$$$$ To evaluate the differences between human judgment types we first align the hypothesis to the references using a fixed set of edit costs, identical to the weights in Table 1, and then optimize the edit costs to maximize the correlation, without realigning.
Lemma is added later in the TERplus extension (Snover et al, 2009). $$$$$ Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.
Lemma is added later in the TERplus extension (Snover et al, 2009). $$$$$ This work was supported, in part, by BBN Technologies under the GALE Program, DARPA/IPTO Contract No.
Lemma is added later in the TERplus extension (Snover et al, 2009). $$$$$ Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.

Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. $$$$$ This set consists of the output of ten MT systems, 3 Arabic-to-English systems and 7 Chineseto-English systems, consisting of a total, across all systems and both language pairs, of 7,452 segments across 900 documents.
Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. $$$$$ (50 ton → 50 tons) (caused clouds → causing clouds) (syria deny → syria denies) Given this distribution of the pivot-based paraphrases, we experimented with a variant of TERp that did not use the paraphrase probability at all but instead only used the actual edit distance between the two phrases to determine the final cost of a phrase substitution.
Additionally, the average scores for adequacy and fluency were themselves averaged into a single score, following (Snover et al, 2009), and the Spearman's correlation of each of the automatic metrics with these scores are given in Table 4. $$$$$ The inclusion of stem, synonym and paraphrase edits allows TERp to overcome some of the weaknesses of the TER metric and better align hypothesized translations with reference translations.

The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ These phrases are generated by considering possible paraphrases of the reference words.
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent.
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/.

TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. $$$$$ In order to evaluate the state of automatic MT evaluation, NIST tested metrics across a number of conditions across 8 test sets.
TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. $$$$$ This edit cost for phrasal substitutions is, therefore, specified by four parameters, w1, w2, w3 and w4.
TER= min edits avg ref length (4) TER-Plus (TERp) (Snover et al, 2009) extends TER by allowing the cost of edit operations to be tuned in order to maximize the metric's agreement with human judgments. $$$$$ This confirms our intuition that the probability computed via the pivot-method is not a very useful predictor of semantic equivalence for use in TERp.

However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. $$$$$ With the exception of the phrase substitutions, the cost for all other edit operations is the same regardless of what the words in question are.
However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. $$$$$ We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases.
However, instead of ITG alignments that were used in (Karakos et al, 2008), alignments based on TER-plus (Snover et al, 2009) were used now as the core system alignment algorithm. $$$$$ The generation of paraphrases, as well as the effect of varying the source of paraphrases, is discussed in Section 4.

These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). $$$$$ While all three types of human judgements differ from the alignment costs used in alignment, the HTER edit costs differ most significantly.
These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). $$$$$ This new targeted reference is then used as the reference translation when scoring the MT output using Translation Edit Rate (TER) (Snover et al., 2006) or when used with other automatic metrics such as BLEU or METEOR (Banerjee and Lavie, 2005).
These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). $$$$$ This penalty against stem matches can be attributed to Fluency requirements in HTER that specifically penalize against incorrect morphology.
These operations relax the shifting constraints of TER; shifts are now allowed if the words of one string are synonyms or share the same stem as the words of the string they are compared to (Snover et al, 2009). TER-plus identifies words with the same stem using the Porter stemming algorithm (Porter et al, 1980), and identifies synonyms using the WordNet database (Miller et al, 1995). $$$$$ Correlation results with human judgments, including independent results from the 2008 NIST Metrics MATR evaluation, where TERp was consistently one of the top metrics, are presented in Section 3 to show the utility of TERp as an evaluation metric.

After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. $$$$$ The set of possible correct translations is very large—possibly infinite— and any single reference translation is just a single point in that space.
After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. $$$$$ We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases.
After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. $$$$$ Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.
After the extraction, pruning techniques (Snover et al, 2009) can be applied to increase the precision of the extracted paraphrases. $$$$$ While this can make no judgement as to the preference of one type of human judgment over another, it indicates differences between these human judgment types, and in particular the difference between HTER and Adequacy and Fluency.

TERp (Snover et al, 2009) $$$$$ The test sets included translations from Arabic-to-English, Chinese-toEnglish, Farsi-to-English, Arabic-to-French, and English-to-French MT systems involved in NIST’s MTEval 2008, the GALE (Olive, 2005) Phase 2 and Phrase 2.5 program, Transtac January and July 2007, and CESTA run 1 and run 2, covering multiple genres.
TERp (Snover et al, 2009) $$$$$ These constraints are intended to simulate the way in which a human editor might choose the words to shift.
TERp (Snover et al, 2009) $$$$$ On the other hand, deletion of content words might not lower the fluency of a translation but the adequacy would suffer.

The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. $$$$$ HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/.
The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. $$$$$ Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.
The scores were case-insensitive and edit costs from Snover et al (2009) were used to produce scores tuned for fluency and adequacy. $$$$$ The inclusion of additional more specific edit types could lead to a more detailed understanding of which translation phenomenon and translation errors are most emphasized or ignored by which types of human judgments.

This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. $$$$$ We compare TERp with BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006).
This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. $$$$$ The inclusion of stem, synonym and paraphrase edits allows TERp to overcome some of the weaknesses of the TER metric and better align hypothesized translations with reference translations.
This algorithm is not equivalent to an incremental TER Plus (Snover et al, 2009) due to different shift constraints and the lack of paraphrase matching 30 1cat (1) 2sat (1) mat (1) (a) Skeleton hypothesis. $$$$$ The results further support our claim that the pivot paraphrase probability is not a very useful indicator of semantic relatedness.

Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. $$$$$ These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality.
Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. $$$$$ Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments.
Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. $$$$$ This work was supported, in part, by BBN Technologies under the GALE Program, DARPA/IPTO Contract No.
Its automatic versions TER and TERp (Snover et al 2009), however, remain sentence based metrics. $$$$$ HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/.

In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997). $$$$$ For example, errors in tense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent.
In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997). $$$$$ HR0011-06-C-0022 and in part by the Human Language Technology Center of Excellence.. TERp is available on the web for download at: http://www.umiacs.umd.edu/∼snover/terp/.
In addition to the BLEU metric, models can be trained to optimize other popular evaluation metrics such as METEOR (Lavie and Denkowski, 2009), TERp (Snover et al, 2009) ,mWER (Nieen et al, 2000), and PER (Tillmann et al, 1997). $$$$$ Unlike Adequacy and Fluency which have a low edit cost for insertions and a very high cost for deletions, HTER has a balanced cost for the two edit types.
