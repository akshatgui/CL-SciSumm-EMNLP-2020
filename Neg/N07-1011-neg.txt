Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ A standard machine learning approach is to perform a set of independent binary classifications of the form “Is mention a coreferent with mention b?” This approach of decomposing the problem into pairwise decisions presents at least two related difficulties.
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ Given a set of noun phrases xj = {xi}, let the binary random variable yj be 1 if all the noun phrases xi E xj are coreferent.

In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ Future work will extend our approach to a wider variety of tasks.
In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ The model we have described here is specific to clustering tasks; however a similar formulation could be used to approach a number of language processing tasks, such as parsing and relation extraction.
In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems.

The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.
The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ FIRST-ORDER MIRA is our proposed model that takes advantage of first-order features of the data and is trained with error-driven and rank-based methods.
The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ The model we have described here is specific to clustering tasks; however a similar formulation could be used to approach a number of language processing tasks, such as parsing and relation extraction.

Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ This problem has recently been addressed by a number of researchers.
Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ The model we have described here is specific to clustering tasks; however a similar formulation could be used to approach a number of language processing tasks, such as parsing and relation extraction.
Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ Our experiments show the advantages of this ranking-based loss function.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ A short-term extension would be to consider features over entire clusterings, such as the number of clusters.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems.

However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ Milch et al. (2005) address these issues by constructing a generative probabilistic model, where noun clusters are sampled from a generative process.
However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ Precision for xi is defined as ci divided by the number of noun phrases in xi’s cluster.
However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ These tasks could benefit from first-order features, and the present work can guide the approximations required in those domains.
However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method.

Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ This problem has recently been addressed by a number of researchers.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ Future work will extend our approach to a wider variety of tasks.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ Sutton and McCallum (2005) show that such a piecewise approximation can be theoretically justified as minimizing an upper bound of the exact loss function.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ While in theory a metaclassifier can flexibly represent features, they do not explore features using the full flexibility of firstorder logic.

 $$$$$ We use the true entity segmentation, and parse each sentence in the corpus using a phrase-structure grammar, as is common for this task.
 $$$$$ This approach can therefore be viewed as a type of piecewise approximation of exact parameter estimation in these models (Sutton and McCallum, 2005).
 $$$$$ A standard machine learning approach is to perform a set of independent binary classifications of the form “Is mention a coreferent with mention b?” This approach of decomposing the problem into pairwise decisions presents at least two related difficulties.

As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ A factor graph for the First-Order Model is presented in Figure 3 for three noun phrases.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ For the Pairwise Model, a corresponding undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the pairwise noun phrase compatibility as fc(yij, xij) = exp(Ek λkfk(yij, xij)).

Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ Future work will extend our approach to a wider variety of tasks.
Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ A standard machine learning approach is to perform a set of independent binary classifications of the form “Is mention a coreferent with mention b?” This approach of decomposing the problem into pairwise decisions presents at least two related difficulties.
Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ We thank Robert Hall for helpful contributions.

For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems.
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ Additionally, we average the parameters calculated at each iteration to improve convergence.

Many of our features are similar to those described in Culotta et al (2007). $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.
Many of our features are similar to those described in Culotta et al (2007). $$$$$ Also note that the Pairwise baseline obtains results similar to those in Ng and Cardie (2002).
Many of our features are similar to those described in Culotta et al (2007). $$$$$ We refer to the training and inference methods described in this section as the Pairwise Model.

Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ This is an example of a model with extremely flexible representational power, but for which exact inference is intractable.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ We thank Robert Hall for helpful contributions.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ Here, transitivity is a bit more complicated, since it also requires that if yj = 1, then for any subset xk C_ xj, yk = 1.

For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ However, as shown in recent work (McCallum and Wellner, 2003; Singla and Domingos, 2005), better performance can be obtained by performing relational inference to directly consider the dependence among a set of predictions.
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ This is an example of a model with extremely flexible representational power, but for which exact inference is intractable.
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ The simple approximations we have described here have enabled this more flexible model to outperform a model that is simplified for tractability.
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ However, because there may exists subsets of the cluster that are coreferent, features representing these positive subsets may be unjustly penalized.
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ Here, each fc is a “piece” of the model trained independently.
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ This allows us to use the full flexibility of first-order logic to construct features about sets of nouns.

Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ Error analysis indicates that often noun xi is correctly not merged with a cluster xj when xj has a strong internal coherence.
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ Future work will extend our approach to a wider variety of tasks.
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.

Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates $$$$$ Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms.
Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates $$$$$ This problem has recently been addressed by a number of researchers.

 $$$$$ These tasks could benefit from first-order features, and the present work can guide the approximations required in those domains.
 $$$$$ The features fk and weights Ak are defined as before, but now the features can represent arbitrary attributes over the entire set xj.
 $$$$$ We see that both the first-order features and the training enhancements improve performance consistently. wise Model in F1 measure for both standard training and error-driven training.
 $$$$$ While in theory a metaclassifier can flexibly represent features, they do not explore features using the full flexibility of firstorder logic.

 $$$$$ In general, we would like to construct arbitrary features over a cluster of noun phrases using the full expressivity of first-order logic.
 $$$$$ This approach allows the update to only penalize the difference between the two features of examples, thereby not penalizing features representing any overlapping coreferent clusters.
 $$$$$ For example, if all 5 mentions of France in a document are string identical, then the system will be extremely cautious of merging a noun that is not equivalent to France into xj, since this will turn off the “All-String-Match” feature for cluster xj.

We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ In general, we would like to construct arbitrary features over a cluster of noun phrases using the full expressivity of first-order logic.
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ For the Pairwise Model, a corresponding undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the pairwise noun phrase compatibility as fc(yij, xij) = exp(Ek λkfk(yij, xij)).
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ This is an example of a model with extremely flexible representational power, but for which exact inference is intractable.
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ However, because there may exists subsets of the cluster that are coreferent, features representing these positive subsets may be unjustly penalized.
