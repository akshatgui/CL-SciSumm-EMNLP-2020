Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ Here, each fc is a “piece” of the model trained independently.
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ Factor ft enforces the transitivity constraints by ft(·) = −oc if transitivity is not satisfied, 1 otherwise.
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ The problem stems from the transitivity constraints of coreference: If a and b are coreferent, and b and c are coreferent, then a and c must be coreferent.
Culotta et al (2007) present a system which uses an online learning approach to train a classifier to judge whether two entities are coreferential or not. $$$$$ At testing time, we perform standard greedy agglomerative clustering, where the score for each merger is proportional to the probability of the newly formed clustering according to the model.

In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.
In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.
In our study, we also tested the "Most-X" strategy for the first-order features as in (Culotta et al., 2007), but got similar results without much difference (±0.5% F-measure) in performance. $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.

The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ It updates the parameter vector with two constraints: (1) the positive example must have a higher score by a given margin, and (2) the change to A should be minimal.
The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ To implement this update, we use MIRA (Margin Infused Relaxed Algorithm), a relaxed, online maximum margin training algorithm (Crammer and Singer, 2003).
The one exception is the size of a cluster (Culotta et al, 2007). $$$$$ For the First-Order model, an undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the cluster-wise noun phrase compatibility as fc(yj, xj) = exp(Ek λkfk(yj, xj)).

Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ 336 documents are used for training, and the remainder for testing.
Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ These pairwise decisions are made collectively using relational inference; however, as pointed out in Milch et al. (2004), this model has limited representational power since it does not capture features of entities, only of pairs of mention.
Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.
Culotta et al (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm. $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.

Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ This approach can therefore be viewed as a type of piecewise approximation of exact parameter estimation in these models (Sutton and McCallum, 2005).
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ For the Pairwise Model, a corresponding undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the pairwise noun phrase compatibility as fc(yij, xij) = exp(Ek λkfk(yij, xij)).
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ For the Pairwise Model, a corresponding undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the pairwise noun phrase compatibility as fc(yij, xij) = exp(Ek λkfk(yij, xij)).
However, with the advent of the ACE data, many systems either evaluated only true mentions, i.e. mentions which are included in the annotation, the so-called key, or even received true information for mention boundaries, heads of mentions and mention type (Culotta et al, 2007, inter alia). $$$$$ Factor ft enforces the transitivity constraints by ft(·) = −oc if transitivity is not satisfied, 1 otherwise.

Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ Consider a training example cluster with a negative label, indicating that not all of the noun phrases it contains are coreferent.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ We follow Soon et al. (2001) and Ng and Cardie (2002) to generate most of our features for the Pairwise Model.
Culotta et al (2007) introduce a first-order probabilistic model which implements features over sets of mentions. $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are the author(s)’ and do not necessarily reflect those of the sponsor.

 $$$$$ This restriction can be detrimental if there exist features of sets of noun phrases that cannot be captured by a combination of pairwise features.
 $$$$$ A simple method to generate training examples is to sample positive and negative cluster examples uniformly at random from the training data.
 $$$$$ Luo et al. (2004) do enable features between mention-cluster pairs, but do not perform the error-driven and ranking enhancements proposed in our work.
 $$$$$ Note that this model gives us the representational power of recently proposed Markov logic networks (Richardson and Domingos, 2006); that is, we can construct arbitrary formulae in first-order logic to characterize the noun coreference task, and can learn weights for instantiations of these formulae.

As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ However, they do not investigate rank-based loss functions.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ Traditional noun phrase coreference resolution systems represent features only of pairs of noun phrases.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ However, naively grounding the corresponding Markov logic network results in a combinatorial explosion of variables.
As we show, this combination of a pairwise model and strong features produces a 1.5 percentage point increase in B-Cubed F-Score over a complex model in the state-of-the-art system by Culotta et al. (2007), although their system uses a complex, non-pairwise model, computing features over partial clusters of mentions. $$$$$ First, because each training example consists of a subset of noun phrases, the number of possible training examples we can generate is exponential in the number of noun phrases.

Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ A short-term extension would be to consider features over entire clusterings, such as the number of clusters.
Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ These pieces are combined at prediction time using clustering algorithms to enforce transitivity.
Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ A short-term extension would be to consider features over entire clusterings, such as the number of clusters.
Culotta et al (2007) introduced a model that predicts whether a pair of equivalence classes should be merged, using features computed over all the mentions in both classes. $$$$$ Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems.

For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ This is similar to the model presented in McCallum and Wellner (2005).
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ The methods described in Sections 2, 3 and 4 can be viewed as estimating the parameters of each factor fc independently.
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ Future work will extend our approach to a wider variety of tasks.
For our experiments in Section 5, we use gold mention types as is done by Culotta et al (2007) and Luo and Zitouni (2005). $$$$$ A factor graph for the First-Order Model is presented in Figure 3 for three noun phrases.

Many of our features are similar to those described in Culotta et al (2007). $$$$$ A short-term extension would be to consider features over entire clusterings, such as the number of clusters.
Many of our features are similar to those described in Culotta et al (2007). $$$$$ We outline a set of approximations that make this approach practical, and apply our method to the ACE coreference dataset, achieving a 45% error reduction over a comparable method that only considers features of pairs of noun phrases.
Many of our features are similar to those described in Culotta et al (2007). $$$$$ Positive examples are generated by first sampling a true cluster, then sampling a subset of that cluster.

Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ By relying on sampling methods at training time and approximate inference methods at testing time, this approach can be made scalable.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ Recall for xi is defined as the ci divided by the number of mentions in the gold standard cluster for xi.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ Nicolae and Nicolae (2006) combine pairwise classification with graph-cut algorithms.
Our test set contains the same 107 documents as Culotta et al (2007). $$$$$ For the First-Order model, an undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the cluster-wise noun phrase compatibility as fc(yj, xj) = exp(Ek λkfk(yj, xj)).

For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ Again, factor ft enforces the transitivity constraints by ft(·) = −oc if transitivity is not satisfied, 1 otherwise.
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ We apply our approach to the noun coreference ACE 2004 data, containing 443 news documents with 28,135 noun phrases to be coreferenced.
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ The algorithm is as follows: Given initial parameters A, perform greedy agglomerative clustering on training document i until an incorrect cluster is formed.
For the experiments in Section 5, following Culotta et al (2007), to make experiments more comparable across systems, we assume that perfect mention boundaries and mention type labels are given. $$$$$ This work was supported in part by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under contract #NBCHD030010, in part by U.S. Government contract #NBCH040171 through a subcontract with BBNT Solutions LLC, in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant #IIS-0326249, in part by Microsoft Live Labs, and in part by the Defense Advanced Research Projects Agency (DARPA) under contract #HR0011-06-C-0023.

Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ There have been a number of machine learning approaches to coreference resolution, traditionally factored into classification decisions over pairs of nouns (Soon et al., 2001; Ng and Cardie, 2002).
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ We see that both the first-order features and the training enhancements improve performance consistently. wise Model in F1 measure for both standard training and error-driven training.
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ A standard machine learning approach is to perform a set of independent binary classifications of the form “Is mention a coreferent with mention b?” This approach of decomposing the problem into pairwise decisions presents at least two related difficulties.
Culotta et al (2007) is the best comparable system of which we are aware. $$$$$ (However, we should note that there are also small differences in the feature sets used for error-driven and standard training results.)

Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ For example, McCallum and Wellner (2005) apply a graph partitioning algorithm on a weighted, undirected graph in which vertices are noun phrases and edges are weighted by the pairwise score between noun phrases.
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005).
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ This result demonstrates an example of how a firstorder logic representation can be incorporated into a probabilistic model and scaled efficiently.
Note that since we report results on Dev-Eval, the results in Table 6 are not directly comparable with Culotta et al (2007). $$$$$ Collins and Roark (2004) present an incremental perceptron algorithm for parsing that uses “early update” to update the parameters when an error is encountered.

Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL. $$$$$ Of course, we do not enumerate this set; rather, we incrementally instantiate y variables as needed during prediction.
Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL. $$$$$ A simple approach is to perform the transitive closure of the pairwise decisions.
Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL. $$$$$ In this paper, we propose a machine learning method enables features over noun phrases, resulting in a first-order probabilistic model for coreference.
Motivated in part by Culotta et al (2007), we create cluster-level features from the relational features in our feature set using four predicates: NONE, MOST FALSE, MOST-TRUE, and ALL. $$$$$ Below we outline methods to scale training and prediction with this representation.

 $$$$$ A factor graph for the First-Order Model is presented in Figure 3 for three noun phrases.
 $$$$$ We refer to the system described in this section as First-Order MIRA.
 $$$$$ In addition to the listed features, we also include conjunctions of size 2, for example “Genders match AND numbers match”.
 $$$$$ The number of y variables increases exponentially in the number of x variables.

 $$$$$ Below we outline methods to scale training and prediction with this representation.
 $$$$$ For the First-Order model, an undirected graphical model can be defined as where Zx is the input-dependent normalizer and factor fc parameterizes the cluster-wise noun phrase compatibility as fc(yj, xj) = exp(Ek λkfk(yj, xj)).
 $$$$$ FIRST-ORDER MIRA is our proposed model that takes advantage of first-order features of the data and is trained with error-driven and rank-based methods.

We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ However, naively grounding the corresponding Markov logic network results in a combinatorial explosion of variables.
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ Perhaps the most related is “learning as search optimization” (LASO) (Daum´e III and Marcu, 2005b; Daum´e III and Marcu, 2005a).
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ Milch et al. (2005) address these issues by constructing a generative probabilistic model, where noun clusters are sampled from a generative process.
We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al (2007) and Bengston and Roth (2008). $$$$$ Again, factor ft enforces the transitivity constraints by ft(·) = −oc if transitivity is not satisfied, 1 otherwise.
