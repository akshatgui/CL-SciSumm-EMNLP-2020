As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model $$$$$ Computational theories of discourse, in particular focusing (see Grosz (1977) and Sidner (1979)) and centering (Grosz et al. (1983; 1995)), have heavily influenced coreference research in the 1970s and 1980s, leading to the development of numerous centering algorithms (see Walker et al.
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model $$$$$ The MUC-6 and MUC-7 corpora, though relatively small (60 documents each) and homogeneous w.r.t. document type (newswire articles only), have been extensively used for training and evaluating coreference models.

(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue refer to the same real-world entity, has been at the core of natural language processing (NLP) since the 1960s.
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ Several algorithms that address one or both of these problems have been used for coreference clustering.
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ The widespread popularity of machine learning approaches to coreference resolution can be attributed in part to the public availability of annotated coreference corpora.
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.

This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ It is worth noting that Tetreault (2005) has employed Grosz and Sidner’s (1986) discourse theory and Veins Theory (Ide and Cristea, 2000) to identify and remove candidate antecedents that are not referentially accessible to an anaphoric pronoun in his heuristic pronoun resolvers.
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ Besides simple string-matching operations such as exact string match, substring match, and head noun match for different kinds of NPs (see Daum´e III and Marcu (2005)), slightly more sophisticated stringmatching facilities have been attempted, including minimum edit distance (Strube et al., 2002) and longest common subsequence (Casta˜no et al., 2002).

A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ As Mitkov (2001) puts it, coreference resolution is a “difficult, but not intractable problem,” and we have been making “slow, but steady progress” on improving machine learning approaches to the problem in the past fifteen years.
A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Publicly available coreference systems currently include JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), BART (Versley et al., 2008b), CoRTex (Denis and Baldridge, 2008), the Illinois Coreference Package (Bengtson and Roth, 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), and Charniak and Elsner’s (2009) pronoun resolver.
A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Expressive models (e.g., those that can exploit cluster-level features) generally offer better performance, and so are models that are “global” in nature.

In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ Memorization features have been used as binary-valued features indicating the presence or absence of their words (Luo et al., 2004) or as probabilistic features indicating the probability that the two heads are coreferent according to the training data (Ng, 2007b).
In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ While we have focused our discussion on supervised approaches, coreference researchers have also attempted to reduce a resolver’s reliance on annotated data by combining a small amount of labeled data and a large amount of unlabeled data using general-purpose semi-supervised learning algorithms such as co-training (M¨uller et al., 2002), self-training (Kehler et al., 2004a), and EM (Cherry and Bergsma, 2005; Ng, 2008).

The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ (2006) employ these trees directly as structured features for pronoun resolution.
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ As noted above, the primary purpose of training instance creation is to reduce class skewness.
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ Ge et al. (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs’s (1978) seminal syntax-based pronoun resolution algorithm.

In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ In addition, researchers have developed approaches that are targeted at handling certain kinds of anaphora present in non-English languages, such as zero anaphora (e.g., Iida et al. (2007a), Zhao and Ng (2007)).
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.

For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ In addition, researchers have developed approaches that are targeted at handling certain kinds of anaphora present in non-English languages, such as zero anaphora (e.g., Iida et al. (2007a), Zhao and Ng (2007)).
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ In practice, these general-purpose methods are typically used to provide scores that complement those obtained via the three coreference scorers discussed above.
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ Computational theories of discourse, in particular focusing (see Grosz (1977) and Sidner (1979)) and centering (Grosz et al. (1983; 1995)), have heavily influenced coreference research in the 1970s and 1980s, leading to the development of numerous centering algorithms (see Walker et al.

Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ Despite its improved expressiveness, the entitymention model has not yielded particularly encouraging results.
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ Due to space limitations, however, we will restrict our discussion to the most commonly investigated kind of coreference relation: the identity relation for NPs, excluding coreference among clauses and bridging references (e.g., part/whole and set/subset relations).
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ An important issue with ranking models that we have eluded so far concerns the identification of non-anaphoric NPs.
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ This work was supported in part by NSF Grant IIS-0812261.

We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ This work was supported in part by NSF Grant IIS-0812261.
We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ Being inherently a clustering task, coreference has also received a lot of attention in the machine learning community.

The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Expressive models (e.g., those that can exploit cluster-level features) generally offer better performance, and so are models that are “global” in nature.
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.

Interested readers can refer to the literature review by Ng (2010). $$$$$ Other publicly available coreference corpora of interest include two annotated by Ruslan Mitkov’s research group: (1) a 55,000-word corpus in the domain of security/terrorism (Hasler et al., 2006); and (2) training data released as part of the 2007 Anaphora Resolution Exercise (Or˘asan et al., 2008), a coreference resolution shared task.
Interested readers can refer to the literature review by Ng (2010). $$$$$ These include (1) the English Penn Treebank (Marcus et al., 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al., 2006); (2) the T¨ubingen Treebank (Telljohann et al., 2004), which is a collection of German news articles consisting of 27,125 sentences; (3) the Prague Dependency Treebank (Haji˘c et al., 2006), which consists of 3168 news articles taken from the Czech National Corpus; (4) the NAIST Text Corpus (Iida et al., 2007b), which consists of 287 Japanese news articles; (5) the AnCora Corpus (Recasens and Marti, 2009), which consists of Spanish and Catalan journalist texts; and (6) the GENIA corpus (Ohta et al., 2002), which contains 2000 MEDLINE abstracts.
Interested readers can refer to the literature review by Ng (2010). $$$$$ The focus of coreference research underwent a gradual shift from heuristic approaches to machine learning approaches in the 1990s.
Interested readers can refer to the literature review by Ng (2010). $$$$$ It would be interesting to incorporate this idea into a learning-based resolver.

Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ There are also two that consist of spoken dialogues: the TRAINS93 corpus (Heeman and Allen, 1995) and the Switchboard data set (Calhoun et al., in press).
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ The MUC-6 and MUC-7 corpora, though relatively small (60 documents each) and homogeneous w.r.t. document type (newswire articles only), have been extensively used for training and evaluating coreference models.

For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ There are also two that consist of spoken dialogues: the TRAINS93 corpus (Heeman and Allen, 1995) and the Switchboard data set (Calhoun et al., in press).
For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Ge et al. (1998) implement a Hobbs distance feature, which encodes the rank assigned to a candidate antecedent for a pronoun by Hobbs’s (1978) seminal syntax-based pronoun resolution algorithm.
For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ In addition, Massimo Poesio and his colleagues are leading an annotation project that aims to collect large amounts of coreference data for English via a Web Collaboration game called Phrase Detectives2.

 $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
 $$$$$ Ranking models, on the other hand, allow us to determine which candidate antecedent is most probable given an NP to be resolved.
 $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
