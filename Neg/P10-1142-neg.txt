As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model: Generation of (transitively) redundant pairs, as the formation of co reference sets (co reference clustering) is done after pairwise classification. $$$$$ Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)), such as: whether NPj/k appears in the first paragraph of the Wiki page that has NPk/j as the title or in the list of categories to which this page belongs, and the degree of overlap between the two pages that have the two NPs as their titles (see Poesio et al. (2007) for other uses of encyclopedic knowledge for coreference resolution).
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model: Generation of (transitively) redundant pairs, as the formation of co reference sets (co reference clustering) is done after pairwise classification. $$$$$ In particular, while existing survey papers discuss learning-based coreference research primarily in the context of the influential mention-pair model, we additionally survey recently proposed learning-based coreference models, which attempt to address the weaknesses of the mention-pair model.
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model: Generation of (transitively) redundant pairs, as the formation of co reference sets (co reference clustering) is done after pairwise classification. $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
As recently discussed in (Ng, 2010), the so called mention-pair model suffers from several design flaws which originate from the locally confined perspective of the model: Generation of (transitively) redundant pairs, as the formation of co reference sets (co reference clustering) is done after pairwise classification. $$$$$ We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.

(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ Given an anaphoric noun phrase3, NPk, Soon et al.’s method creates a positive instance between NPk and its closest preceding antecedent, NPj, and a negative instance by pairing NPk with each of the intervening NPs, NPj+1, ..., NPk−1.
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ For example, a memorization feature is a word pair composed of the head nouns of the two NPs involved in an instance (Bengtson and Roth, 2008).
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ Despite its simple task definition, coreference is generally considered a difficult NLP task, typically involving the use of sophisticated knowledge sources and inference procedures (Charniak, 1972).
(Ng, 2010) discusses the entity-mention model which operates on emerging co reference sets to create features describing the relation of an anaphor candidate and established co reference sets. $$$$$ To obtain the set of NPs to be partitioned by a resolver, three methods are typically used.

This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ First, how do we obtain the set of NPs that a resolver will partition?
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ Second, how do we score the partition it produces?
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ In particular, while existing survey papers discuss learning-based coreference research primarily in the context of the influential mention-pair model, we additionally survey recently proposed learning-based coreference models, which attempt to address the weaknesses of the mention-pair model.
This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be co referent so far (for more discussion on the model types, see, e.g., (Ng, 2010)). $$$$$ We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.

A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Computational theories of discourse, in particular focusing (see Grosz (1977) and Sidner (1979)) and centering (Grosz et al. (1983; 1995)), have heavily influenced coreference research in the 1970s and 1980s, leading to the development of numerous centering algorithms (see Walker et al.
A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Other publicly available coreference corpora of interest include two annotated by Ruslan Mitkov’s research group: (1) a 55,000-word corpus in the domain of security/terrorism (Hasler et al., 2006); and (2) training data released as part of the 2007 Anaphora Resolution Exercise (Or˘asan et al., 2008), a coreference resolution shared task.
A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
A better idea of the progress in the field can be obtained by reading recent survey articles (Ng, 2010) and tutorials (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Another thread of supervised coreference research concerns the development of linguistic features.

In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ Other commonly-used logical predicates for creating cluster-level features include relaxed versions of the ALL predicate, such as MOST, which is true if NPk agrees in number with more than half of the NPs in Cj, and ANY, which is true as long as NPk agrees in number with just one of the NPs in Cj.
In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al.’s method: if NPk is non-pronominal, a positive instance should be formed between NPk and its closest preceding nonpronominal antecedent instead.
In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ It is worth mentioning that there is a trend towards evaluating a resolver against multiple scorers, which can indirectly help to counteract the bias inherent in a particular scorer.
In computational linguistics, the increasing availability of annotated coreference corpora has led to developments in machine learning approaches to automatic co reference resolution (see Ng, 2010). $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.

The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ Despite its simple task definition, coreference is generally considered a difficult NLP task, typically involving the use of sophisticated knowledge sources and inference procedures (Charniak, 1972).
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ While mention rankers have consistently outperformed the mention-pair model (Versley, 2006; Denis and Baldridge, 2007b), they are not more expressive than the mention-pair model, as they are unable to exploit cluster-level features, unlike the entitymention model.
The task of automatic NP coreference resolution is to determine "which NPs in a text [...] refer to the same real-world entity" (Ng, 2010, p. 1396). $$$$$ In particular, while existing survey papers discuss learning-based coreference research primarily in the context of the influential mention-pair model, we additionally survey recently proposed learning-based coreference models, which attempt to address the weaknesses of the mention-pair model.

In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ This work was supported in part by NSF Grant IIS-0812261.
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ Equally popular are the corpora produced by the Automatic Content Extraction (ACE1) evaluations in the past decade: while the earlier ACE corpora (e.g., ACE-2) consist of solely English newswire and broadcast news articles, the later ones (e.g., ACE 2005) have also included Chinese and Arabic documents taken from additional sources such as broadcast conversations, webblog, usenet, and conversational telephone speech.
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ This is not an easy question, as researchers have been evaluating their resolvers on different corpora using different evaluation metrics and preprocessing tools.
In principle, this algorithm is too greedy and sometimes results in unreasonable partition (Ng, 2010). $$$$$ To ensure further progress, researchers should compare their results against a baseline that is stronger than the commonly-used Soon et al. (2001) system, which relies on a weak model (i.e., the mention-pair model) and a small set of linguistic features.

For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ Two important issues surround the evaluation of a coreference resolver.
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ These include (1) the English Penn Treebank (Marcus et al., 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al., 2006); (2) the T¨ubingen Treebank (Telljohann et al., 2004), which is a collection of German news articles consisting of 27,125 sentences; (3) the Prague Dependency Treebank (Haji˘c et al., 2006), which consists of 3168 news articles taken from the Czech National Corpus; (4) the NAIST Text Corpus (Iida et al., 2007b), which consists of 287 Japanese news articles; (5) the AnCora Corpus (Recasens and Marti, 2009), which consists of Spanish and Catalan journalist texts; and (6) the GENIA corpus (Ohta et al., 2002), which contains 2000 MEDLINE abstracts.
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ Some clustering algorithms bear a closer resemblance to the way a human creates coreference clusters.
For a historical account and assessent of work in automated anaphora resolution in this period and afterwards, we direct the reader to Strube (2007), Ng (2010) and Stede (2012). $$$$$ Compared to earlier work on anaphoricity determination, recently proposed approaches are more “global” in nature, taking into account the pairwise decisions made by the mention-pair model when making anaphoricity decisions.

Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ Grammatical features encode the grammatical properties of one or both NPs involved in an instance.
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ To obtain the set of NPs to be partitioned by a resolver, three methods are typically used.
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver.
Ng (2010) provides an excellent overview of the history and recent developments within the field. $$$$$ Computational theories of discourse, in particular focusing (see Grosz (1977) and Sidner (1979)) and centering (Grosz et al. (1983; 1995)), have heavily influenced coreference research in the 1970s and 1980s, leading to the development of numerous centering algorithms (see Walker et al.

We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al.’s method: if NPk is non-pronominal, a positive instance should be formed between NPk and its closest preceding nonpronominal antecedent instead.
We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e.g., Converse (2006)), Japanese (e.g., Iida (2007)), Arabic (e.g., Luo and Zitouni (2005)), Dutch (e.g., Hoste (2005)), German (e.g., Wunsch (2010)), Swedish (e.g., Nilsson (2010)), and Czech (e.g., Ngu.y et al. (2009)).
We follow the standard architecture where mentions are extracted in the first step, then they are clustered using a pair-wise classifier (see e.g., (Ng, 2010)). $$$$$ Learning-based coreference research has remained vibrant since then, with results regularly published not only in general NLP conferences, but also in specialized conferences (e.g., the biennial Discourse Anaphora and Anaphor Resolution Colloquium (DAARC)) and workshops (e.g., the series of Bergen Workshop on Anaphora Resolution (WAR)).

The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Learning-based coreference research has remained vibrant since then, with results regularly published not only in general NLP conferences, but also in specialized conferences (e.g., the biennial Discourse Anaphora and Anaphor Resolution Colloquium (DAARC)) and workshops (e.g., the series of Bergen Workshop on Anaphora Resolution (WAR)).
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.
The two most common decoding algorithms often found in literature are the so-called BestFirst (henceforth BF) and ClosestFirst (CF) algorithms (Ng, 2010). $$$$$ Being inherently a clustering task, coreference has also received a lot of attention in the machine learning community.

Interested readers can refer to the literature review by Ng (2010). $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
Interested readers can refer to the literature review by Ng (2010). $$$$$ Additional coreference data will be available in the near future.
Interested readers can refer to the literature review by Ng (2010). $$$$$ While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their coreference algorithm, Stoyanov et al. (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver.

Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ There are also features that encode general linguistic preferences either for or against coreference.
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.
Excellent surveys are provided by Strube (2007) and Ng (2010). Unresolved anaphora can add significant translation ambiguity, and their incorrect translation can significantly decrease a reader's ability to understand a text. $$$$$ The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade.

For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Publicly available coreference systems currently include JavaRAP (Qiu et al., 2004), GuiTaR (Poesio and Kabadjov, 2004), BART (Versley et al., 2008b), CoRTex (Denis and Baldridge, 2008), the Illinois Coreference Package (Bengtson and Roth, 2008), CherryPicker (Rahman and Ng, 2009), Reconcile (Stoyanov et al., 2010), and Charniak and Elsner’s (2009) pronoun resolver.
For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Any opinions, findings, and conclusions or recommendations expressed are those of the author and do not necessarily reflect the views or official policies, either expressed or implied, of the NSF.
For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ Below we discuss how these weaknesses are addressed by the entity-mention model and ranking models.
For a detailed survey of the progress in this field, we refer the reader to a recent article (Ng, 2010) and a tutorial (Ponzetto and Poesio, 2009) dedicated to this subject. $$$$$ For example, Ng and Cardie (2002c) report that bestfirst clustering is better than closest-first clustering.

 $$$$$ Some researchers simply use the first sense (Soon et al., 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006).
 $$$$$ Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue refer to the same real-world entity, has been at the core of natural language processing (NLP) since the 1960s.
 $$$$$ Our goal in this paper is to provide NLP researchers with a survey of the major milestones in supervised coreference research, focusing on the computational models, the linguistic features, the annotated corpora, and the evaluation metrics that were developed in the past fifteen years.
 $$$$$ We thank the three anonymous reviewers for their invaluable comments on an earlier draft of the paper.
