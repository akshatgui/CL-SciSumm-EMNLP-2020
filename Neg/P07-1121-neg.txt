This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007).
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ Figure 1 shows two sample logical forms and their English glosses.
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ To compute the yield of this MR parse tree, we start from the leaf nodes: apply Ax1.state(x1) to the argument (x1), and Ax1.Ax2.area(x1,x2) to the arguments (x1, x2).
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ In this work, we extend the WASP algorithm by adding a variable-binding mechanism based on λ-calculus, which allows for compositional semantics for logical forms.

They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ Certain applications may require different meaning representations, e.g. frame semantics.
They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ All derivations start with a pair of co-indexed start symbols, (S1 , S1 ).
They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.
They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ The parser maintains a set of possible entity types for each logical variable introduced in a partial derivation (except those that are no longer visible).

 $$$$$ We have described the A-WASP algorithm which generates logical forms based on A-calculus.
 $$$$$ Although Prolog logical forms are the main focus of this paper, our algorithm makes minimal assumptions about the target MRL.
 $$$$$ The average length of a sentence is 7.57 words.
 $$$$$ A semantic parser is learned given a set of training sentences and their correct logical forms using standard SMT techniques.

 $$$$$ It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing.
 $$$$$ The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms).
 $$$$$ Figure 2(a) shows a sample parse tree of a logical form, where each CFG production corresponds to a subformula.
 $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.

 $$$$$ As a concrete example, Figure 2(b) shows an MR parse tree that corresponds to the English parse, [What is the [smallest [state] [by area]]], based on the A-SCFG rules in Figure 3.
 $$$$$ Throughout this paper, we use the notation x1, x2,... for logical variables.
 $$$$$ This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.

It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ By keeping occurrences of a logical variable in close proximity in the MR parse tree, we also avoid unnecessary variable bindings in the extracted rules.
It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ The yield of a derivation is a pair of terminal strings, (e, f), where e is an NL sentence and f is the MR translation of e. For convenience, we call an SCFG production a rule throughout this paper.
It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ For example, to handle long-distance dependencies that occur in open-domain text, CCG and TAG would be more appropriate than CFG.
It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ Synchronous grammars have also been used in other NLP tasks, most notably semantic parsing, which is the construction of a complete, formal meaning representation (MR) of an NL sentence.

 $$$$$ This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005).
 $$$$$ If there is a logical variable that cannot refer to any types of entities (i.e. the set of entity types is empty), then the partial derivation is considered invalid.
 $$$$$ Figure 2(a) shows a sample parse tree of a logical form, where each CFG production corresponds to a subformula.

 $$$$$ For example, it should be straightforward to adapt A-WASP to the NL generation task—all one needs is a decoder that can handle input logical forms.
 $$$$$ In this work, we mainly consider the GEOQUERY domain, where a query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996).
 $$$$$ We evaluated the A-WASP algorithm in the GEOQUERY domain.

 $$$$$ For example, a state cannot possibly be a river (Figure 7(a)).
 $$$$$ This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.
 $$$$$ No rules can be extracted for the state predicate, because the shortest NL substring that covers the word states and the argument string Texas, i.e. states bordering Texas, contains the word bordering, which is linked to an MRL production outside the MR sub-parse rooted at state.
 $$$$$ The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms).

Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.
Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms).
Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ A semantic parser is learned given a set of training sentences and their correct logical forms using standard SMT techniques.
Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ Acknowledgments: We thank Rohit Kate, Razvan Bunescu and the anonymous reviewers for their valuable comments.

We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ To compute the yield of this MR parse tree, we start from the leaf nodes: apply Ax1.state(x1) to the argument (x1), and Ax1.Ax2.area(x1,x2) to the arguments (x1, x2).
We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ Figure 8 shows the learning curves for the AWASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and Mooney, 2005), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&C), a CCG-based algorithm which uses Prolog logical forms.
We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ Figure 8 shows the learning curves for the AWASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and Mooney, 2005), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&C), a CCG-based algorithm which uses Prolog logical forms.
We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ The parser maintains a set of possible entity types for each logical variable introduced in a partial derivation (except those that are no longer visible).

For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ Applying this A-function to (x1) gives the MR string smallest(x2,(state(x1),area(x1,x2))).
For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule.
For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ The resulting parser is shown to be the bestperforming system so far in a database query domain.

 $$$$$ Other tasks that can potentially benefit from this include question answering and interlingual MT.
 $$$$$ The average length of a sentence is 7.57 words.
 $$$$$ Figure 1 shows two sample logical forms and their English glosses.
 $$$$$ Figure 1 shows two sample logical forms and their English glosses.

Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. $$$$$ [smallest [state] [by area]] in Figure 3.
Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. $$$$$ In contrast, SCISSOR cannot be used directly on the nonEnglish data, because syntactic annotations are only available in English.
Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. $$$$$ This procedure is repeated for all conjunctions that appear in a logical form.
Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. $$$$$ Each edge in F indicates a possible edge in the transformed MR parse tree.

 $$$$$ Third, A-WASP has the best F-measure.
 $$$$$ In this work, we mainly consider the GEOQUERY domain, where a query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996).

The systems that we compared with are $$$$$ Errors that do not involve type mismatch.
The systems that we compared with are $$$$$ Other tasks that can potentially benefit from this include question answering and interlingual MT.
The systems that we compared with are $$$$$ It is also similar to Schuler’s (2003) use of model-theoretic interpretations to guide syntactic parsing.

 $$$$$ In our previous work (Wong and Mooney, 2006), semantic parsing is cast as a machine translation task, where an SCFG is used to model the translation of an NL into a formal meaning-representation language (MRL).
 $$$$$ To see why the current lexical acquisition algorithm can be problematic, consider the word alignment in Figure 5 (for the sentence pair in Figure 1(b)).

Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ First, we compared the performance of A-WASP with and without conjunct regrouping (Section 4).
Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ This work was supported by a gift from Google Inc.
Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as predicate logic.
Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ This work shows that it is possible to use standard SMT methods in tasks where logical forms are involved.

 $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.
 $$$$$ This problem can be ameliorated by transforming the logical form of each training sentence so that the NL and MR parse trees are maximally isomorphic.
 $$$$$ The parser maintains a set of possible entity types for each logical variable introduced in a partial derivation (except those that are no longer visible).
 $$$$$ Rule extraction continues in this manner until the root of the MR parse tree is reached.
