This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ In future work, we plan to further generalize the synchronous parsing framework to allow different combinations of grammar formalisms.
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ In this work, we mainly consider the GEOQUERY domain, where a query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996).
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.
This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. $$$$$ Throughout this paper, we use the notation x1, x2,... for logical variables.

They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ To represent the logical form in Figure 4, we use its linearized parse—a list of MRL productions that generate the logical form, in top-down, left-most order (cf.
They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ Figure 1 shows two sample logical forms and their English glosses.
They solved the problem of aligning sentences and meanings by iteratively retraining an existing supervised semantic parser, WASP (Wong and Mooney, 2007b) or KRISP (Kate and Mooney, 2006), or an existing supervised natural-language generator, WASP-1 (Wong and Mooney, 2007a). $$$$$ For some domains, this problem can be avoided by transforming a logical language into a variable-free, functional language (e.g. the GEOQUERY functional query language in Wong and Mooney (2006)).

 $$$$$ The resulting parser is shown to be the bestperforming system so far in a database query domain.
 $$$$$ Other tasks that can potentially benefit from this include question answering and interlingual MT.
 $$$$$ The use of type checking is to exploit the fact that people tend not to ask questions that obviously have no valid answers (Grice, 1975).

 $$$$$ [smallest [state] [by area]] in Figure 3.
 $$$$$ More recently, we show that our SCFG-based parser can be inverted to produce a state-of-the-art NL generator, where a formal MRL is translated into an NL (Wong and Mooney, 2007).
 $$$$$ Certain applications may require different meaning representations, e.g. frame semantics.
 $$$$$ Step 3.

 $$$$$ Remove edges from F whose inclusion in the MR parse tree would prevent the NL and MR parse trees from being isomorphic.
 $$$$$ Rules are then extracted from each word alignment as follows.
 $$$$$ Errors that do not involve type mismatch.
 $$$$$ Table 1 summarizes the results at the end of the learning curves (792 training examples for AWASP, WASP and SCISSOR, 600 for Z&C).

It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ In this work, we mainly consider the GEOQUERY domain, where a query language based on Prolog is used to query a database on U.S. geography (Zelle and Mooney, 1996).
It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ To see the relative importance of each component of the A-WASP algorithm, we performed two ablation studies.
It may also alleviate the non-isomorphism issue that was commonly faced by researchers when mapping meaning representations and sentences (Wong and Mooney, 2007b). $$$$$ Step 3.

 $$$$$ A semantic parser is learned given a set of training sentences and their correct logical forms using standard SMT techniques.
 $$$$$ This is because grammar formalisms such as SCFG do not have a principled mechanism for handling logical variables.
 $$$$$ The parser maintains a set of possible entity types for each logical variable introduced in a partial derivation (except those that are no longer visible).
 $$$$$ The resulting parser is shown to be the bestperforming system so far in a database query domain.

 $$$$$ The questions were manually translated into Prolog logical forms.
 $$$$$ In particular, A-WASP has the best recall by far.
 $$$$$ Also it is awkward to talk about the population density of a state’s highest point (Figure 7(b)).
 $$$$$ Table 2 shows the results.

 $$$$$ By keeping occurrences of a logical variable in close proximity in the MR parse tree, we also avoid unnecessary variable bindings in the extracted rules.
 $$$$$ This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005).
 $$$$$ The only restriction on the MRL is that it be defined by an unambiguous context-free grammar (CFG) that divides a logical form into subformulas (and terms into subterms).
 $$$$$ Of the many features that we have tried, one feature set stands out as being the most effective, the two-level rules in Collins and Koo (2005), which give the number of times a given rule is used to expand a non-terminal in a given parent rule.

Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.
Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ Each step of a derivation involves the rewriting of a pair of co-indexed non-terminals by the same SCFG production.
Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ The resulting parser is shown to be the bestperforming system so far in a database query domain.
Our approach outperforms the previous system WASP? 1++ (Wong and Mooney, 2007a) significantly, and achieves comparable or slightly better performance as compared to Lu et al (2009). $$$$$ Acknowledgments: We thank Rohit Kate, Razvan Bunescu and the anonymous reviewers for their valuable comments.

We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ To compute the yield of a derivation, each fj is applied to its corresponding arguments xj to obtain an MR string free of λoperators with logical variables properly named.
We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ Figure 1 shows two sample logical forms and their English glosses.
We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ On the other hand, x1 appears in Q′, and it appears outside smallest(...) (as an argument of answer), so x1 must be bound.
We follow the standard evaluation procedure for Geo250, using 10-fold cross validation experiments with the same splits of the data as Wong and Mooney (2007). $$$$$ Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006).

For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ Originally developed as a theory of compiling programming languages (Aho and Ullman, 1972), synchronous grammars have seen a surge of interest recently in the statistical machine translation (SMT) community as a way of formalizing syntax-based translation models between natural languages (NL).
For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ Like most existing work on syntax-based SMT (Chiang, 2005; Galley et al., 2006), we construct G using rules extracted from word alignments.
For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ This work shows that it is possible to use standard SMT methods in tasks where logical forms are involved.
For GeoQuery, this includes the ZC05, ZC07 (Zettlemoyer and Collins, 2005, 2007), ?-WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011). $$$$$ Both α and β are strings of terminal and nonterminal symbols.

 $$$$$ Each step of a derivation involves the rewriting of a pair of co-indexed non-terminals by the same SCFG production.
 $$$$$ This results in two MR strings: state(x1) and area(x1,x2).

Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. $$$$$ We performed a single run of 10-fold cross validation, and measured the performance of the learned parsers using precision (percentage of translations that were correct), recall (percentage of test sentences that were correctly translated), and Fmeasure (harmonic mean of precision and recall).
Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. $$$$$ This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.

 $$$$$ We did not pursue this, as it is not needed in our experiments so far.
 $$$$$ Using statistical machine translation techniques, a semantic parser based on a synchronous grammar augmented with operators is learned given a set of training sentences and their correct logical forms.
 $$$$$ Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006).
 $$$$$ This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005).

The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ The resulting parser is shown to be the bestperforming system so far in a database query domain.
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ Both formalisms have led to SMT systems whose performance is state-of-the-art (Chiang, 2005; Galley et al., 2006).
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ All derivations start with a pair of co-indexed start symbols, (S1 , S1 ).
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ Figure 1 shows two sample logical forms and their English glosses.

 $$$$$ In this section, we propose two methods for modeling the target MRL.
 $$$$$ Remove edges from F whose inclusion in the MR parse tree would prevent the NL and MR parse trees from being isomorphic.
 $$$$$ This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.
 $$$$$ As a concrete example, Figure 2(b) shows an MR parse tree that corresponds to the English parse, [What is the [smallest [state] [by area]]], based on the A-SCFG rules in Figure 3.

Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ We present a method for regrouping conjuncts to promote isomorphism between NL and MR parse trees.2 Given a conjunction, it does the following: (See Figure 6 for the pseudocode, and Figure 5 for an illustration.)
Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ Figure 8 shows the learning curves for the AWASP algorithm compared to: (1) the original WASP algorithm which uses a functional query language (FunQL); (2) SCISSOR (Ge and Mooney, 2005), a fully-supervised, combined syntacticsemantic parsing algorithm which also uses FunQL; and (3) Zettlemoyer and Collins (2005) (Z&C), a CCG-based algorithm which uses Prolog logical forms.
Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ In particular, we extend the WASP semantic parsing algorithm by adding variable-binding λ-operators to the underlying SCFG.
Note the results for SCISSOR, KRISPandLU on GEOQUERY are based on a different meaning representation language, FUNQL, which has been shown to produce lower results (Wong and Mooney, 2007). $$$$$ Each m-place predicate is associated with a list of m-tuples showing all valid combinations of entity types that the m arguments can refer to: These m-tuples of entity types are given as domain knowledge.

 $$$$$ Second, Z&C has the best precision, although their results are based on 280 test examples only, whereas our results are based on 10-fold cross validation.
 $$$$$ This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms.
 $$$$$ The resulting parser is shown to be the bestperforming system so far in a database query domain.
 $$$$$ This is unfortunate because most existing work on computational semantics is based on predicate logic, where logical variables play an important role (Blackburn and Bos, 2005).
