One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ However, the restriction to Indo-European languages does make the results less conclusive when one wishes to transfer a parser from English to Chinese, for example.
One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.

Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ However, the gold POS tag assumption weakens any conclusions that can be drawn, as part-of-speech are also a form of syntactic analysis, only shallower.
Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ This generates a parser that emulates the direct transfer parser, but DP: dependency parser, i.e., DP : x -+ y that are considered ‘good’ by some external metric.
Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ In fact, if we randomly sampled from the multi-source data until the training set size was equivalent to the size of the English data, then the results still hold (and in fact go up slightly for some languages).
Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ First, we train a source side English parser that, crucially, is delexicalized so that its predictions rely soley on the part-of-speech tags of the input sentence, in the same vein as Zeman and Resnik (2008).

More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ This generates a parser that emulates the direct transfer parser, but DP: dependency parser, i.e., DP : x -+ y that are considered ‘good’ by some external metric.
More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ It is not surprising that a parser transferred from annotated resources does significantly better than unsupervised systems since it has much more information from which to learn.
More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.
More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ These assumptions help to give the model traction.

If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ We evaluate in the former setting to compare to previous studies that make this assumption.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ From this table we can see that direct transfer is a very strong baseline and is over 20% absolute better than the DMV model for both gold and predicted POS tags.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ In this paper we focus on transferring dependency parsers between languages.

It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ The previous section focused on transferring an English parser to a new target language.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ We then use a constraint driven learning algorithm to adapt the transferred parsers to the respective target language, obtaining an additional 16% error reduction on average in a multi-source setting.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ The third (multi-dir.) is the multi-source direct transfer system.

It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).
It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available).
It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ The model is trained to update towards parses that are in high agreement with a source side English parse based on constraints drawn from alignments in the parallel data.

McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ A dependency parser takes a tokenized input sentence (optionally part-ofspeech tagged) and produces a connected tree where directed arcs represent a syntactic head-modifier relationship.
McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ Ganchev et al. also report results for Bulgarian.
McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ For Spanish we can see that the multi-source direct transfer parser is better (75.1% versus 70.6%), and this is also true for the multi-source projected parser (73.2%).
McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.

A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.
A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ The previous section focused on transferring an English parser to a new target language.
A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English.

By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ The second fundamental observation is that when available, multiple sources should be used.
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008).
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ These do not effect the result.

We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ Table 4 gives results comparing the models presented in this work to those three systems.
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available).
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ In this work we present a method for creating dependency parsers for languages for which no labeled training data is available.
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ Second, we can generate USR results for all eight languages and not just for the languages that they report.

The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ An empirical comparison to Ganchev et al. (2009) is given in Section 5.
The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ We focused on this subset of languages because they are Indo-European and a significant amount of parallel data exists for each language.
The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ The study of unsupervised grammar induction has many merits.

Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ For example, on the CoNLL test sets, a DMV model obtains UAS of 28.7/41.8/34.6% for ar/zh/ja respectively, whereas an English direct transfer parser obtains 32.1/53.8/32.2% and a multi-source direct transfer parser obtains 39.9/41.7/43.3%.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ One fundamental point the above experiments illustrate is that even for languages for which no resources exist, simple methods for transferring parsers work remarkably well.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ These assumptions help to give the model traction.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.

 $$$$$ Central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models between languages.
 $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.
 $$$$$ Some of these variations are expected, e.g., the Romance languages (Spanish, Italian and Portuguese) tend to transfer well to one another.

Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria.
Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.
Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ Central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models between languages.
Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ However, their results show that this method often did not significantly outperform uniform mixing.

We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.
We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.
We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ Central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models between languages.

 $$$$$ This result holds in the presence of both gold POS tags as well as automatic tags projected from English.
 $$$$$ The first (best-source) is the direct transfer results for the oracle single-best source language per target language.
 $$$$$ Even through naive multi-source methods (concatenating data), it is possible to build a system that has comparable accuracy to the single-best source for all languages.
 $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.

The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ By presenting results on eight languages our study is already more comprehensive than most previous work in this area.
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ For this we used the Europarl corpus version 5 (Koehn, 2005).
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ In particular, if one can transfer part-of-speech tags, then a large part of transferring unlabeled dependencies has been solved.
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ Subsequently, researchers have begun to look at both porting these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007).

Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ First, it makes the comparison more direct.
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ We focus on using this parsing system for two reasons.
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ To simplify discussion, we first focus on the most common instantiation of parser transfer in the literature: transferring from English to other languages.
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ In this paper we focus on transferring dependency parsers between languages.

Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ For all data sets we used the predefined training and testing splits.
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available).
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ We empirically show that directly transferring delexicalized models (i.e. parsing a foreign language POS sequence with an English parser) already outperforms state-of-the-art unsupervised parsers by a significant margin.
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.

The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008).
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ The algorithm is given in Figure 2.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text.
