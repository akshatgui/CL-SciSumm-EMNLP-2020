One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ For this we used the universal tagset from Petrov et al. (2011), which includes: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions orpostpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all tag).
One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ This is useful for two reasons.
One of several drawbacks of this practice is that it weakens any conclusions that could be drawn about how computers (and possibly humans) learn in the absence of explicit feedback (McDonald et al, 2011). $$$$$ Results are given in Table 1 under the column enproj.

Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.
Part-of-speech tags are known to contain significant amounts of information for unlabeled dependency parsing (McDonald et al, 2011), so we find it reassuring that our latest grammar inducer is less dependent on gold tags than its predecessors. $$$$$ Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.

More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ This observation should lead to a new baseline in unsupervised and projected grammar induction – the UAS of a delexicalized English parser.
More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.
More recently, McDonald et al (2011) demonstrated an impressive alternative to grammar induction by projecting reference parse trees from languages that have annotations to ones that are resource-poor. $$$$$ This emphasizes that even for languages with no syntactic resources – or possibly even parallel data – simple transfer methods can already be more powerful than grammar induction systems.

If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ In this setting the English projected model gets 72.0% on Spanish.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.
If parallel data is available, it can be further used to enforce model agreement on this data to adjust for discrepancies between the two languages, for example by means of projected transfer (McDonald et al, 2011). $$$$$ This suggests that even better transfer models can be produced by separately weighting each of the sources depending on the target language – either weighting by hand, if we know the language group of the target language, or automatically, if we do not.

It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ These results are also given in Table 4 under USR†.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ Overall, Table 2 does indicate that there are possible gains in accuracy through the inclusion of additional languages.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
It also retains compatibility with any refinement procedures similar to projected transfer (McDonald et al, 2011) that may have been designed to work in conjunction with direct model transfer. $$$$$ The key observation is that part-of-speech tags contain a significant amount of information for unlabeled dependency parsing.

It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ We report results for sentences of all lengths, as well as with gold and automatically induced part-of-speech tags.
It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ There is also a wide range of values depending on the source training data and/or target testing data, e.g., Portuguese as a source tends to parse target languages much better than Danish, and is also more amenable as a target testing language.
It remains to be seen which one would produce the best results and how multi-source feature representation projection would compare to, for example, multi-source projected transfer (McDonald et al, 2011). $$$$$ Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008).

McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ Preliminary experiments for Arabic (ar), Chinese (zh), and Japanese (ja) suggest similar direct transfer methods are applicable.
McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.
McDonald et al (2011) showed that such projection produce better structures than the current unsupervised parsers do. $$$$$ We evaluate in the latter setting to measure performance in a more realistic scenario – when no target language resources are available.

A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ The fourth and final result set (multi-proj.) is the multi-source projected system.
A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ Most notably, it increases our understanding of how computers (and possibly humans) learn in the absence of any explicit feedback.
A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ The resulting parser consistently improves on the directly transferred delexicalized parser, reducing relative errors by 8% on average, and as much as 18% on some languages.
A more generally applicable class of methods exploits the notion of universal part of speech tags (Petrov et al., 2011; Das and Petrov, 2011) to train parsers that can run on any language with no adaptation (McDonald et al., 2011) or unsupervised adaptation (Cohen et al., 2011). $$$$$ We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.

By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ These do not effect the result.
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ There are a couple of differences.
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ The third (multi-dir.) is the multi-source direct transfer system.
By instantiating the basic MST Parser features over coarse parts of speech, we construct a state-of-the-art delexicalized parser in the style of McDonald et al (2011). $$$$$ Preliminary experiments for Arabic (ar), Chinese (zh), and Japanese (ja) suggest similar direct transfer methods are applicable.

We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks.
We emphasize again that these baseline features are entirely standard, and all the DELEX feature set does is recreate an MST Parser-based analogue of the direct transfer parser described by McDonald et al (2011). $$$$$ Our results consistently outperform the previous state-of-the-art across all languages and training configurations.

The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ Acknowledgements: We would like to thank Kuzman Ganchev, Valentin Spitkovsky and Dipanjan Das for numerous discussions on this topic and comments on earlier drafts of this paper.
The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ We use English (en) only as a source language throughout the paper.
The first variable we consider is whether we have access to a small number of target language trees or only pre-existing tree banks in a number of other languages; while not our actual target language, these other tree banks can still serve as a kind of proxy for learning which features generally transfer useful in formation (McDonald et al., 2011). $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.

Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ However, this is not necessary as treebanks are available for a number of language groups, e.g., Indo-European, Altaic, Semitic, and Sino-Tibetan.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ Overall, Table 2 does indicate that there are possible gains in accuracy through the inclusion of additional languages.
Following McDonald et al (2011), we strip punctuation from all tree banks for the results of Section 3.3. $$$$$ We then use a constraint driven learning algorithm to adapt the transferred parsers to the respective target language, obtaining an additional 16% error reduction on average in a multi-source setting.

 $$$$$ In this case the ALIGN function returns a value of 2.
 $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.
 $$$$$ We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
 $$$$$ Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.

Following the procedure from McDonald et al (2011), for each language, we train both our DELEX and DELEX+PROJ features on a concatenation of 2000 sentences from each other CoNLL training set, plus 2000 sentences from the Penn Treebank. $$$$$ But again, there is an overall trend to better models.

We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.
We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.
We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ However, their results show that this method often did not significantly outperform uniform mixing.
We include for reference the baseline results of McDonald et al (2011) and Ta?ckstro?m et al (2012) (multi-direct transfer and no clusters) and the improvements from their best methods using lexical information (multi-projected transfer and cross lingual clusters). $$$$$ For these experiments we still only use English-target parallel data because that is the format of the readily available data in the Europarl corpus.

 $$$$$ In this setting the English projected model gets 72.0% on Spanish.
 $$$$$ (2009), Smith and Eisner (2009) inter alia.
 $$$$$ The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available).
 $$$$$ So far we have not defined the ALIGN function that is used to score potential parses.

The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ Overall, Table 2 does indicate that there are possible gains in accuracy through the inclusion of additional languages.
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ The second (avg-source) is the mean UAS over all source languages per target language.
The first parser is a non-lexicalized version of the MST parser (McDonald et al 2005) successfully used in the multilingual context (McDonald et al 2011). $$$$$ Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.

Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ Table 4 gives results comparing the models presented in this work to those three systems.
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ The second fundamental observation is that when available, multiple sources should be used.
Surprisingly, our model slightly outperforms the mapping of (Petrov et al 2011), yielding an average accuracy of 56.7% as compared to the 55.4% achieved by its manually constructed counterpart for the direct transfer method (McDonald et al 2011). $$$$$ Unlike our work, Søgaard found that simply concatenating all the data led to degradation in performance.

Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ We would also like to thank Shay Cohen, Dipanjan Das, Noah Smith and Anders Søgaard for sharing early drafts of their recent related work.
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ Most previous work has assumed gold part-of-speech tags, but as the code for USR is publicly available we were able to train it using the same projected part-of-speech tags used in our models.
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.
Specifically, McDonald et al (2011) have demonstrated that projecting from a single oracle chosen language can lead to good parsing performance, and our technique may allow such projection without an oracle. $$$$$ We evaluate in the latter setting to measure performance in a more realistic scenario – when no target language resources are available.

The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ Our final parsers achieve state-of-the-art accuracies on eight Indo-European languages, significantly outperforming previous unsupervised and projected systems.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ This subtracts 1.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.
The first section of the table is for the direct transfer of the MST parser (McDonald et al 2011). $$$$$ Ganchev et al. also report results for Bulgarian.
