Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ The different translation tables form dif ferent components in the log-linear model, whose weights are set using standard minimum error rate training methods.The alternative path model outperforms the sur face form model with POS LM, with an BLEU scoreof 19.47% vs. 19.05%.
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ The framework of factored translation models is very general.
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ From a training point of view thismeans that we need to learn translation and gener ation tables from the word-aligned parallel corpus and define scoring methods that help us to choose between ambiguous mappings.

Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence.
Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ Each word form is treated as a token in it self.
Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach.

We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ 868
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ Such a model can be defined straight-forward as a factored translation model.
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ One or more scoring functions may be defined overthis table, in our experiments we used both condi tional probability distributions, e.g., p(fish|NN) andp(NN|fish), obtained by maximum likelihood esti mation.
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ See Figure 2 for an illustration of this model in our framework.

Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ Given the multiple choices for each step (reflecting the ambiguity in translation), each input phrase may be ex panded into a list of translation options.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ For surface forms with rich evidence in the training data, we prefer surface form mappings, and for surface forms with poor or no evidence in the training data we decompose surface forms into lemma and morphology information and map theseseparately.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.

Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ Och et al (2004) report minor improvements with linguistic features on a Chinese-English task, Koehn and Knight (2003) show some success in re-rankingnoun phrases for German-English.
Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ Many more models that incorporatedifferent factors can be quickly built using the ex isting implementation.
Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ For instance, for a genera tion step that maps surface forms to part-of-speech, a table with entries such as (fish,NN) is constructed.

We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ In their approaches, first, an n-best list with the best transla tions is generated for each input sentence.
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ neue h?user werden gebaut new houses are builtFigure 3: Example sentence translation by a standard phrase model.
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ The current state-of-the-art approach to statistical machine translation, so-called phrase-based models,is limited to the mapping of small text chunks with out any explicit use of linguistic information, may it be morphological, syntactic, or semantic.

A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ Since all mapping steps operate on the same phrase segmentation, the expansions of these mapping steps can be efficiently pre-computed prior to the heuristic beam search, and stored astranslation options.
A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ 6.1 Syntactically Enriched Output.
A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ German morphological analysis and POS tagging was done using LoPar Schmidt and Schulte im Walde (2000), English POS tagging was done with Brill?s tagger (Brill, 1995), followed by a simple lemmatizer based on tagging results.
A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ Moreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps mayoperate on different phrase segmentations (for instance a part-of-speech model for large scale re ordering).

Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ One example to illustrate the short-comings of thetraditional surface word approach in statistical machine translation is the poor handling of morphology.
Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.
Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ While this problem does not show up as stronglyin English ? due to the very limited morphological inflection in English ? it does constitute a sig nificant problem for morphologically rich languages such as Arabic, German, Czech, etc. Thus, it may be preferably to model translation between morphologically rich languages on the levelof lemmas, and thus pooling the evidence for differ ent word forms that derive from a common lemma.

Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ In their approaches, first, an n-best list with the best transla tions is generated for each input sentence.
Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach.
Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ Translate morphological and POS factors.
Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ Adding all features results inlower performance (27.04% BLEU), than consider ing only case, number and gender (27.45% BLEU) or additionally verbial (person, tense, and aspect) and prepositional (lemma and case) morphology (27.62% BLEU).

Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ lemma lemma part-of-speech OutputInput morphology part-of-speech word word morphologyFigure 2: Example factored model: morphologi cal analysis and generation, decomposed into three mapping steps (translation of lemmas, translation ofpart-of-speech and morphological information, gen eration of surface forms).
Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ In fact, some preliminary ex periments have shown that word alignment based on lemmas or stems yields improved alignment quality.
Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ This means that the translation model treats, say, the word house completely independent of the word houses.
Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ We reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence.

Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ However, moving from a surface word translation mapping to a lemma/morphology mapping leads to a deterioration of performance to a BLEU score of 14.46%.Note that this model completely ignores the sur face forms of input words and only relies on the 2Made available for the WMT07 workshop shared task http://www.statmt.org/wmt07/ 873 German?English Model BLEU baseline (surface) 18.19% + POS LM 19.05% pure lemma/morph model 14.46% backoff lemma/morph model 19.47% Table 2: Experimental results with morphological analysis and generation model (Figure 2), using News Commentary corpus more general lemma and morphology information.

Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.
Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ Translation models that operate on more gen eral representations, such as lemmas instead of surface forms of words, can draw on richer statistics and overcome the data sparseness problems caused by limited training data.?
Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ While this problem does not show up as stronglyin English ? due to the very limited morphological inflection in English ? it does constitute a sig nificant problem for morphologically rich languages such as Arabic, German, Czech, etc. Thus, it may be preferably to model translation between morphologically rich languages on the levelof lemmas, and thus pooling the evidence for differ ent word forms that derive from a common lemma.
Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ Translation models that operate on more gen eral representations, such as lemmas instead of surface forms of words, can draw on richer statistics and overcome the data sparseness problems caused by limited training data.?

 $$$$$ The current state-of-the-art approach to statistical machine translation, so-called phrase-based models,is limited to the mapping of small text chunks with out any explicit use of linguistic information, may it be morphological, syntactic, or semantic.
 $$$$$ We reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence.
 $$$$$ This means that the translation model treats, say, the word house completely independent of the word houses.
 $$$$$ The translation of factored representations of input words into the factored representations of out put words is broken up into a sequence of mapping steps that either translate input factors into output factors, or generate additional output factors from existing output factors.

Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ In fact, compu tational problems hold back experiments with morecomplex factored models that are theoretically pos sible but too computationally expensive to carry out.Our current focus is to develop a more efficient im plementation that will enable these experiments.
Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ 5.1 Training.
Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ 5.1 Training.

Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ The baseline system produces often phrases such as zur(to) zwischenstaatlichen(inter-governmental) methoden(methods), with a mismatch between the determiner (singular) and the noun (plural), while the adjective is ambiguous.
Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ Such a model can be defined straight-forward as a factored translation model.

While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1 $$$$$ See Figure 2 for an illustration of this model in our framework.
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1 $$$$$ This work is implemented as part of the open source Moses1 system (Koehn et al, 2007).
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1 $$$$$ See Figure 2 for an illustration of this model in our framework.
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1 $$$$$ 6.1 Syntactically Enriched Output.

Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ The word alignment plays no role here.
Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.

We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ For instance: re ordering at the sentence level is mostly driven word word part-of-speech OutputInput morphology part-of-speech morphology word class lemma word class lemma ......Figure 1: Factored representations of input and out put words incorporate additional annotation into the statistical translation model.
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ The framework of factored translation models is very general.
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ See Figure 2 for an illustration of this model in our framework.

Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ The representation of ha?user in German is: surface-form ha?user | lemma haus | part-of-speech NN | count plural | case nominative | gender neutral.
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ Moreover, we expect to overcome the constraints of the currently implemented synchronous factored models by developing a more general asynchronous framework, where multiple translation steps mayoperate on different phrase segmentations (for instance a part-of-speech model for large scale re ordering).
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ In their approaches, first, an n-best list with the best transla tions is generated for each input sentence.

We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ The word alignment methods may operate on the surface forms of words, or on anyof the other factors.
We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
