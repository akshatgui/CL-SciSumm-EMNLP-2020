Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ 6.4 Integrated Recasing.
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ For instance: re ordering at the sentence level is mostly driven word word part-of-speech OutputInput morphology part-of-speech morphology word class lemma word class lemma ......Figure 1: Factored representations of input and out put words incorporate additional annotation into the statistical translation model.
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al 2006) and (Koehn and Hoang 2007). $$$$$ We reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence.

Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ If the input language is morphologically richer than the out put language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006).
Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ We reported on experiments that showed gains over standard phrase-based models, both in terms of automatic scores (gains of up to 2% BLEU), as well as a measure of grammatical coherence.
Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ We are currently exploringthese possibilities, for instance use of syntactic in formation in reordering and models with augmented input information.We have not addressed all computational problems of factored translation models.
Eventually, we would like to replace the functionality of factored translation models (Koehn and Hoang, 2007) with lattice transformation and augmentation. $$$$$ We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.

We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ Compared to phrase-based models, the decomposi tion of phrase translation into several mapping stepscreates additional computational complexity.
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ This paper describes the motivation, the modeling aspects and the computationally efficient decoding methods of factored translation models.
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach.
We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al, 2003) and factored statistical machine translation (Koehn and Hoang, 2007). $$$$$ We call the application of these mapping stepsto an input phrase expansion.

Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ Adding all features results inlower performance (27.04% BLEU), than consider ing only case, number and gender (27.45% BLEU) or additionally verbial (person, tense, and aspect) and prepositional (lemma and case) morphology (27.62% BLEU).
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ Compared to a baseline English?Chinese system, adding word classes on theoutput side as additional factors (in a model as pre English?Chinese Model BLEU baseline (surface) 19.54% surface + word class 21.10% Table 3: Experimental result with automatic word classes obtained by word clustering Chinese?English Recase Method BLEU Standard two-pass: SMT + recase 20.65% Integrated factored model (optimized) 21.08% OutputInput mixed-cased lower-cased lower-casedTable 4: Experimental result with integrated recas ing (IWSLT 2006 task) viously illustrated in Figure 4) to be exploited by a 7-gram sequence model, we observe a gain 1.5% BLEU absolute.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ While this problem does not show up as stronglyin English ? due to the very limited morphological inflection in English ? it does constitute a sig nificant problem for morphologically rich languages such as Arabic, German, Czech, etc. Thus, it may be preferably to model translation between morphologically rich languages on the levelof lemmas, and thus pooling the evidence for differ ent word forms that derive from a common lemma.
Any way to enforce linguistic constraints will result in a reduced need for data, and ultimately in more complete models, given the same amount of data (Koehn and Hoang, 2007). $$$$$ Structural problems have also been addressed bypre-processing: Collins et al (2005) reorder the in put to a statistical system to closer match the word order of the output language.On the other end of the translation pipeline, addi tional information has been used in post-processing.

Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ The integrated recas ing model outperform the standard approach with an BLEU score of 21.08% to 20.65%.
Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ If the input language is morphologically richer than the out put language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006).
Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). $$$$$ Our framework is more general and goes beyond recent work on models that back off to representations with richer statistics (Nie?en and Ney, 2001; Yang and Kirchhoff, 2006; Talbot andOsborne, 2006) by keeping a more complex repre sentation throughout the translation process.Rich morphology often poses a challenge to sta tistical machine translation, since a multitude of word forms derived from the same lemma fragmentthe data and lead to sparse data problems.

We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach.
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ Note that while we illustrate the use of factored translation models on such a linguistically motivated 869 example, our framework also applies to models that incorporate statistically defined word classes, or any other annotation.
We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the target side, with a sequence model on part-of-speech. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.

A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ The goal of integrating syntactic informationinto the translation model has prompted many re searchers to pursue tree-based transfer models (Wu, 1997; Alshawi et al, 1998; Yamada and Knight, 2001; Melamed, 2004; Menezes and Quirk, 2005; Galley et al, 2006), with increasingly encouraging results.
A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ Having such information avail able to the translation model allows the directmodeling of these aspects.
A tight integration of morpho syntactic information into the translation model was proposed by (Koehn and Hoang, 2007) where lemma and morphological information are translated separately, and this information is combined on the output side to generate the translation. $$$$$ For instance, if we want to add part-of-speech information on the input and output side, we need to obtain part-of-speech tagged training data.

Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ In this model the translation process is broken up into the following three mapping steps: 1.
Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ Any instance of house in the training data does not add any knowledge to the translation of houses.
Koehn and Hoang (2007) generalise the phrase-based model's representation of the word from a string to a vector, allowing additional features such as part-of-speech and morphology to be associated with, or even to replace, surface forms during search. $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.

Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ The new approach allows additional annotation at the word level.
Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
Factored models (Koehn and Hoang, 2007) facilitate the translation by breaking it down into several factors which are further combined using a log-linear model (Och and Ney, 2002). $$$$$ An extended description of these experiments is in the JHU workshop report (Koehn et al, 2006).

Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ In future work, these approaches may be combined.
Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ Generation steps map outputfactors within individual output words.
Unlike with factored models (Koehn and Hoang, 2007) or additional translation lexicons (Schwenk et al, 2008), we do not generate the surface form back from the lemma translation, which means that tense, gender and number information are lost. $$$$$ Our framework is more general and goes beyond recent work on models that back off to representations with richer statistics (Nie?en and Ney, 2001; Yang and Kirchhoff, 2006; Talbot andOsborne, 2006) by keeping a more complex repre sentation throughout the translation process.Rich morphology often poses a challenge to sta tistical machine translation, since a multitude of word forms derived from the same lemma fragmentthe data and lead to sparse data problems.

Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ Each word form is treated as a token in it self.
Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. $$$$$ The English?German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006).

Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ The additional features are used to rescore the n-best list, resulting possibly in a better best translation for the sentence.
Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ In stead of a simple table look-up to obtain the possible translations for an input phrase, now multiple tables have to be consulted and their content combined.
Factored models are introduced in (Koehn and Hoang, 2007) for better integration of morpho syntactic information. Gime ?nez and Ma`rquez (2005) merge multiple word alignments obtained from several linguistically-tagged versions of a Spanish-Englishcorpus, but only standard tokens are used in decoding. $$$$$ by general syntactic principles, local agreement constraints show up in morphology, etc.Therefore, we extended the phrase-based ap proach to statistical translation to tightly integrate additional information.

 $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
 $$$$$ As in phrase-based models, factored translation models can be seen as the combination of several components (language model, reordering model,translation steps, generation steps).
 $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
 $$$$$ This means that the translation model treats, say, the word house completely independent of the word houses.

Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ The current state-of-the-art approach to statistical machine translation, so-called phrase-based models,is limited to the mapping of small text chunks with out any explicit use of linguistic information, may it be morphological, syntactic, or semantic.
Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ Many attempts have been made to add richer in formation to statistical machine translation models.Most of these focus on the pre-processing of the in put to the statistical system, or the post-processing of its output.
Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ For a given input phrase, all pos sible translation options are thus computed before word word part-of-speech OutputInput 3 g r a m 7 g r a mFigure 4: Syntactically enriched output: By gener ating additional linguistic factors on the output side,high-order sequence models over these factors sup port syntactical coherence of the output.
Yeniterzi and Oflazer (2010) mapped the syntax of the English side to the morphology of the Turkish side with the factored model (Koehn and Hoang, 2007). $$$$$ We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.

Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ While this allows the translation of word forms withknown lemma and unknown surface form, on balance it seems to be disadvantage to throw away sur face form information.To overcome this problem, we introduce an al ternative path model: Translation options in this model may come either from the surface form modelor from the lemma/morphology model we just de scribed.
Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ For instance, re call our earlier example: a scoring function for a generation model component that is a conditional probability distribution between input and output factors, e.g., ?(fish,NN,singular) = p(NN|fish).
Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ by general syntactic principles, local agreement constraints show up in morphology, etc.Therefore, we extended the phrase-based ap proach to statistical translation to tightly integrate additional information.
Koehn and Hoang (2007) have reported an in crease of 0.86 BLEU points for German-to-English translation for small training data. $$$$$ Let us now take a closer look at one example, thetranslation of the one-word phrase ha?user into En glish.

While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. $$$$$ Then, the n-best list is enriched with additional features, for instance by syntactically parsing each candidate translation and adding a parse score.
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. $$$$$ The framework of factored translation models is very general.
While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. $$$$$ One example to illustrate the short-comings of thetraditional surface word approach in statistical machine translation is the poor handling of morphology.

Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ In such a model, we would want to translate lemmaand morphological information separately, and com bine this information on the output side to ultimately generate the output surface words.
Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ Each word form is treated as a token in it self.
Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ In fact, compu tational problems hold back experiments with morecomplex factored models that are theoretically pos sible but too computationally expensive to carry out.Our current focus is to develop a more efficient im plementation that will enable these experiments.
Our approach is built on top of the factor-based SMT model proposed by Koehn and Hoang (2007), as an extension of the traditional phrase based SMT framework. $$$$$ Acknowledgments This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No NR0011-06-C-0022 and inpart under the EuroMatrix project funded by the Eu ropean Commission (6th Framework Programme).We also benefited greatly from a 2006 summer workshop hosted by the Johns Hopkins Uni versity and would like thank the other workshop participants for their support and insights, namelyNicola Bertoldi, Ondrej Bojar, Chris Callison Burch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst Christine Moran, Wade Shen, and Richard Zens.

We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ If the input language is morphologically richer than the out put language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006).
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ We present an extension of phrase-based statistical machine translation models that enables the straight-forward integration of additional annotation at the word-level ?may it be linguistic markup or automatically generated word classes.
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ Such a model can be defined straight-forward as a factored translation model.
We also construct a Hebrew-to-English MT system using Moses' factored translation model (Koehn and Hoang, 2007). $$$$$ We presented an extension of the state-of-the-artphrase-based approach to statistical machine trans lation that allows the straight-forward integration of additional information, may it come from linguistic tools or automatically acquired word classes.

Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ If the input language is morphologically richer than the out put language, it helps to stem or segment the input in a pre-processing step, before passing it on to the translation system (Lee, 2004; Sadat and Habash, 2006).
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ The model that incorporates both POS and morphology (18.22% BLEU vs. baseline 18.04% BLEU) ensures better local grammatical coherence.
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ Structural problems have also been addressed bypre-processing: Collins et al (2005) reorder the in put to a statistical system to closer match the word order of the output language.On the other end of the translation pipeline, addi tional information has been used in post-processing.
Factored translation models (Koehn and Hoang, 2007) approach the idea of integrating annotation into translation from the opposite direction. $$$$$ These experiments demonstrate that within the frameworkof factored translation models additional informa tion can be successfully exploited to overcome someshort-comings of the currently dominant phrase based statistical approach.

We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ In a num ber of experiments we show that factoredtranslation models lead to better translation performance, both in terms of auto matic scores, as well as more grammatical coherence.
We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ lemma lemma part-of-speech OutputInput morphology part-of-speech word word morphologyFigure 2: Example factored model: morphologi cal analysis and generation, decomposed into three mapping steps (translation of lemmas, translation ofpart-of-speech and morphological information, gen eration of surface forms).
We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ These are called translation options.
We translate from Spanish into English using phrase-based decoding with Moses (Koehn and Hoang, 2007) as our decoder. $$$$$ linguistic factorsFactored translation models build on the phrase based approach (Koehn et al, 2003) that breaks up the translation of a sentence into the translation of small text chunks (so-called phrases).
