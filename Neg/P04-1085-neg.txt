Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. $$$$$ This improved accuracy with DA information should of course not be considered as the actual accuracy of our system, since DA information is difficult to acquire automatically (Stolcke et al., 2000).
Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. $$$$$ For example, if a speaker disagrees with another person once in the conversation, is he more likely to disagree with him again?

An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ We are grateful to Mari Ostendorf and Dustin Hillard for providing us with their agreement and disagreement labeled data.
An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ Structural features encode some helpful information regarding ordering and overlap of spurts.
An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ If we hypothesize that only a limited set of paired DAs (e.g. offer-accept, question-answer, and apologydownplay) can be realized as adjacency pairs, then knowing the DA category of the B part and of all potential A parts should help in finding the most meaningful dialog act tag among all potential A parts; for example, the question-accept pair is admittedly more likely to correspond to an AP than e.g. backchannel-accept.
An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ The annotation of the corpus with adjacency pairs is described in (Shriberg et al., 2004; Dhillon et al., 2004).

Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training.
Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available.
Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ This relaxation on a strict adjacency requirement is particularly important in interactions of multiple speakers since other speakers have more opportunities to insert utterances between the two elements of the AP construction (e.g. interrupted, abandoned or ignored utterances; backchannels; APs with multiple second elements, e.g. a question followed by answers of multiple speakers).2 Information provided by adjacency pairs can be used to identify the target of an agreeing or disagreeing utterance.
Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ IIS-012196.

Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ In this analysis subsection and for all classification results presented thereafter, we used a value of .
Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ We are grateful to Mari Ostendorf and Dustin Hillard for providing us with their agreement and disagreement labeled data.
Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ Table 7 summarizes the performance of our classifier with the different feature sets in this classification task, distinguishing the case where the four label-dependency pragmatic features are available during decoding from the case where they are not.
Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ Performance is reported in Table 6.

More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). $$$$$ Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960).
More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). $$$$$ We also plan to incorporate acoustic features to increase the robustness of our procedure in the case where only speech recognition output is available.
More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). $$$$$ Switchboard, (Stolcke et al., 2000)).

Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). $$$$$ Meetings in general run just under an hour each; they have an average of 6.5 participants.
Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). $$$$$ We are grateful to Mari Ostendorf and Dustin Hillard for providing us with their agreement and disagreement labeled data.
Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). $$$$$ Our accuracy for classifying agreements and disagreements is 86.9%, which is a 4.9% improvement over (Hillard et al., 2003).

Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ The next subsection describes the machine learning framework used to significantly outperform this already quite effective baseline algorithm.
Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ These features are shown to be informative and to help the classification task, yielding a substantial improvement (1.3% to reach a 86.9% accuracy in three-way classification).
Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.

This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ While none of the results characterize any strong conditioning of by and , we can nevertheless notice some interesting phenomena.
This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ Our accuracy for classifying agreements and disagreements is 86.9%, which is a 4.9% improvement over (Hillard et al., 2003).
This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement.
This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ Adjacency pairs (AP) are considered fundamental units of conversational organization (Schegloff and Sacks, 1973).

 $$$$$ Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960).
 $$$$$ We assume in that study that accurate AP labeling is available, but for the purpose of building and testing a classifier, we use only automatically extracted adjacency pair information.
 $$$$$ Finally, we observe that by incorporating label-dependency features representing pragmatic influences, we further improve the performance (about 1% in Table 7).
 $$$$$ In the second set of experiments, we aimed at reducing the expected variance of our experimental results and performed N-fold cross-validation in a four-way classification task, at each step retaining the hand-labeled data of a meeting for testing and the rest of the data for training.

 $$$$$ Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task.
 $$$$$ We then turn to our work on identifying adjacency pairs.

The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ One of the main features of meetings is the occurrence of agreement and disagreement among participants.
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ The ICSI Meeting corpus (Janin et al., 2003) is a collection of 75 meetings collected at the International Computer Science Institute (ICSI), one among the growing number of corpora of humanto-human multi-party conversations.
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET.5 Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package.6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2.
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)).

The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ Second, the table corroborates the findings of (Hillard et al., 2003) that lexical information make the most helpful local features.
The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ Backchannels (e.g.
The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ The ICSI Meeting corpus (Janin et al., 2003) is a collection of 75 meetings collected at the International Computer Science Institute (ICSI), one among the growing number of corpora of humanto-human multi-party conversations.
The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ We are grateful to Mari Ostendorf and Dustin Hillard for providing us with their agreement and disagreement labeled data.

Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ This improved accuracy with DA information should of course not be considered as the actual accuracy of our system, since DA information is difficult to acquire automatically (Stolcke et al., 2000).
Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ Most previous work in that area is limited to interaction between two speakers (e.g.
Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ These features are shown to be informative and to help the classification task, yielding a substantial improvement (1.3% to reach a 86.9% accuracy in three-way classification).

It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ IIS-012196.
It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ They reflect the structure of conversations as paired utterances such as questionanswer and offer-acceptance, and their labeling is used in our work to determine who are the addressees in agreements and disagreements.

Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.
Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.
Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ This material is based on research supported by the National Science Foundation under Grant No.

To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). $$$$$ Often meetings include long stretches of controversial discussion before some consensus decision is reached.
To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). $$$$$ This material is based on research supported by the National Science Foundation under Grant No.
To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). $$$$$ “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement.
To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). $$$$$ Similarly with the same probability distribution, a tendency to agree is confirmed in 25% of the cases.

 $$$$$ We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.
 $$$$$ One of the main features of meetings is the occurrence of agreement and disagreement among participants.
 $$$$$ Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of (Hillard et al., 2003).
 $$$$$ Nonetheless, we believe this model can as well be used to capture salient interactional patterns specific to meetings with different social dynamics.
