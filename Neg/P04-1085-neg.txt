Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. $$$$$ Second, the table corroborates the findings of (Hillard et al., 2003) that lexical information make the most helpful local features.
Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. $$$$$ While the actual classification problem incorporates four classes, the BACKCHANNEL class is igTable 3.
Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. $$$$$ Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960).
Galley et al (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. $$$$$ We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels.

An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ Contextual information about agreements and disagreements can also provide useful cues regarding who is the addressee of a given utterance.
An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ This material is based on research supported by the National Science Foundation under Grant No.
An adjacent pair is said to consist of two parts that are ordered, adjacent, and produced by different speakers (Galley et al, 2004). $$$$$ We also wanted to determine if information about dialog acts (DA) helps the ranking task.

Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ Performance is reported in Table 6.
Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ We then turn to our work on identifying adjacency pairs.
Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance.
Using them, Galley et al (2004) report an 8% increase in speaker identification. $$$$$ We first performed several empirical analyses in order to determine to what extent contextual information helps in discriminating between agreement and disagreement.

Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.
Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ This seems to indicate that modeling label dependencies in our classification problem is useful.
Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET.5 Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package.6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2.
Galley et al 2004 show the value of using durational and structural features for identifying agreement and disagreement in spoken conversational speech. $$$$$ We had 8135 spurts available for training and testing, and performed two sets of experiments to evaluate the performance of our system.

More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). $$$$$ This notation makes it obvious that we do not necessarily assume that agreements and disagreements are reflexive 7The annotation of DA is particularly fine-grained with a choice of many optional tags that can be associated with each DA.
More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). $$$$$ To train the maximum entropy ranking model, we used the generalized iterative scaling algorithm (Darroch and Ratcliff, 1972) as implemented in YASMET.5 Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets: the performance is 89.39% when using all feature sets, and reaches 90.2% after applying Gaussian smoothing and using incremental feature selection as described in (Berger et al., 1996) and implemented in the yasmetFS package.6 Note that restricting ourselves to only backward looking features decreases the performance significantly, as we can see in Table 2.
More sophisticated approaches have been proposed (Hillard et al, 2003), including an extension that, in an interesting reversal of our problem, makes use of sentiment polarity indicators within speech segments (Galley et al, 2004). $$$$$ “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement.

Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). $$$$$ The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation .
Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). $$$$$ Even when APs are not directly adjacent, the same constraints between pairs and mechanisms for selecting the next speaker remain in place (e.g. the case of embedded question and answer pairs).
Classifying agree/disagree opinions in conversational debates using Bayesian networks was presented in (Galley et al, 2004). $$$$$ Switchboard, (Stolcke et al., 2000)).

Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ This might be due to some additional features the latter work didn’t exploit (e.g. structural features and adjective polarity), and to the fact that the learning algorithm used in our experiments might be more accurate than decision trees in the given task.
Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ The table yields some interesting results, showing quite significant variations in class distribution when it is conditioned on various types of contextual information.
Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information.
Other researchers have developed models for detecting agreement and disagreement in meetings, using models that combine lexical features with prosodic features (e.g., pause, duration, F0, speech rate) (Hillard et al, 2003) and structural information (e.g., the previous and following speaker) (Galley et al, 2004). $$$$$ We define: as the tag of the most recent spurt before that is produced by Y and addresses X.

This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels.
This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ Table 7 summarizes the performance of our classifier with the different feature sets in this classification task, distinguishing the case where the four label-dependency pragmatic features are available during decoding from the case where they are not.
This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ We model context using Bayesian networks that allows capturing of these pragmatic dependencies.
This research has tackled issues such as the automatic detection of agreement and disagreement (Galley et al, 2004), and of the level of involvement of conversational participants (Gatica-Perez et al, 2005). $$$$$ Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)).

 $$$$$ This might be due to some additional features the latter work didn’t exploit (e.g. structural features and adjective polarity), and to the fact that the learning algorithm used in our experiments might be more accurate than decision trees in the given task.
 $$$$$ Regarding lexical features, we used a countbased feature selection algorithm to remove many first-word and last-word features that occur infrequently and that are typically uninformative for the task at hand.
 $$$$$ Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.

 $$$$$ IIS-012196.
 $$$$$ Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960).
 $$$$$ Note that with only the first feature listed in the table, the maximum entropy ranker matches exactly the performance of the baseline algorithm (79.8% accuracy).

The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable, since the kappa coefficient was .63 (Cohen, 1960).
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ We also wanted to determine if information about dialog acts (DA) helps the ranking task.
The contrast classifier is also competitive with the best case result in (Galley et al, 2004) (last entry), which adds speaker change, segment duration, and adjacency pair sequence dependency features using a dynamic Bayesian network. $$$$$ We used the DA annotation that we also had available, and used the DA tag sequence of part A and B as a feature.7 When we add the DA feature set, the accuracy reaches 91.34%, which is only slightly better than our 90.20% accuracy, which indicates that lexical, durational, and structural features capture most of the informativeness provided by DAs.

The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ These features are shown to be informative and to help the classification task, yielding a substantial improvement (1.3% to reach a 86.9% accuracy in three-way classification).
The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ We had 8135 spurts available for training and testing, and performed two sets of experiments to evaluate the performance of our system.
The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels.
The experiments here kept the feature set fixed, but results of (Galley et al, 2004) suggest that further gains can be achieved by augmenting the feature set. $$$$$ The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation .

Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ “uhhuh” and “okay”) were treated as a separate category, since they are generally used by listeners to indicate they are following along, while not necessarily indicating agreement.
Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision.
Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ Note that all features in Table 1 are “backwardlooking”, in the sense that they result from an analysis of context preceding B.
Galley et al (2004) proposed the use of Bayesian networks to model pragmatic dependencies of previous agreement or disagreement on the current utterance. $$$$$ Given feature functions and model parameters , the probability of the maximum entropy model is defined as: each speaker is either classified as speaker A or not, but we abandoned that approach, since it gives much worse performance.

It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ Regarding lexical features, we used a countbased feature selection algorithm to remove many first-word and last-word features that occur infrequently and that are typically uninformative for the task at hand.
It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ In the first set of experiments, we reproduced the experimental setting of (Hillard et al., 2003), a three-way classification (BACKCHANNEL and OTHER are merged) using hand-labeled data of a single meeting as a test set and the remaining data as training material; for this experiment, we used the same training set as (Hillard et al., 2003).
It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ We then turn to our work on identifying adjacency pairs.
It is to be expected that the a-part provides a useful cue for identification of addressee of the b-part (Galley et al, 2004). $$$$$ We define the problem of AP identification as follows: given the second element (B) of an adjacency pair, determine who is the speaker of the first element (A).

Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ This finding is consistent with previous work (Ravichandran et al., 2003) that compares maximum entropy classification and re-ranking on a question answering task.
Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002).
Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ In the first set of experiments, we reproduced the experimental setting of (Hillard et al., 2003), a three-way classification (BACKCHANNEL and OTHER are merged) using hand-labeled data of a single meeting as a test set and the remaining data as training material; for this experiment, we used the same training set as (Hillard et al., 2003).
Identification of this fine-grained structure of an interaction has been studied in prior work, with applications in agreement detection (Galley et al, 2004), addressee detection (op den Akker and Traum, 2009), and real-world applications, such as customer service conversations (Kim et al, 2010). $$$$$ Our accuracy for classifying agreements and disagreements is 86.9%, which is a 4.9% improvement over (Hillard et al., 2003).

To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). $$$$$ In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither.
To find these pairs automatically, we trained a non-sequential log-linear model that achieves a .902 accuracy (Galley et al, 2004). $$$$$ We used the labeled adjacency pairs of 50 meetings and selected 80% of the pairs for training.

 $$$$$ In the second set of experiments, we aimed at reducing the expected variance of our experimental results and performed N-fold cross-validation in a four-way classification task, at each step retaining the hand-labeled data of a meeting for testing and the rest of the data for training.
 $$$$$ We describe a statistical approach for modeling agreements and disagreements in conversational interaction.
 $$$$$ Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data.
 $$$$$ Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse.
