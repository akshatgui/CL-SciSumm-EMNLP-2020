The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). $$$$$ The hierarchy we chose for semantic matching is the semantic network of WordNet 1M190], [MI93].
The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). $$$$$ The problem with successful resolution of ambiguous prepositional phrase attachment is that we need to employ various types of knowledge.
The state of the art is a supervised algorithm that employs a semantically tagged corpus (Stetina and Nagao, 1997). $$$$$ At each internal node, we follow the branch labelled by the attribute value which is the semantic ancestor of the attribute value of the quadruple (i.e. the branch attribute value is a semantic ancestori0 of the value of the quadruple attribute).

clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). $$$$$ The above experiments confirmed the expectations that using the semantic information in combination with even a very limited context leads to a substantial improvement of NLP techniques.
clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). $$$$$ It is not a trivial problem and has been approached by many researchers [GCY92], [YA93], [B&W94], [RE95], [YA95], [KAE96], fL196}, etc.
clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). $$$$$ We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.
clSN induces a decision tree with a sense-tagged corpus, using a semantic dictionary (Stetina and Nagao, 1997 ). $$$$$ For our purposes, we have based the semantic distance calculation on a combination of the path distance between two nodes and their depth.

At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. $$$$$ We will try to overcome this problem through the supervised learning algorithm described herein.
At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. $$$$$ In the case of verbs, the situation is even more complex, because many verbs do not share the same hierarchy, and therefore there is no direct path between the concepts they represent.
At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. $$$$$ Then, we calculate the overall heterogeneity (OH) of all these subsets as a weighted sum of their expected information: OH = -y p(A = A.
At the other extreme, Stetina and Nagao (1997) developed a customized, explicit WSD algorithm as part of their decision tree system. $$$$$ The training and testing data, extracted from the Penn Tree Bank [MA93], are identical to that used by IRRR94], [C&B951 for comparison purposes&quot;.

Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ The problem with successful resolution of ambiguous prepositional phrase attachment is that we need to employ various types of knowledge.
Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ This, however, is done only once and the disambiguated corpus is stored for future classifications of unseen quadruples.
Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ The same concepts have a distance equal to 0; concepts with no common ancestor have a distance equal to 1.
Stetina and Nagao (1997) trained on a version of the Ratnaparkhi et al (1994) dataset that contained modifications similar to those by Collins and Brooks (1995) and excluded forms not present in WordNet. $$$$$ This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.

Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. $$$$$ Our algorithm, on the other hand, has substantially reduced the number of classifications based on fewer words.
Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. $$$$$ Most of the examples in this category possibly require a wider sentential context for further improvement of accuracy.
Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. $$$$$ Consider, for example, the following sentence:
Finally, Greenberg (2013) implemented a decision tree that reimplemented the WSD module from Stetina and Nagao (1997), and also used WordNet morphosemantic (teleological) links, WordNet evocations, and a list of phrasal verbs as features. $$$$$ The data contained 20801 training and 3097 testing quadruples with 51 prepositions and ensured that there was no implicit training of the method on the test set itself.

We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). $$$$$ The more abstract the concepts are (the higher in hierarchy), the bigger the distance.
We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). $$$$$ As shown in the following table, our algorithm appears to have surpassed many existing methods and is very close to human performance on the same testing data13.
We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). $$$$$ For example, the root entity is directly followed by the concept of life...farm, while a sedan, a type of a car, is in terms of path more distant from the concept of express_train, although they are both vehicles and therefore closer concepts.
We explored the effect of excluding quadruples with lexically-specified prepositions (usually tagged PPCLR in WSJ), removing sentences in which there was no actual V, N 1, P, N 2 string found, manually removing encountered misclassifications, and reimplementing data sparsity modifications from Collins and Brooks (1995) and Stetina and Nagao (1997). $$$$$ Having ascertained the nearest common ancestor in the hierarchy, we calculate the distance as an average of the distance of the two concepts to their nearest common ancestor divided by the depth in the WordNet Hierarchy:

Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). $$$$$ The training and testing data, extracted from the Penn Tree Bank [MA93], are identical to that used by IRRR94], [C&B951 for comparison purposes&quot;.
Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). $$$$$ All are based on matching the words from the analysed sentence against the words in the training set.
Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). $$$$$ There have been numerous attempts to substitute context by superficial knowledge extracted from a large corpus.
Additional meanings derived from specific synsets have been attached to the words as described in (Stetina and Nagao, 1997). $$$$$ The more abstract the concepts are (the higher in hierarchy), the bigger the distance.

 $$$$$ We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.
 $$$$$ The problem with successful resolution of ambiguous prepositional phrase attachment is that we need to employ various types of knowledge.
 $$$$$ The above experiments confirmed the expectations that using the semantic information in combination with even a very limited context leads to a substantial improvement of NLP techniques.
 $$$$$ We feel that a bigger corpus, would provide us with an increase of accuracy of &quot;certainty I&quot; attachments, which partly includes attachments based on the small leaves.

The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. $$$$$ Two words are similar if their semantic distance is less than 1.0 and if either their character strings are different or if one of the words has been previously disambiguated.
The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. $$$$$ 2.
The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. $$$$$ Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags.
The current state of the art is 88% reported by Stetina and Nagao (1997) using the WSJ text in conjunction with WordNet. $$$$$ Consider, however, the next sentence:

The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. $$$$$ It is obvious that without some contextual information we cannot disambiguate such a sentence correctly.
The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. $$$$$ We propose a new supervised learning method for PPattachment based on a semantically tagged corpus.
The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. $$$$$ Although our method exhibits an accuracy close to the human performance, we feel that there is still a space for improvement, particularly in using a wider sentential context (human performance on full sentential context is over 93%), more training data and/or more accurate sense disambiguation technique.
The best published results over RRR are those of Stetina and Nagao (1997), who employ WordNet sense predictions from an unsupervised WSD method within a decision tree classifier. $$$$$ For our purposes, we have based the semantic distance calculation on a combination of the path distance between two nodes and their depth.

The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. $$$$$ Consider, however, the next sentence:
The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. $$$$$ The training and testing data, extracted from the Penn Tree Bank [MA93], are identical to that used by IRRR94], [C&B951 for comparison purposes&quot;.
The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. $$$$$ The most computationally expensive part of the system is the word sense disambiguation of the training corpus.
The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important, supporting the findings of Stetina and Nagao (1997) over a standalone PP attachment task. $$$$$ Therefore, we used the syntactically analysed corpus [MA931 and assigned the word senses ourselves.

Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ For example, the root entity is directly followed by the concept of life...farm, while a sedan, a type of a car, is in terms of path more distant from the concept of express_train, although they are both vehicles and therefore closer concepts.
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ The PP for children can be either adjectival and attach to the object noun books or adverbial and attach to the verb buy, leaving us with the ambiguity of two possible syntactic structures: adj) VP(VB---buy, NP(NNS—books, PP(IN=for,NP(NN--children)))) adv) VP(VB=buy, NP(NNS—books), PP(IN=for,NP(NN=children))).
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ It is obvious that without some contextual information we cannot disambiguate such a sentence correctly.
Supervised training methods already applied to PP attachment range from stochastic maximum likelihood (Collins and Brooks, 1995) or maxi mum entropy models (Ratnaparkhi et al, 1994) to the induction of transformation rules (Brill and Resnik, 1994), decision trees (Stetina and Nagao, 1997) and connectionist models (Sopena et al, 1998). $$$$$ We believe that the word sense disambiguation can be accompanied by PP attachment resolution, and that they complement each other.

The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. $$$$$ At first, we have to specify the semantic hierarchy.
The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. $$$$$ It is obvious that without some contextual information we cannot disambiguate such a sentence correctly.
The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. $$$$$ At the same time we would like to note, that PP attachment and sense disambiguation are heavily contextually dependent problems.
The state-of-the-art is set by (Stetina and Nagao, 1997) who generalize corpus observations to semantically similar words as they can be derived from the WordNet hierarchy. $$$$$ Verb hierarchies are more shallow than those of nouns, as nouns tend to be more easily organised by the is-a relation, while this is not always possible for verbs.

 $$$$$ It is not a trivial problem and has been approached by many researchers [GCY92], [YA93], [B&W94], [RE95], [YA95], [KAE96], fL196}, etc.
 $$$$$ This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.
 $$$$$ This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity.
 $$$$$ )], where p(PPADvIA=Aw) and p(PADJIA=Aw) represent the conditional probabilities of the adverbial and adjectival attachments, respectively.
