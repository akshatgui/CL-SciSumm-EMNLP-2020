Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ IRI9712068.
Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ IRI9712068.
Long before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification

As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. $$$$$ Arguably the most widely used is the mutual information (Hindle, 1990; Church and Hanks, 1990; Dagan et al., 1995; Luk, 1995; D. Lin, 1998a).
As an asymmetric measure, we examine skew divergence defined by the following equation (Lee, 1999), where Px denotes a probability distribution estimated from a feature set Fx. $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.

We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ Test-set performance was measured by the error rate, defined as —1(# of incorrect choices + (# of ties)/2) , where T is the number of test triple tokens in the set, and a tie results when both alternatives are deemed equally likely by the language model in question.
We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ We consider here the question of how to estimate the conditional cooccurrence probability P(v In) of an unseen word pair (n, v) drawn from some finite set N x V. Two state-of-the-art technologies are Katz's (1987) backoff method and Jelinek and Mercer's (1980) interpolation method.
We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ IRI9712068.
We use verb-object relations in both active and passive voice constructions as did Pereira et al. (1993) and Lee (1999), among others. $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.

We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). $$$$$ When grouped by average performance, they fell into several coherent classes, which corresponded to the extent to which the functions focused on the intersection of the supports (regions of positive probability) of the distributions.
We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). $$$$$ The overall goal of the work described here was to discover these key characteristics.
We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). $$$$$ Previously, we found the Jensen-Shannon divergence (Rao, 1982; J. Lin, 1991) to be a useful measure of the distance between distributions: The function D is the KL divergence, which measures the (always nonnegative) average inefficiency in using one distribution to code for another (Cover and Thomas, 1991): The function avgq denotes the average distribution avgq,r(v) = (q(v) + r(v))/2; observe that its use ensures that the Jensen-Shannon divergence is always defined.
We use the cosine similarity measure for window based contexts and the following commonly used similarity measures for the syntactic vector space: Hindle's (1990) measure, the weighted Linmeasure (Wu and Zhou, 2003), the Skew divergence measure (Lee, 1999), the Jensen-Shannon (JS) divergence measure (Lin, 1991), Jaccard's coefficient (van Rijsbergen, 1979) and the Confusion probability (Essen and Steinbiss, 1992). $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification

While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). $$$$$ To perform the evaluation, we incorporated each similarity function into a decision rule as follows.
While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). $$$$$ Both use P(v) to estimate P(v In) when (n, v) is unseen, essentially ignoring the identity of n. An alternative approach is distance-weighted averaging, which arrives at an estimate for unseen cooccurrences by combining estimates for where S(n) is a set of candidate similar words and sim(n, m) is a function of the similarity between n and m. We focus on distributional rather than semantic similarity (e.g., Resnik (1995)) because the goal of distance-weighted averaging is to smooth probability distributions — although the words &quot;chance&quot; and &quot;probability&quot; are synonyms, the former may not be a good model for predicting what cooccurrences the latter is likely to participate in.
While Lee (1999) argues that -Skew's asymmetry can be advantageous for nouns, this probably does not hold for verbs: verb hierarchies have much shallower structure than noun hierarchies with most verbs concentrated on one level (Miller et al, 1990). $$$$$ The Jensen-Shannon divergence and the L1 norm can be computed simply by knowing the values of q and r on Vqr.

In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.
In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
In general we may be able to identify related phrases (for example with distributional similarity (Lee, 1999)), but would like to be able to automatically classify the related phrases by the type of the relationship. $$$$$ If the base language model probabilities obey certain Bayesian consistency conditions (Dagan et al., 1999), as is the case for relative frequencies, then we may write the confusion probability as follows: Note that it incorporates unigram probabilities as well as the two distributions q and r. Finally, Kendall's T which appears in work on clustering similar adjectives (Hatzivassiloglou and McKeown, 1993; Hatzivassiloglou, 1996), is a nonparametric measure of the association between random variables (Gibbons, 1993).

Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.
Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ The overall goal of the work described here was to discover these key characteristics.
Similarity measures such as Cosine, Jaccard, Dice, etc (Lee, 1999), can be employed to compute the similarities between the seeds and other feature expressions. $$$$$ In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.

The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. $$$$$ Table 1 lists these alternative forms in order of performance.
The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the -skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure. $$$$$ IRI9712068.

Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
Internally, the ranking of attributes uses Jensen-Shannon (Lee, 1999) to compute similarity scores between internal representations of seed attributes, on one hand, and each of the newly acquired attributes, on the other hand. $$$$$ It does not apply in the present setting because it does not measure the similarity between two arbitrary probability distributions (in our case, P(Vin) and P(V1m)), but rather the similarity between a joint distribution P(Xi, X2) and the corresponding product distribution P(Xi)P(X2)• Hamming-type metrics (Cardie, 1993; Zavrel and Daelemans, 1997) are intended for data with symbolic features, since they count feature label mismatches, whereas we are dealing feature values that are probabilities.

For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.
For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ Finally, we observe that the skew metric seems quite promising.
For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. $$$$$ distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.

 $$$$$ An inherent problem for statistical methods in natural language processing is that of sparse data — the inaccurate representation in any training corpus of the probability of low frequency events.
 $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification

We are interested in some distributional similarities (Lee, 1999) given certain context. $$$$$ The intuition behind Kendall's T is as follows.
We are interested in some distributional similarities (Lee, 1999) given certain context. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification

Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. $$$$$ D. Lin also found that the choice of similarity function can affect the quality of automatically-constructed thesauri to a statistically significant degree (1998a) and the ability to determine common morphological roots by as much as 49% in precision (1998b).
Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.
Dagan et al (1997) find that the symmetric information radius measure performs best on a pseudo word sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence, a generalisation of Kullback-Leibler divergence performs best for improving probability estimates for unseen word co-occurrences. $$$$$ 587,833 (80%) of the pairs served as a training set from which to calculate base probabilities.

The second approach is from Lee (1999). $$$$$ IRI9712068.
The second approach is from Lee (1999). $$$$$ The reason we used a restricted version of the distance-weighted averaging model was that we sought to discover fundamental differences in behavior.
The second approach is from Lee (1999). $$$$$ This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al., 1999).
The second approach is from Lee (1999). $$$$$ In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.

This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ The overall goal of the work described here was to discover these key characteristics.
This choice of similarity measures was motivated by results of studies by (Levy et al 1998) and (Lee 1999) which compared several well known measures on similar tasks and found these three to be superior to many others. $$$$$ We view the empirical approach taken in this paper as complementary to Lin's.

Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999). $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.
Another reason for this choice is that there are different ideas underlying these measures: while the Jaccard's coefficient is a binary measure, L1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being a version of the information theoretic Kullback Leibler divergence (cf., Lee 1999). $$$$$ Test-set performance was measured by the error rate, defined as —1(# of incorrect choices + (# of ties)/2) , where T is the number of test triple tokens in the set, and a tie results when both alternatives are deemed equally likely by the language model in question.

Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. $$$$$ 4 The Skew Divergence Based on the results just described, it appears that it is desirable to have a similarity function that focuses on the verbs that cooccur with both of the nouns being compared.
Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. $$$$$ These unseen events generally make up a substantial portion of novel data; for example, Essen and Steinbiss (1992) report that 12% of the test-set bigrams in a 75%-25% split of one million words did not occur in the training partition.
Some enhanced KL models were developed to prevent these problems such as Jensen-Shannon (Jianhua, 1991), which introducing a probabilistic variable m, or -Skew Divergence (Lee, 1999), by adopting adjustable variable. $$$$$ In previous work (Dagan et al., 1999), we compared the performance of three different functions: the Jensen-Shannon divergence (total divergence to the average), the L1 norm, and the confusion probability.

Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ An inherent problem for statistical methods in natural language processing is that of sparse data — the inaccurate representation in any training corpus of the probability of low frequency events.
Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ Thanks to Claire Cardie, Jon Kleinberg, Fernando Pereira, and Stuart Shieber for helpful discussions, the anonymous reviewers for their insightful comments, Fernando Pereira for access to computational resources at AT&T, and Stuart Shieber for the opportunity to pursue this work at Harvard University under NSF Grant No.
Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ Whether we mean f or C — f should be clear from context.
Similarly, McCarthy (2000) uses skew divergence (a variant of KL divergence proposed by Lee, 1999) to compare the sense profile of one argument of a verb (e.g., the subject position of the intransitive) to another argument of the same verb (e.g., the object position of the transitive), to determine if the verb participates in an argument alternation involving the two positions. $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification

 $$$$$ IRI9712068.

We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos). $$$$$ IRI9712068.
We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos). $$$$$ Our contributions are three-fold: an empirical comparison of a broad range of measures; a classification
We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos). $$$$$ IRI9712068.
We compare SPD to other measures applied directly to the (unpropagated) probability profiles given by the Clark-Weir method: the probability distribution distance given by skew divergence (skew) (Lee, 1999), as well as the general vector distance given by cosine (cos). $$$$$ distributional similarity measures for the purpose of improving probability estimation for unseen cooccurrences.
