The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). $$$$$ All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search.
The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). $$$$$ Nevertheless Wikipedia offers promising results, which we expect to improve as well as the encyclopedia goes under further development.
The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). $$$$$ Our approach to WordNet here is to cast the search results in terms of semantic similarity measures.

This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form.
This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ The MaxEnt model produces a probability for each category y (coreferent or not) of a candidate pair, conditioned on the context x in which the candidate occurs.
This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ To the best of our knowledge, we do not know of any previous work using Wikipedia or SRL for coreference resolution.
This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ The resolution requires an encyclopedia (i.e.

Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ We thank Katja Filippova, Margot Mieskes and the three anonymous reviewers for their useful comments.
Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ For the ACE 2003 data, 11,406 of 32,502 automatically extracted noun phrases were tagged with 2,801 different argument-predicate pairs.
Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.

However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). $$$$$ Lincoln).
However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). $$$$$ That is, we take the cross product of each antecedent and anaphor category to form pairs of ‘Wikipedia synsets’.

Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ Additionally, we use the Wikipedia category graph.
Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ The generated model is able to learn selectional preferences in cases where surface morpho-syntactic features do not suffice, i.e. pronoun and common name resolution.
Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ The results are somehow surprising, as one would not expect a community-generated categorization to be almost as informative as a well structured lexical taxonomy such as WordNet.

Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases.
Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ Their output can be used as features for a learner.

Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). $$$$$ We believe that an interesting aspect of Wikipedia is that it offers large coverage resources for many languages, thus making it a natural choice for multilingual NLP systems.
Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). $$$$$ In order to correctly resolve the anaphoric expressions highlighted in bold, it seems that some kind of lexical semantic and encyclopedic knowledge is required.
Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). $$$$$ in contrast to Harabagiu et al. (2001), who weight WordNet relations differently in order to compute the confidence measure of the path.

Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ We thank Katja Filippova, Margot Mieskes and the three anonymous reviewers for their useful comments.
Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ This is due to making more data available to the classifier, as the SRL features are very sparse and inherently suffer from data fragmentation.
Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ Tables 3 and 4 show a comparison of the performance between our baseline system and the ones incremented with semantic features.
Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ As an example, consider a fragment from the Automatic Content Extraction (ACE) 2003 data.

The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ (2004)).
The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ Kehler et al. (2004) observe no significant improvement due to predicate argument statistics.
The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ The first author has been supported by a KTF grant (09.003.2004).

Semantic features $$$$$ Therefore, the Wikipedia-based measures are to be taken as semantic relatedness measures.
Semantic features $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
Semantic features $$$$$ The improvement reported by Yang et al. (2005) is rather caused by their twin-candidate model than by the semantic knowledge.
Semantic features $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.

Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ These measures are not specifically developed for coreference resolution but simply taken ‘off-the-shelf’ and applied to our task without any specific tuning — i.e.
Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ Tables 3 and 4 show a comparison of the performance between our baseline system and the ones incremented with semantic features.
Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.
Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.

Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ Nevertheless Wikipedia offers promising results, which we expect to improve as well as the encyclopedia goes under further development.
Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ The generated model is able to learn selectional preferences in cases where surface morpho-syntactic features do not suffice, i.e. pronoun and common name resolution.
Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ DISTANCE).

Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). $$$$$ If a disambiguation page is hit, we first get all the hyperlinks in the page.
Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). $$$$$ Then, we moved on and developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1.

Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ More specifically, whether a machine learning based approach to coreference resolution can be improved and which phenomena are affected by such information.
Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998).
Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ While the results given by using ‘the free encyclopedia that anyone can edit’ are satisfactory, major improvements can come from developing efficient query strategies – i.e. a more refined disambiguation technique taking advantage of the context in which the queries (e.g. referring expressions) occur.
Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ When combining different feature sources, we register an accuracy improvement on pronouns and common nouns, as well as an increase in F-measure due to a higher recall.

We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ While the results given by using ‘the free encyclopedia that anyone can edit’ are satisfactory, major improvements can come from developing efficient query strategies – i.e. a more refined disambiguation technique taking advantage of the context in which the queries (e.g. referring expressions) occur.
We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ We create a positive training instance from each pair of adjacent coreferent REs.
We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ Wikipedia) look-up and reasoning on the content relatedness holding between the different expressions (i.e. as a path measure along the links of the WordNet and Wikipedia taxonomies).
We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ Event representations seem also to be important for coreference resolution, as shown below: In this example, knowing that the Interfax news agency is the AGENT of the report predicate and It being the AGENT of say could trigger the (semantic parallelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow.

 $$$$$ Zx is a normalization constant.
 $$$$$ This suggests that semantics plays a role in pronoun and common noun resolution, where surface features cannot account for complex preferences and semantic knowledge is required.
 $$$$$ WN SIMILARITY BEST the highest similarity score from all (SENSEREi�ry,,, SENSEREj�m) synset pairs.

As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). $$$$$ In the case of SRL, this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than (still related) work involving predicate argument statistics.
As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). $$$$$ Our approach to WordNet here is to cast the search results in terms of semantic similarity measures.
As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). $$$$$ Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution.

Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. $$$$$ While the results given by using ‘the free encyclopedia that anyone can edit’ are satisfactory, major improvements can come from developing efficient query strategies – i.e. a more refined disambiguation technique taking advantage of the context in which the queries (e.g. referring expressions) occur.

Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows $$$$$ To prevent the model from overfitting, we employed a tunable Gaussian prior as a smoothing method.
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows $$$$$ Semantics plays indeed a role in coreference resolution.
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows $$$$$ In addition, since May 2004 it provides also a taxonomy by means of the category feature: articles can be placed in one or more categories, which are further categorized to provide a category tree.
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows $$$$$ Nevertheless Wikipedia offers promising results, which we expect to improve as well as the encyclopedia goes under further development.

In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ I SEMROLE the semantic role argumentpredicate pairs of REi.
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ We then choose the best performing feature based on the MUC score F-measure and add it to the model.
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
