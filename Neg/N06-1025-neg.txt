The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). $$$$$ Employing SRL is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter.

This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ Feature selection always improves results.
This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ WN SIMILARITY BEST the highest similarity score from all (SENSEREi�ry,,, SENSEREj�m) synset pairs.
This last conjecture is somewhat validated by Ponzetto and Strube (2006b), who reported that including predicate argument pairs as features improved the performance of a coreference resolver. $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.

Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ In addition, we report the accuracy score for all three types of ACE mentions, namely pronouns, common nouns and proper names.
Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ Given such level of semantic information available at the RE level, we introduce two new features6.
Also, on WS-353, our hybrid sense-filtered variants and word-cos-ll obtained a correlation score higher than published results using WordNet-based measures (Jarmasz and Szpakowicz, 2003) (.33 to .35) and Wikipedia based methods (Ponzetto and Strube, 2006) (.19 to .48); and very close to the results obtained by thesaurus-based (Jarmasz and Szpakowicz, 2003) (.55) and LSA-based methods (Finkelstein et al, 2002) (.56). $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.

However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.
However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). $$$$$ Future work will include turning Wikipedia into an ontology with well defined taxonomic relations, as well as exploring its usefulness of for other NLP applications.
However, the use of related verbs is similar in spirit to Bean and Riloff's (2004) use of patterns for inducing contextual role knowledge, and the use of semantic roles is also discussed in Ponzetto and Strube (2006). $$$$$ To prevent the model from overfitting, we employed a tunable Gaussian prior as a smoothing method.

Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ We follow Soon et al. (2001) in performing a simple one-tailed, paired sample t-test between the baseline system’s MUC score F-measure and each of the other systems’ F-measure scores on the test documents. nouns, whereas SRL improves pronouns.
Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system.
Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
Note that there has been a recent surge of interest in extracting world knowledge from online encyclopedias such as Wikipedia (e.g., Ponzetto and Strube (2006, 2007), Poesio et al). $$$$$ More specifically, whether a machine learning based approach to coreference resolution can be improved and which phenomena are affected by such information.

Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ The improvement reported by Yang et al. (2005) is rather caused by their twin-candidate model than by the semantic knowledge.
Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ We create a positive training instance from each pair of adjacent coreferent REs.
Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below. $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.

Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). $$$$$ Wikipedia is a multilingual Web-based free-content encyclopedia5.
Traditional learning-based coreference resolvers operate by training a model for classifying whether two mentions are co-referring or not (e.g., Soon et al (2001), Ng and Cardie (2002b), Kehler et al (2004), Ponzetto and Strube (2006)). $$$$$ We investigate the use of the WordNet and Wikipedia taxonomies for extracting semantic similarity and relatedness measures, as well as semantic parsing information in terms of semantic role labeling (Gildea & Jurafsky, 2002, SRL henceforth).

Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia).
Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ This paper explores whether coreference resolution can benefit from semantic knowledge sources.
Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ For determining the relevant feature sets we follow an iterative procedure similar to the wrapper approach for feature selection (Kohavi & John, 1997) using the development data.
Given that WordNet is a static ontology and as such has limitation on coverage, more recently, there have been successful attempts to utilize information from much larger, collaboratively built resources such as Wikipedia (Ponzetto and Strube, 2006). $$$$$ In order to correctly resolve the anaphoric expressions highlighted in bold, it seems that some kind of lexical semantic and encyclopedic knowledge is required.

The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ Given PREi/j and the lists of categories CREi/j they belong to, we factorise over all possible category pairs.
The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ I SEMROLE the semantic role argumentpredicate pairs of REi.
The use of semantic knowledge for coreference resolution has been studied before in a number of works, among them (Ponzetto and Strube, 2006), (Bengtson and Roth, 2008), (Lee et al, 2011), and (Rahman and Ng, 2011). $$$$$ Our approach to WordNet here is to cast the search results in terms of semantic similarity measures.

Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006) For English, the number agreement and gender agreement features can be obtained through the gender corpus provided. $$$$$ In addition, we report the accuracy score for all three types of ACE mentions, namely pronouns, common nouns and proper names.
Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006) For English, the number agreement and gender agreement features can be obtained through the gender corpus provided. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006) For English, the number agreement and gender agreement features can be obtained through the gender corpus provided. $$$$$ Feature selection always improves results.
Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006) For English, the number agreement and gender agreement features can be obtained through the gender corpus provided. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.

Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ In order to correctly resolve the anaphoric expressions highlighted in bold, it seems that some kind of lexical semantic and encyclopedic knowledge is required.
Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ We believe that an interesting aspect of Wikipedia is that it offers large coverage resources for many languages, thus making it a natural choice for multilingual NLP systems.
Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ Acknowledgements: This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany.
Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. $$$$$ That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004).

Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.
Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ The measures from Rada et al. (1989), Leacock & Chodorow (1998) and Wu & Palmer (1994) are computed in the same way as for WordNet.
Some researchers simply use the first sense (Soon et al, 2001) or all possible senses (Ponzetto and Strube, 2006a), while others overcome this problem with word sense disambiguation (Nicolae and Nicolae, 2006). $$$$$ Their output can be used as features for a learner.

Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). $$$$$ In order to correctly resolve the anaphoric expressions highlighted in bold, it seems that some kind of lexical semantic and encyclopedic knowledge is required.
Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). $$$$$ Our approach to WordNet here is to cast the search results in terms of semantic similarity measures.
Knowledge has also been mined from Wikipedia for measuring the semantic relatedness of two NPs, NPj and NPk (Ponzetto and Strube (2006a; 2007)). $$$$$ Performance improvements are highlighted in bold7.

Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ DISTANCE).
Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ We believe that an interesting aspect of Wikipedia is that it offers large coverage resources for many languages, thus making it a natural choice for multilingual NLP systems.
Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
Contextual roles (Bean and Riloff, 2004), semantic relations (Ji et al, 2005), semantic roles (Ponzetto and Strube, 2006b; Kong et al, 2009), and animacy (Orasan and Evans, 2007) have also been exploited to improve coreference resolution. $$$$$ Wikipedia is a multilingual Web-based free-content encyclopedia5.

We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ In this paper we investigated the effects of using different semantic knowledge sources within a machine learning based coreference resolution system.
We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ Labels have the form “ARG1 pred1 ... ARGn predn” for n semantic roles filled by a constituent, where each semantic argument label is always defined with respect to a predicate.
We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ Semantics plays indeed a role in coreference resolution.
We use Wikipedia differently from (Ponzetto and Strube, 2006) who focus on using WikiRelate, a Wikipedia-based relatedness metric (Strube and Ponzetto, 2006). $$$$$ In order to correctly resolve the anaphoric expressions highlighted in bold, it seems that some kind of lexical semantic and encyclopedic knowledge is required.

 $$$$$ In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources.

As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). $$$$$ We investigate the use of the WordNet and Wikipedia taxonomies for extracting semantic similarity and relatedness measures, as well as semantic parsing information in terms of semantic role labeling (Gildea & Jurafsky, 2002, SRL henceforth).
As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). $$$$$ In the case of SRL, this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than (still related) work involving predicate argument statistics.
As a result, researchers have re-adopted the once-popular knowledge-rich approach ,investigating a variety of semantic knowledge sources for common noun resolution, such as the semantic relations between two NPs (e.g., Ji et al (2005)), their semantic similarity as computed using WordNet (e.g., Poesio et al (2004)) or Wikipedia (Ponzetto and Strube, 2006), and the contextual role played by an NP (see Bean and Riloff (2004)). $$$$$ Given PREi/j and the lists of categories CREi/j they belong to, we factorise over all possible category pairs.

Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.
Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. $$$$$ For determining the relevant feature sets we follow an iterative procedure similar to the wrapper approach for feature selection (Kohavi & John, 1997) using the development data.
Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. $$$$$ We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns.
Following Ponzetto and Strube (2006), we consider an anaphoric reference, NPi, correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition. $$$$$ This is due to the high regions of the Wikipedia category tree being too strongly connected.

Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1. $$$$$ Else if they are both defined and agree T; else F. PROPER NAME T if both REi and REQ are proper names; else F. APPOSITIVE T if REQ is in apposition with REi; else F. In the baseline system semantic information is limited to WordNet semantic class matching.
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1. $$$$$ When combining different feature sources, we register an accuracy improvement on pronouns and common nouns, as well as an increase in F-measure due to a higher recall.
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1. $$$$$ The generated model is able to learn selectional preferences in cases where surface morpho-syntactic features do not suffice, i.e. pronoun and common name resolution.
Following previous work (e.g., Soon et al (2001) and Ponzetto and Strube (2006)), we generate training instances as follows: a positive instance is created for each anaphoric NP, NPj, and its closest antecedent, NPi; and a negative instance is created for NPj paired with each of the intervening NPs, NPi+1, NPi+2,..., NPj-1. $$$$$ These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels.

In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ We believe that the lack of semantics in the current systems leads to a performance bottleneck.
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ Future work will include turning Wikipedia into an ontology with well defined taxonomic relations, as well as exploring its usefulness of for other NLP applications.
In alternative, it has been recently shown that Wikipedia can be a promising source of semantic knowledge for coreference resolution between nominals (Ponzetto and Strube, 2006). $$$$$ The first author has been supported by a KTF grant (09.003.2004).
