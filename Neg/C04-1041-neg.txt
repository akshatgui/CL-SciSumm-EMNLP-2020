The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ The constraints SUPERTAGGING/PARSING USAGE CONSTRAINTS DISK MEMORY ? = 0.01 ? 0.05 ? 0.1 17 GB 31 GB CCGbank constraints 13 GB 23 GB Eisner constraints 9 GB 16 GB ? = 0.05 ? 0.1 2 GB 4 GB Table 3: Space requirements for model training dataprevent any constituent which is the result of a forward (backward) composition serving as the primary functor in another forward (backward) composition or a forward (backward) application.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ This could further increase parsing effficiency.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ A statisti cal model can be used to determine the most likelycategories given the word?s context.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ The second row has the CCGbank rule restriction applied, and the third row the Eisner normal-form restrictions.The next three rows correspond to our new strat egy of starting with the least restrictive setting of thesupertagger (?

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The final row corresponds to a more restrictive setting on the supertagger, in which a value of ? = 0.05 is used initially and ? = 0.1 is used if thenode limit is exceeded.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ It also dramatically increases the speed of the parser.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The first value of ? is 0.01; if the parser cannot find a spanning analysis, this ischanged to ? = 0.01k=100; if the node limit is ex ceeded (for these experiments set at 1,000,000), ? is changed to 0.05.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The techniques we have presented inthis paper increase the speed of the parser by a factor of 77.

A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al, 2000).
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ , dm.
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ As far as we are aware, the times reported here are an orderof magnitude faster than any reported for compara ble systems using linguistically motivated grammar formalisms.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ This research was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ We follow Clark (2002) in ignoring the featuresbased on the previously assigned categories; there fore every tagging decision is local and the Viterbi algorithm is not required.

The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al, 2000).
The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number.
The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ This paper describes the role of supertagging in a wide-coverage CCG parser which uses a log-linear model to select an analysis.
The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ did find that LTAG supertagging increased pars ing speed, but at a significant cost in coverage: only 1,324 sentences out of a test set of 2,250 received a parse.

However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ This makes this parser suitable for large scale NLP tasks.The results also suggest that further improvements can be obtained by improving the supertagger, which should be possible given the simple tag ging approach currently being used.The novel parsing strategy of allowing the grammar to decide if the supertagging is likely to be cor rect suggests a number of interesting possibilities.In particular, we would like to investigate only re pairing those areas of the chart that are most likely to contain errors, rather than parsing the sentence from scratch using a new set of lexical categories.
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ As far as we are aware, the times reported here are an orderof magnitude faster than any reported for compara ble systems using linguistically motivated grammar formalisms.
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.

This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ A statisti cal model can be used to determine the most likelycategories given the word?s context.
This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number.
This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ Acknowledgements We would like to thank Julia Hockenmaier, whosework creating the CCGbank made this research possible, and Mark Steedman for his advice and guid ance.
This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%.

The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ This research was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ This research was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ Packed charts are created for approximately 94% of the sentences in sec tions 2-21 of CCGbank.
The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ The threshold was set at300,000 nodes in the chart.

The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ As well as investigating parser efficiency, we have also evaluated the accuracy of the parser onsection 00 of CCGbank, using both parsing strate gies together with the normal-form constraints.
The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ Clark (2002) compares the size of grammarsextracted from CCGbank with automatically extracted LTAGs.
The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ Thefirst, following Hockenmaier (2003), is to only al low categories to combine if the combination hasbeen seen in sections 2-21 of CCGbank.

Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). $$$$$ Supertag ging was introduced for LTAG as a way of increasingparsing efficiency by reducing the number of struc tures assigned to each word (Bangalore and Joshi, 1999).
Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). $$$$$ Following Clark (2002), we apply a fre quency cutoff to the training set, only using thosecategories which appear at least 10 times in sections 2-21.
Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). $$$$$ The estimation method maximises the following objective function: L?(?)
Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages: first, CCG lexical categories are assigned to the words in the sentence, and then the categories are combined by the parser (Clark and Curran, 2004a). $$$$$ Whilst training theparser, the supertagger can be thought of as supply ing a number of plausible but incorrect categoriesfor each word; these, together with the correct cat egories, determine the parts of the parse space that are used in the estimation process.

The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ In this paper weuse the normal-form model, which defines proba bilities with the conditional log-linear form in (1), where y is a derivation and x is a sentence.
The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ Our wide-coverage CCG parser uses a log-linear model to select an analysis.
The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ It also dramatically increases the speed of the parser.

Clark and Curran (2004b) describes two log-linear parsing models for CCG: a normal-form derivation model and a dependency model. $$$$$ We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.

We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ L(?)
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ Clark (2002) shows how the models in (1) can be used to define a multi-tagger which can assign more than one category to a word.
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ , dm.
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ In Clark andCurran (2004) we show that the parsing model re sulting from training data generated in this way produces state-of-the-art CCG dependency recovery: 84.6 F-score over labelled dependencies.

Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ Acknowledgements We would like to thank Julia Hockenmaier, whosework creating the CCGbank made this research possible, and Mark Steedman for his advice and guid ance.
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ Our previous approach to parsing unseen data (Clark et al, 2002; Clark and Curran, 2003) wasto use the least restrictive setting of the supertagger which still allows a reasonable compromise be tween speed and accuracy.
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ The third row shows a further reduction in size when using the Eisner normal-form constraints.

Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ L(?)
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ Parsing using CCG can be viewed as a two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories to gether using CCG?s combinatory rules.1 The first stage can be accomplished by simply assigning to each word all categories from the word?s entry in the lexicon (Hockenmaier, 2003).
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ ?G(?)
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ We hypothesise this is a key factor in the higher accuracy for supertaggingusing a CCG grammar compared with an automati cally extracted LTAG.

The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ Acknowledgements We would like to thank Julia Hockenmaier, whosework creating the CCGbank made this research possible, and Mark Steedman for his advice and guid ance.
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ Our results confirm that wide-coverage CCG parsing is feasible for many large-scale NLP tasks.
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ Clark et al (2002) and Clark and Curran (2004) give a detailed description of the dependency structures.
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ 2The numbers differ slightly from those reported in Clark (2002) since a newer version of CCGbank is being used here.

The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ This could further increase parsing effficiency.
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ Curran and Clark(2003) describes the model and explains how Gen eralised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights.

The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ The coverage is not 100%because, for some sentences, the parser cannot pro vide an analysis, and some charts exceed the node limit even at the ? = 0.1 level.
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ The first row of the table corresponds to using the least restrictive ? value of 0.01, and reverting to ? = 0.05, and finally ? = 0.1, if the chart size exceeds some threshold.
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ Section 4.2discusses the use of various settings on the supertag ger.
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories.

Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes. $$$$$ Supertagging has more recently been applied to CCG (Clark, 2002; Curran and Clark, 2003).Supertagging accuracy is relatively high for man ually constructed LTAGs (Bangalore and Joshi,1999).
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes. $$$$$ We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b): dependency features representing predicate-argument dependencies (with and without distance measures); rule instantiation features encoding the combining categories together with the result category (wit hand without a lexical head); lexical category features, consisting of word category pairs at the leaf nodes; and root category features, consisting of head word category pairs at the root nodes. $$$$$ Acknowledgements We would like to thank Julia Hockenmaier, whosework creating the CCGbank made this research possible, and Mark Steedman for his advice and guid ance.

The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis. $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.
The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis. $$$$$ ?G(?)
The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis. $$$$$ It also dramatically increases the speed of the parser.
The CCG parsing consists of two phases: first the supertagger assigns the most probable categories toeach word, and then the small number of combinatory rules, plus the type-changing and punctuation rules, are used with the CKY algorithm to build a packed chart. We use the method described in Clark and Curran (2004b) for integrating the supertagger with the parser: initially a small number of categories is assigned to each word, and more categories are requested if the parser can not find a spanning analysis. $$$$$ The final two columns give the number of sentences, and the number of ? CATS/ 0.1 FIRST 0.01 FIRST WORD PARSES % PARSES % 0.1 1.4 1689 88.4 0 0.0 0.075 1.5 43 2.3 7 0.4 0.05 1.7 51 2.7 39 2.0 0.01 2.9 79 4.1 1816 95.1 0.01k=100 3.5 33 1.7 33 1.7 NO SPAN 15 0.8 15 0.8 Table 5: Supertagger ? levels used on section 00words, parsed per second.
