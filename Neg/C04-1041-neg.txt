The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ SUPERTAGGING/PARSING TIME SENTS WORDS CONSTRAINTS SEC /SEC /SEC ? = 0.01?
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ Since an auto matically extracted CCG grammar can produce anextremely large number of parses, the use of a su pertagger is crucial in limiting the total number of parses for the training data to a computationally manageable number.
The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second. $$$$$ The estimation method maximises the following objective function: L?(?)

The parser used in this paper is described in Clark and Curran (2004b). $$$$$ This demonstratesthe large amount of information in the lexical categories, and the potential for improving parser ac curacy and efficiency by improving the supertagger.
The parser used in this paper is described in Clark and Curran (2004b). $$$$$ The techniques we have presented inthis paper increase the speed of the parser by a factor of 77.

A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ Our philosophy was to give the parser the greatest possibility of finding thecorrect parse, by giving it as many categories as pos sible, while still retaining reasonable efficiency.4Another possible solution would be to use sampling meth ods, e.g. Osborne (2000).
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ This makes this parser suitable for large scale NLP tasks.The results also suggest that further improvements can be obtained by improving the supertagger, which should be possible given the simple tag ging approach currently being used.The novel parsing strategy of allowing the grammar to decide if the supertagging is likely to be cor rect suggests a number of interesting possibilities.In particular, we would like to investigate only re pairing those areas of the chart that are most likely to contain errors, rather than parsing the sentence from scratch using a new set of lexical categories.
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.
A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories. $$$$$ It also dramatically increases the speed of the parser.

In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ For words which appear less than 20 times in the training data, the dictionary based on the word?s POS tag is used.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ Our results confirm that wide-coverage CCG parsing is feasible for many large-scale NLP tasks.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ A packed chart is used to efficiently represent all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart.
In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG. $$$$$ To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis.

The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The conditional probabilities have the following log-linear form: p(y|x) = 1 Z(x)e ? i ?i fi(y,x) (1) where fi is a feature, ?i is the corresponding weight, and Z(x) is a normalisation constant.
The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms.
The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word). $$$$$ The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.

However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ The result is an accurate wide-coverage CCG parserwhich is an order of magnitude faster than comparable systems for other linguistically moti vated formalisms.
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ is a Gaussian prior term used to avoid overfitting (n is the number of features; ?i is the weight for feature fi; and ? is a parameter of theGaussian).
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ L(?)
However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery. $$$$$ ?G(?)

This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ As far as we are aware, the times reported here are an orderof magnitude faster than any reported for compara ble systems using linguistically motivated grammar formalisms.
This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ is a Gaussian prior term used to avoid overfitting (n is the number of features; ?i is the weight for feature fi; and ? is a parameter of theGaussian).
This is particularly true of the C&C parser, which exploits CCG? s lexicalisation to divide the parsing task between two integrated models (Clark and Curran, 2004). $$$$$ This could further increase parsing effficiency.

The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ The grammars of Chen and Vijay Shanker (2000) contain between 2,000 and 9,000 tree frames, depending on the parameters used inthe extraction process, significantly more elemen tary structures than the number of lexical categories derived from CCGbank.
The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ The advantage of this supertagging approach is that the number of categories assigned to each word can be re duced, with a correspondingly massive reduction in the number of derivations.
The dependency parsing model of Clark and Curran (2004b) is extended to exploit this partial training data. $$$$$ Features are defined for each word in the window and for the POS tag of each word.

The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ To give one example, the number of categories in the tag dictionary?s entry for the wordis is 45 (only considering categories which have appeared at least 10 times in the training data).
The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ The percentage of sentences without an analysis is now 6%, but the parser is extremely fast,processing almost 50 sentences a second.
The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ This paper has shown that by tightly integrating a supertagger with a CCG parser, very fast parse times can be achieved for Penn Treebank WSJ text.
The formalism we use is Combinatory Categorial Grammar (Steedman, 2000), together with a parsing model described in Clark and Curran (2004b) which we adapt for use with partial data. $$$$$ To achieve maximum speed, the supertagger initially assigns only a small number of CCG categories toeach word, and the parser only requests more cate gories from the supertagger if it cannot provide an analysis.

Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages $$$$$ We also report signifi cantly higher parsing speeds on newspaper text than any previously reported for a full wide-coverage parser.
Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages $$$$$ In this way the parser interacts much more closely with the supertagger.
Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages $$$$$ This makes this parser suitable for large scale NLP tasks.The results also suggest that further improvements can be obtained by improving the supertagger, which should be possible given the simple tag ging approach currently being used.The novel parsing strategy of allowing the grammar to decide if the supertagging is likely to be cor rect suggests a number of interesting possibilities.In particular, we would like to investigate only re pairing those areas of the chart that are most likely to contain errors, rather than parsing the sentence from scratch using a new set of lexical categories.
Parsing with Combinatory Categorial Grammar (CCG) takes place in two stages $$$$$ This paper has shown that by tightly integrating a supertagger with a CCG parser, very fast parse times can be achieved for Penn Treebank WSJ text.

The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ Whilst training theparser, the supertagger can be thought of as supply ing a number of plausible but incorrect categoriesfor each word; these, together with the correct cat egories, determine the parts of the parse space that are used in the estimation process.
The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ We also report signifi cantly higher parsing speeds on newspaper text than any previously reported for a full wide-coverage parser.
The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ In Clark and Curran(2004) we show that using this more restrictive set ting has a small negative impact on the accuracy of the resulting parser (about 0.6 F-score over labelled dependencies).
The partial training method uses the log-linear dependency model described in Clark and Curran (2004b), which uses sets of predicate-argument dependencies, rather than derivations, for training. $$$$$ Our results confirm that wide-coverage CCG parsing is feasible for many large-scale NLP tasks.

Clark and Curran (2004b) describes two log-linear parsing models for CCG $$$$$ The estimation method maximises the following objective function: L?(?)
Clark and Curran (2004b) describes two log-linear parsing models for CCG $$$$$ In this way the parser interacts much more closely with the supertagger.
Clark and Curran (2004b) describes two log-linear parsing models for CCG $$$$$ This research was supported by EPSRC grant GR/M96889, and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author.
Clark and Curran (2004b) describes two log-linear parsing models for CCG $$$$$ The table shows that the normal-form constraints have a significant impact on the speed, reducing theparse times for the old strategy by 72%, and reduc ing the times for the new strategy by 84%.

We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ This configuration of the system would be useful for obtaining data for lexical knowledge acquisition, for ex ample, for which large amounts of data are required.
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ The drop in accuracy is ex pected given the importance of POS tags as features.
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ (These are used to analysesome coordination and extraction cases, for example.)
We define the probability of a dependency structure as the sum of the probabilities of all those derivations leading to that structure (Clark and Curran, 2004b). $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.

Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ It also dramatically increases the speed of the parser.
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ The WSJ is a publication that I enjoy reading NP/N N (S[dcl]\NP)/NP NP/N N (NP\NP)/(S[dcl]/NP) NP (S[dcl]\NP)/(S[ng]\NP) (S[ng]\NP)/NP Figure 1: Example sentence with CCG lexical categories frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00 cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat 1 1 225 0 0 12 (0.03%) 12 (0.6%) 10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%) Table 1: Statistics for the lexical category setAn alternative is to use a statistical tagging approach to assign one or more categories.
Clark and Curran (2004b) describes the training procedure for the dependency model, which uses a discriminative estimation method by maximising the conditional likelihood of the model given the data (Riezler et al, 2002). $$$$$ We also report signifi cantly higher parsing speeds on newspaper text than any previously reported for a full wide-coverage parser.

Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ As far as we are aware, the times reported here are an orderof magnitude faster than any reported for compara ble systems using linguistically motivated grammar formalisms.
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ The supertagger reduces the derivation space over which model estimation is performed, reducing the space required for discriminative training.
Clark and Curran (2003) shows how the sum over the complete derivation space can be performed efficiently using a packed chart and the inside-outside algorithm, and Clark and Curran (2004b) extends this method to sum over all derivations leading to a gold-standard dependency structure. $$$$$ The training sen tences are parsed using a number of nodes from a 64-node Beowulf cluster.3 The time taken to parse the training sentences depends on the supertagging and parsing constraints, and the number of nodes used, but is typically around 30 minutes.

The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ To handle this we have developed a parallel implementation of the estimation algorithm which runs on a Beowulf cluster.
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.
The definitions of the objective function (4) and the gradient (5) for training remain the same in the partial-data case; the only differences are that (pi) is now defined to be those derivations which are con sis tent with the partial dependency structure pi, and the gold-standard dependency structures pij are the partial structures extracted from the gold-standard lexical category sequences. Clark and Curran (2004b) gives an algorithm for finding all derivations in a packed chart which produce a particular set of dependencies. $$$$$ The supertagger uses probabilities p(y|x) where y is a lexical category and x is a context.

The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ Eachentry in the tag dictionary is a list of all the cate gories seen with that word in the training data.
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.
The lexical category sequences for the sentences in 2-21 can easily be read off the CCGbank derivations. The derivations licenced by a lexical category sequence were created using the CCG parser described in Clark and Curran (2004b). $$$$$ The final parser beam row combines thebeam search with the fast, reduced coverage config uration of the parser, producing speeds of over 50 sentences per second.

The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ However, for LTAGs extracted automati cally from the Penn Treebank, performance is much lower (Chen et al, 1999; Chen et al, 2002).
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ 4.1 Normal-Form Constraints.
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ The techniques we have presented inthis paper increase the speed of the parser by a factor of 77.
The training data for the dependency model was created by first supertagging the sentences in sections 2-21, using the supertagger described in Clark and Curran (2004b). The average number of categories Since our training method is intended to be applicable in the absence of derivation data, the use of such rules may appear suspect. $$$$$ The oracle row shows the parser speed when it is provided with only the correct lexical categories.The parser is extremely fast, and in Clark and Cur ran (2004) we show that the F-score for labelled dependencies is almost 98%.

Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b) $$$$$ The techniques we have presented inthis paper increase the speed of the parser by a factor of 77.
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b) $$$$$ The model paramaters are estimated using a discriminative method, that is,one which requires all incorrect parses for a sentence as well as the correct parse.
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b) $$$$$ In fact, performance for such grammars is below that needed for successful integration into a full parser (Sarkar et al, 2000).
Approximate memory usage in each case was 17.6 GB of RAM.The dependency model uses the same set of features described in Clark and Curran (2004b) $$$$$ In Clark and Curran (2004) wedescribe efficient methods for performing the cal culations using packed charts.

The CCG parsing consists of two phases $$$$$ We show that large increases in speedcan be obtained by tightly integrating the su pertagger with the CCG grammar and parser.This is the first work we are aware of to success fully integrate a supertagger with a full parser which uses an automatically extracted grammar.We also further reduce the derivation space us ing constraints on category combination.
The CCG parsing consists of two phases $$$$$ 4.1 Normal-Form Constraints.
The CCG parsing consists of two phases $$$$$ The next section describes how normal-form constraints can further reduce the derivation space.
