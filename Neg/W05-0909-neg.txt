
Surface-form oriented metrics such as BLEU (Papineni et al, 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al, 2006), WER (Nie? en et al, 2000), and TER (Snover et al, 2006) do not correctly reflect the meaning similarities of the input sentence. $$$$$ High-levels of correlation at the segment level are important because they are likely to yield a metric that is sensitive to minor differences between systems and to minor differences between different versions of the same system* Furthermore, current levels of correlation at the sentence level are still rather low, offering a very significant space for improvement* The results reported in this paper demonstrate that all of the individual components included within METEOR contribute to improved correlation with human judgments* In particular, METEOR is shown to have statistically significant better correlation compared to unigram-precision, unigramrecall and the harmonic FI combination of the two* We are currently in the process of exploring several further enhancements to the current METEOR metric, which we believe have the potential to significantly further improve the sensitivity of the metric and its level of correlation with human judgments* Our work on these directions is described in further detail in Section 4*

Results are presented in terms of BLEU (Papineniet al, 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005 ) metrics. $$$$$ As the number of chunks goes to 1, penalty decreases, and its lower bound is decided by the number of unigrams matched* The parameters if this penalty function were determined based on some experimentation with deveopment data, but have not yet been trained to be optimal* Finally, the METEOR Score for the given alignment is computed as follows: data sets, the MT evaluation systems analyzed in this paper only evaluate English sentences produced by translation systems by comparing them to English reference sentences)* This has the effect of reducing the Fmean by the maximum of 50% if there are no bigram or longer matches* For a single system translation, METEOR computes the above score for each reference translation, and then reports the best score as the score for the translation* The overall METEOR score for a system is calculated based on aggregate statistics accumulated over the entire test set, similarly to the way this is done in BLEU* We calculate aggregate precision, aggregate recall, an aggregate penalty, and then combine them using the same formula used for scoring individual segments*

We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines. $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision: the proportion of the matched n-grams out of the total number of n-grams in the evaluated translation.
We consider four widely used MT metrics (BLEU, NIST, METEOR (Banerjee and Lavie, 2005) (v0.7), and TER) as our baselines. $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision: the proportion of the matched n-grams out of the total number of n-grams in the evaluated translation.

In order to attack these problems, some metrics have been proposed to include more linguistic information into the process of matching ,e.g., Meteor (Banerjee and Lavie, 2005) metric and MaxSim (Channad Ng, 2008) metrics, which improve the lexical level by the synonym dictionary or stemming technique. $$$$$ High-levels of correlation at the segment level are important because they are likely to yield a metric that is sensitive to minor differences between systems and to minor differences between different versions of the same system* Furthermore, current levels of correlation at the sentence level are still rather low, offering a very significant space for improvement* The results reported in this paper demonstrate that all of the individual components included within METEOR contribute to improved correlation with human judgments* In particular, METEOR is shown to have statistically significant better correlation compared to unigram-precision, unigramrecall and the harmonic FI combination of the two* We are currently in the process of exploring several further enhancements to the current METEOR metric, which we believe have the potential to significantly further improve the sensitivity of the metric and its level of correlation with human judgments* Our work on these directions is described in further detail in Section 4*

Another approach is taken by two other commonly used metrics, ME TEOR (Banerjee and Lavie, 2005) and TER (Snoveret al, 2006). $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall â€” the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric: The Lack of Recall: We believe that the fixed brevity penalty in BLEU does not adequately compensate for the lack of recall* Our experimental results strongly support this claim* Use of Higher Order N-grams: Higher order N-grams are used in BLEU as an indirect measure of a translation's level of grammatical wellformedness* We believe an explicit measure for the level of grammaticality (or word order) can better account for the importance of grammaticality as a factor in the MT metric, and result in better correlation with human judgments of translation quality* Lack of Explicit Word-matching Between Translation and Reference: N-gram counts don't require an explicit word-to-word matching, but this can result in counting incorrect &quot;matches&quot;, particularly for common function words* Use of Geometric Averaging of N-grams: Geometric averaging results in a score of &quot;zero&quot; whenever one of the component n-gram scores is zero* Consequently, BLEU scores at the sentence (or segment) level can be meaningless* Although BLEU was intended to be used only for aggregate counts over an entire test-set (and not at the sentence level), scores at the sentence level can be useful indicators of the quality of the metric* In experiments we conducted, a modified version of BLEU that uses equal-weight arithmetic averaging of n-gram scores was found to have better correlation with human judgments* METEOR was designed to explicitly address the weaknesses in BLEU identified above* It evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a reference translation* If more than one reference translation is available, the given translation is scored against each reference independently, and the best score is reported* This is discussed in more detail later in this section* Given a pair of translations to be compared (a system translation and a reference translation), METEOR creates an alignment between the two strings* We define an alignment as a mapping between unigrams, such that every unigram in each string maps to zero or one unigram in the other string, and to no unigrams in the same string* Thus in a given alignment, a single unigram in one string cannot map to more than one unigram in the other string* This alignment is incrementally produced through a series of stages, each stage consisting of two distinct phases* In the first phase an external module lists all the possible unigram mappings between the two strings* Thus, for example, if the word &quot;computer&quot; occurs once in the system translation and twice in the reference translation, the external module lists two possible unigram mappings, one mapping the occurrence of &quot;computer&quot; in the system translation to the first occurrence of &quot;computer&quot; in the reference translation, and another mapping it to the second occurrence* Different modules map unigrams based on different criteria* The &quot;exact&quot; module maps two unigrams if they are exactly the same (e*g* &quot;computers&quot; maps to &quot;computers&quot; but not &quot;computer&quot;)* The &quot;porter stem&quot; module maps two unigrams if they are the same after they are stemmed using the Porter stemmer (e*g*: &quot;computers&quot; maps to both &quot;computers&quot; and to &quot;computer&quot;)* The &quot;WN synonymy&quot; module maps two unigrams if they are synonyms of each other* In the second phase of each stage, the largest subset of these unigram mappings is selected such that the resulting set constitutes an alignment as defined above (that is, each unigram must map to at most one unigram in the other string)* If more than one subset constitutes an alignment, and also has the same cardinality as the largest set, METEOR selects that set that has the least number of unigram mapping crosses* Intuitively, if the two strings are typed out on two rows one above the other, and lines are drawn connecting unigrams that are mapped to each other, each line crossing is counted as a &quot;unigram mapping cross&quot;* Formally, two unigram mappings (ti, rj) and (tk, rl) (where ti and tk are unigrams in the system translation mapped to unigrams rj and rl in the reference translation respectively) are said to cross if and only if the following formula evaluates to a negative number: where pos(tx) is the numeric position of the unigram tx in the system translation string, and pos(ry) is the numeric position of the unigram ry in the reference string* For a given alignment, every pair of unigram mappings is evaluated as a cross or not, and the alignment with the least total crosses is selected in this second phase* Note that these two phases together constitute a variation of the algorithm presented in (Turian et al, 2003)* Each stage only maps unigrams that have not been mapped to any unigram in any of the preceding stages* Thus the order in which the stages are run imposes different priorities on the mapping modules employed by the different stages* That is, if the first stage employs the &quot;exact&quot; mapping module and the second stage employs the &quot;porter stem&quot; module, METEOR is effectively preferring to first map two unigrams based on their surface forms, and performing the stemming only if the surface forms do not match (or if the mapping based on surface forms was too &quot;costly&quot; in terms of the total number of crosses)* Note that METEOR is flexible in terms of the number of stages, the actual external mapping module used for each stage, and the order in which the stages are run* By default the first stage uses the &quot;exact&quot; mapping module, the second the &quot;porter stem&quot; module and the third the &quot;WN synonymy&quot; mo dule* In section 4 we evaluate each of these configurations of METEOR* Once all the stages have been run and a final alignment has been produced between the system translation and the reference translation, the METEOR score for this pair of translations is computed as follows* First unigram precision (P) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the system translation* Similarly, unigram recall (R) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the reference translation* Next we compute Fmean by combining the precision and recall via a harmonic-mean (van Rijsbergen, 1979) that places most of the weight on recall* We use a harmonic mean of P and 9R* The resulting formula used is: Precision, recall and Fmean are based on unigram matches* To take into account longer matches, METEOR computes a penalty for a given alignment as follows* First, all the unigrams in the system translation that are mapped to unigrams in the reference translation are grouped into the fewest possible number of chunks such that the unigrams in each chunk are in adjacent positions in the system translation, and are also mapped to unigrams that are in adjacent positions in the reference translation* Thus, the longer the n-grams, the fewer the chunks, and in the extreme case where the entire system translation string matches the reference translation there is only one chunk* In the other extreme, if there are no bigram or longer matches, there are as many chunks as there are unigram matches* The penalty is then computed through the following formula: ï¿½ #unigrams matched _ For example, if the system translation was &quot;the president spoke to the audience&quot; and the reference translation was &quot;the president then spoke to the audience&quot;, there are two chunks: &quot;the president&quot; and &quot;spoke to the audience&quot;* Observe that the penalty increases as the number of chunks increases to a maximum of 0.5.

Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision: the proportion of the matched n-grams out of the total number of n-grams in the evaluated translation.
Instead we carry out extrinsic evaluation on the MT quality using the well known automatic MT evaluation metrics $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.

In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium? s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al, 2003), Translation Error Rate (TER) (Snover et al, 2006) 1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment. $$$$$ As the number of chunks goes to 1, penalty decreases, and its lower bound is decided by the number of unigrams matched* The parameters if this penalty function were determined based on some experimentation with deveopment data, but have not yet been trained to be optimal* Finally, the METEOR Score for the given alignment is computed as follows: data sets, the MT evaluation systems analyzed in this paper only evaluate English sentences produced by translation systems by comparing them to English reference sentences)* This has the effect of reducing the Fmean by the maximum of 50% if there are no bigram or longer matches* For a single system translation, METEOR computes the above score for each reference translation, and then reports the best score as the score for the translation* The overall METEOR score for a system is calculated based on aggregate statistics accumulated over the entire test set, similarly to the way this is done in BLEU* We calculate aggregate precision, aggregate recall, an aggregate penalty, and then combine them using the same formula used for scoring individual segments*


We also used the paraphrase database TERp for METEOR (Banerjee and Lavie, 2005). $$$$$ The main principle behind IBM's BLEU metric (Papineni et al, 2002) is the measurement of the overlap in unigrams (single words) and higher order n-grams of words, between a translation being evaluated and a set of one or more reference translations* The main component of BLEU is n-gram precision: the proportion of the matched n-grams out of the total number of n-grams in the evaluated translation.

For evaluation we have selected a set of 8 metric variants corresponding to seven different families $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall â€” the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric: The Lack of Recall: We believe that the fixed brevity penalty in BLEU does not adequately compensate for the lack of recall* Our experimental results strongly support this claim* Use of Higher Order N-grams: Higher order N-grams are used in BLEU as an indirect measure of a translation's level of grammatical wellformedness* We believe an explicit measure for the level of grammaticality (or word order) can better account for the importance of grammaticality as a factor in the MT metric, and result in better correlation with human judgments of translation quality* Lack of Explicit Word-matching Between Translation and Reference: N-gram counts don't require an explicit word-to-word matching, but this can result in counting incorrect &quot;matches&quot;, particularly for common function words* Use of Geometric Averaging of N-grams: Geometric averaging results in a score of &quot;zero&quot; whenever one of the component n-gram scores is zero* Consequently, BLEU scores at the sentence (or segment) level can be meaningless* Although BLEU was intended to be used only for aggregate counts over an entire test-set (and not at the sentence level), scores at the sentence level can be useful indicators of the quality of the metric* In experiments we conducted, a modified version of BLEU that uses equal-weight arithmetic averaging of n-gram scores was found to have better correlation with human judgments* METEOR was designed to explicitly address the weaknesses in BLEU identified above* It evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a reference translation* If more than one reference translation is available, the given translation is scored against each reference independently, and the best score is reported* This is discussed in more detail later in this section* Given a pair of translations to be compared (a system translation and a reference translation), METEOR creates an alignment between the two strings* We define an alignment as a mapping between unigrams, such that every unigram in each string maps to zero or one unigram in the other string, and to no unigrams in the same string* Thus in a given alignment, a single unigram in one string cannot map to more than one unigram in the other string* This alignment is incrementally produced through a series of stages, each stage consisting of two distinct phases* In the first phase an external module lists all the possible unigram mappings between the two strings* Thus, for example, if the word &quot;computer&quot; occurs once in the system translation and twice in the reference translation, the external module lists two possible unigram mappings, one mapping the occurrence of &quot;computer&quot; in the system translation to the first occurrence of &quot;computer&quot; in the reference translation, and another mapping it to the second occurrence* Different modules map unigrams based on different criteria* The &quot;exact&quot; module maps two unigrams if they are exactly the same (e*g* &quot;computers&quot; maps to &quot;computers&quot; but not &quot;computer&quot;)* The &quot;porter stem&quot; module maps two unigrams if they are the same after they are stemmed using the Porter stemmer (e*g*: &quot;computers&quot; maps to both &quot;computers&quot; and to &quot;computer&quot;)* The &quot;WN synonymy&quot; module maps two unigrams if they are synonyms of each other* In the second phase of each stage, the largest subset of these unigram mappings is selected such that the resulting set constitutes an alignment as defined above (that is, each unigram must map to at most one unigram in the other string)* If more than one subset constitutes an alignment, and also has the same cardinality as the largest set, METEOR selects that set that has the least number of unigram mapping crosses* Intuitively, if the two strings are typed out on two rows one above the other, and lines are drawn connecting unigrams that are mapped to each other, each line crossing is counted as a &quot;unigram mapping cross&quot;* Formally, two unigram mappings (ti, rj) and (tk, rl) (where ti and tk are unigrams in the system translation mapped to unigrams rj and rl in the reference translation respectively) are said to cross if and only if the following formula evaluates to a negative number: where pos(tx) is the numeric position of the unigram tx in the system translation string, and pos(ry) is the numeric position of the unigram ry in the reference string* For a given alignment, every pair of unigram mappings is evaluated as a cross or not, and the alignment with the least total crosses is selected in this second phase* Note that these two phases together constitute a variation of the algorithm presented in (Turian et al, 2003)* Each stage only maps unigrams that have not been mapped to any unigram in any of the preceding stages* Thus the order in which the stages are run imposes different priorities on the mapping modules employed by the different stages* That is, if the first stage employs the &quot;exact&quot; mapping module and the second stage employs the &quot;porter stem&quot; module, METEOR is effectively preferring to first map two unigrams based on their surface forms, and performing the stemming only if the surface forms do not match (or if the mapping based on surface forms was too &quot;costly&quot; in terms of the total number of crosses)* Note that METEOR is flexible in terms of the number of stages, the actual external mapping module used for each stage, and the order in which the stages are run* By default the first stage uses the &quot;exact&quot; mapping module, the second the &quot;porter stem&quot; module and the third the &quot;WN synonymy&quot; mo dule* In section 4 we evaluate each of these configurations of METEOR* Once all the stages have been run and a final alignment has been produced between the system translation and the reference translation, the METEOR score for this pair of translations is computed as follows* First unigram precision (P) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the system translation* Similarly, unigram recall (R) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the reference translation* Next we compute Fmean by combining the precision and recall via a harmonic-mean (van Rijsbergen, 1979) that places most of the weight on recall* We use a harmonic mean of P and 9R* The resulting formula used is: Precision, recall and Fmean are based on unigram matches* To take into account longer matches, METEOR computes a penalty for a given alignment as follows* First, all the unigrams in the system translation that are mapped to unigrams in the reference translation are grouped into the fewest possible number of chunks such that the unigrams in each chunk are in adjacent positions in the system translation, and are also mapped to unigrams that are in adjacent positions in the reference translation* Thus, the longer the n-grams, the fewer the chunks, and in the extreme case where the entire system translation string matches the reference translation there is only one chunk* In the other extreme, if there are no bigram or longer matches, there are as many chunks as there are unigram matches* The penalty is then computed through the following formula: ï¿½ #unigrams matched _ For example, if the system translation was &quot;the president spoke to the audience&quot; and the reference translation was &quot;the president then spoke to the audience&quot;, there are two chunks: &quot;the president&quot; and &quot;spoke to the audience&quot;* Observe that the penalty increases as the number of chunks increases to a maximum of 0.5.
For evaluation we have selected a set of 8 metric variants corresponding to seven different families $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall â€” the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric: The Lack of Recall: We believe that the fixed brevity penalty in BLEU does not adequately compensate for the lack of recall* Our experimental results strongly support this claim* Use of Higher Order N-grams: Higher order N-grams are used in BLEU as an indirect measure of a translation's level of grammatical wellformedness* We believe an explicit measure for the level of grammaticality (or word order) can better account for the importance of grammaticality as a factor in the MT metric, and result in better correlation with human judgments of translation quality* Lack of Explicit Word-matching Between Translation and Reference: N-gram counts don't require an explicit word-to-word matching, but this can result in counting incorrect &quot;matches&quot;, particularly for common function words* Use of Geometric Averaging of N-grams: Geometric averaging results in a score of &quot;zero&quot; whenever one of the component n-gram scores is zero* Consequently, BLEU scores at the sentence (or segment) level can be meaningless* Although BLEU was intended to be used only for aggregate counts over an entire test-set (and not at the sentence level), scores at the sentence level can be useful indicators of the quality of the metric* In experiments we conducted, a modified version of BLEU that uses equal-weight arithmetic averaging of n-gram scores was found to have better correlation with human judgments* METEOR was designed to explicitly address the weaknesses in BLEU identified above* It evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a reference translation* If more than one reference translation is available, the given translation is scored against each reference independently, and the best score is reported* This is discussed in more detail later in this section* Given a pair of translations to be compared (a system translation and a reference translation), METEOR creates an alignment between the two strings* We define an alignment as a mapping between unigrams, such that every unigram in each string maps to zero or one unigram in the other string, and to no unigrams in the same string* Thus in a given alignment, a single unigram in one string cannot map to more than one unigram in the other string* This alignment is incrementally produced through a series of stages, each stage consisting of two distinct phases* In the first phase an external module lists all the possible unigram mappings between the two strings* Thus, for example, if the word &quot;computer&quot; occurs once in the system translation and twice in the reference translation, the external module lists two possible unigram mappings, one mapping the occurrence of &quot;computer&quot; in the system translation to the first occurrence of &quot;computer&quot; in the reference translation, and another mapping it to the second occurrence* Different modules map unigrams based on different criteria* The &quot;exact&quot; module maps two unigrams if they are exactly the same (e*g* &quot;computers&quot; maps to &quot;computers&quot; but not &quot;computer&quot;)* The &quot;porter stem&quot; module maps two unigrams if they are the same after they are stemmed using the Porter stemmer (e*g*: &quot;computers&quot; maps to both &quot;computers&quot; and to &quot;computer&quot;)* The &quot;WN synonymy&quot; module maps two unigrams if they are synonyms of each other* In the second phase of each stage, the largest subset of these unigram mappings is selected such that the resulting set constitutes an alignment as defined above (that is, each unigram must map to at most one unigram in the other string)* If more than one subset constitutes an alignment, and also has the same cardinality as the largest set, METEOR selects that set that has the least number of unigram mapping crosses* Intuitively, if the two strings are typed out on two rows one above the other, and lines are drawn connecting unigrams that are mapped to each other, each line crossing is counted as a &quot;unigram mapping cross&quot;* Formally, two unigram mappings (ti, rj) and (tk, rl) (where ti and tk are unigrams in the system translation mapped to unigrams rj and rl in the reference translation respectively) are said to cross if and only if the following formula evaluates to a negative number: where pos(tx) is the numeric position of the unigram tx in the system translation string, and pos(ry) is the numeric position of the unigram ry in the reference string* For a given alignment, every pair of unigram mappings is evaluated as a cross or not, and the alignment with the least total crosses is selected in this second phase* Note that these two phases together constitute a variation of the algorithm presented in (Turian et al, 2003)* Each stage only maps unigrams that have not been mapped to any unigram in any of the preceding stages* Thus the order in which the stages are run imposes different priorities on the mapping modules employed by the different stages* That is, if the first stage employs the &quot;exact&quot; mapping module and the second stage employs the &quot;porter stem&quot; module, METEOR is effectively preferring to first map two unigrams based on their surface forms, and performing the stemming only if the surface forms do not match (or if the mapping based on surface forms was too &quot;costly&quot; in terms of the total number of crosses)* Note that METEOR is flexible in terms of the number of stages, the actual external mapping module used for each stage, and the order in which the stages are run* By default the first stage uses the &quot;exact&quot; mapping module, the second the &quot;porter stem&quot; module and the third the &quot;WN synonymy&quot; mo dule* In section 4 we evaluate each of these configurations of METEOR* Once all the stages have been run and a final alignment has been produced between the system translation and the reference translation, the METEOR score for this pair of translations is computed as follows* First unigram precision (P) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the system translation* Similarly, unigram recall (R) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the reference translation* Next we compute Fmean by combining the precision and recall via a harmonic-mean (van Rijsbergen, 1979) that places most of the weight on recall* We use a harmonic mean of P and 9R* The resulting formula used is: Precision, recall and Fmean are based on unigram matches* To take into account longer matches, METEOR computes a penalty for a given alignment as follows* First, all the unigrams in the system translation that are mapped to unigrams in the reference translation are grouped into the fewest possible number of chunks such that the unigrams in each chunk are in adjacent positions in the system translation, and are also mapped to unigrams that are in adjacent positions in the reference translation* Thus, the longer the n-grams, the fewer the chunks, and in the extreme case where the entire system translation string matches the reference translation there is only one chunk* In the other extreme, if there are no bigram or longer matches, there are as many chunks as there are unigram matches* The penalty is then computed through the following formula: ï¿½ #unigrams matched _ For example, if the system translation was &quot;the president spoke to the audience&quot; and the reference translation was &quot;the president then spoke to the audience&quot;, there are two chunks: &quot;the president&quot; and &quot;spoke to the audience&quot;* Observe that the penalty increases as the number of chunks increases to a maximum of 0.5.



There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003). $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003). $$$$$ As the number of chunks goes to 1, penalty decreases, and its lower bound is decided by the number of unigrams matched* The parameters if this penalty function were determined based on some experimentation with deveopment data, but have not yet been trained to be optimal* Finally, the METEOR Score for the given alignment is computed as follows: data sets, the MT evaluation systems analyzed in this paper only evaluate English sentences produced by translation systems by comparing them to English reference sentences)* This has the effect of reducing the Fmean by the maximum of 50% if there are no bigram or longer matches* For a single system translation, METEOR computes the above score for each reference translation, and then reports the best score as the score for the translation* The overall METEOR score for a system is calculated based on aggregate statistics accumulated over the entire test set, similarly to the way this is done in BLEU* We calculate aggregate precision, aggregate recall, an aggregate penalty, and then combine them using the same formula used for scoring individual segments*
There exists a variety of different metrics ,e.g., word error rate, position-independent word error rate, BLEU score (Papineni et al, 2002), NIST score (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al, 2003). $$$$$ High-levels of correlation at the segment level are important because they are likely to yield a metric that is sensitive to minor differences between systems and to minor differences between different versions of the same system* Furthermore, current levels of correlation at the sentence level are still rather low, offering a very significant space for improvement* The results reported in this paper demonstrate that all of the individual components included within METEOR contribute to improved correlation with human judgments* In particular, METEOR is shown to have statistically significant better correlation compared to unigram-precision, unigramrecall and the harmonic FI combination of the two* We are currently in the process of exploring several further enhancements to the current METEOR metric, which we believe have the potential to significantly further improve the sensitivity of the metric and its level of correlation with human judgments* Our work on these directions is described in further detail in Section 4*


e.g. Meteor (Banerjee and Lavie, 2005). $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall â€” the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric: The Lack of Recall: We believe that the fixed brevity penalty in BLEU does not adequately compensate for the lack of recall* Our experimental results strongly support this claim* Use of Higher Order N-grams: Higher order N-grams are used in BLEU as an indirect measure of a translation's level of grammatical wellformedness* We believe an explicit measure for the level of grammaticality (or word order) can better account for the importance of grammaticality as a factor in the MT metric, and result in better correlation with human judgments of translation quality* Lack of Explicit Word-matching Between Translation and Reference: N-gram counts don't require an explicit word-to-word matching, but this can result in counting incorrect &quot;matches&quot;, particularly for common function words* Use of Geometric Averaging of N-grams: Geometric averaging results in a score of &quot;zero&quot; whenever one of the component n-gram scores is zero* Consequently, BLEU scores at the sentence (or segment) level can be meaningless* Although BLEU was intended to be used only for aggregate counts over an entire test-set (and not at the sentence level), scores at the sentence level can be useful indicators of the quality of the metric* In experiments we conducted, a modified version of BLEU that uses equal-weight arithmetic averaging of n-gram scores was found to have better correlation with human judgments* METEOR was designed to explicitly address the weaknesses in BLEU identified above* It evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a reference translation* If more than one reference translation is available, the given translation is scored against each reference independently, and the best score is reported* This is discussed in more detail later in this section* Given a pair of translations to be compared (a system translation and a reference translation), METEOR creates an alignment between the two strings* We define an alignment as a mapping between unigrams, such that every unigram in each string maps to zero or one unigram in the other string, and to no unigrams in the same string* Thus in a given alignment, a single unigram in one string cannot map to more than one unigram in the other string* This alignment is incrementally produced through a series of stages, each stage consisting of two distinct phases* In the first phase an external module lists all the possible unigram mappings between the two strings* Thus, for example, if the word &quot;computer&quot; occurs once in the system translation and twice in the reference translation, the external module lists two possible unigram mappings, one mapping the occurrence of &quot;computer&quot; in the system translation to the first occurrence of &quot;computer&quot; in the reference translation, and another mapping it to the second occurrence* Different modules map unigrams based on different criteria* The &quot;exact&quot; module maps two unigrams if they are exactly the same (e*g* &quot;computers&quot; maps to &quot;computers&quot; but not &quot;computer&quot;)* The &quot;porter stem&quot; module maps two unigrams if they are the same after they are stemmed using the Porter stemmer (e*g*: &quot;computers&quot; maps to both &quot;computers&quot; and to &quot;computer&quot;)* The &quot;WN synonymy&quot; module maps two unigrams if they are synonyms of each other* In the second phase of each stage, the largest subset of these unigram mappings is selected such that the resulting set constitutes an alignment as defined above (that is, each unigram must map to at most one unigram in the other string)* If more than one subset constitutes an alignment, and also has the same cardinality as the largest set, METEOR selects that set that has the least number of unigram mapping crosses* Intuitively, if the two strings are typed out on two rows one above the other, and lines are drawn connecting unigrams that are mapped to each other, each line crossing is counted as a &quot;unigram mapping cross&quot;* Formally, two unigram mappings (ti, rj) and (tk, rl) (where ti and tk are unigrams in the system translation mapped to unigrams rj and rl in the reference translation respectively) are said to cross if and only if the following formula evaluates to a negative number: where pos(tx) is the numeric position of the unigram tx in the system translation string, and pos(ry) is the numeric position of the unigram ry in the reference string* For a given alignment, every pair of unigram mappings is evaluated as a cross or not, and the alignment with the least total crosses is selected in this second phase* Note that these two phases together constitute a variation of the algorithm presented in (Turian et al, 2003)* Each stage only maps unigrams that have not been mapped to any unigram in any of the preceding stages* Thus the order in which the stages are run imposes different priorities on the mapping modules employed by the different stages* That is, if the first stage employs the &quot;exact&quot; mapping module and the second stage employs the &quot;porter stem&quot; module, METEOR is effectively preferring to first map two unigrams based on their surface forms, and performing the stemming only if the surface forms do not match (or if the mapping based on surface forms was too &quot;costly&quot; in terms of the total number of crosses)* Note that METEOR is flexible in terms of the number of stages, the actual external mapping module used for each stage, and the order in which the stages are run* By default the first stage uses the &quot;exact&quot; mapping module, the second the &quot;porter stem&quot; module and the third the &quot;WN synonymy&quot; mo dule* In section 4 we evaluate each of these configurations of METEOR* Once all the stages have been run and a final alignment has been produced between the system translation and the reference translation, the METEOR score for this pair of translations is computed as follows* First unigram precision (P) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the system translation* Similarly, unigram recall (R) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the reference translation* Next we compute Fmean by combining the precision and recall via a harmonic-mean (van Rijsbergen, 1979) that places most of the weight on recall* We use a harmonic mean of P and 9R* The resulting formula used is: Precision, recall and Fmean are based on unigram matches* To take into account longer matches, METEOR computes a penalty for a given alignment as follows* First, all the unigrams in the system translation that are mapped to unigrams in the reference translation are grouped into the fewest possible number of chunks such that the unigrams in each chunk are in adjacent positions in the system translation, and are also mapped to unigrams that are in adjacent positions in the reference translation* Thus, the longer the n-grams, the fewer the chunks, and in the extreme case where the entire system translation string matches the reference translation there is only one chunk* In the other extreme, if there are no bigram or longer matches, there are as many chunks as there are unigram matches* The penalty is then computed through the following formula: ï¿½ #unigrams matched _ For example, if the system translation was &quot;the president spoke to the audience&quot; and the reference translation was &quot;the president then spoke to the audience&quot;, there are two chunks: &quot;the president&quot; and &quot;spoke to the audience&quot;* Observe that the penalty increases as the number of chunks increases to a maximum of 0.5.

Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.

For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.
For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. $$$$$ Precision is calculated separately for each n-gram order, and the precisions are combined via a geometric averaging* BLEU does not take recall into account directly* Recall â€” the proportion of the matched n-grams out of the total number of n-grams in the reference translation, is extremely important for assessing the quality of MT output, as it reflects to what degree the translation covers the entire content of the translated sentence* BLEU does not use recall because the notion of recall is unclear when matching simultaneously against a set of reference translations (rather than a single reference)* To compensate for recall, BLEU uses a Brevity Penalty, which penalizes translations for being &quot;too short&quot;* The NIST metric is conceptually similar to BLEU in most aspects, including the weaknesses discussed below* BLEU and NIST suffer from several weaknesses, which we attempt to address explicitly in our proposed METEOR metric: The Lack of Recall: We believe that the fixed brevity penalty in BLEU does not adequately compensate for the lack of recall* Our experimental results strongly support this claim* Use of Higher Order N-grams: Higher order N-grams are used in BLEU as an indirect measure of a translation's level of grammatical wellformedness* We believe an explicit measure for the level of grammaticality (or word order) can better account for the importance of grammaticality as a factor in the MT metric, and result in better correlation with human judgments of translation quality* Lack of Explicit Word-matching Between Translation and Reference: N-gram counts don't require an explicit word-to-word matching, but this can result in counting incorrect &quot;matches&quot;, particularly for common function words* Use of Geometric Averaging of N-grams: Geometric averaging results in a score of &quot;zero&quot; whenever one of the component n-gram scores is zero* Consequently, BLEU scores at the sentence (or segment) level can be meaningless* Although BLEU was intended to be used only for aggregate counts over an entire test-set (and not at the sentence level), scores at the sentence level can be useful indicators of the quality of the metric* In experiments we conducted, a modified version of BLEU that uses equal-weight arithmetic averaging of n-gram scores was found to have better correlation with human judgments* METEOR was designed to explicitly address the weaknesses in BLEU identified above* It evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a reference translation* If more than one reference translation is available, the given translation is scored against each reference independently, and the best score is reported* This is discussed in more detail later in this section* Given a pair of translations to be compared (a system translation and a reference translation), METEOR creates an alignment between the two strings* We define an alignment as a mapping between unigrams, such that every unigram in each string maps to zero or one unigram in the other string, and to no unigrams in the same string* Thus in a given alignment, a single unigram in one string cannot map to more than one unigram in the other string* This alignment is incrementally produced through a series of stages, each stage consisting of two distinct phases* In the first phase an external module lists all the possible unigram mappings between the two strings* Thus, for example, if the word &quot;computer&quot; occurs once in the system translation and twice in the reference translation, the external module lists two possible unigram mappings, one mapping the occurrence of &quot;computer&quot; in the system translation to the first occurrence of &quot;computer&quot; in the reference translation, and another mapping it to the second occurrence* Different modules map unigrams based on different criteria* The &quot;exact&quot; module maps two unigrams if they are exactly the same (e*g* &quot;computers&quot; maps to &quot;computers&quot; but not &quot;computer&quot;)* The &quot;porter stem&quot; module maps two unigrams if they are the same after they are stemmed using the Porter stemmer (e*g*: &quot;computers&quot; maps to both &quot;computers&quot; and to &quot;computer&quot;)* The &quot;WN synonymy&quot; module maps two unigrams if they are synonyms of each other* In the second phase of each stage, the largest subset of these unigram mappings is selected such that the resulting set constitutes an alignment as defined above (that is, each unigram must map to at most one unigram in the other string)* If more than one subset constitutes an alignment, and also has the same cardinality as the largest set, METEOR selects that set that has the least number of unigram mapping crosses* Intuitively, if the two strings are typed out on two rows one above the other, and lines are drawn connecting unigrams that are mapped to each other, each line crossing is counted as a &quot;unigram mapping cross&quot;* Formally, two unigram mappings (ti, rj) and (tk, rl) (where ti and tk are unigrams in the system translation mapped to unigrams rj and rl in the reference translation respectively) are said to cross if and only if the following formula evaluates to a negative number: where pos(tx) is the numeric position of the unigram tx in the system translation string, and pos(ry) is the numeric position of the unigram ry in the reference string* For a given alignment, every pair of unigram mappings is evaluated as a cross or not, and the alignment with the least total crosses is selected in this second phase* Note that these two phases together constitute a variation of the algorithm presented in (Turian et al, 2003)* Each stage only maps unigrams that have not been mapped to any unigram in any of the preceding stages* Thus the order in which the stages are run imposes different priorities on the mapping modules employed by the different stages* That is, if the first stage employs the &quot;exact&quot; mapping module and the second stage employs the &quot;porter stem&quot; module, METEOR is effectively preferring to first map two unigrams based on their surface forms, and performing the stemming only if the surface forms do not match (or if the mapping based on surface forms was too &quot;costly&quot; in terms of the total number of crosses)* Note that METEOR is flexible in terms of the number of stages, the actual external mapping module used for each stage, and the order in which the stages are run* By default the first stage uses the &quot;exact&quot; mapping module, the second the &quot;porter stem&quot; module and the third the &quot;WN synonymy&quot; mo dule* In section 4 we evaluate each of these configurations of METEOR* Once all the stages have been run and a final alignment has been produced between the system translation and the reference translation, the METEOR score for this pair of translations is computed as follows* First unigram precision (P) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the system translation* Similarly, unigram recall (R) is computed as the ratio of the number of unigrams in the system translation that are mapped (to unigrams in the reference translation) to the total number of unigrams in the reference translation* Next we compute Fmean by combining the precision and recall via a harmonic-mean (van Rijsbergen, 1979) that places most of the weight on recall* We use a harmonic mean of P and 9R* The resulting formula used is: Precision, recall and Fmean are based on unigram matches* To take into account longer matches, METEOR computes a penalty for a given alignment as follows* First, all the unigrams in the system translation that are mapped to unigrams in the reference translation are grouped into the fewest possible number of chunks such that the unigrams in each chunk are in adjacent positions in the system translation, and are also mapped to unigrams that are in adjacent positions in the reference translation* Thus, the longer the n-grams, the fewer the chunks, and in the extreme case where the entire system translation string matches the reference translation there is only one chunk* In the other extreme, if there are no bigram or longer matches, there are as many chunks as there are unigram matches* The penalty is then computed through the following formula: ï¿½ #unigrams matched _ For example, if the system translation was &quot;the president spoke to the audience&quot; and the reference translation was &quot;the president then spoke to the audience&quot;, there are two chunks: &quot;the president&quot; and &quot;spoke to the audience&quot;* Observe that the penalty increases as the number of chunks increases to a maximum of 0.5.
For the evaluation of our system, we used a number of widely accepted automatic metrics, namely BLEU (Papineni et al, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al, 2006) and inverse F-Score based on token-level precision and recall. $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.

In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al, 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). $$$$$ ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a more important role than precision in obtaining high-levels of correlation with human judgments* The previous analysis focused on correlation with human judgments at the system level* In this paper, we focus our attention on improving correlation between METEOR score and human judgments at the segment level.

Metrics based on word alignment between MT outputs and the references (Banerjee and Lavie,2005). $$$$$ As the number of chunks goes to 1, penalty decreases, and its lower bound is decided by the number of unigrams matched* The parameters if this penalty function were determined based on some experimentation with deveopment data, but have not yet been trained to be optimal* Finally, the METEOR Score for the given alignment is computed as follows: data sets, the MT evaluation systems analyzed in this paper only evaluate English sentences produced by translation systems by comparing them to English reference sentences)* This has the effect of reducing the Fmean by the maximum of 50% if there are no bigram or longer matches* For a single system translation, METEOR computes the above score for each reference translation, and then reports the best score as the score for the translation* The overall METEOR score for a system is calculated based on aggregate statistics accumulated over the entire test set, similarly to the way this is done in BLEU* We calculate aggregate precision, aggregate recall, an aggregate penalty, and then combine them using the same formula used for scoring individual segments*
