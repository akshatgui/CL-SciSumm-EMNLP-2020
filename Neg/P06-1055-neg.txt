The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ Merging, by allowing only the most beneficial annotations, helps mitigate this risk, but it is not the only way.
The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ A third category specializes in monetary units, and so on.
The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.

 $$$$$ By using a split-and-merge strategy and beginning with the barest possible initial structure, our method reliably learns a PCFG that is remarkably good at parsing.
 $$$$$ While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.
 $$$$$ Then, we parse with the larger annotated grammar, but, at each span, we prune away any symbols whose posterior probability under the baseline grammar falls below a certain threshold (e−8 in our experiments).

Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ Even though our baseline grammar has a very low accuracy, we found that this pruning barely impacts the performance of our better grammars, while significantly reducing the computational cost.
Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ We used the EVALB parseval reference implementation, available from Sekine and Collins (1997), for scoring.
Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.
Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.

We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.
We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ We would like to see only one sort of this tag because, despite its frequency, it always produces the terminal comma (barring a few annotation errors in the treebank).
We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ Such patterns are common, but often less easy to predict.
We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.

For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set.
For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ For example, after 4 SM cycles, the Fl scores of the 4 trained grammars have a variance of only 0.024, which is tiny compared to the deviation of 0.43 obtained by Matsuzaki et al. (2005)).

In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ In most cases, the categories are recognizable as either classic subcategories or an interpretable division of some other kind.
In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ We take a different, fully automated approach.

We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ Many other interesting patterns emerge, including many classical distinctions not specifically mentioned or modeled in previous work.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ The estimated probability of a production px = P(Ax → By C,) is interpolated with the average over all subsymbols of A.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.

 $$$$$ Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set.
 $$$$$ These posterior rule probabilities are still given by (1), but, since the structure of the tree is no longer known, we must sum over it when computing the inside and outside probabilities: For efficiency reasons, we use a coarse-to-fine pruning scheme like that of Caraballo and Charniak (1998).
 $$$$$ Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.
 $$$$$ Verbs of communication (says) and propositional attitudes (beleives) that tend to take inflected sentential complements dominate two classes, while control verbs (wants) fill out another.

However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ Because smoothing is most necessary when production statistics are least reliable, we expect smoothing to help more with larger numbers of subsymbols.
However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ Since A now combines the statistics of A1 and A2, its production probabilities are the sum of those of A1 and A2, weighted by their relative frequency p1 and p2 in the training data.
However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ The relevant rule is the very probable VP-2 → VP-2 ADVP-6; adding this ADVP to a growing VP does not change the VP subsymbol.

 $$$$$ Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005).
 $$$$$ Beginning with this baseline grammar, we repeatedly split and re-train the grammar.
 $$$$$ For example, NP would be split into NP-1 through NP-8.

We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000).
We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ Additionally, splitting symbols like the comma is not only unnecessary, but potentially harmful, since it needlessly fragments observations of other symbols’ behavior.
We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ Another contribution is that, unlike previous work, we investigate smoothed models, allowing us to split grammars more heavily before running into the oversplitting effect discussed in Klein and Manning (2003), where data fragmentation outweighs increased expressivity.

 $$$$$ S-12’s children usually include NP-8, which in turn usually includes PRP-0, the capitalized nominative pronouns, DT-{1,2,61 (the capitalized determiners), and so on.
 $$$$$ Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005).
 $$$$$ Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.
 $$$$$ What is more, contrary to the gain in likelihood for splitting, the loss in likelihood for merging can be efficiently approximated.7 Let T be a training tree generating a sentence w. Consider a node n of T spanning (r, t) with the label A; that is, the subtree rooted at n generates wT:t and has the label A.

We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ Hierarchical splitting leads to better parameter estimates over directly estimating a grammar with 2k subsymbols per symbol.
We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ As another example of a specific trend which was mentioned by Klein and Manning (2003), adverbs (RB) do contain splits for adverbs under ADVPs (also), NPs (only), and VPs (not).
We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ All reported development set results are averages over four runs.
We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ For example, the symbol NP might be split into the subsymbol NP&quot;S in subject position and the subsymbol NP&quot;VP in object position.

In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ For example, NP would be split into NP-1 through NP-8.
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ Empirically, hierarchical splitting increases the accuracy and lowers the variance of the learned grammars.
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ However, in practice, and given limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level more fine than POS tag but less fine than lexical identity.

Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ Other interesting phenomena also emerge.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ Personal pronouns (PRP) are well-divided into three categories, roughly: nominative case, accusative case, and sentence-initial nominative case, which each correlate very strongly with syntactic position.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ We feel it is reasonable to present only a single grammar because all the grammars are very similar.

To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ By using a split-and-merge strategy and beginning with the barest possible initial structure, our method reliably learns a PCFG that is remarkably good at parsing.
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).

This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. $$$$$ Here, α is a small constant: we found 0.01 to be a good value, but the actual quantity was surprisingly unimportant.
This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. $$$$$ At the same time, each bin gives a less robust estimate of the grammar probabilities, leading to overfitting.

We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.
We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set.
We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ For example, even beginning with an X-bar grammar (see Section 1.1) with 98 symbols, our best grammar, using 1043 symbols, achieves a test set F, of 90.2%.

 $$$$$ Such patterns are common, but often less easy to predict.
 $$$$$ Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.
 $$$$$ We found that merging 50% of the newly split symbols dramatically reduced the grammar size after each splitting round, so that after 6 SM cycles, the grammar was only 17% of the size it would otherwise have been (1043 vs. 6273 subcategories), while at the same time there was no loss in accuracy (Figure 3).

In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ The estimated probability of a production px = P(Ax → By C,) is interpolated with the average over all subsymbols of A.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ Other work has also investigated aspects of automatic grammar refinement; for example, Chiang and Bikel (2002) learn annotations such as head rules in a constrained declarative language for tree-adjoining grammars.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well.
