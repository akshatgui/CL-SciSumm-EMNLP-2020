The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ Despite its simplicity, our best grammar achieves 90.2% on the Penn Treebank, higher than fully lexicalized systems.
The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
The trees are binarized (Petrov et al, 2006) and for the EM algorithm we use the initialization method described in Matsuzaki et al. (2005). $$$$$ Functional categories generally show fewer splits, but those splits that they do exhibit are known to be strongly correlated with syntactic behavior.

 $$$$$ In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
 $$$$$ We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.
 $$$$$ Despite its simplicity, our best grammar achieves 90.2% on the Penn Treebank, higher than fully lexicalized systems.

Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.
Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
Following Petrov et al (2006) latent annotations and probabilities for the associated rules are learnt incrementally following an iterative process consisting of the repetition of three steps. $$$$$ Another contribution is that, unlike previous work, we investigate smoothed models, allowing us to split grammars more heavily before running into the oversplitting effect discussed in Klein and Manning (2003), where data fragmentation outweighs increased expressivity.

We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ Hierarchical split/merge training enables us to learn compact but accurate grammars, ranging from extremely compact (an Fl of 78% with only 147 symbols) to extremely accurate (an Fl of 90.2% for our largest grammar with only 1043 symbols).
We demonstrate that likelihood-based hierarchical EM training (Petrov et al, 2006) and cluster-based language modeling methods (Goodman, 2001) are superior to both rank-based and random-projection methods. $$$$$ It shows most of the manually introduced annotations discussed by Klein and Manning (2003), but also learns other linguistic phenomena.

For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.
For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ However, as demonstrated in Charniak (1996) and Klein and Manning (2003), a PCFG which simply takes the empirical rules and probabilities off of a treebank does not perform well.
For example in the domain of syntactic parsing with probabilistic context-free grammars (PCFGs), a surprising recent result is that automatically induced grammar refinements can outperform sophisticated methods which exploit substantial manually articulated structure (Petrov et al, 2006). $$$$$ However, it also has a very low parsing performance: 65.8/59.8 LP/LR on the development set.

In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ For our lexicon, we used a simple yet robust method for dealing with unknown and rare words by extracting a small number of features from the word and then computing appproximate tagging probabilities.3 EM is only guaranteed to find a local maximum of the likelihood, and, indeed, in practice it often gets stuck in a suboptimal configuration.
In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ The external correlation turns out to be that people and countries are more likely to possess a subject NP, while organizations are more likely to possess an object NP.
In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ Smoothing the productions of each subsymbol by shrinking them towards their common base symbol gives us a more reliable estimate, allowing them to share statistical strength.
In this paper, we consider a much more automatic, data-driven approach to learning HMM structure for acoustic modeling, analagous to the approach taken by Petrov et al (2006) for learning PCFGs. $$$$$ Klein and Manning (2003) addressed this question from a linguistic perspective, starting with a Markov grammar and manually splitting symbols in response to observed linguistic trends in the data.

We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ As a final example, the cardinal number nonterminal (CD) induces various categories for dates, fractions, spelled-out numbers, large (usually financial) digit sequences, and others.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ The second approximation that Matsuzaki et al. (2005) present is the Viterbi parse under a new sentence-specific PCFG, whose rule probabilities are given as the solution of a variational approximation of the original grammar.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.
We approximate the loss in data likelihood for a merge with the following likelihood ratio (Petrov et al, 2006). $$$$$ Verbal categories are also heavily split.

 $$$$$ In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
 $$$$$ While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.
 $$$$$ The smaller this fraction, the higher the risk of overfitting.

However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).
However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser (Petrov et al, 2006)). $$$$$ For example, even beginning with an X-bar grammar (see Section 1.1) with 98 symbols, our best grammar, using 1043 symbols, achieves a test set F, of 90.2%.

 $$$$$ Despite its simplicity, our best grammar achieves 90.2% on the Penn Treebank, higher than fully lexicalized systems.
 $$$$$ Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000).
 $$$$$ In contrast with previous work, we are able to split various terminals to different degrees, as appropriate to the actual complexity in the data.
 $$$$$ Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.

We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ Other interesting phenomena also emerge.
We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ In this paper, we investigate the learning of a grammar consistent with a treebank at the level of evaluation symbols (such as NP, VP, etc.) but split based on the likelihood of the training trees.
We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.
We have so far dealt with the adequacy of representation and we plan to test whether more sophisticated estimation (e.g., split-merge-smooth estimation as in (Petrov et al, 2006)) can obtain further improvements from the explicit representation of agreement. $$$$$ Although we show only the binary component here, of course there are both binary and unary productions that are included.

 $$$$$ We present a method that combines the strengths of both manual and automatic approaches while addressing some of their common shortcomings.
 $$$$$ Hierarchical split/merge training enables us to learn compact but accurate grammars, ranging from extremely compact (an Fl of 78% with only 147 symbols) to extremely accurate (an Fl of 90.2% for our largest grammar with only 1043 symbols).
 $$$$$ Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation.

We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ Functional categories generally show fewer splits, but those splits that they do exhibit are known to be strongly correlated with syntactic behavior.
We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
We combine multiple word representations based on semantic clusters extracted from the (Brown et al, 1992) algorithm and syntactic clusters obtained from the Berkeley parser (Petrov et al, 2006) in order to improve discriminative dependency parsing in the MST Parser framework (McDonald et al, 2005). $$$$$ Analyzing the splits of phrasal nonterminals is more difficult than for lexical categories, and we can merely give illustrations.

In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ Therefore, a variety of techniques have been developed to both enrich and generalize the naive grammar, ranging from simple tree annotation and symbol splitting (Johnson, 1998; Klein and Manning, 2003) to full lexicalization and intricate smoothing (Collins, 1999; Charniak, 2000).
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ The grammars recover patterns like those discussed in Klein and Manning (2003), heavily articulating complex and frequent categories like NP and VP while barely splitting rare or simple ones (see Section 3 for an empirical analysis).
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ All of section 23 was reserved for the final test.
In this paper, we obtain syntactic clusters from the Berkeley parser (Petrov et al., 2006). $$$$$ In order to overcome data fragmentation and overfitting, we smooth our parameters.

Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ In the latent model, its label A is split up into several latent labels, Ax.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.
Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al, 2006). $$$$$ Probabilistic context-free grammars (PCFGs) underlie most high-performance parsers in one way or another (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005).

To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ For example, even beginning with an X-bar grammar (see Section 1.1) with 98 symbols, our best grammar, using 1043 symbols, achieves a test set F, of 90.2%.
To generate parse trees, we use the Berkeley parser (Petrov et al, 2006), and use Collins head rules (Collins, 2003) to head-out binarize each tree. $$$$$ Personal pronouns (PRP) are well-divided into three categories, roughly: nominative case, accusative case, and sentence-initial nominative case, which each correlate very strongly with syntactic position.

This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.
This enables us to compare against the results of Fowler and Penn (2010), who trained the Petrov parser (Petrov et al, 2006) on CCGbank. $$$$$ Hierarchical split/merge training enables us to learn compact but accurate grammars, ranging from extremely compact (an Fl of 78% with only 147 symbols) to extremely accurate (an Fl of 90.2% for our largest grammar with only 1043 symbols).

We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ Verb phrases, unsurprisingly, also receive a full set of subsymbols, including categories for infinitive VPs, passive VPs, several for intransitive VPs, several for transitive VPs with NP and PP objects, and one for sentential complements.
We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set.
We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ Hierarchical split/merge training enables us to learn compact but accurate grammars, ranging from extremely compact (an Fl of 78% with only 147 symbols) to extremely accurate (an Fl of 90.2% for our largest grammar with only 1043 symbols).
We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy (Petrov et al, 2006). $$$$$ On the other hand, our grammars are much more compact and substantially more accurate than previous work on automatic annotation.

 $$$$$ Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.
 $$$$$ By using a split-and-merge strategy and beginning with the barest possible initial structure, our method reliably learns a PCFG that is remarkably good at parsing.
 $$$$$ We ran our experiments on the Wall Street Journal (WSJ) portion of the Penn Treebank using the standard setup: we trained on sections 2 to 21, and we used section 1 as a validation set for tuning model hyperparameters.
 $$$$$ We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank.

In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ Nonterminal splits can also be used to relay information between distant tree nodes, though untangling this kind of propagation and distilling it into clean examples is not trivial.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ Since A now combines the statistics of A1 and A2, its production probabilities are the sum of those of A1 and A2, weighted by their relative frequency p1 and p2 in the training data.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ In fact, extra subsymbols may need to be added to several nonterminals before they can cooperate to pass information along the parse tree.
In particular, a cluster learning algorithm that permits clusters to split and/or merge, as in Petrov et al (2006) or in Pereira et al (1993), may be appropriate. $$$$$ Hierarchical split/merge training enables us to learn compact but accurate grammars, ranging from extremely compact (an Fl of 78% with only 147 symbols) to extremely accurate (an Fl of 90.2% for our largest grammar with only 1043 symbols).
