(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003).
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ This allows us to evaluate the hard constraints outside the context of supervised learning.
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ This work adds a cloning operation to the tree-to-string generative model in (Yamada and Knight, 2001).
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.

Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ It can be shown that this will occur within a polynomial number of iterations (Tsochantaridis et al., 2004).
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ As the model is generative, it is much more difficult to incorporate a wide variety of features as we do here.
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ An ITG parser is used for the alignment search, exposing two syntactic features: the use of inverted productions, and the use of spans that would not be available in a tree-to-string system.
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training set needs to be iteratively aligned.

At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree.
At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks.
At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ Discriminative training is conducted using a highly modular SVM for structured output, which allows code reuse between the syntactic aligner and a maximum matching baseline.

An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001).
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology.
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ There have been many attempts to incorporate syntax into alignment; we will not present a complete list here.

The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ This allows subtrees to move during translation.
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ An ITG parser is used for the alignment search, exposing two syntactic features: the use of inverted productions, and the use of spans that would not be available in a tree-to-string system.
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings.

(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints.
(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.
(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ Let l = (Ej, Fk) be a potential link between the jth word of English sentence E and the kth word of Foreign sentence F. To measure correlation between Ej and Fk we use conditional link probability (Cherry and Lin, 2003) in place of the Dice coefficient: where the link counts are determined by wordaligning 50K sentence pairs with another matching SVM that uses the Ï†2 measure (Gale and Church, 1991) in place of Dice.

The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ We have shown that these features produce a 22% relative reduction in error rate with respect to a strong flat-string model.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996).

Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search.
Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ Its AER of 0.086 represents a 22% relative error reduction compared to the matching system.
Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ The ITG is the bottleneck, so training time could be improved by optimizing the parser.
Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ The gold standard is divided into sure and possible link sets 5 and P (Och and Ney, 2003).

Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ Several other aligners have used discriminative training.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ Suppose we are given a parse tree for one of the two sentences in our sentence pair.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ The first tests the dependency-augmented ITG described in Section 3.2 as an aligner with hard cohesion constraints.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ Though Tsochantaridis et al. (2004) provide several ways to incorporate loss into the SVM objective, we will use margin re-scaling, as it corresponds to loss usage in another max-margin alignment approach (Taskar et al., 2005).

One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ All aligners use the same simple objective function.
One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ Viewed generatively, an ITG writes to two streams at once.
One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.

The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ (Liu et al., 2005) uses a log-linear model with a greedy search.
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ This allows subtrees to move during translation.
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ An ITG will search all alignments that conform to a possible binary constituency tree.
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ The effect of the hard cohesion constraint has been greatly diminished after discriminative training.
