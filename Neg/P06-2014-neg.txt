(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link.
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ The remainder of this paper will develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations.
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ We have presented a discriminative, syntactic word alignment method.
(Cherry and Lin, 2006) used dependency structures as soft constraints to improve word alignment in an ITG framework. $$$$$ When this occurs, the alignment is said to maintain phrasal cohesion.

Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001).
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree.
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ However, this hard constraint can also rule out correct alignments, and its utility decreases as alignment models become more complex.
Cherry and Lin (2006) introduce soft syntactic ITG (Wu, 1997) constraints into a discriminative model, and use an ITG parser to constrain the search for a Viterbi alignment. $$$$$ Discriminative training is conducted using a highly modular SVM for structured output, which allows code reuse between the syntactic aligner and a maximum matching baseline.

At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings.
At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ A terminal production rl that corresponds to a link l is given that link’s features from the matching system: OT (rl) = O(l).
At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al, 2009). $$$$$ The value of this penalty will be determined through discriminative training, as described in Section 4.

An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ We will refer to the parsed language as English, and the unparsed language as Foreign.
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ The algorithm then iterates: the optimizer minimizes (3) again with the new constraint set, and solves the max cost problem for i = i + 1 with the new ~w, growing the constraint set if necessary.
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ Terminal productions produce a token in each stream, or a token in one stream with the null symbol 0 in the other.
An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. $$$$$ We create ITG trees from the gold standard using the following sorted priorities during tree construction: This creates trees that represent high scoring alignments, using a minimal number of invalid spans.

The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ We have presented a discriminative, syntactic word alignment method.
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ We conduct two experiments.
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ Similarly, any index x E/ [i, k] is external to T[Z,k].
The first two rows repeat the experiments of Taskar et al (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. $$$$$ This second feature creates a soft phrasal cohesion constraint.

(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ In (Zhang and Gildea, 2004), this model was tested on the same annotated French-English sentence pairs that we divided into training and test sets for our experiments; it achieved an AER of 0.15.
(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005).
(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ The first tests the dependency-augmented ITG described in Section 3.2 as an aligner with hard cohesion constraints.
(Cherry and Lin, 2006) modify an ITG aligner by introducing a penalty for induced parses that violate syntactic bracketing constraints. $$$$$ We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.

The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ Once the end of the training set is reached, the learner loops back to the beginning.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree.
The first is to relax or update the independence assumptions based on more information, usually syntactic, from the language pairs (Cherry and Lin, 2006). $$$$$ Both sentences are decomposed into constituent phrases simultaneously, producing a word alignment as a byproduct.

Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ Several other aligners have used discriminative training.
Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ Since all ITG movement can be explained by inversions, this constrained ITG cannot interrupt one dependency phrase with part of another.
Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment. $$$$$ Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints.

Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ They maximize summed link values v(l), where v(l) is defined as follows for an l = (Ej, Fk): All three aligners link based on 02 correlation scores, breaking ties in favor of closer pairs.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has exactly one generator in the source.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign.
Fox (2002) demonstrated and counted cases where cohesion was not maintained in hand aligned sentence-pairs, while Cherry and Lin (2006) showed that a soft cohesion constraint is superior to a hard constraint for word alignment. $$$$$ The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts.

One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ An ITG will search all alignments that conform to a possible binary constituency tree.
One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot ... le, is interrupted by the projection of its head, cause.
One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ The remainder of this paper will develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations.
One can further refine existing word alignment models with syntactic constraints (e.g. (Cherry and Lin, 2006)). $$$$$ The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser.

The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has exactly one generator in the source.
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ We use a publicly available structured output SVM to create a max-margin syntactic aligner with a soft cohesion constraint.
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks.
The syntactic constraints are specifically imposed on the n words involved in 1-to-n alignments, which is different from the cohesion constraints (Fox, 2002) as explored by Cherry and Lin (2006), where knowledge of cross-lingual syntactic projection is used. $$$$$ A loss function A(yi, y) quantifies just how incorrect a particular structure y is.
