We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. $$$$$ }: ??Tz ? Dirichlet(?
We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. $$$$$ To cope with unknown words, we replace any word appearing fewer than 5 timesin the training set with one of 50 unknown word to kens derived from 10 word-form features.
We could use other word-based dependency trees such as trees by the infinite PCFG model (Liang et al, 2007) and syntactic-head or semantic-head dependency trees in Nakazawa and Kurohashi (2012), although it is not our major focus. $$$$$ The question of how to select the appropriate gram mar complexity has been studied in earlier work.It is well known that more complex models nec essarily have higher likelihood and thus a penaltymust be imposed for more complex grammars.

However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large. $$$$$ Probabilistic context-free grammars (PCFGs) havebeen a core modeling technique for many aspects of linguistic structure, particularly syntac tic phrase structure in treebank parsing (Charniak, 1996; Collins, 1999).
However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large. $$$$$ We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process (HDP).
However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large. $$$$$ While our primarycontribution is the elucidation of the model and algorithm, we have also explored some important empirical properties of the HDP-PCFG and also demon strated the potential of variational HDP-PCFGs on a full-scale parsing task.
However, in all these cases the effective size of the state space (i.e., the number of sub-symbols in the infinite PCFG (Liang et al, 2007), or the number of adapted productions in the adaptor grammar (Johnson et al, 2007)) was not very large. $$$$$ Though their model is also based on hierarchical Dirichlet processes and issimilar to ours, they present a different inference al gorithm which is based on sampling.

For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al, 2007), and use probabilistic grammar models other than PCFGs. $$$$$ In this scenario, the set of symbols is known, but we do not know howmany subsymbols to allocate per symbol.
For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al, 2007), and use probabilistic grammar models other than PCFGs. $$$$$ K PCFG PCFG (smoothed) HDP-PCFG F1 Size F1 Size F1 Size 1 60.47 2558 60.36 2597 60.5 2557 2 69.53 3788 69.38 4614 71.08 4264 8 74.32 4262 79.26 120598 79.15 50629 12 70.99 7297 78.8 160403 78.94 86386 16 66.99 19616 79.2 261444 78.24 131377 20 64.44 27593 79.27 369699 77.81 202767 Table 2: Shows development F1 and grammar sizes (the number of effective rules) as we increase the truncation K. in K. We used these values of ?B in the following experiments.
For example, we can infer the number of nonterminals with a nonparametric Bayesian model (Liang et al., 2007), infer the model more robustly based on a Markov chain Monte Carlo inference (Johnson et al, 2007), and use probabilistic grammar models other than PCFGs. $$$$$ Examples of such penalized likelihood procedures in clude Stolcke and Omohundro (1994), which used an asymptotic Bayesian model selection criterion and Petrov et al (2006), which used a split-merge algorithm which procedurally determines when to switch between grammars of various complexities.

 $$$$$ From the grammar in Figure 6, we generated 2000 trees.
 $$$$$ These techniques are model selection techniquesthat use heuristics to choose among competing sta tistical models; in contrast, the HDP-PCFG relies on the Bayesian formalism to provide implicit control over model complexity within the framework of a single probabilistic model.Johnson et al (2006) also explored nonparametric grammars, but they do not give an inference algorithm for recursive grammars, e.g., grammars in cluding rules of the form A ? BC and B ? DA.
 $$$$$ The standard PCFG uses all 20 subsymbols of both S and X to explain the data, resulting in 8320 effective rules; in contrast, the HDP-PCFG uses only 4 subsymbols for X and 1 forS, resulting in only 68 effective rules.
 $$$$$ The question of ?how many clusters (symbols)??

While Liang et al (2007) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. $$$$$ While our primarycontribution is the elucidation of the model and algorithm, we have also explored some important empirical properties of the HDP-PCFG and also demon strated the potential of variational HDP-PCFGs on a full-scale parsing task.
While Liang et al (2007) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. $$$$$ Finkel et al(2007) independently developed another nonpara metric model of grammars.
While Liang et al (2007) demonstrated empirical gains on a synthetic corpus, our experiments focus on unsupervised category refinement on real language data. $$$$$ a nonparametric Bayesian model of syntactic tree structures based on Dirichlet processes.

(General grammars with infinite numbers of nonterminals were studied by (Liang et al, 2007b)). $$$$$ Examples of such penalized likelihood procedures in clude Stolcke and Omohundro (1994), which used an asymptotic Bayesian model selection criterion and Petrov et al (2006), which used a split-merge algorithm which procedurally determines when to switch between grammars of various complexities.
(General grammars with infinite numbers of nonterminals were studied by (Liang et al, 2007b)). $$$$$ Then define the binary production distribu tion as their cross-product ?Bz = lzr T z . This alsoyields a distribution over symbol pairs and hence de fines a different type of nonparametric PCFG.

In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a). $$$$$ Examples of such penalized likelihood procedures in clude Stolcke and Omohundro (1994), which used an asymptotic Bayesian model selection criterion and Petrov et al (2006), which used a split-merge algorithm which procedurally determines when to switch between grammars of various complexities.
In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a). $$$$$ As a result, many generative approaches to parsing construct refinements of the treebankgrammar which are more suitable for the model ing task.
In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a). $$$$$ As a result, many generative approaches to parsing construct refinements of the treebankgrammar which are more suitable for the model ing task.
In addition to the block sampler used by Bhattacharya and Getoor (2006), we are investigating general-purpose split merge samplers (Jain and Neal, 2000) and the permutation sampler (Liang et al, 2007a). $$$$$ Onsynthetic data, we recover the correct grammar without having to specify its complexity in advance.

However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007). $$$$$ We compare an ordinary PCFG estimated with maximum likelihood (Matsuzaki et al, 2005) andthe HDP-PCFG estimated using the variational in ference algorithm described in Section 2.6.To parse new sentences with a grammar, we com pute the posterior distribution over rules at each spanand extract the tree with the maximum expected cor rect number of rules (Petrov and Klein, 2007).
However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007). $$$$$ We ranexperiments on the Wall Street Journal (WSJ) por tion of the Penn Treebank.
However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007). $$$$$ GEM(?)
However, because the latent variable grammars are not explicitly regularized, EM keeps fitting the training data and eventually begins over fitting (Liang et al, 2007). $$$$$ In EM, the E-step involves a dynamic program that exploits the Markov structure of the parse tree, and the M-step involves computing ratios based onexpected counts extracted from the E-step.

 $$$$$ does not mean ?noparameters?; rather, it means that the effective num ber of parameters can grow adaptively as the amount of data increases, which is a desirable property of a learning algorithm.As models increase in complexity, so does the un certainty over parameter estimates.
 $$$$$ 696
 $$$$$ Finkel et al(2007) independently developed another nonpara metric model of grammars.
 $$$$$ Note that ?nonparametric?

We'd like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive state splitting found in the iHMM and iPCFG (Liang et al., 2007). $$$$$ In this regime, point estimates are unreliable since they do not take into account the fact that there are different amountsof uncertainty in the various components of the pa rameters.
We'd like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive state splitting found in the iHMM and iPCFG (Liang et al., 2007). $$$$$ We also show that our tech niques can be applied to full-scale parsingapplications by demonstrating its effective ness in learning state-split grammars.
We'd like to learn the number of paradigm classes from the data, but doing this would probably require extending adaptor grammars to incorporate the kind of adaptive state splitting found in the iHMM and iPCFG (Liang et al., 2007). $$$$$ Onsynthetic data, we recover the correct grammar without having to specify its complexity in advance.

It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al (2007). $$$$$ The HDP-PCFG is a Bayesian model which naturally handles this uncertainty.
It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al (2007). $$$$$ While our primarycontribution is the elucidation of the model and algorithm, we have also explored some important empirical properties of the HDP-PCFG and also demon strated the potential of variational HDP-PCFGs on a full-scale parsing task.
It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al (2007). $$$$$ In addition to presenting a fully Bayesianmodel for the PCFG, we also develop an ef ficient variational inference procedure.

First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007). $$$$$ As a result, many generative approaches to parsing construct refinements of the treebankgrammar which are more suitable for the model ing task.
First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007). $$$$$ 696
First, we can let the number of nonterminals grow unboundedly, as in the Infinite PCFG, where the nonterminals of the grammar can be indefinitely refined versions of a base PCFG (Liang et al, 2007). $$$$$ }: ??z ? G0 [draw component parameters] For each data point i ? {1, . . .

Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of alpha; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics. $$$$$ We also show that our tech niques can be applied to full-scale parsingapplications by demonstrating its effective ness in learning state-split grammars.
Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of alpha; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics. $$$$$ Our HDP-PCFG model allows the complexity of the grammar to grow as more training data is available.
Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of alpha; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics. $$$$$ Kurihara and Sato (2004) and Kurihara and Sato (2006) applied variational inference to PCFGs.
