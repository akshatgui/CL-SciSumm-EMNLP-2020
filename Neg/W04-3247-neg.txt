Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone. $$$$$ We provide an evaluation of our method on DUC 2004 data.
Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone. $$$$$ In this paper, we focus on multi-document generic text summarization, where the goal is to produce a summary of multiple documents about the same, but unspecified topic.
Earlier experiments with graph-based ranking algorithms for text summarization, as previously re ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone. $$$$$ We compare our new methods and centroidbased summarization using a feature-based generic summarization toolkit, MEAD, and show that new features outperform Centroid in most of the cases.

 $$$$$ Our summarization approach is to assess the centrality of each sentence in a cluster and include the most important ones in the summary.
 $$$$$ Second is how to compute the overall prestige of a sentence given its similarity to other sentences.
 $$$$$ Centroid-based summarization has given promising results in the past (Radev et al., 2001).
 $$$$$ This process can be viewed as choosing the most central sentences in a (multi-document) cluster that give the necessary and enough amount of information related to the main theme of the cluster.

 $$$$$ We provide an evaluation of our method on DUC 2004 data.
 $$$$$ Centrality of a sentence is often defined in terms of the centrality of the words that it contains.
 $$$$$ 3 d2s2 Ramadan told reporters in Baghdad that ”Iraq cannot deal positively with whoever represents the Security Council unless there was a clear stance on the issue of lifting the blockade off of it.

The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ Table 3 shows the LexPageRank scores for the graphs in Figure 2 setting the damping factor to .
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ We fixed Length cutoff at 9, and the weight of the Position feature at 1 in all of the policies.
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ As an extreme example, consider a noisy cluster where all the documents are related to each other, but only one of them is about a somewhat different topic.

Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ We call this new measure of sentence similarity lexical PageRank, or LexPageRank.
Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ The degree of a node in the cosine similarity graph is an indication of how much common information the sentence has with other sentences.
Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ We define degree centrality as the degree of each node in the similarity graph.
Lex PageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ Constructing the similarity graph of sentences provides us with a better view of important sentences compared to the centroid approach, which is prone to overgeneralization of the information in a document cluster.

 $$$$$ One of the most successful applications of prestige is PageRank (Page et al., 1998), the underlying technology behind the Google search engine.
 $$$$$ In centroid-based summarization (Radev et al., 2000), the sentences that contain more words from the centroid of the cluster are considered as central.
 $$$$$ All of our experiments shown in Section 4.3 use CSIS reranker.

Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. $$$$$ Test data for our experiments is taken from Document Understanding Conferences (DUC) 2004 summarization evaluation to compare our system also with other state-of-the-art summarization systems.
Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. $$$$$ The reranker in the example is a word-based MMR reranker with a cosine similarity threshold, 0.5.
Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. $$$$$ Text summarization is the process of automatically creating a compressed version of a given text that provides useful information for the user.

We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). $$$$$ We are now considering an approach for computing sentence importance based on the concept of eigenvector centrality (prestige) that we call LexPageRank.
We represent the sentences in A or B as a text graph constructed using the same approach as was used in Erkan and Radev (2004a, 2004b). $$$$$ This is a measure of how close the sentence is to the centroid of the cluster.

Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. $$$$$ There are two points to clarify in this definition of centrality.
Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. $$$$$ Test data for our experiments is taken from Document Understanding Conferences (DUC) 2004 summarization evaluation to compare our system also with other state-of-the-art summarization systems.
Erkan and Radev (2004a and 2004b) represented the documents as a weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function. $$$$$ The results of applying these methods on extractive summarization is quite promising.

In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). $$$$$ We have introduced two different methods, Degree and LexPageRank, for computing prestige in similarity graphs.
In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). $$$$$ This situation can be avoided by considering where the votes come from and taking the prestige of the voting node into account in weighting each vote.
In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). $$$$$ However, this may have a negative effect in the quality of the summaries in some cases where several unwanted sentences vote for each and raise their prestiges.
In the recent years graph based techinques have become very popular in automatic text summarization (Erkan and Radev, 2004), (Mihalcea, 2005). $$$$$ In the following sections, we discuss two methods to compute sentence prestige using this matrix.

Lex PageRank (Erkan and Radev, 2004) is one of such methods. $$$$$ Extractive summarization produces summaries by choosing a subset of the sentences in the original documents.
Lex PageRank (Erkan and Radev, 2004) is one of such methods. $$$$$ We have presented a novel approach to define sentence centrality based on graph-based prestige scoring of sentences.
Lex PageRank (Erkan and Radev, 2004) is one of such methods. $$$$$ This method can be directly applied to the cosine similarity graph to find the most prestigious sentences in a document.

TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. $$$$$ All the numbers are normalized so that the highest ranked sentence gets the score .
TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. $$$$$ Our summarization approach is to assess the centrality of each sentence in a cluster and include the most important ones in the summary.
TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. $$$$$ Constructing the similarity graph of sentences provides us with a better view of important sentences compared to the centroid approach, which is prone to overgeneralization of the information in a document cluster.
TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. $$$$$ For the similarity metric, we use cosine.

Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. $$$$$ Even the simplest approach we have taken, degree centrality, is a good enough heuristic to perform better than lead-based and centroid-based summaries.
Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. $$$$$ Too low thresholds may mistakenly take weak similarities into consideration while too high thresholds may lose much of the similarity relations in a cluster.
Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. $$$$$ First of all, it accounts for information subsumption among sentences.
Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. $$$$$ If the cosine similarity between two sentences exceeds a particular predefined threshold, a corresponding edge is added to the connectivity matrix.

Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. $$$$$ Test data for our experiments is taken from Document Understanding Conferences (DUC) 2004 summarization evaluation to compare our system also with other state-of-the-art summarization systems.
Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. $$$$$ ROUGE is a recallbased metric for fixed-length summaries which is based on n-gram co-occurence.
Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. $$$$$ ‘random’ indicates a method where we have picked random sentences from the cluster to produce a summary.
Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. $$$$$ In this paper, we focus on multi-document generic text summarization, where the goal is to produce a summary of multiple documents about the same, but unspecified topic.

LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ In Section 2, we present centroid-based summarization, a wellknown method for judging sentence centrality.
LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ As seen in Table 1, the choice of cosine threshold dramatically influences the interpretation of centrality.
LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ Table 2 and Table 3 show the ROUGE scores we have got in the experiments with using LexPageRank, Degree, and Centroid in Tasks 2 and 4, respectively, sorted by ROUGE-1 scores.
LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. $$$$$ First is how to define similarity between two sentences.

The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). $$$$$ Since we are interested in significant similarities, we can eliminate some low values in this matrix by defining a threshold so that the cluster can be viewed as an (undirected) graph, where each sentence of the cluster is a node, and significantly similar sentences are connected to each other.
The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). $$$$$ In PageRank, the score of a page is determined depending on the number of pages that link to that page as well as the individual scores of the linking pages.
The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). $$$$$ We define degree centrality as the degree of each node in the similarity graph.
The underlying hypothesis of cross-document inference is that the salience of a fact should be calculated by taking into consideration both its confidence and the confidence of other facts connected to it, which is inspired by PageRank (Page et al, 1998) and LexRank (Erkan and Radev, 2004). $$$$$ 4 d2s3 Baghdad had decided late last October to completely cease cooperating with the inspectors of the United Nations Special Commission (UNSCOM), in charge of disarming Iraq’s weapons, and whose work became very limited since the fifth of August, and announced it will not resume its cooperation with the Commission even if it were subjected to a military operation.

Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). $$$$$ In Section 2, we present centroid-based summarization, a wellknown method for judging sentence centrality.
Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). $$$$$ This is a measure of how close the sentence is to the centroid of the cluster.
Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). $$$$$ In Section 2, we present centroid-based summarization, a wellknown method for judging sentence centrality.
Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al, 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al, 2007), and LSA based methods (Gong and Liu, 2001). $$$$$ We call this new measure of sentence similarity lexical PageRank, or LexPageRank.

The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ It is obvious from the figures that threshold choice affects the LexPageRank rankings of some sentences.
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ Multidocument extractive summarization relies on the concept of sentence centrality to identify the most important sentences in a document.
The method first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality (Erkan and Radev, 2004). $$$$$ The results show that our approach outperforms centroid-based summarization and is quite successful compared to other summarization systems.

 $$$$$ Test data for our experiments is taken from Document Understanding Conferences (DUC) 2004 summarization evaluation to compare our system also with other state-of-the-art summarization systems.
 $$$$$ We have presented a novel approach to define sentence centrality based on graph-based prestige scoring of sentences.
 $$$$$ As seen in Table 1, the choice of cosine threshold dramatically influences the interpretation of centrality.
 $$$$$ 8 d4s1 The Special Representative of the United Nations Secretary-General in Baghdad, Prakash Shah, announced today, Wednesday, after meeting with the Iraqi Deputy Prime Minister Tariq Aziz, that Iraq refuses to back down from its decision to cut off cooperation with the disarmament inspectors.

 $$$$$ Task 2 involves summarization of 50 TDT English clusters.
 $$$$$ Computation of Degree can be done on the fly as a side product of LexPageRank just before the power method is applied on the similarity graph.
 $$$$$ In this model, a sentence connectivity matrix is constructed based on cosine similarity.
