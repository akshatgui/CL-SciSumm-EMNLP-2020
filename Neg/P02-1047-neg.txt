A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). $$$$$ In this paper, we describe experiments aimed at building robust discourse-relation classification systems.
A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). $$$$$ To achieve this, we used the patterns in Table 2 to extract examples of discourse relations from the BLIPP corpus.
A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). $$$$$ We employed our classifiers on the manually labeled examples extracted from Carlson et al.’s corpus (2001).
A similar approach has been advocated for the interpretation of discourse relations by Marcu and Echihabi (2002). $$$$$ For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), we can automatically collect many examples of CONTRAST relations.

Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. $$$$$ If we had access to robust semantic interpreters, we could, for example, infer from sentence 1.a that “cannot buy arms legally(libya)”, infer from sentence 1.b that “can buy arms legally(rwanda)”, use our background knowledge in order to infer that “similar(libya,rwanda)”, and apply Hobbs’s (1990) definitions of discourse relations to arrive at the conclusion that a CONTRAST relation holds between the sentences in (1).
Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. $$$$$ We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. $$$$$ This ensures that our classifiers do not learn, for example, that the word pair if – then is a good indicator of a CONDITION relation, which would simply amount to learning to distinguish between the extraction patterns used to construct the corpus.
Apart from the fact that we present an alternative model, our work differs from Marcu and Echihabi (2002) in two important ways. $$$$$ We hypothesize that we can determine that a CONTRAST relation holds between the sentences in (3) even if we cannot semantically interpret the two sentences, simply because our background knowledge tells us that good and fails are good indicators of contrastive statements.

Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations $$$$$ We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations $$$$$ What the experiments manually labeled RST relations that hold between elementary discourse units.
Inspired by Marcu and Echihabi (2002), to construct relatively low noise discourse instances for unsupervised methods using cue phrases, we grouped the 13 relations into the following 5 relations $$$$$ For example, by extracting sentence pairs that have the keyword “But” at the beginning of the second sentence, as the sentence pair shown in (1), we can automatically collect many examples of CONTRAST relations.

Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). $$$$$ In Lascarides and Asher’s theory (1993), we would label the relation between 2.a and 2.b as EXPLANATION because the event in 2.b explains why the event in 2.a happened (perhaps by CAUSING it).
Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). $$$$$ As in the case of CONTRAST and CONDITION, the NO-RELATION examples are also noisy because long distance relations are common in well-written texts.
Cue-phrase-based patterns could find only limited number of discourse instances with high precision (Marcu and Echihabi, 2002). $$$$$ The results in Table 5 show that the classifiers learned from automatically generated training data can be used to distinguish between certain types of RST relations.

Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002). $$$$$ Also, since the learning curve for the BLIPP corpus is steeper than the learning curve for the Raw corpus, this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs (unannotated data).
Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002). $$$$$ This ensures that our classifiers do not learn, for example, that the word pair if – then is a good indicator of a CONDITION relation, which would simply amount to learning to distinguish between the extraction patterns used to construct the corpus.
Nouns (except for named entities) and verbs were most representative words in discourse recognition (Marcu and Echihabi, 2002). $$$$$ First, we need a means to acquire vast amounts of background knowledge from which we can derive, for example, that the word pairs good – fails and embargo – legally are good indicators of CONTRAST relations.

 $$$$$ And by extracting sentences that contain the keyword “because”, we can automatically collect many examples of CAUSE-EXPLANATION-EVIDENCE relations.
 $$$$$ Consider, for example, the sentence/clause pairs below. from the sale of expensive, high-technology systems like laser-designated missiles, aircraft electronic warfare systems, tactical radios, anti-radiation bombs and battlefield mobility systems.
 $$$$$ If we adopt, for example, Knott and Sanders’s (1998) account, we would say that the relation between sentences 1.a and 1.b is ADDITIVE, because no causal connection exists between the two sentences, PRAGMATIC, because the relation pertains to illocutionary force and not to the propositional content of the sentences, and NEGATIVE, because the relation involves a CONTRAST between the two sentences.

Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. $$$$$ The results in Table 5 show that the classifiers learned from automatically generated training data can be used to distinguish between certain types of RST relations.
Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. $$$$$ To test this hypothesis, we need to solve two problems.
Presently, there exist methods for learning oppositional terms (Marcu and Echihabi, 2002) and paraphrase learning has been thoroughly studied, but successfully extending these techniques to learn incompatible phrases poses difficulties because of the data distribution. $$$$$ In other respects though, our approach is more ambitious because it focuses on the problem of recognizing such discourse relations in unrestricted texts.

Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). $$$$$ (We define these relations in Section 2.2.)
Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). $$$$$ We also extracted all the sentences that contained the word “but” in the middle of a sentence; we split each extracted sentence into two spans, one containing the words from the beginning of the sentence to the occurrence of the keyword “but” and one containing the words from the occurrence of “but” to the end of the sentence; and we labeled the relation between the two resulting text spans as CONTRAST as well.

(Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. $$$$$ (3) Paul fails almost every class he takes.
(Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. $$$$$ Our experiments show that discourse relation classifiers that use very simple features achieve unexpectedly high levels of performance when trained on extremely large data sets.
(Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. $$$$$ Also, since the learning curve for the BLIPP corpus is steeper than the learning curve for the Raw corpus, this suggests that discourse relation classifiers trained on most representative word pairs and millions of training examples can achieve higher levels of performance than classifiers trained on all word pairs (unannotated data).
(Marcu and Echihabi, 2002) used a pattern based approach to extract instances of discourse relations such as Contrast and Elaboration from unlabeled corpora. $$$$$ The discourse relation definitions proposed by others (Mann and Thompson, 1988; Lascarides and Asher, 1993; Knott and Sanders, 1998) are not easier to apply either because they assume the ability to automatically derive, in addition to the semantics of the text spans, the intentions and illocutions associated with them as well.

There are other efforts that attempt to extend the work of (Marcu and Echihabi, 2002). $$$$$ In our paper, we show that massive amounts of data can have a major impact on discourse processing research as well.
There are other efforts that attempt to extend the work of (Marcu and Echihabi, 2002). $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
There are other efforts that attempt to extend the work of (Marcu and Echihabi, 2002). $$$$$ The approach we advocate in this paper is in some respects less ambitious than current approaches to discourse relations because it relies upon a much smaller set of relations than those used by Mann and Thompson (1988) or Martin (1992).

(Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. $$$$$ Table 3 shows the performance of all discourse relation classifiers.
(Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. $$$$$ We wrote a simple program that extracted the nouns, verbs, and cue phrases in each sentence/clause.
(Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. $$$$$ We have also built a six-way classifier to distinguish between all six relation types.

(Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ Because Carlson et al.’s corpus is small, all unmarked relations will be likely labeled as ELABORATIONs.
(Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ The extraction patterns described in Table 2 enable us to solve this problem.1 Second, given vast amounts of training material, we need a means to learn which pairs of lexical items are likely to co-occur in conjunction with each discourse relation and a means to apply the learned parameters to any pair of text spans in order to determine the discourse relation that holds between them.
(Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ Consider, for example, the sentence/clause pairs below. from the sale of expensive, high-technology systems like laser-designated missiles, aircraft electronic warfare systems, tactical radios, anti-radiation bombs and battlefield mobility systems.

To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. $$$$$ As previous research in linguistics (Halliday and Hasan, 1976; Schiffrin, 1987) and computational linguistics (Marcu, 2000) show, some occurrences of “but” and “because” do not have a discourse function; and others signal other relations than CONTRAST and CAUSE-EXPLANATION.
To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. $$$$$ This classifier has a performance of 49.7%, with a baseline of 16.67%, which is achieved by labeling all relations as CONTRASTS.
To overcome the shortage of manually annotated training data, (Marcu and Echihabi, 2002) proposed a pattern-based approach to automatically generate training data from raw corpora. $$$$$ Consider, for example, the sentence/clause pairs below. from the sale of expensive, high-technology systems like laser-designated missiles, aircraft electronic warfare systems, tactical radios, anti-radiation bombs and battlefield mobility systems.

(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. $$$$$ To these examples, we added 58,000 NO-RELATION-SAME-TEXT and 58,000 NO-RELATION-DIFFERENT-TEXTS relations.
(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. $$$$$ The second, called BLIPP, is a corpus of only 1,796,386 sentences that were parsed automatically by Charniak (2000).
(Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. $$$$$ The analysis above is informative only from a machine learning perspective.

Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. $$$$$ We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. $$$$$ First, a discourse relation recognizer would enable the development of improved discourse parsers and, consequently, of high performance single document summarizers (Marcu, 2000).
Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. $$$$$ To build such systems, we train a family of Naive Bayes classifiers on a large set of examples that are generated automatically from two corpora: a corpus of 41,147,805 English sentences that have no annotations, and BLIPP, a corpus of 1,796,386 automatically parsed English sentences (Charniak, 2000), which is available from the Linguistic Data Consortium (www.ldc.upenn.edu).
Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. $$$$$ Fortunately, with the classifiers described here, one can label some of the unmarked discourse relations correctly.

(Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. $$$$$ We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
(Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. $$$$$ In general, we hypothesize that lexical item pairs can provide clues about the discourse relations that hold between the text spans in which the lexical items occur.
(Marcu and Echihabi 2002) proposed a method to identify discourse relations between text segments using Naive Bayes classifiers trained on a huge corpus. $$$$$ In Hobbs’s theory (1990), we would also label the relation between 2.a and 2.b as EXPLANATION because the event asserted by 2.b CAUSED or could CAUSE the event asserted in 2.a.

When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.
When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. $$$$$ For example, the most representative words of the sentence in example (4), are those shown in italics.
When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. $$$$$ We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
When we consider the frequency of discourse relations, i.e. 43% for ELABORATION, 32% for CONTRAST etc., the weighted accuracy was 53% using only lexical information, which is comparable to the similar experiment by (Marcu and Echihabi 2002) of 49.7%. $$$$$ Unfortunately, the state of the art in NLP does not provide us access to semantic interpreters and general purpose knowledge bases that would support these kinds of inferences.

An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. $$$$$ So we can expect the examples we extract to be noisy.
An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. $$$$$ The results in Table 5 show that the classifiers learned from automatically generated training data can be used to distinguish between certain types of RST relations.
An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. $$$$$ For each discourse relation pair , we train a word-pair-based classifier using the automatically derived training examples in the Raw corpus, from which we first removed the cue-phrases used for extracting the examples.
An unsupervised approach was proposed to recognize discourse relations in (Marcu and Echihabi, 2002), which extracts discourse relations that hold between arbitrary spans of text making use of cue phrases. $$$$$ We present an unsupervised approach to discourse relations of hold between arbitrary spans of texts.

We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ Unfortunately, cue phrases do not signal all relations in a text.
We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. $$$$$ In question-answering, it would enable the development of systems capable of answering sophisticated, non-factoid queries, such as “what were the causes ofX?” or “what contradicts Y?”, which are beyond the state of the art of current systems (TREC, 2001).

We draw on and extend the work of Marcu and Echihabi (2002). $$$$$ The first corpus, which we call Raw, is a corpus of 1 billion words of unannotated English (41,147,805 sentences) that we created by catenating various corpora made available over the years by the Linguistic Data Consortium.
We draw on and extend the work of Marcu and Echihabi (2002). $$$$$ We show that discourse relation classifiers trained on examples that are automatically extracted from massive amounts of text can be used to distinguish between some of these relations with accuracies as high as 93%, even when the relations are not explicitly marked by cue phrases.
We draw on and extend the work of Marcu and Echihabi (2002). $$$$$ We also examined the learning curves of various classifiers and noticed that, for some of them, the addition of training examples does not appear to have a significant impact on their performance.
