They train a statistical parser on trees with only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al, 1993). $$$$$ The any-consistent rate is a measure of the grammar's coverage of linguistic phenomena.
They train a statistical parser on trees with only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al, 1993). $$$$$ The grammarian's task remains that of improving the any-consistent rate.
They train a statistical parser on trees with only semantic labels on the nodes; however, they do not integrate syntactic and semantic parsing. History-based models of parsing were first introduced in (Black et al, 1993). $$$$$ Our most common test set consists of 1600 sentences that are never seen by the grammarian.

In recent years, re-ranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al, 1993). $$$$$ We have chosen computer manuals as a task domain.
In recent years, re-ranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al, 1993). $$$$$ In particular, when considering only immediate parents, unit rules such as NP2 —■ NP1 prevent the probabilistic model from allowing the NP1 constituent to interact with the VP rule which is the functional parent of NP1.
In recent years, re-ranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al, 1993). $$$$$ We only have built a decision tree to the rule probability component (3) of the model.

The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al, 1993). $$$$$ Because of this the HBG formulation allows one to handle any grammar formalism that has a derivation process.
The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al, 1993). $$$$$ There is a significant gap between the labeled Viterbi and any-consistent rates: 30 percentage points.
The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al, 1993). $$$$$ Conclusions The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG.
The major alternative to PCFG-based approaches are so-called history-based parsers (Black et al, 1993). $$$$$ One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG.

Instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, Black et al (1993) call this the history). $$$$$ The success of the HBG model encourages future development of general history-based grammars as a more promising approach than the usual P-CFG.
Instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, Black et al (1993) call this the history). $$$$$ We propose a probabilistic model of context for disambiguation in parsing, HBG, which incorporates the intuitions of these previous works into one unified framework.
Instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, Black et al (1993) call this the history). $$$$$ But, in general, the constituent XP2 does not contain enough useful information for ambiguity resolution.
Instead, the dependency tree is built stepwise and the decision about what step to take next (e.g. which dependency to insert) can be based on information about, in theory all, previous steps and their results (in the context of generative probabilistic parsing, Black et al (1993) call this the history). $$$$$ Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node.
