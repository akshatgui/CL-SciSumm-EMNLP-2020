(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. $$$$$ Perhaps more encouragingly, gains in Fmeasure are accompanied by simultaneous increase in recall and precision for all three data sets.
(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. $$$$$ As mentioned above, the standard approach trains and optimizes a coreference classifier without necessarily optimizing for clustering-level accuracy.
(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. $$$$$ Although we are not aware of any previous attempt on training a ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, Harabagiu et al. (2001) introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions.
(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems. $$$$$ Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.

The main difference between this approach and ours is that (Ng, 2005)'s approach takes coreference resolution one step further, by comparing the results of multiple systems, while our system is a single resolver; furthermore, he emphasizes the global optimization of ranking clusters obtained locally, whereas our focus is on globally optimizing the clusterization method inside the resolver. $$$$$ We train an SVM-based ranker for ranking candidate partitions by means of Joachimsâ€™ (2002) SVM package, with all the parameters set to their default values.
The main difference between this approach and ours is that (Ng, 2005)'s approach takes coreference resolution one step further, by comparing the results of multiple systems, while our system is a single resolver; furthermore, he emphasizes the global optimization of ranking clusters obtained locally, whereas our focus is on globally optimizing the clusterization method inside the resolver. $$$$$ Random ranking.

There are many different training example generation algorithms, e.g., McCarthy and Lehnert's method, Soon et als method, Ng and Cardies method (Ng, 2005). $$$$$ First, design decisions such as the choice of the learning algorithm and the clustering procedure are apparently critical to system performance, but are often made in an ad-hoc and unprincipled manner that may be suboptimal from an empirical point of view.
There are many different training example generation algorithms, e.g., McCarthy and Lehnert's method, Soon et als method, Ng and Cardies method (Ng, 2005). $$$$$ Although we are not aware of any previous attempt on training a ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, Harabagiu et al. (2001) introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions.

Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps $$$$$ We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al., 1996).
Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps $$$$$ (2004) have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers.
Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps $$$$$ We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.

To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005). $$$$$ The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task.
To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005). $$$$$ Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.
To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005). $$$$$ Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.
To our knowledge, the best results on this dataset were obtained by the meta-classification scheme of Ng (2005). $$$$$ At each step of this search process, candidate partitions are ranked based on their heuristically computed scores.

Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method. $$$$$ An interesting question is: how much does supervised ranking help?
Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method. $$$$$ The features can be divided into four groups: lexical, grammatical, semantic, and positional.
Although our train-test splits may differ slightly, the best B-Cubed F1 score reported in Ng (2005) is 69.3%, which is considerably lower than the 79.3% obtained with our method. $$$$$ Recall that our approach uses labeled data to train both the coreference classifiers and the ranking model.

Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. $$$$$ Two questions naturally arise after examining the above results.
Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. $$$$$ Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.
Ng (2005) learns a meta-classifier to choose the best prediction from the output of several coreference systems. $$$$$ Although we are not aware of any previous attempt on training a ranking model using global features of an NP partition, there is some related work on partition ranking where the score of a partition is computed via a heuristic function of the probabilities of its NP pairs being coreferent.2 For instance, Harabagiu et al. (2001) introduce a greedy algorithm for finding the highest-scored partition by performing a beam search in the space of possible partitions.

This could be incorporated in a ranking scheme, as in Ng (2005). $$$$$ To expand our set of candidate partitions, we can potentially incorporate more high-performing coreference systems into our framework, which is flexible enough to accommodate even those that adopt knowledge-based (e.g., Harabagiu et al. (2001)) and unsupervised approaches (e.g., Cardie and Wagstaff (1999), Bean and Riloff (2004)).
This could be incorporated in a ranking scheme, as in Ng (2005). $$$$$ First, design decisions such as the choice of the learning algorithm and the clustering procedure are apparently critical to system performance, but are often made in an ad-hoc and unprincipled manner that may be suboptimal from an empirical point of view.

The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set. $$$$$ We consider three previously-proposed methods of creating training instances.
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set. $$$$$ In this paper, we view coreference resolution as a problem of ranking candidate partitions generated by different coreference systems.
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set. $$$$$ In this section we will focus on discussing related work along these two dimensions.
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure of about 62% for the same data set. $$$$$ This is essentially an upper limit on how much our approach can improve upon the baselines given the current set of candidate partitions.

 $$$$$ We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.
 $$$$$ Once again, we may use previous work to guide our choices.
 $$$$$ Ng and Cardie (2002a) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program.
 $$$$$ Our approach compares favorably to two state-of-the-art coreference systems when evaluated on three standard coreference data sets.

 $$$$$ In contrast, we attempt to optimize our ranking model with respect to the target coreference scoring function, essentially by training it in such a way that a higher scored candidate partition (according to the scoring function) would be assigned a higher rank (see Section 3.2 for details).
 $$$$$ The central idea behind the majority of these learningbased approaches is to recast coreference resolution as a binary classification task.
 $$$$$ We propose a set of partition-based features to learn a ranking model for distinguishing good and bad partitions.
 $$$$$ Instead of committing ourselves to a particular resolution method as in previous approaches, our framework makes it possible to leverage the strengths of different methods by allowing them to participate in the generation of candidate partitions.

MUC and B3 metrics (Ng, 2005a). $$$$$ (2004) have explored the use of SVM, voted perceptron, and logistic regression for training coreference classifiers.
MUC and B3 metrics (Ng, 2005a). $$$$$ In contrast, we attempt to optimize our ranking model with respect to the target coreference scoring function, essentially by training it in such a way that a higher scored candidate partition (according to the scoring function) would be assigned a higher rank (see Section 3.2 for details).
MUC and B3 metrics (Ng, 2005a). $$$$$ For pronouns, the most confident antecedent is simply its closest preceding antecedent.
MUC and B3 metrics (Ng, 2005a). $$$$$ As mentioned above, the standard approach trains and optimizes a coreference classifier without necessarily optimizing for clustering-level accuracy.

Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. $$$$$ Ranking candidate partitions.
Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. $$$$$ Random ranking.
Recent work has examined such models; Luo et al. (2004) using Bell trees, and McCallum and Wellner (2004) using conditional random fields, and Ng (2005) using rerankers. $$$$$ (2004)).

A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. $$$$$ Table 1 summarizes the previous work on coreference resolution that employs the learning algorithms, clustering algorithms, feature sets, and instance creation methods discussed above.
A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. $$$$$ Our approach.
A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems. $$$$$ Baseline systems.

Other work on global models of coreference (as opposed to pairwise models) has included $$$$$ To address the first question, we take the 54 coreference systems that were trained on half of the available training texts (see Section 4) and apply them to the three ACE test data sets.
Other work on global models of coreference (as opposed to pairwise models) has included $$$$$ First, let us assume that is the -th nominal feature in N&Câ€™s feature set and is the -th possible value of .

Similarly, the method of (Ng, 2005) ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents. $$$$$ Optimizing for clustering-level accuracy.
Similarly, the method of (Ng, 2005) ranks base models according to their performance on separate tuning set, and then uses the highest-ranked base model for predicting on test documents. $$$$$ Since the most likely antecedent is chosen for each NP, best-first clustering may produce partitions with higher precision than closest-first clustering.

The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. $$$$$ In this section we will focus on discussing related work along these two dimensions.
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. $$$$$ Unlike the MUC results, using more features to train the ranking model does not always yield better performance with respect to the B-CUBED scorer (see rows 3-5 of Table 4).
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. $$$$$ Ranking candidate partitions.
The results are comparable to those reported in (Ng, 2005) which uses similar features and gets an F-measure ranging in 50-60% for the same data set. $$$$$ Unlike the MUC results, using more features to train the ranking model does not always yield better performance with respect to the B-CUBED scorer (see rows 3-5 of Table 4).

According to Ng (2005), most learning based coreference systems can be defined by four elements $$$$$ As mentioned above, the standard approach trains and optimizes a coreference classifier without necessarily optimizing for clustering-level accuracy.
According to Ng (2005), most learning based coreference systems can be defined by four elements $$$$$ Second, the MUC scorer applies the same penalty to each erroneous merging decision, whereas B-CUBED penalizes erroneous merging decisions involving two large clusters more heavily than those involving two small clusters.
According to Ng (2005), most learning based coreference systems can be defined by four elements $$$$$ The large number of instances can potentially make the training process inefficient.

This strategy has been described as best-first clustering by Ng (2005). $$$$$ Ng and Cardie (2002a) attempt to optimize their rulebased coreference classifier for clustering-level accuracy, essentially by finding a subset of the learned rules that performs the best on held-out data with respect to the target coreference scoring program.
This strategy has been described as best-first clustering by Ng (2005). $$$$$ Ng and Cardieâ€™s system, on the other hand, employs RIPPER to train a coreference classifier on instances created by N&Câ€™s method and represented by N&Câ€™s feature set, inducing a partition on the given NPs via best-first clustering.

In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ (2004)).
In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ We consider three learning algorithms, namely, the C4.5 decision tree induction system (Quinlan, 1993), the RIPPER rule learning algorithm (Cohen, 1995), and maximum entropy classification (Berger et al., 1996).
In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ Of course, we can also expand our pre-selected set of coreference systems via incorporating additional learning algorithms, clustering algorithms, and feature sets.
