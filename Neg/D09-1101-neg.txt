 $$$$$ Second, they have limitations in their expressiveness: the information extracted from the two mentions alone may not be sufficient for making an informed coreference decision, especially if the candidate antecedent is a pronoun (which is semantically empty) or a mention that lacks descriptive information such as gender (e.g., Clinton).
 $$$$$ Experimental results on the ACE data sets demonstrate its superior performance to competing approaches.
 $$$$$ Specifically, the mentions are processed in a left-to-right manner.
 $$$$$ The cluster ranker described above can be used to determine which preceding cluster a discourseold mention should be linked to, but it cannot be used to determine whether a mention is discoursenew or not.

 $$$$$ Gazetteers (8): Eight dictionaries containing pronouns (77 entries), common words and words that are not names (399.6k), person names (83.6k), person titles and honorifics (761), vehicle words (226), location names (1.8k), company names (77.6k), and nouns extracted from WordNet that are hyponyms of PERSON (6.3k).
 $$$$$ This also represents a significant improvement over the mention ranker adopting the joint architecture, the best of the baselines, and suggests that cluster ranking is abetter precision-enhancing model than mention ranking.
 $$$$$ We propose a cluster-ranking approach to coreference resolution that combines the strengths of mention rankers and entitymention models.

For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). $$$$$ In Yang et al.’s (2004) entity-mention model, a training instance is composed of an active mention mk, a preceding cluster C, and a mention mj in C that is closest in distance to mk in the associated text.
For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). $$$$$ In this section, we describe three coreference models that will serve as our baselines: the mentionpair model, the entity-mention model, and the mention-ranking model.
For comparison purposes, the B3None variant used on A05RA is calculated slightly differently than other B3None results; see Rahman and Ng (2009). $$$$$ Given an active mention mk, we follow Denis and Baldridge (2008) and use an independently-trained classifier to determine whether mk is discourse-new.

The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ More recently, Yang et al. (2008) have proposed another entity-mention model trained by inductive logic programming.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ After training, the resulting cluster ranker processes the mentions in a test text in a left-to-right manner.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Morphological (8): wi’s prefixes and suffixes of length one, two, three, and four.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution.

The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). $$$$$ Next, we train our second baseline, the entity-mention coreference classifier, using the SVM learner.
The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). $$$$$ We thank the three anonymous reviewers for their invaluable comments on the paper.
The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). $$$$$ This work was supported in part by NSF Grant IIS-0812261.
The set of features that we use, listed in Table 5, is an extension of the set by Rahman and Ng (2009). $$$$$ We evaluate each coreference model using both true mentions (i.e., gold standard mentions4) and system mentions (i.e., automatically identified mentions).

Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). $$$$$ Crucially, our approach combines the strengths of entity-mention models and mention-ranking models.
Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). $$$$$ Pleonastic its have been detected using heuristics (e.g., Kennedy and Boguraev (1996)) and learning-based techniques such as rule learning (e.g., M¨uller (2006)), kernels (e.g., Versley et al. (2008)), and distributional methods (e.g., Bergsma et al.
Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). $$$$$ Our cluster-ranking model.
Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent re search on narrative event schema (Chambers and Jurafsky, 2009). $$$$$ Specifically, the class value of a training instance i(cj, mk) created for mk is the rank of cj among the competing clusters, which is 2 if mk belongs to cj, and 1 otherwise.

We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). $$$$$ In Yang et al.’s (2004) entity-mention model, a training instance is composed of an active mention mk, a preceding cluster C, and a mention mj in C that is closest in distance to mk in the associated text.
We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). $$$$$ Finally, mk will be linked to the closest preceding cluster that is classified as coreferent with mk.
We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). $$$$$ After training, the resulting cluster ranker processes the mentions in a test text in a left-to-right manner.
We created a baseline system based on the cluster-ranking model proposed by Rahman and Ng (2009). $$$$$ More recently, Yang et al. (2008) have proposed another entity-mention model trained by inductive logic programming.

Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. $$$$$ Crucially, our approach combines the strengths of entity-mention models and mention-ranking models.
Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. $$$$$ Recall that a cluster ranker ranks a set of preceding clusters for an active mention mk.
Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. $$$$$ Experimental results on the ACE 2005 corpus show that (1) jointly learning coreference resolution and discourse-new detection allows the cluster ranker to achieve better performance than adopting a pipeline coreference architecture; and (2) our cluster ranker significantly outperforms the mention ranker, the best of the three baseline coreference models, under both the pipeline architecture and the joint architecture.
Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. $$$$$ By contrast, our resolution strategy is learned without applying hand-coded constraints in a separate filtering step.

In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). $$$$$ After training, the resulting cluster ranker processes the mentions in a test text in a left-to-right manner.
In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). $$$$$ We thank the three anonymous reviewers for their invaluable comments on the paper.
In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). $$$$$ Our work advances the state-of-the-art in coreference resolution by bringing learningbased coreference systems to the next level of performance.
In each query we include a null-cluster instance, to allow joint learning of discourse-new detection, following (Rahman and Ng, 2009). $$$$$ As a result, errors in discourse-new detection could be propagated to the resolver, possibly leading to a deterioration of coreference performance (see Ng and Cardie (2002a)).

We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. $$$$$ In our running example shown in Figure 1, three training instances will be generated for He: i(Monday, He), i(secretary of state, He), and i(his, He).
We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. $$$$$ While machine learning approaches to coreference resolution have received a lot of attention since the mid90s, popular learning-based coreference frameworks such as the mention-pair model are arguably rather unsatisfactory from a linguistic point of view.
We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. $$$$$ Unlike mention-pair models, these entity-mention models are trained to determine whether an active mention belongs to a preceding, possibly partially-formed, coreference cluster.
We follow Rahman and Ng (2009) in jointly learning to detect anaphoric mentions along with resolving coreference relations. $$$$$ While entity-mention models have previously been shown to be worse or at best marginally better than their mention-pair counterparts (Luo et al., 2004; Yang et al., 2008), our cluster-ranking models, which are a natural extension of entity-mention models, significantly outperformed all competing approaches.

We follow the procedure described in (Rahman and Ng, 2009). $$$$$ We additionally show how our cluster-ranking framework naturally allows discourse-new entity detection to be learned jointly with coreference resolution.
We follow the procedure described in (Rahman and Ng, 2009). $$$$$ Experimental results on the ACE 2005 corpus show that (1) jointly learning coreference resolution and discourse-new detection allows the cluster ranker to achieve better performance than adopting a pipeline coreference architecture; and (2) our cluster ranker significantly outperforms the mention ranker, the best of the three baseline coreference models, under both the pipeline architecture and the joint architecture.
We follow the procedure described in (Rahman and Ng, 2009). $$$$$ We thank the three anonymous reviewers for their invaluable comments on the paper.
We follow the procedure described in (Rahman and Ng, 2009). $$$$$ If so, mk will not be resolved.

In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ Our work bridges this gap by realizing in a new machine learning framework ideas rooted in Lappin and Leass’s (1994) heuristic-based pronoun resolver, which in turn was motivated by classic salience-based approaches to anaphora resolution.
In ACE05-ALL, we have the full ACE 2005 training set and use the standard train/test splits reported in Rahman and Ng (2009) and Haghighi and Klein (2010). $$$$$ To identify discourse-new mentions, we employ two methods.

 $$$$$ To allow for the possibility that mk is discourse-new, we create an additional test instance that contains features that solely describe the active mention (similar to what we did in the training step above).
 $$$$$ To address the second weakness, researchers have investigated the acquisition of entity-mention coreference models (e.g., Luo et al. (2004), Yang et al.
 $$$$$ To allow for the possibility that mk is discourse-new, we create an additional test instance that contains features that solely describe the active mention (similar to what we did in the training step above).

Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). $$$$$ (2004), as described below.
Also, the B3 variant used by Rahman and Ng (2009) is slightly different from other systems (they remove all and only the singleton twinless system mentions, so it is neither B3All nor B3None). $$$$$ Mention ranking.

The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ They use the ANY predicate to generate clusterlevel features as follows: given a binary-valued feature X defined over a pair of mentions, they introduce an ANY-X cluster-level feature, which has the value TRUE if X is true between the active mention and any mention in the preceding cluster under consideration.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ Three training instances will be generated for He: i({Monday}, He), i({secretary of state}, He), and i({Barack Obama, his}, He).
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ Semantic (1): The named entity (NE) tag of wi obtained using the Stanford CRF-based NE recognizer (Finkel et al., 2005).
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ This contrasts with classification-based approaches, where many clustering algorithms have been employed to co-ordinate the pairwise coreference decisions (because it is unclear which one is the best).

To extract NPs from the ACE-annotated documents, we train a mention extractor on the training texts (see Section 5.1 of Rahman and Ng (2009) for details), which recalls 83.6% of the NPs in the test set. $$$$$ Two independent lines of recent research have attempted to improve these mention-pair one by learning a mentionto rank preceding mentions for a given anaphor, and the other training an to determine whether a preceding cluster is coreferent with a given mention.

Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. $$$$$ (1998), and Mitkov (2002)).
Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. $$$$$ Three training instances will be generated for He: i({Monday}, He), i({secretary of state}, He), and i({Barack Obama, his}, He).
Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. $$$$$ We thank the three anonymous reviewers for their invaluable comments on the paper.
Since space limitations preclude a description of these features, we refer the reader to Rahman and Ng (2009) for details. $$$$$ We thank the three anonymous reviewers for their invaluable comments on the paper.

Details of the CR model can be found in Rahman and Ng (2009). $$$$$ Luo et al. (2004) represent one of the earliest attempts to investigate learning-based entity-mention models.
Details of the CR model can be found in Rahman and Ng (2009). $$$$$ (2008)).
Details of the CR model can be found in Rahman and Ng (2009). $$$$$ Note that we only remove twinless (as opposed to all) system mentions that are singletons: this allows us to reward a resolver for successful identification of singleton mentions that have twins, thus overcoming a major weakness of and common criticism against the MUC scorer.

Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). $$$$$ All the test instances are then presented to the classifier.
Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). $$$$$ Following Florian et al. (2004), we recast mention extraction as a sequence labeling task, where we assign to each token in a test text a label that indicates whether it begins a mention, is inside a mention, or is outside a mention.
Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). $$$$$ Note that these partial clusters preceding mk are formed incrementally based on the predictions of the ranker for the first k−1 mentions; no gold-standard coreference information is used in their formation.
Also, the results show that the CR model is stronger than the MP model, corroborating previous empirical findings (Rahman and Ng, 2009). $$$$$ Contrary to common wisdom, this entity-mention model underperforms its mention-pair counterpart in spite of the generalization from mention-pair to cluster-level features.

The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster. $$$$$ Traditional learning-based coreference reoperate by training a mentionfor determining whether two mentions are coreferent or not.
The cluster ranking model of Rahman and Ng (2009) proceeds in a left-to-right fashion and adds the current discourse old mention to the highest scoring preceding cluster. $$$$$ Specifically, for each feature X shown in the last two blocks in Table 1, we first convert X into an equivalent set of binary-valued features if it is multi-valued.
