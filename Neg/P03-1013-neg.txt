Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ An error analysis revealed that many of the errors of the Collins model in Experiment 1 are chunking errors.
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ Most importantly, Negra follows the dependency grammar tradition in assuming flat syntactic representations: (a) There is no S —* NP VP rule.
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ However, this approach differs in the number of ways from the results reported here: (a) a hand-written grammar (instead of a treebank grammar) is used; (b) training is carried out on unannotated data; (c) the grammar and the training set cover only subordinate and relative clauses, not unrestricted text.
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ However, the learning curves for the three models failed to produce any evidence that they suffer from sparse data.

 $$$$$ However, this comes at the price of an unacceptable reduction in coverage.
 $$$$$ We pooled the estimates for pairs of conjoined and nonconjoined daughter categories (S and CS, NP and CNP, etc.
 $$$$$ For transitive verbs, for instance, we need the rules S —* V NP NP, S —* NP V NP, and S —* NP NP V to account for verb initial, verb second, and verb final order (assuming a flat S, see Section 2.2).
 $$$$$ As Experiment 1 showed, this cannot be taken for granted.

Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ Let l(C) be the lexical head of the constituent C. The rule probability is then defined as (see also Beil et al. 2002): probability that the (non-head) category C has the lexical head l(C) given that its mother is P with lexical head l(P).
Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ For perfect tags, sister-head dependencies lead to an improvement for NPs, PPs, and all categories.
Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ For perfect tags, sister-head dependencies lead to an improvement for NPs, PPs, and all categories.
Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ The flatness of the Negra annotation reflects the syntactic properties of German, in particular its semi-free wordorder.

However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ Learning curves show that the poor performance of the lexicalized models is not due to lack of training data.
However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ They also report the results of a task-based evaluation (extraction of sucategorization frames).
However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ A slow increase occurs as more training data is added.
However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ Section 3 describes two standard lexicalized models (Carroll and Rooth, 1998; Collins, 1997), as well as an unlexicalized baseline model.

It turns out, however, that lexicalization is not unproblematic $$$$$ Becker and Frank (2002) train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks).
It turns out, however, that lexicalization is not unproblematic $$$$$ For non-recursive NPs, Collins (1997) does not use the probability function in (5), but instead substitutes Pr (and, by analogy, Pl) by: Here the head H is substituted by the sister Ri−1 (and Li−1).
It turns out, however, that lexicalization is not unproblematic $$$$$ German exhibits a number of syntactic properties that distinguish it from English, the language that has been the focus of most research in parsing.
It turns out, however, that lexicalization is not unproblematic $$$$$ In contrast to Carroll and Rooth’s (1998) approach, the model proposed by Collins (1997) does not compute rule probabilities directly.

There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ This is a way of accounting for the semi-free wordorder of German (see Section 2.1): the first NP within an S need not be the subject.
There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ This is at odds with what has been reported for English.
There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ The aim of the present paper is to test if this finding carries over to German and to the Negra corpus.
There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ The Collins model gains only about 1%.

Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ In Experiment 2, we therefore investigated an alternative hypothesis: the poor performance of the lexicalized models is due to the fact that the rules in Negra are flatter than in the Penn Treebank, which makes lexical head-head dependencies less useful for correctly determining constituent boundaries.
Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ We added grammatical functions to both the baseline model and the C&R model, as we predicted that this would allow the model to better capture the wordorder facts of German.
Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ Another prediction was that adding Negra-specific information to the models will increase parsing performance.
Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ There is some research on treebank-based parsing of languages other than English.

Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. $$$$$ Collins et al. (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.
Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. $$$$$ While verb order is fixed in German, the order of complements and adjuncts is variable, and influenced by a variety of syntactic and non-syntactic factors, including pronominalization, information structure, definiteness, and animacy (e.g., Uszkoreit 1987).
Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. $$$$$ All models were run on POS-tagged input; this input was created by tagging the test set with a separate POS tagger, for both known and unknown words.

Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ A coordinated sentence has the category CS, a coordinate NP has the category CNP, etc.
Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ Lexicalization can increase parsing performance dramatically for English (Carroll and Rooth, 1998; Charniak, 1997, 2000; Collins, 1997), and the lexicalized model proposed by Collins (1997) has been successfully applied to Czech (Collins et al., 1999) and Chinese (Bikel and Chiang, 2000).
Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ This paper is structured as follows.
Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ The first position in a declarative sentence, for example, can be occupied by various constituents, including the subject (er ‘he’ in (1c)), the object (Musik ‘music’ in (2a)), an adjunct (gestern ‘yesterday’ in (2b)), or the non-finite verb (komponiert ‘composed’ in (2c)).

Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German $$$$$ Each context-free rule RHS —* LHS is annotated with an expansion probability P(RHS|LHS).
Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German $$$$$ We propose an alternative model that uses sister-head dependencies instead.
Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German $$$$$ In this experiment, we therefore investigate an alternative hypothesis, viz., that the lexicalized models do not cope well with the fact that Negra rules are so flat (see Section 2.2).

This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ For instance, subject and object NPs have different wordorder preferences (subjects tend to be preverbal, while objects tend to be postverbal), a fact that is captured if subjects have the label NP-SB, while objects are labeled NP-OA (accusative object), NP-DA (dative object), etc.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ Using cascaded Markov models, Brants (2000) reports an improved performance on the same task (LR 84.4%, LP 88.3%).
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ The head-lexicalized model of Carroll and Rooth (1998) has been applied to German by Beil et al. (1999, 2002).
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ The tagging accuracy was 97.12% on the development set.

The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance. $$$$$ This model outperforms the two original lexicalized models, as well as the unlexicalized baseline.
The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance. $$$$$ There are currently no probabilistic, treebanktrained parsers available for German (to our knowledge).
The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance. $$$$$ The flatness of the Negra annotation reflects the syntactic properties of German, in particular its semi-free wordorder.

 $$$$$ The results show that both lexicalized models fail to outperform the unlexicalized baseline.
 $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
 $$$$$ Section 5 presents an error analysis for Collins’s (1997) lexicalized model, which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra.

 $$$$$ Section 5 presents an error analysis for Collins’s (1997) lexicalized model, which shows that the head-head dependencies used in this model fail to cope well with the flat structures in Negra.
 $$$$$ Verb order is largely fixed: in subordinate clauses such as (1a), both the finite verb hat ‘has’ and the non-finite verb komponiert ‘composed’ are in sentence final position. because er yesterday music composed has ‘Because he has composed music yesterday.’ In yes/no questions such as (1b), the finite verb is sentence initial, while the non-finite verb is sentence final.
 $$$$$ The poor performance of the lexicalized models could be due to a lack of sufficient training data: our Negra training set contains approximately 18,000 sentences, and is therefore significantly smaller than the Penn Treebank training set (about 40,000 sentences).

 $$$$$ In contrast to Carroll and Rooth’s (1998) approach, the model proposed by Collins (1997) does not compute rule probabilities directly.
 $$$$$ This paper is structured as follows.
 $$$$$ This is a way of dealing with the fact that there is no separate NP node within PPs in Negra.
 $$$$$ A possible reason for this is sparse data: a grammar augmented with grammatical functions contains many additional categories, which means that many more parameters have to be estimated using the same training set.

The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ The poor performance of the lexicalized models could be due to a lack of sufficient training data: our Negra training set contains approximately 18,000 sentences, and is therefore significantly smaller than the Penn Treebank training set (about 40,000 sentences).
The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ Example are MO (modifier), HD (head), SB (subject), and OC (clausal object).
The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ A number of chunking models have been proposed, however.
The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.

To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). $$$$$ In this experiment, we therefore investigate an alternative hypothesis, viz., that the lexicalized models do not cope well with the fact that Negra rules are so flat (see Section 2.2).
To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). $$$$$ The aim of the present paper is to test if this finding carries over to German and to the Negra corpus.

Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Learning curves show that this effect is not due to lack of training data.
Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ We added grammatical functions to both the baseline model and the C&R model, as we predicted that this would allow the model to better capture the wordorder facts of German.
Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Both lexicalized models perform substantially worse.
Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Learning curves show that this effect is not due to lack of training data.

 $$$$$ The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.
 $$$$$ Learning curves show that this effect is not due to lack of training data.
 $$$$$ For instance, subject and object NPs have different wordorder preferences (subjects tend to be preverbal, while objects tend to be postverbal), a fact that is captured if subjects have the label NP-SB, while objects are labeled NP-OA (accusative object), NP-DA (dative object), etc.

able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ It can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures.
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ This is at odds with what has been reported for English.
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Both Lopar and the Collins model use various backoff distributions to smooth the estimates.
