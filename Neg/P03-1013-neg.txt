Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ Neither Collins et al. (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages.
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ This paper is structured as follows.
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ However, the resulting performance is significantly lower than the performance of the same model for English (see Table 1).
Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set; for NEGRA we followed Dubey and Keller (2003) in using the first 18,602 sentences for training, the last 1,000 for development, and the previous 1,000 for testing. $$$$$ However, the learning curve for Negra (see Figure 1) indicates that the performance of the Collins (1997) model is stable, even for small training sets.

 $$$$$ The aim of the present paper is to test if this finding carries over to German and to the Negra corpus.
 $$$$$ This model achieves up to 74% recall and precision, thus outperforming the unlexicalized baseline model.
 $$$$$ (L0 and R0 are stop categories.)
 $$$$$ To add lexical sensitivity, the Ph, Pr and Pl probability functions also take into account head words and their POS tags:

Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ At this point, the model is still unlexicalized.
Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ This can the thought of as a way of binarizing the flat rules in Negra.
Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ The best performance was obtained for a model that uses sister-head dependencies for all categories.
Additionally, we show a limited number of results on the Negra corpus, using the standard training/development/test splits, defined in (Dubey and Keller, 2003). $$$$$ Of the remaining 2,000 sentences, the first 1,000 served as the test set, and the last 1000 as the development set.

However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ More specifically, we used a standard probabilistic context-free grammar (PCFG; see Charniak 1993).
However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ We observe that existing lexicalized parsing models using head-head dependencies, while successful for English, fail to outperform an unlexicalized baseline model for German.
However, Dubey and Keller (2003) have demonstrated that lexicalization does not help a Collins-style parser that is trained on this corpus, and Levy and Manning (2004) have shown that its context-free representation is a poor approximation to the underlying dependency structure. $$$$$ This is a way of accounting for the semi-free wordorder of German (see Section 2.1): the first NP within an S need not be the subject.

It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). $$$$$ Let l(C) be the head word of C and t(C) the tag of the head word of C. Then the probability of a rule is defined as: Here, Ph is the probability of generating the head, and Pl and Pr are the probabilities of generating the nonterminals to the left and right of the head, respectively; d(i) is a distance measure.
It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). $$$$$ Let l(C) be the head word of C and t(C) the tag of the head word of C. Then the probability of a rule is defined as: Here, Ph is the probability of generating the head, and Pl and Pr are the probabilities of generating the nonterminals to the left and right of the head, respectively; d(i) is a distance measure.
It turns out, however, that lexicalization is not unproblematic: First, there is evidence that full lexicalization does not carry over across different tree-banks for other languages, annotations or domains (Dubey and Keller, 2003). $$$$$ In order to obtain an upper bound for the performance of the parsing models, we also ran the parsers on the test set with the correct tags (as specified in Negra), again for both known and unknown words.

There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ The head-lexicalized model of Carroll and Rooth (1998) has been applied to German by Beil et al. (1999, 2002).
There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.
There are even studies showing that lexicalization can be harmful when parsing richly inflected languages like German (Dubey and Keller, 2003) and Turkish (Eryigit and Oflazer, 2006). $$$$$ Learning curves show that this effect is not due to lack of training data.

Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ This model outperforms the two original lexicalized models, as well as the unlexicalized baseline.
Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ Becker and Frank (2002) train an unlexicalized PCFG on Negra to perform a different chunking task, viz., the identification of topological fields (sentence-based chunks).
Efforts have been made to adapt existing CFG models to German (Dubey and Keller, 2003), but the results still don't compare to state-of-the art parsing of English. $$$$$ This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.

Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. $$$$$ In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model (the baseline), Carroll and Rooth’s (1998) head-lexicalized model, and Collins’s (1997) model based on head-head dependencies.
Dubey and Keller (2003) analyze the difficulties that Germanim poses on parsing. $$$$$ The work by Collins et al. (1999) and Bikel and Chiang (2000) has demonstrated the applicability of the Collins (1997) model for Czech and Chinese.

Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ This is a way of taking the idiosyncrasies of the Negra annotation into account, and resulted in a small improvement in performance.
Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ The aim of the present paper is to test if this finding carries over to German and to the Negra corpus.
Earlier studies by Dubey and Keller (2003) and Dubey (2005) using the Negra treebank (Skut et al, 1997) reports that lexicalization of PCFGs decrease the parsing accuracy when parsing Negra's flat constituent structures. $$$$$ The performance of the Collins model is even less affected by training set size.

Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%. $$$$$ Verb order is largely fixed: in subordinate clauses such as (1a), both the finite verb hat ‘has’ and the non-finite verb komponiert ‘composed’ are in sentence final position. because er yesterday music composed has ‘Because he has composed music yesterday.’ In yes/no questions such as (1b), the finite verb is sentence initial, while the non-finite verb is sentence final.
Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%. $$$$$ This is at odds with what has been reported for English.
Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%. $$$$$ Skut and Brants (1998) used Negra to train a maximum entropy-based chunker, and report LR and LP of 84.4% for NP and PP chunking.
Relevant, in principle, to our discussion here, are also the results obtained with tree bank grammars for German: (Dubey and Keller, 2003) have trained a PCFG on the Negra corpus (Skut et al, 1998), reporting labelled precision and recall between 70 and 75%. $$$$$ The C&R model performs worse than the baseline, at 68.04% LR and 60.07% LP (for TnT tags).

This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ This finding goes some way towards explaining why the parsing performance reported for the Penn Treebank is substantially higher than the results for Negra: the Penn Treebank contains split PPs, which means that there are lot of brackets that are easy to get right.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ We first added sister-head dependencies for NPs (following Collins’s (1997) original proposal) and then for PPs, which are flat in Negra, and thus similar in structure to NPs (see Section 2.2).
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ The aim of the present paper is to test if this finding carries over to German and to the Negra corpus.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.

The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance. $$$$$ At this point, the model is still unlexicalized.
The learning curves over increasing training data (e.g., for German (Dubey and Keller, 2003)) show that tree bank size can not be the sole factor to account for the inferior performance. $$$$$ For transitive verbs, for instance, we need the rules S —* V NP NP, S —* NP V NP, and S —* NP NP V to account for verb initial, verb second, and verb final order (assuming a flat S, see Section 2.2).

 $$$$$ Most importantly, Negra follows the dependency grammar tradition in assuming flat syntactic representations: (a) There is no S —* NP VP rule.
 $$$$$ The work by Collins et al. (1999) and Bikel and Chiang (2000) has demonstrated the applicability of the Collins (1997) model for Czech and Chinese.
 $$$$$ They report an LR and LP of 93%.
 $$$$$ Both lexicalized models perform substantially worse.

 $$$$$ We also tested variants of the baseline model and the C&R model that include grammatical function information, as we hypothesized that this information might help the model to handle wordorder variation more adequately, as explained above.
 $$$$$ This hypothesis cannot be tested, as the authors do not present learning curves for their models.
 $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
 $$$$$ Skut and Brants (1998) used Negra to train a maximum entropy-based chunker, and report LR and LP of 84.4% for NP and PP chunking.

 $$$$$ Collins et al. (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.
 $$$$$ We present a probabilistic parsing model for German trained on the Negra treebank.
 $$$$$ This model outperforms the baseline, achieving a labeled precision and recall of up to 74%.
 $$$$$ They also report the results of a task-based evaluation (extraction of sucategorization frames).

The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ We also found that using lexical sister-head dependencies for all categories leads to a larger improvement than using them only for NPs or PPs (see Table 5).
The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ Section 3 describes two standard lexicalized models (Carroll and Rooth, 1998; Collins, 1997), as well as an unlexicalized baseline model.
The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ As a possible explanation we considered lack of training data: Negra is about half the size of the Penn Treebank.
The comparison of the experiments with (line 2) and without grammatical functions (line 1) confirms the findings of Dubeyand Keller (2003) that the task of assigning correct grammatical functions is harder than mere constituent-based parsing. $$$$$ A grammar rule LHS  RHS can be written as P  Lm ...L1 H R1 ...Rn where P is the mother and H is the head daughter.

To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). $$$$$ Parameter pooling increases both LR and LP by about 1%.
To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). $$$$$ The performance reported by these authors is substantially lower than the one reported for English, which might be due to the fact that less training data is available for Czech and Chinese (see Table 1).
To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). $$$$$ There are currently no probabilistic, treebanktrained parsers available for German (to our knowledge).
To allow comparisons with earlier work on NEGRA parsing, we use the same split of training, development and testing data as used in Dubey and Keller (2003). $$$$$ Testing our sister-head model on these languages is a topic for future research.

Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ The annotation scheme (Skut et al., 1997) is modeled to a certain extent on that of the Penn Treebank (Marcus et al., 1993), with crucial differences.
Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Collins et al. (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.
Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ To add lexical sensitivity, the Ph, Pr and Pl probability functions also take into account head words and their POS tags:
Overall, the best-performing model, using Brants smoothing, achieves a labelled bracketing F-score of 76.2, higher than earlier results reported by Dubey and Keller (2003) and Schiehlen (2004). $$$$$ However, the learning curve for Negra (see Figure 1) indicates that the performance of the Collins (1997) model is stable, even for small training sets.

 $$$$$ This indicates that sister-head dependencies are more appropriate for treebanks with very flat structures such as Negra.
 $$$$$ But instead the parser produces two constituents as in (6b)): The reason for this problem is that neben is the head of the constituent in (6), and the Collins model uses a crude distance measure together with head-head dependencies to decide if additional constituents should be added to the PP.
 $$$$$ Adding grammatical function again reduces performance slightly.
 $$$$$ This is a way of dealing with the fact that there is no separate NP node within PPs in Negra.

able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ We propose an alternative model that uses sister-head dependencies instead of head-head dependencies.
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Testing our sister-head model on these languages is a topic for future research.
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ Such annotation schemes are often used for languages that (unlike English) have a free or semi-free wordorder.
able 4 lists the result of the best model presented here against the earlier work on NEGRA parsing described in Dubey and Keller (2003) and Schiehlen (2004). $$$$$ This model uses lexical sisterhead dependencies, which makes it particularly suitable for parsing Negra’s flat structures.
