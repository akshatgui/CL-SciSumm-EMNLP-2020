Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. $$$$$ Based on these confidence metrics, we designed the inference rules in Table 4.
Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. $$$$$ For each related document r returned by INDRI, we repeat the within-sentence event extraction and cross-sentence inference procedure, and get an expanded event mention set EMSett +r .
Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. $$$$$ We construct an INDRI query from the triggers and arguments, each weighted by local confidence and frequency in the test document.

We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ The argument classifier is applied to the remaining mentions in the sentence; for any argument passing that classifier, the role classifier is used to assign a role to it.
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ We share the view of using global inference to improve event extraction with some recent research.
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ ACE defines the following terminology: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) event trigger: the main word which most clearly expresses an event occurrence event arguments: the mentions that are involved in an event (participants) event mention: a phrase or sentence within which an event is described, including trigger and arguments The 2005 ACE evaluation had 8 types of events, with 33 subtypes; for the purpose of this paper, we will treat these simply as 33 distinct event types.
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD), so we have used the idea of sense consistency introduced in (Yarowsky, 1995), extending it to operate across related documents.

Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ The gain obtained by applying successive rules can be seen in the progression of successive points towards higher recall and, for argument labeling, precision4.
Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ EMSet represents a set of event mentions which is gradually updated.
Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task.

(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ Identifying events of a particular type within individual documents – ‘classical’ information extraction – remains a difficult task.
(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ We employ a similar approach to propagate consistent event arguments across sentences and documents.
(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ Its training and test procedures are as follows.
(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ EMSet represents a set of event mentions which is gradually updated.

We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ In this paper we demonstrate that appreciable improvements are possible over the variety of event types in the ACE (Automatic Content Extraction) evaluation through the use of cross-sentence and cross-document evidence.
We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ The statistics of some trigger examples are presented in table 2.
We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ For example, The above test sentence doesn’t include an explicit trigger word to indicate “Vivendi” as a “seller” of a “Transaction_Transfer-Ownership” event mention, but “Vivendi” is correctly identified as “seller” in many other related sentences (by matching patterns “[Seller] sell” and “buy [Seller]’s”).
We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ We can take advantage of these alternate descriptions to improve event extraction in the original document, by favoring consistency of interpretation across sentences and documents.

 $$$$$ ...
 $$$$$ XSent-Trigger-Freq(trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all sentences within a document XDoc-Trigger-Freq (trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all documents in a cluster XDoc-Trigger-BestFreq (trigger) Maximum over all etypes of XDoc-Trigger-Freq (trigger, etype) XDoc-Arg-Freq(arg, etype) The weighted frequency of arg appearing as an argument of an event of type etype across all documents in a cluster XDoc-Role-Freq(arg, etype, role) The weighted frequency of arg appearing as an argument of an event of type etype with role role across all documents in a cluster XDoc-Role-BestFreq(arg) Maximum over all etypes and roles of XDoc-Role-Freq(arg, etype, role) XSent-Trigger-Margin(trigger) The margin value of trigger in XSent-Trigger-Freq XDoc-Trigger-Margin(trigger) The margin value of trigger in XDoc-Trigger-Freq XDoc-Role-Margin(arg) The margin value of arg in XDoc-Role-Freq

There are two common assumptions within a cluster of related documents (Ji and Grishman 2008): Trigger Consistency Per Cluster: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type. $$$$$ We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related documents.
There are two common assumptions within a cluster of related documents (Ji and Grishman 2008): Trigger Consistency Per Cluster: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type. $$$$$ So if we can determine the sense (event type) of a word in the related documents, this will allow us to infer its sense in the test document.
There are two common assumptions within a cluster of related documents (Ji and Grishman 2008): Trigger Consistency Per Cluster: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type. $$$$$ For each test text we retrieved 25 related texts from English TDT5 corpus which in total consists of 278,108 texts (from April to September of 2003).

For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task.
For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ This section gives more details about the baseline withinsentence event tagger, and the information retrieval system we use to obtain related documents.
For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ “the Davao Medical Center” is mistakenly tagged as “Place” for a “Life_Die” event mention.

The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ We can take advantage of these alternate descriptions to improve event extraction in the original document, by favoring consistency of interpretation across sentences and documents.
The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ Although the rules may seem complex, they basically serve two functions:
The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ We then use the INDRI retrieval system (Strohman et al., 2005) to obtain the top N (N=25 in this paper3) related documents.
The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ In the following example, The Davao Medical Center, a regional government hospital, recorded 19 deaths with 50 wounded.

Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ ...
Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ We apply within-sentence event extraction to get an initial set of event mentions EMSett , and conduct cross-sentence inference (details will be presented in section 5) to get an updated set of event mentions EMSett .
Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ We employ a similar approach to propagate consistent event arguments across sentences and documents.
Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ Identifying events of a particular type within individual documents – ‘classical’ information extraction – remains a difficult task.

Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006).
Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ One of the initial goals for IE was to create a database of relations and events from the entire input corpus, and allow further logical reasoning on the database.
Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ Besides some obvious overlooked cases (it’s probably difficult for a human to remember 33 different event types during annotation), most difficulties were caused by judging generic verbs, nouns and adjectives.
Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ As we can see from the table, the likelihood of a candidate word being an event trigger in the test document is closer to its distribution in the collection of related documents than the uniform training corpora.

Although this figure is very low, it is not surprising: the results on the English ACE 2005 corpus show that the inter-annotator agreement on trigger identification is only about 40% (Ji and Grishman, 2008). $$$$$ We employ a similar approach to propagate consistent event arguments across sentences and documents.
Although this figure is very low, it is not surprising: the results on the English ACE 2005 corpus show that the inter-annotator agreement on trigger identification is only about 40% (Ji and Grishman, 2008). $$$$$ For each related document r returned by INDRI, we repeat the within-sentence event extraction and cross-sentence inference procedure, and get an expanded event mention set EMSett +r .
Although this figure is very low, it is not surprising: the results on the English ACE 2005 corpus show that the inter-annotator agreement on trigger identification is only about 40% (Ji and Grishman, 2008). $$$$$ Figure 1 depicts the general procedure of our approach.
Although this figure is very low, it is not surprising: the results on the English ACE 2005 corpus show that the inter-annotator agreement on trigger identification is only about 40% (Ji and Grishman, 2008). $$$$$ Figure 1 depicts the general procedure of our approach.

The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ We then use the INDRI retrieval system (Strohman et al., 2005) to obtain the top N (N=25 in this paper3) related documents.
The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ Identifying events of a particular type within individual documents – ‘classical’ information extraction – remains a difficult task.
The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ For example, we can naturally get an event-driven summary for the collection of related documents; the sentences including high-confidence events can be used as additional training data to bootstrap the event tagger; from related events in different timeframes we can derive entailment rules; the refined consistent events can serve better for other NLP tasks such as template based question-answering.

We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ We used 10 newswire texts from ACE 2005 training corpora (from March to May of 2003) as our development set, and then conduct blind test on a separate set of 40 ACE 2005 newswire texts.
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ In addition, Patwardhan and Riloff (2007) also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction.
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ On the other hand, if a word is not tagged as an event trigger in most related documents, then it’s less likely to be correct in the test sentence despite its high local confidence.
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ We investigate this hypothesis by automatically obtaining 25 related documents for each test text.

Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ The central idea of inference is to obtain document-wide and cluster-wide statistics about the frequency with which triggers and arguments are associated with particular types of events, and then use this information to correct event and argument identification and classification.
Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ In the short term, the approach provides a platform for many byproducts.
Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ The system combines pattern matching with statistical models.
Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.

Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ In addition, a set of Maximum Entropy based classifiers are trained: In the test procedure, each document is scanned for instances of triggers from the training corpus.
Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ This section gives more details about the baseline withinsentence event tagger, and the information retrieval system we use to obtain related documents.
Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ Error analysis on the inference procedure shows that the propagation rules (3), (4), (7) and (9) produced a few extra false alarms.
Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ In the next section we shall focus on describing the inference procedure.

Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ In this paper we propose a new approach to break down the document boundaries for event extraction.
Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents.
Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ The labeled point on each curve shows the best F-measure that can be obtained on the development set by adjusting the threshold for that rule.
Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ ACE defines the following terminology: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) event trigger: the main word which most clearly expresses an event occurrence event arguments: the mentions that are involved in an event (participants) event mention: a phrase or sentence within which an event is described, including trigger and arguments The 2005 ACE evaluation had 8 types of events, with 33 subtypes; for the purpose of this paper, we will treat these simply as 33 distinct event types.

A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ This pattern-matching process, if successful, will assign some of the mentions in the sentence as arguments of a potential event mention.
A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ Figure 1 depicts the general procedure of our approach.

We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ Yangarber et al. (Yangarber and Jokipii, 2005; Yangarber, 2006; Yangarber et al., 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time.
We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ We employ a similar approach to propagate consistent event arguments across sentences and documents.
We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ We then use the INDRI retrieval system (Strohman et al., 2005) to obtain the top N (N=25 in this paper3) related documents.

Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ In this section we shall present our motivations based on error analysis for the baseline event tagger.
Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ The artificial constraint that extraction should be done independently for each document was introduced in part to simplify the task and its evaluation.
Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ We gather together event extraction results from a set of related documents, and then apply inference and constraints to enhance IE performance.
