Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. $$$$$ In addition, a set of Maximum Entropy based classifiers are trained: In the test procedure, each document is scanned for instances of triggers from the training corpus.
Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. $$$$$ This section gives more details about the baseline withinsentence event tagger, and the information retrieval system we use to obtain related documents.
Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. $$$$$ We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents.

We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ EMSet represents a set of event mentions which is gradually updated.
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ We construct an INDRI query from the triggers and arguments, each weighted by local confidence and frequency in the test document.
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ We summarize the frequency and confidence metrics in Table 3.
We then apply the cross-document inference techniques as described in (Ji and Grishman, 2008) to improve trigger and argument labeling performance by favoring interpretation consistency across the test events and background events. $$$$$ We investigate this hypothesis by automatically obtaining 25 related documents for each test text.

Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ This pattern-matching process, if successful, will assign some of the mentions in the sentence as arguments of a potential event mention.
Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ We construct an INDRI query from the triggers and arguments, each weighted by local confidence and frequency in the test document.
Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. $$$$$ The system combines pattern matching with statistical models.

(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ So if we can determine the sense (event type) of a word in the related documents, this will allow us to infer its sense in the test document.
(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ The artificial constraint that extraction should be done independently for each document was introduced in part to simplify the task and its evaluation.
(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ In addition, we also measured the performance of two human annotators who prepared the ACE 2005 training data on 28 newswire texts (a subset of the blind test set).
(Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. $$$$$ For each related document r returned by INDRI, we repeat the within-sentence event extraction and cross-sentence inference procedure, and get an expanded event mention set EMSett +r .

We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event ex- Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.
We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ Inspired by the observation about trigger distribution, we propose a similar hypothesis – one argument role per cluster for event arguments.
We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ ACE defines the following terminology: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) event trigger: the main word which most clearly expresses an event occurrence event arguments: the mentions that are involved in an event (participants) event mention: a phrase or sentence within which an event is described, including trigger and arguments The 2005 ACE evaluation had 8 types of events, with 33 subtypes; for the purpose of this paper, we will treat these simply as 33 distinct event types.
We followed Ji and Grishman (2008)'s approach and used the INDRI retrieval system (Strohman et al, 2005) to obtain the top N related documents for each annotated document in the training corpus. $$$$$ We used 10 newswire texts from ACE 2005 training corpora (from March to May of 2003) as our development set, and then conduct blind test on a separate set of 40 ACE 2005 newswire texts.

 $$$$$ The statistics of some trigger examples are presented in table 2.
 $$$$$ Based on the above motivations we propose to incorporate global evidence from a cluster of related documents to refine local decisions.
 $$$$$ ACE defines the following terminology: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) event trigger: the main word which most clearly expresses an event occurrence event arguments: the mentions that are involved in an event (participants) event mention: a phrase or sentence within which an event is described, including trigger and arguments The 2005 ACE evaluation had 8 types of events, with 33 subtypes; for the purpose of this paper, we will treat these simply as 33 distinct event types.

There are two common assumptions within a cluster of related documents (Ji and Grishman 2008) $$$$$ For example, in a document about “the advance into Baghdad”: Most US army commanders believe it is critical to pause the breakneck advance towards Baghdad to secure the supply lines and make sure weapons are operable and troops resupplied.... British and US forces report gains in the advance on Baghdad and take control of Umm Qasr, despite a fierce sandstorm which slows another flank.
There are two common assumptions within a cluster of related documents (Ji and Grishman 2008) $$$$$ For each related document r returned by INDRI, we repeat the within-sentence event extraction and cross-sentence inference procedure, and get an expanded event mention set EMSett +r .
There are two common assumptions within a cluster of related documents (Ji and Grishman 2008) $$$$$ Some other errors happen on nouns and adjectives.
There are two common assumptions within a cluster of related documents (Ji and Grishman 2008) $$$$$ Although the rules may seem complex, they basically serve two functions:

For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ We then use the INDRI retrieval system (Strohman et al., 2005) to obtain the top N (N=25 in this paper3) related documents.
For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ When an instance is found, the system tries to match the environment of the trigger against the set of patterns associated with that trigger.
For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ This system extracts events independently for each sentence.
For ST_IR and ST_GI, we retrieved the best N (using N=25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). $$$$$ We can take advantage of these alternate descriptions to improve event extraction in the original document, by favoring consistency of interpretation across sentences and documents.

The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ EMSet represents a set of event mentions which is gradually updated.
The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ As we can see from the table, the likelihood of a candidate word being an event trigger in the test document is closer to its distribution in the collection of related documents than the uniform training corpora.
The most salient work for event extraction is Grishman et al (2005) and Jiand and Grishman (2008). $$$$$ For every event mention in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments.

Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ Based on these confidence metrics, we designed the inference rules in Table 4.
Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ Several recent studies involving specific event types have stressed the benefits of going beyond traditional singledocument extraction; in particular, Yangarber (2006) has emphasized this potential in his work on medical information extraction.
Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ For a set of event mentions we tabulate the following document-wide and cluster-wide confidence-weighted frequencies: coreferential with or related to the argument, the frequency of the event type and role.
Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents. Our work shares some similarities. $$$$$ For argument labeling we can see that crosssentence inference improved both identification (3.7% higher F-Measure) and classification (6.1% higher accuracy); and cross-document inference mainly provided further gains (1.9%) in classification.

Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related documents.
Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ We investigate this hypothesis by automatically obtaining 25 related documents for each test text.
Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic related documents. $$$$$ For example it’s hard to decide whether “named” represents a “Personnel_Nominate” or “Personnel_Start-Position” event mention; “hacked to death” represents a “Life_Die” or “Conflict_Attack” event mention without following more specific annotation guidelines.

Although this figure is very low, it is not surprising $$$$$ We employ a similar approach to propagate consistent event arguments across sentences and documents.
Although this figure is very low, it is not surprising $$$$$ We share the view of using global inference to improve event extraction with some recent research.
Although this figure is very low, it is not surprising $$$$$ The argument classifier is applied to the remaining mentions in the sentence; for any argument passing that classifier, the role classifier is used to assign a role to it.
Although this figure is very low, it is not surprising $$$$$ In the next section we shall focus on describing the inference procedure.

The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ In the short term, the approach provides a platform for many byproducts.
The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ The event extraction task we are addressing is that of the Automatic Content Extraction (ACE) evaluations2.
The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ Based on the fact that many other instances of “hurt” are not “Life_Injure” triggers in the related documents, we can successfully remove this wrong event mention in the test document.
The second reason is that it is hard to identify an event mention due to the failure of following specified annotation guidelines, as mentioned in Ji and Grishman (2008). $$$$$ We construct an INDRI query from the triggers and arguments, each weighted by local confidence and frequency in the test document.

We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ ACE defines the following terminology: entity: an object or a set of objects in one of the semantic categories of interest mention: a reference to an entity (typically, a noun phrase) event trigger: the main word which most clearly expresses an event occurrence event arguments: the mentions that are involved in an event (participants) event mention: a phrase or sentence within which an event is described, including trigger and arguments The 2005 ACE evaluation had 8 types of events, with 33 subtypes; for the purpose of this paper, we will treat these simply as 33 distinct event types.
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ The central idea of inference is to obtain document-wide and cluster-wide statistics about the frequency with which triggers and arguments are associated with particular types of events, and then use this information to correct event and argument identification and classification.
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ Then we apply
We use a state-of-the-art IE system (Ji and Grishman, 2008) developed for the Automatic Content Extraction (ACE) program to process texts and automatic speech recognition output. $$$$$ XSent-Trigger-Freq(trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all sentences within a document XDoc-Trigger-Freq (trigger, etype) The weighted frequency of string trigger appearing as the trigger of an event of type etype across all documents in a cluster XDoc-Trigger-BestFreq (trigger) Maximum over all etypes of XDoc-Trigger-Freq (trigger, etype) XDoc-Arg-Freq(arg, etype) The weighted frequency of arg appearing as an argument of an event of type etype across all documents in a cluster XDoc-Role-Freq(arg, etype, role) The weighted frequency of arg appearing as an argument of an event of type etype with role role across all documents in a cluster XDoc-Role-BestFreq(arg) Maximum over all etypes and roles of XDoc-Role-Freq(arg, etype, role) XSent-Trigger-Margin(trigger) The margin value of trigger in XSent-Trigger-Freq XDoc-Trigger-Margin(trigger) The margin value of trigger in XDoc-Trigger-Freq XDoc-Role-Margin(arg) The margin value of arg in XDoc-Role-Freq

Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ Such methods allow us to build dynamic background knowledge as required to interpret a document and can compensate for the limited annotated training data which can be provided for each event type.
Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ Based on these confidence metrics, we designed the inference rules in Table 4.
Our work can also be considered as an extension of global back ground inference (e.g. Ji and Grishman, 2008) to cross-media paradigm. $$$$$ We can take advantage of these alternate descriptions to improve event extraction in the original document, by favoring consistency of interpretation across sentences and documents.

Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ In this section we shall present our motivations based on error analysis for the baseline event tagger.
Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ For each argument we also add other names coreferential with or bearing some ACE relation to the argument.
Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ Such methods allow us to build dynamic background knowledge as required to interpret a document and can compensate for the limited annotated training data which can be provided for each event type.
Ji and Grishman (2008) enforce event role consistency across different documents. $$$$$ Then we apply

Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ As shown in table 5 the inter-annotator agreement on trigger identification is only about 40%.
Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ Almost all the current event extraction systems focus on processing single documents and, except for coreference resolution, operate a sentence at a time (Grishman et al., 2005; Ahn, 2006; Hardy et al., 2006).
Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ The system combines pattern matching with statistical models.
Finkel et al (2005) and Ji and Grishman (2008) incorporate global information by enforcing event role or label consistency over a document or across related documents. $$$$$ In the next section we shall focus on describing the inference procedure.

A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ Its training and test procedures are as follows.
A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ Across a heterogeneous document corpus, a particular verb can sometimes be trigger and sometimes not, and can represent different event types.
A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ When an instance is found, the system tries to match the environment of the trigger against the set of patterns associated with that trigger.
A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al, 2011), and thus fails to shed light on the users focus/interests. $$$$$ We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents.

We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ In this paper we demonstrate that appreciable improvements are possible over the variety of event types in the ACE (Automatic Content Extraction) evaluation through the use of cross-sentence and cross-document evidence.
We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ We construct an INDRI query from the triggers and arguments, each weighted by local confidence and frequency in the test document.
We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ We expand the idea to more general event types and use information retrieval techniques to obtain wider background knowledge from related documents.
We have also attempted using the results from Dependency Parsing, Relation Extraction and Event Extraction tools (Ji and Grishman, 2008) to enrich the link types. $$$$$ Identifying events of a particular type within individual documents – ‘classical’ information extraction – remains a difficult task.

Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ Mann (2007) encoded specific inference rules to improve extraction of CEO (name, start year, end year) in the MUC management succession task.
Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ The argument classifier is applied to the remaining mentions in the sentence; for any argument passing that classifier, the role classifier is used to assign a role to it.
Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ In the next section we shall focus on describing the inference procedure.
Ji and Grishman (2008) extracts event mentions (belonging to a predefined list of target event types) and their associated arguments. $$$$$ Several recent studies involving specific event types have stressed the benefits of going beyond traditional singledocument extraction; in particular, Yangarber (2006) has emphasized this potential in his work on medical information extraction.
