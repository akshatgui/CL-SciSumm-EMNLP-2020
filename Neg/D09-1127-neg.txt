Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. $$$$$ In fact, the conjunction of these two features, is another feature with even stronger discrimination power.
Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.
Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. $$$$$ The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese.

The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.
The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art with negligible efficiency overhead, thus much faster than biparsing.
The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art with negligible efficiency overhead, thus much faster than biparsing.

 $$$$$ By contrast, Burkett and Klein (2008) reported their approach of “monolingual k-best parsing followed by bilingual k2-best reranking” to be “3.8 times slower” than monolingual parsing.
 $$$$$ We now develop this idea into bilingual contiguity features.
 $$$$$ Informally, if the correct decision is a reduction, then it is likely that the corresponding words of st−1 and st on the target-side should also form a contiguous span.
 $$$$$ However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.

 $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.
 $$$$$ This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack.
 $$$$$ We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004).
 $$$$$ Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.).

 $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.
 $$$$$ 2006AA010108.
 $$$$$ We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.
 $$$$$ This target span is contiguous, because no word within this span is aligned to a source word outside of the source span.

 $$$$$ On the other hand, English can help Chinese parsing as well, for example in deciding the scope of relative clauses which is unambiguous in English but ambiguous in Chinese.
 $$$$$ We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules.
 $$$$$ 3).
 $$$$$ So we will engineer more such features, especially with lexicalization and soft alignments (Liang et al., 2006), and study the impact of alignment quality on parsing improvement.

Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.
Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.).
Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set.
Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ However, in Figure 3(b), the source span is still [saw .. Bill], but this time maps onto a much longer span on the Chinese side.

We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ The complexity of this algorithm is O(nk), which subsumes the determinstic mode as a special case (k = 1).
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees.
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity.

 $$$$$ Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is O(n6) as opposed to the monolingual O(n3) time.
 $$$$$ We now develop this idea into bilingual contiguity features.
 $$$$$ By contrast, Burkett and Klein (2008) reported their approach of “monolingual k-best parsing followed by bilingual k2-best reranking” to be “3.8 times slower” than monolingual parsing.
 $$$$$ We suspect (and will finish in the future work) that using manual alignments would result in a better correlation, though for the main parsing results (see below) we can only afford automatic alignments in order for our approach to be widely applicable to any bitext.

Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ Specifically, we showed a simple method of incorporating alignment features as soft evidence on top of a state-of-the-art shift-reduce dependency parser, which helped better resolve shift-reduce conflicts with fractional efficiency overhead.
Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ Among the 197 first mistakes (other 76 sentences have perfect output), the vast majority, 190 of them (96.4%), are shift-reduce errors (equally distributed between shift-becomesreduce and reduce-becomes-shift), and only 7 (3.6%) are due to reduce-reduce conflicts.8 These statistics confirm our intuition that shift-reduce decisions are much harder to make during parsing, and contribute to the overwhelming majority of errors, which is studied in the next hypothesis.
Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ In fact, the conjunction of these two features, is another feature with even stronger discrimination power.
Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ In this work we choose shift-reduce dependency parsing for its simplicity and efficiency.

Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ The second and third authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No.
Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.
Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with goldstandard parse trees (Bies et al., 2007).

 $$$$$ From a linguistics point of view, we would like to see how linguistics distance affects this approach, e.g., we suspect EnglishFrench would not help each other as much as English-Chinese do; and it would be very interesting to see what types of syntactic ambiguities can be resolved across different language pairs.
 $$$$$ The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese.
 $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.
 $$$$$ Our wordalignments are generated from the HMM aligner of Liang et al. (2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.).

Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.
Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.
Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ Now we face a shift-reduce conflict: we can either combine “saw” and “Bill” in a reduceR action (5a), or shift “Bill” (5b).
Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ Our intuition is, whenever we face a decision whether to combine the stack tops st−1 and st or to shift the current word wi, we will consult the other language, where the word-alignment information would hopefully provide a preference, as in the running example of PP-attachment (see Figure 1).

 $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.
 $$$$$ We thank the anonymous reviewers for pointing to us references about “arc-standard”.
 $$$$$ Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art with negligible efficiency overhead, thus much faster than biparsing.
 $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.

For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008).
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ However, the search space of joint parsing is inevitably much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack.
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ In this work we choose shift-reduce dependency parsing for its simplicity and efficiency.

The semantics of the system is described in (Huang et al, 2009). $$$$$ The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese.
The semantics of the system is described in (Huang et al, 2009). $$$$$ For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees.
The semantics of the system is described in (Huang et al, 2009). $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.
The semantics of the system is described in (Huang et al, 2009). $$$$$ Computing the bilingual features further slows it down, but only fractionally so (just 1.06 times as slow as the baseline at k=16), which is appealing in practice.

Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.
Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.
Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ For example, in Figure 2, “I” is a left dependent of “saw”, and can in principle wait until “Bill” and “with” are reduced, and then finally combine with “saw”.

In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.
In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ This aligner outputs “soft alignments”, i.e., posterior probabilities for each source-target word pair.
In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.
In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ Shiftreduce conflicts overwhelmingly dominate.

 $$$$$ We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.
 $$$$$ The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese.
 $$$$$ Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results.
 $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.

Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning.
Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.
Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ Furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (Sagae and Lavie, 2006).
Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.
