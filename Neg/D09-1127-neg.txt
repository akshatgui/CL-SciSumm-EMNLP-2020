Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. $$$$$ Ambiguity resolution is a central task in Natural Language Processing.
Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).
Finally, Huang et al (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, target language parser. $$$$$ In this work we choose shift-reduce dependency parsing for its simplicity and efficiency.

The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ So in total we got three bilingual feature (templates), which in practice amounts to 24 instances (after cross-product with {−, +} and the three actions).
The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ Our intuition is, whenever we face a decision whether to combine the stack tops st−1 and st or to shift the current word wi, we will consult the other language, where the word-alignment information would hopefully provide a preference, as in the running example of PP-attachment (see Figure 1).
The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.
The first consists of a joint segmentation and POS-tagging model (Zhang and Clark, 2010) and a word-based dependency parsing model using the arc-standard algorithm (Huang et al, 2009). $$$$$ Specifically, we make the following contributions:

 $$$$$ In this work we choose shift-reduce dependency parsing for its simplicity and efficiency.
 $$$$$ We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004).
 $$$$$ 2006AA010108.

 $$$$$ We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004).
 $$$$$ In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k2 tree pairs, which covers a tiny fraction of the whole space (Huang, 2008).
 $$$$$ Specifically, we make the following contributions:

 $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).
 $$$$$ On the other hand, English can help Chinese parsing as well, for example in deciding the scope of relative clauses which is unambiguous in English but ambiguous in Chinese.
 $$$$$ Furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (Sagae and Lavie, 2006).
 $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).

 $$$$$ 2006AA010108.
 $$$$$ We will use features extracted from the configuration to resolve the conflict.
 $$$$$ We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously.

Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that 5Our definition implies that we only consider faithful spans to be contiguous (Galley et al., 2004).
Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ Following this idea, Ganchev et al. (2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results.
Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules.
Please refer to page 573 of (Huang et al, 2009b) for more details about how to convert tree-to-string rules to SCFG rules. $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).

We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.
We follow Huang et al (2009b) to keep the probabilities of a natural rule unchanged and set those of a virtual rule to 1. $$$$$ We thank the anonymous reviewers for pointing to us references about “arc-standard”.

 $$$$$ We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.
 $$$$$ We incorporate the three bilingual features (again, with automatic alignments) into the baseline parser, retrain it, and test its performance on the English dev set, with varying beam size.
 $$$$$ The second and third authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No.
 $$$$$ However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.

Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ In terms of speed, both parsers run proportionally slower with larger beams, as the time complexity is linear to the beam-size.
Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ For example, Hwa et al. (2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees.
Huang et al (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. $$$$$ Furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (Sagae and Lavie, 2006).

Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ We thank the anonymous reviewers for pointing to us references about “arc-standard”.
Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ Specifically, we showed a simple method of incorporating alignment features as soft evidence on top of a state-of-the-art shift-reduce dependency parser, which helped better resolve shift-reduce conflicts with fractional efficiency overhead.
Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.
Huang et al (2009) proposed features based on reordering between languages for a shift-reduce parser. $$$$$ Here we propose a much simpler monowhere a source-language parser learns to exploit reorderings as adobservation, but to build the target-side tree as well.

 $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.
 $$$$$ Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity.
 $$$$$ However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.
 $$$$$ This could be explained by the fact that beam-search is more robust than the deterministic mode, where in the latter, if our bilingual features misled the parser into a mistake, there is no chance of getting back, while in the former multiple configurations are being pursued in parallel.

Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ As a final example, in Figure 3(d), Chinese word kandao aligns to “saw”, which is already recognized, and this violates the right contiguity.
Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ T(·) denotes the POS tag of a given word, and lc(·) and rc(·) represent the leftmost and rightmost child.
Table 7 lists the results, where Huang 2009 refers to the result of Huang et al (2009), Chen2010BI refers to the result of using bilingual features in Chen et al (2010), and Chen2010ALL refers to the result of using all of the features in Chen et al (2010). $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).

 $$$$$ So to conclude, this bilingual hypothesis is empirically justified.
 $$$$$ However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.
 $$$$$ This method is much simpler than joint parsing because it remains monolingual in the backbone, with alignment information merely as soft evidence, rather than hard constraints since automatic word alignment is far from perfect.
 $$$$$ We suspect (and will finish in the future work) that using manual alignments would result in a better correlation, though for the main parsing results (see below) we can only afford automatic alignments in order for our approach to be widely applicable to any bitext.

For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ To make things worse, languages are non-isomorphic, i.e., there is no 1to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004).
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ Hypothesis 2 is verified in Table 7.
For the transition-based parsers, we used the arc-eager (ARCE) variant of the freely available MALT parser (Nivre et al,2006), and our own implementation of an arc standard parser (ARCS) as described in (Huang et al., 2009). $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.

The semantics of the system is described in (Huang et al, 2009). $$$$$ We argue that this kind of shift-reduce conflicts are the major source of parsing errors, since the other type of conflict, reduce-reduce conflict (i.e., whether left or right) is relatively easier to resolve given the part-of-speech information.
The semantics of the system is described in (Huang et al, 2009). $$$$$ The second and third authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No.
The semantics of the system is described in (Huang et al, 2009). $$$$$ On the other hand, English can help Chinese parsing as well, for example in deciding the scope of relative clauses which is unambiguous in English but ambiguous in Chinese.
The semantics of the system is described in (Huang et al, 2009). $$$$$ This method is much simpler than joint parsing because it remains monolingual in the backbone, with alignment information merely as soft evidence, rather than hard constraints since automatic word alignment is far from perfect.

Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ We use section 22 as dev set to determine the optimal number of iterations in perceptron training.
Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).
Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ In shift-reduce parsing, further mistakes are often caused by previous ones, so only the first mistake in each sentence (if there is one) is easily identifiable;7 this is also the argument for “early update” in applying perceptron learning to these incremental parsing algorithms (Collins and Roark, 2004) (see also Section 2).
Fossum and Knight (2008) and Huang et al (2009) improve English prepositional phrase attachment using features from an unparsed Chinese sentence. $$$$$ We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.

In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set.
In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ In Figure 3(c), when considering shifting “with”, the source span becomes [Bill .. with] which maps to [na .. Bi’er] on the Chinese side.
In fact, it is worse than the deterministic parser of Huang et al (2009), which uses (almost) the same set of features. $$$$$ We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees.

 $$$$$ So we will engineer more such features, especially with lexicalization and soft alignments (Liang et al., 2006), and study the impact of alignment quality on parsing improvement.
 $$$$$ Similarly, we can develop another feature cR(st, wi) for the shift action.
 $$$$$ We choose to use the heuristic of “shortest stack” that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999).
 $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.

Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ Jointly parsing two languages has been shown to improve accuracies on either or both sides.
Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008).
Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al, 2009). $$$$$ Our final results on the test set (290 sentences) are summarized in Table 9.
