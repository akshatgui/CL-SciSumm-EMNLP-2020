Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text planning and surface generation (realization). $$$$$ We have insufficient engineering data at present to make any well-substantiated claims about whether the one-way pipeline has the optimal cost/benefit tradeoff or not (and in any case this will probably_ depend somewhat on the circumstances of each application [Reiter and Mellish, 1993]), but the circumstantial evidence on this question is striking; despite the fact that so many theoretical papers have argued against pipelines and very few (if any) have argued for pipelines, every one of the applicationsoriented systems examined in this survey chose to use the one-way pipeline architecture.
Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text planning and surface generation (realization). $$$$$ I would like to conclude this paper by encouraging generation researchers to regard the results of engineering analyses to be as interesting and as important to the understanding of language as conventional linguistic analyses.
Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text planning and surface generation (realization). $$$$$ It goes without saying, of course, that the views represented are my own, and that any factual errors are entirely my fault.
Superficially, the architecture of our system conforms to the standard emerged in natural anguage generation (NLG) (as expressed, for instance in Reiter, 1994) in that it includes the stages of content specification, text planning and surface generation (realization). $$$$$ In other words, despite different theoretical claims, there is a remarkable level of similarity in how these systems 'really work'; that is, a de facto 'consensus architecture' seems to be emerging for how applied NLG systems should generate text.

This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
This separation is now fairly standard and most implementations encapsulate each task in a separate module (Robin 1995), (Reiter 1994). $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.

The summarizer's architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. $$$$$ Unfortunately, to date the results of such analyses have all-too-often been regarded more as embarrassments (since they contradict theory) than as valuable observations, and hence have not been published.
The summarizer's architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. $$$$$ It goes without saying, of course, that the views represented are my own, and that any factual errors are entirely my fault.
The summarizer's architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. $$$$$ None of the systems use the semantic head-driven generation algorithm [Shieber et at., 1990], although this is probably the single best-known algorithm for surface generation; Elhadad [1992, chapter 4] claims that such an algorithm is only necessary for systems that attempt to simultaneously perform both lexical choice and surface generation, which none of the examined systems do.
The summarizer's architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. $$$$$ Cross-fertilization between psycholinguistics and NL engineering will only arise, however, if the results of engineering analyses are reported in the research literature, especially when they suggest going against some theoretical principle.

This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994). $$$$$ Many names have been used for this process; here I use one suggested by Rambow and Korelsky [1992].
This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994). $$$$$ Structure-mapping is based on a dictionary that lists the semantic-net equivalents of linguistic resources [Meteer, 1991] such as content words and grammatical relationships.
This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994). $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
This assumes the existence of a separate higher-level process to produce such a representation, following the canonical pipeline architecture of a full generation system (Reiter, 1994). $$$$$ I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.

In the community of NLG, there is a broad consensus that the generation of natural language should be done in three major steps [Reiter, 1994]. $$$$$ Whether the human language engine is organized as a 'pipeline plus a few feedback loops' or an 'every module talks to every other module' architecture is unknown at this point; hopefully new psycholinguistic experiments will shed more light on this issue.
In the community of NLG, there is a broad consensus that the generation of natural language should be done in three major steps [Reiter, 1994]. $$$$$ Formatting: IDAS, JOYCE, and PENMAN also contain mechanisms for formatting (in the IATEX sense) their output, and/or adding hypertext annotations to enable users to click on portions of the generated text.

Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. $$$$$ I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.
Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
Reiter (1994) proposed an analysis of such systems in terms of a simple three stage pipeline. $$$$$ In FUF it is the 'lexical chooser', in IDAS it is the 'text planner', in JOYCE it is the 'sentence planner', in SPOKESMAN it is the 'text structurer', and in PENMAN it doesn't seem to have a name at all, e.g., Hovy [1988] simply refers to 'pre-generation textplanning tasks'.

Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter& amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al (1999), Soon et al (2001)) .In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter& amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al (1999), Soon et al (2001)) .In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. $$$$$ 50f course, the best way to do something on a machine is often not the best way to do it in nature; e.g., birds and airplanes use different mechanisms to fly.
Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter& amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al (1999), Soon et al (2001)) .In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. $$$$$ I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.
Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter& amp; Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al (1999), Soon et al (2001)) .In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. $$$$$ This is interesting, since backtracking is usually regarded as an essential component of unification-based generation approaches; it is certainly used in the semantic-headdriven algorithm, and in the TEXT generator [McKeown, 1985].

Narratological aspects influence on all architectural modules [Reiter, 1994] or representation levels [Cahill et al, 2000] of NLG. $$$$$ This means that the process as a whole becomes extremely difficult to debug or improve, whether by a human designer or in the course of natural evolution, because a small chance to improve one part has to be accompanied by many simultaneous compensatory changes elsewhere.
Narratological aspects influence on all architectural modules [Reiter, 1994] or representation levels [Cahill et al, 2000] of NLG. $$$$$ This evidence comes from many sources (e.g., cognitive experiments and PET scans of brain activity), but I think perhaps the most convincing evidence is from studies of humans with brain damage.
Narratological aspects influence on all architectural modules [Reiter, 1994] or representation levels [Cahill et al, 2000] of NLG. $$$$$ The systems examined use quite different contentdetermination mechanisms (i.e., there was no consensus); schemas [McKeown, 1985] were the most popular approach.
Narratological aspects influence on all architectural modules [Reiter, 1994] or representation levels [Cahill et al, 2000] of NLG. $$$$$ In this paper I survey some recently-developed NL generation systems that (a) cover the complete generation process and (b) are designed to be used by application programs, as well as (or even instead of) making some theoretical point.

The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). $$$$$ This is again perhaps somewhat surprising, since psycholinguistic plausibility was not in general a goal of the developers of the examined systems.
The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). $$$$$ PENMAN [Penman Natural Language Group, 1989]: Under development at ISI since the early 1980's, PENMAN has been used in several demonstration systems.
The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). $$$$$ SPOKESMAN uses Tree-Adjoining Grammars [Josh, 1987] for syntactic processing.
The RAGS project initially set out to develop a reference architecture based on the three-stage pipeline suggested by Reiter (Reiter, 1994). $$$$$ In doing so, I believe (and again this is a personal belief that probably cannot be substantiated by the existing evidence) that the NL engineer is coming close to the 'reasoning' of the evolutionary process that created the human language system.

In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. $$$$$ I use the JOYCE term here because I think it is the least ambiguous.
In the classic natural language generation (NLG) architecture (Reiter, 1994), sentence boundary decisions are made during the sentence planning stage in which the syntactic structure and wording of sentences are decided. $$$$$ Such a comparison is often difficult to make, because of the many gaps in our current knowledge about how humans speak.

Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions. $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
Although most generation systems pipeline decisions (Reiter, 1994), we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions. $$$$$ Several other universities have also recently begun to use FUF in their research.

Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). $$$$$ Even if this is true, however, if it turns out that adopting this architecture will substantially complicate the design of the overall generation system, and that the most common cases of the phenomena of interest can be adequately handled by adding a few heuristics to the appropriate stage of a simpler architecture, then the engineeringoriented NL worker must ask him- or herself if the benefits of the proposed architecture truly outweigh its costs.
Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). $$$$$ IDAS and PENMAN use variants of the same deep syntactic language, SPL [Kasper, 1989].
Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). $$$$$ Sentence Planning.
Most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (Reiter, 1994). $$$$$ I would like to thank Jean Carletta, Robert Dale, Michael Elhadad, David McDonald, Richard Kittredge, Tanya Korelsky, Chris Mellish, Owen Rambow, and Graeme Ritchie for their very helpful comments on earlier versions of this work.

Furthermore, Reiter (1994), who reviews the architecture of some models of natural language generation, shows that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects. $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
Furthermore, Reiter (1994), who reviews the architecture of some models of natural language generation, shows that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects. $$$$$ This is again perhaps somewhat surprising, since psycholinguistic plausibility was not in general a goal of the developers of the examined systems.
Furthermore, Reiter (1994), who reviews the architecture of some models of natural language generation, shows that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects. $$$$$ I survey some recent applications-oriented NL generation systems, and claim that despite very different theoretical backgrounds, these systems have a remarkably similar architecture in terms of the modules they divide the generation process into, the computations these modules perform, and the way the modules interact with each other.
Furthermore, Reiter (1994), who reviews the architecture of some models of natural language generation, shows that psycholinguistic and engineering approaches often result in systems, which are similar in crucial respects. $$$$$ I would like to thank Jean Carletta, Robert Dale, Michael Elhadad, David McDonald, Richard Kittredge, Tanya Korelsky, Chris Mellish, Owen Rambow, and Graeme Ritchie for their very helpful comments on earlier versions of this work.

In this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-tosay (De Smedt, Horacek& amp; Zock, 1996) and the use of a pipeline architecture, which divides the generation process& quot; into multiple modules, with information flowing in a &apos; pipeline &apos; fashion from one module to the next& quot; (Reiter, 1994). $$$$$ Content determination in the systems examined basically performs two functions: Deep content determination: Determine what information should be communicated to the hearer.
In this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-tosay (De Smedt, Horacek& amp; Zock, 1996) and the use of a pipeline architecture, which divides the generation process& quot; into multiple modules, with information flowing in a &apos; pipeline &apos; fashion from one module to the next& quot; (Reiter, 1994). $$$$$ The engineering argument for modularization is particularly strong; Marr has put this very well in [Man, 1976, page 485]: Any large computation should be split up and implemented as a collection of small subparts that are as nearly independent of one another as the overall task allows.
In this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-tosay (De Smedt, Horacek& amp; Zock, 1996) and the use of a pipeline architecture, which divides the generation process& quot; into multiple modules, with information flowing in a &apos; pipeline &apos; fashion from one module to the next& quot; (Reiter, 1994). $$$$$ From a theoretical perspective, IDAS's main objective was to show that a single representation and reasoning system can be used for both domain and linguistic knowledge [Reiter and Mellish, 1992].
In this paper we ground on two of these common aspects, namely the distinction between what-to-say and how-tosay (De Smedt, Horacek& amp; Zock, 1996) and the use of a pipeline architecture, which divides the generation process& quot; into multiple modules, with information flowing in a &apos; pipeline &apos; fashion from one module to the next& quot; (Reiter, 1994). $$$$$ It goes without saying, of course, that the views represented are my own, and that any factual errors are entirely my fault.

There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). $$$$$ JOYCE [Rambow and Korelsky, 1992]: Developed at Odyssey Research Associates, JOYCE is taken as a representative of several NL generation systems produced by ORA and CoGenTex, including GOSSIP, FOG, and LFS.
There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). $$$$$ I also compare this 'consensus architecture' among applied NLG systems with psycholinguistic knowledge about how humans speak, and argue that at least some aspects of the consensus architecture seem to be in agreement with what is known about human language production, despite the fact that psycholinguistic plausibility was not in general a goal of the developers of the surveyed systems.
There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). $$$$$ I use it here to refer to the &quot;portion of the generation system that knows how grammatical relationships are actually expressed in English (or whatever the target language is).
There has also been a rethinking of the traditional modular NLG architecture (Reiter, 1994). $$$$$ The argument against pipelines and modules is almost always some variant of 'there are linguistic phenomena that can only be properly handled by looking at constraints from different levels (intentional, semantic, syntactic, morphological), and this is difficult to do in a pipeline system.'
