The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ Briscoe et al. (2006) split the 700 sentences in DepBank into a test and development set, but the latter only consists of 140 sentences which was not enough to reliably create the transformation.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ One aspect of parser evaluation not covered in this paper is efficiency.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.

CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). $$$$$ We compare the against the outperformover 5% overall and on the majority of dependency types.
CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). $$$$$ The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank.

For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ Note that some GRs — in The predicate-argument dependencies — includ- this example ncsubj — have a subtype slot, giving ing long-range dependencies — are encoded in the extra information.
For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult.
For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006).
For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ Consider the example The president and chief executive officer said the loss stems from several factors.

Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ Hence, we conclude that accurate, large-scale, linguistically-motivated NLP is now practical with CCG.
Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ To obtain some idea of whether the schemes were converging, we performed the following oracle experiment.
Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult.
Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.

However, cross framework parser evaluation is a difficult problem $$$$$ Note that the difficulties are not unique to CCG, and many would apply to any crossformalism comparison, especially with parsers using automatically extracted grammars.
However, cross framework parser evaluation is a difficult problem $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
However, cross framework parser evaluation is a difficult problem $$$$$ Hence, we conclude that accurate, large-scale, linguistically-motivated NLP is now practical with CCG.
However, cross framework parser evaluation is a difficult problem $$$$$ Despite the difficulties, we have given the first evaluation of a CCG parser outside of CCGbank, outperforming the RASP parser by over 5% overall and on the majority of dependency types.

Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ The results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al. (2006).
Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ Note that the difficulties are not unique to CCG, and many would apply to any crossformalism comparison, especially with parsers using automatically extracted grammars.
Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ Hence, we conclude that accurate, large-scale, linguistically-motivated NLP is now practical with CCG.

While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). $$$$$ Using a cluster of 18 machines we have also parsed the entire Gigaword corpus in less than five days.
While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). $$$$$ The results show that the performance of the CCG parser is higher than RASP overall, and also higher on the majority of GR types (especially the more frequent types).
While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). $$$$$ James Curran was funded under ARC Discovery grants DP0453131 and DP0665973.
While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). $$$$$ We deal with this difference by replacing any pairs of GRs which differ only in their arguments, and where the arguments are coordinated items, with a single GR containing the coordination term as the argument.

More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). $$$$$ RASP uses an unlexicalised parsing model and has not been tuned to newspaper text.
More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). $$$$$ One aspect of parser evaluation not covered in this paper is efficiency.

We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ Despite the difficulties, we have given the first evaluation of a CCG parser outside of CCGbank, outperforming the RASP parser by over 5% overall and on the majority of dependency types.
We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ Without some measure of the difficulty — and effectiveness — of the conversion, there remains a suspicion that the Collins parser is being unfairly penalised.
We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.
We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.

EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons.
EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ This variety of formalisms and output creates a challenge for parser evaluation.
EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison.

SCF and DR $$$$$ James Curran was funded under ARC Discovery grants DP0453131 and DP0665973.
SCF and DR $$$$$ A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.
SCF and DR $$$$$ The final columns of Table 4 show the accuracy of the transformed gold-standard CCGbank dependencies when compared with DepBank; the simple post-processing rules have increased the F-score from 77.86% to 84.76%.
SCF and DR $$$$$ A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison.

The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). $$$$$ As well as providing some measure of the effectiveness of the conversion, this method would also provide an upper bound for the Collins parser, giving the score that a perfect Penn Treebank parser would obtain on DepBank (given the conversion process).
The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). $$$$$ The only change we have made is to turn all ncmod GRs with of as the modifier into iobj GRs (unless the ncmod is a partitive predeterminer).

However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-averaged scores are the mean of the individual scores for each relation (Briscoe et al., 2006).
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ There are parser evaluation suites which have been designed to be formalism-independent and which have been carefully and manually corrected.
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ Such conversions have been performed for other parsers, including parsers producing phrase structure output (Kaplan et al., 2004; Preiss, 2003).

A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ The CCG parser took only 22.6 seconds to parse the 560 sentences in DepBank, with the accuracy given earlier.
A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ Using a cluster of 18 machines we have also parsed the entire Gigaword corpus in less than five days.

Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ The B&C scheme is similar to the defined over local parts of the derivation and include original DepBank scheme (King et al., 2003), but word-word dependencies.
Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ Parsers have been developed for a variety of grammar formalisms, for example HPSG (Toutanova et al., 2002; Malouf and van Noord, 2004), LFG (Kaplan et al., 2004; Cahill et al., 2004), TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000).
Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ The results for CCGbank were obtained using the oracle method described above.

The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation $$$$$ The results for CCGbank were obtained using the oracle method described above.
The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation $$$$$ Examples include any dependencies in which a punctuation mark is one of the arguments; these were removed from the output.

The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.
The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ A log-linear 4 Dependency Conversion to DepBank model scores the alternative parses.
The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ Using a cluster of 18 machines we have also parsed the entire Gigaword corpus in less than five days.

Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). $$$$$ Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.
Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). $$$$$ Hence we challenge other parser developers to map their own parse output into the version of DepBank used here.
Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). $$$$$ Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.

The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ The results for CCGbank were obtained using the oracle method described above.
The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the output of a parser from one representation to another.
The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.

The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-averaged scores are the mean of the individual scores for each relation (Briscoe et al., 2006).
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ We modified the CCGbank sentences — and the CCGbank analyses since these were used for the oracle experiments — to be as close to the DepBank sentences as possible.
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger.
