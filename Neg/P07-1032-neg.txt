The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. this paper we evaluate a on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared.
The evaluation in this paper was based solely on CCGbank, but we have shown in Clark and Curran (2007) that the CCG parser gives state-of-the-art performance, outperforming the RASP parser (Briscoe et al, 2006) by over 5% on DepBank. $$$$$ The overall F-score for the CCG parser, 81.86%, is only 3 points below that for CCGbank, which provides an upper bound for the CCG parser (given the conversion process being used).

CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). $$$$$ The results for CCGbank were obtained using the oracle method described above.
CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
CCG and HPSG parsers also favor the dependency based metrics for evaluation (Clark and Curran, 2007b; Miyao and Tsujii, 2008). $$$$$ For a GR in the parser output to be correct, it has to match the gold-standard GR exactly, including any subtype slots; however, it is possible for a GR to be incorrect at one level but correct at a subsuming level.1 For example, if an ncmod GR is incorrectly labelled with xmod, but is otherwise correct, it will be correct for all levels which subsume both ncmod and xmod, for example mod.

For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types.
For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the output of a parser from one representation to another.
For example, Clark and Curran (2007) developed a set of mapping rules from the output of a Combinatorial Categorial grammar parser to the Grammatical Relations (GR) (Carroll et al, 1998). $$$$$ We compare the against the outperformover 5% overall and on the majority of dependency types.

Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ In this paper we evaluate a CCG parser (Clark and Curran, 2004b) on the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006).
Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ Second, using the same resource for development and testing can lead to an over-optimistic view of parser performance.
Our system used the C & C parser (Clark and Curran, 2007a), which uses the Combinatory Categorial Grammar formalism (CCG, Steedman, 2000). $$$$$ Parser evaluation has improved on the original Parseval measures (Carroll et al., 1998), but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms.

However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. $$$$$ James Curran was funded under ARC Discovery grants DP0453131 and DP0665973.
However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. $$$$$ Briscoe and Carroll (2006) give a rough comparison of RASP with the Parc LFG parser on the different versions of DepBank, obtaining similar results overall, but they acknowledge that the results are not strictly comparable because of the different annotation schemes used.
However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. $$$$$ The CCG parser took only 22.6 seconds to parse the 560 sentences in DepBank, with the accuracy given earlier.
However, cross framework parser evaluation is a difficult problem: previous attempts to evaluate the C & C parser on grammatical relations (Clark and Curran, 2007b) and Penn Treebank-trees (Clark and Curran, 2009) have also produced upper bounds between 80 and 90% F-score. $$$$$ First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the output of a parser from one representation to another.

Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ As an example, the senment slot corresponding to the direct object.
Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
Clark and Curran (2007a) demonstrate the use of techniques like adaptive super tagging, parallelisation and a dynamic-programming chart parsing algorithm to implement the C & C parser, a highly efficient CCG parser that performs well against parsers built on different formalisms (Rimell et al, 2009). $$$$$ First, we demonstrate the considerable difficulties associated with formalism-independent parser evaluation, highlighting the problems in converting the output of a parser from one representation to another.

While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
While this is not ideal, we note that previous efforts at cross-parser evaluation have shown that it is a difficult problem (Clark and Curran (2007b) and Clark and Curran (2009)). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.

More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). $$$$$ A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison.
More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). $$$$$ James Curran was funded under ARC Discovery grants DP0453131 and DP0665973.
More detailed discussions of the obstacles to directly comparing syntactic structures include Preiss (2003), Clark and Curran (2007), and most recently Sagae et al (2008). $$$$$ The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser.

We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ Despite the difficulties, we have given the first evaluation of a CCG parser outside of CCGbank, outperforming the RASP parser by over 5% overall and on the majority of dependency types.
We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ The simple punctuation rules used by the parser do not contain enough information to distinguish between the various cases of ta.
We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-averaged scores are the mean of the individual scores for each relation (Briscoe et al., 2006).
We find that, not only can we produce models that are suitable for kick-starting the treebanking process, but the accuracy of these models is comparable to parsers trained on gold standard data (Clark and Curran, 2007b; Miyao and Tsujii,2008), which have been successfully used in applications (Miyao et al, 2008). $$$$$ We compare the against the outperformover 5% overall and on the majority of dependency types.

EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ The results show that the performance of the CCG parser is higher than RASP overall, and also higher on the majority of GR types (especially the more frequent types).
EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.
EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ In addition we present a method for measuring the effectiveness of the conversion, which provides an upper bound on parsing accuracy.
EDM Fscores of 90% and 83% over in-domain data compare well with dependency-based scores from other parsers, although a direct comparison is very difficult to do (Clark and Curran, 2007a; Miyao et al,2007). $$$$$ The microaveraged scores are calculated by aggregating the counts for all the relations in the hierarchy, including the subsuming relations; the macro-averaged scores are the mean of the individual scores for each relation (Briscoe et al., 2006).

SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C & C CCG parser (Clark and Curran, 2007). $$$$$ All the results were obtained using the RASP evaluation scripts, with the results for the RASP parser taken from Briscoe et al. (2006).
SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C & C CCG parser (Clark and Curran, 2007). $$$$$ The results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al. (2006).
SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C & C CCG parser (Clark and Curran, 2007). $$$$$ Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons.

The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
The focus on labeled dependencies also provides a direct link to recent work on dependency-based evaluation (e.g., Clark and Curran, 2007) and dependency parsing (e.g., CoNLL shared tasks 2006, 2007). $$$$$ Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons.

However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ RASP uses an unlexicalised parsing model and has not been tuned to newspaper text.
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ The subtype slot specifies additional information about the GR; examples include the value obj in a passive ncsubj, indicating that the subject is an underlying object; the value num in ncmod, indicating a numerical quantity; and prt in ncmod to indicate a verb particle.
However, their dependency-based evaluation does not make use of the grammatical function labels, which are provided in the corpora and closely correspond to the representations used in recent work on formalism independent evaluation of parsers (e.g., Clark and Curran, 2007). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.

A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.
A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ A packed chart represen- overall contains less grammatical detail; Briscoe and tation allows efficient decoding, with the Viterbi al- Carroll (2006) describes the differences.
A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ A contribution of this paper has been to highlight the difficulties associated with cross-formalism parser comparison.
A labeled dependency evaluation based on grammatical relations, which links this work to current work on formalism-independent parser evaluation (e.g., Clark and Curran, 2007), shows that the parsing performance for Negra and Tu Ba-D/Z is comparable. $$$$$ Hence we challenge other parser developers to map their own parse output into the version of DepBank used here.

Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ A key question facing the parsing community is how to compare parsers which use different grammar formalisms and produce different output.
Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared.
Conceptually, this conversion is similar to the conversions from deeper structures to GR reprsentations reported by Clark and Curran (2007) and Miyao et al (2007). $$$$$ However, a certain amount of skill and intuition was required to provide a fair conversion of the Collins trees”.

The CCG parser we use (Clark and Curran, 2007b) makes use of three levels of representation: one, a POS tag level based on the fairly coarse-grained POS tags in the Penn Treebank; two, a lexical category level based on the more fine-grained CCG lexical categories, which are assigned to words by a CCG super tagger; and three, a hierarchical level consisting of CCG derivations. $$$$$ This rule applies to only a small subset of the ta cases but has high enough precision to be worthy of inclusion.

The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types.
The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ The final columns of Table 4 show the accuracy of the transformed gold-standard CCGbank dependencies when compared with DepBank; the simple post-processing rules have increased the F-score from 77.86% to 84.76%.
The CCG parser is described in detail in Clark and Curran (2007b) and so we provide only a brief description. $$$$$ This variety of formalisms and output creates a challenge for parser evaluation.

Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). $$$$$ Hence, we conclude that accurate, large-scale, linguistically-motivated NLP is now practical with CCG.
Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). $$$$$ The lexical category (S\NP1)/NP2 is the cate- The GRs are described in Briscoe and Carroll gory of a transitive verb, with the first argument slot (2006) and Briscoe et al. (2006).
Supertagging was originally developed for Lexicalized Tree Adjoining Grammar (Bangalore and Joshi, 1999), but has been particularly successful for wide-coverage CCG parsing (Clark and Curran, 2007b). $$$$$ The CCG parser produces head-dependency relations, so evaluating the parser should simply be a matter of converting the CCG dependencies into those in DepBank.

The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ And third, we provide the first evaluation of a widecoverage CCG parser outside of CCGbank, obtaining impressive results on DepBank and outperforming the RASP parser (Briscoe et al., 2006) by over 5% overall and on the majority of dependency types.
The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser.
The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ Hence we challenge other parser developers to map their own parse output into the version of DepBank used here.
The CCG super tagger is not able to assign a single category to each word with extremely high accuracy - hence the need for it to operate as a multi-tagger - but even in multi-tagger mode it dramatically reduces the ambiguity passed through to the parser (Clark and Curran, 2007b). $$$$$ Second, we develop a method for measuring how effective the conversion process is, which also provides an upper bound for the performance of the parser, given the conversion process being used; this method can be adapted by other researchers to strengthen their own parser comparisons.

The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ Comparison with Penn Treebank parsers would be difficult because, for many constructions, the Penn Treebank trees and CCG derivations are different shapes, and reversing the mapping Hockenmaier used to create CCGbank would be very difficult.
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ James Curran was funded under ARC Discovery grants DP0453131 and DP0665973.
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ The obtains an F-score of 81.9% on labelled dependencies, against an upper of 84.8%.
The parser has been evaluated on DepBank (King et al., 2003), using the GR scheme of Briscoe et al. (2006), and it scores 82.4% labelled precision and 81.2% labelled recall overall (Clark and Curran, 2007a). $$$$$ We would like to thanks the anonymous reviewers for their helpful comments.
