Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. $$$$$ Clearly, the determination of membership of a particular mention into a partition should be conditioned on how well it matches the entity as a whole.
Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. $$$$$ ILP uses separate local classifiers which are learned without knowledge of the output constraints and are then integrated into a larger inference task.
Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. $$$$$ This work was supported by NSF grant IIS0535154.

It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ The task for the anaphoricity determination component is the following: one wants to decide for each mention i in a document whether i is anaphoric or not.
It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ This work was supported by NSF grant IIS0535154.
It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ Another global approach to coreference is the application of Conditional Random Fields (CRFs) (McCallum and Wellner, 2004).

Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.
Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.
Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ As can be seen, the AC-CASCADE system generally provides slightly better precision at the expense of recall than the COREF-PAIRWISE system, but the performance varies across the three datasets.
Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ CRFs estimate a global model that directly uses the constraints of the domain.

 $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.
 $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.
 $$$$$ In future work, we will explore the use of global constraints, similar to those used by (Barzilay and Lapata, 2006) to improve both precision and recall.
 $$$$$ Another option is to pick the antecedent with the best overall probability (Ng and Cardie, 2002b).

Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ Despite having lower precision than COREF-PAIRWISE, the COREF-ILP system obtains very large gains in recall to end up with overall f-score gains of 4.3%, 4.2%, and 3.0% across BNEWS, NPAPER, and NWIRE, respectively.
Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ COREF-ILP uses an objective function that is based on only the coreference classifier and the probabilities it produces.
Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ The source of this variance is likely due to the fact that we applied a uniform anaphoricity threshold of .5 across all datasets; Ng (2004) optimizes this threshold for each of the datasets: .3 for BNEWS and NWIRE and .35 for NPAPER.
Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ Precision is not completely degraded because the optimization performed by ILP utilizes the pairwise probabilities of mention pairs as weights in the objective function to make its assignments.

Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ Both ILP systems are significantly better than the baseline system COREF-PAIRWISE.
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ As mentioned in the introduction, coreference classifiers such as that presented in section 2 suffer from errors in which (a) they assign an antecedent to a non-anaphor mention or (b) they assign no antecedents to an anaphoric mention.
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ These constraints express qualities of what a global assignment of values for these tasks must respect, such as the fact that the arguments to the spouse of relation must be entities with person labels.
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ Roth and Yih thus set this up as problem in which each task is performed separately; their output is used to assign costs associated with indicator variables in an objective function, which is then minimized subject to constraints that relate the two kinds of outputs.

It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys. $$$$$ To counteract this, they add ad hoc constraints based on string matching and extended mention matching which force certain mentions to be resolved as anaphors regardless of the anaphoricity classifier.
It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys. $$$$$ Only the mentions that are deemed anaphoric are considered for coreference resolution.
It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys. $$$$$ Given the rapidly growing size of Bell trees, Luo et al. resort to a beam search algorithm and various pruning strategies, potentially resulting in picking a non-optimal solution.

 $$$$$ This work was supported by NSF grant IIS0535154.
 $$$$$ They only consider proper names, and they only tackled the task of identifying the correct antecedent only for mentions which have a true antecedent.
 $$$$$ In a similar manner, this framework could also be used to capture dependencies among coreference decisions themselves.

 $$$$$ A third global approach is offered by Ng (2005), who proposes a global reranking over partitions generated by different coreference systems.
 $$$$$ Another global approach to coreference is the application of Conditional Random Fields (CRFs) (McCallum and Wellner, 2004).
 $$$$$ Soon et. al.
 $$$$$ This work was supported by NSF grant IIS0535154.

 $$$$$ The other thing to note is that the results in general provide a lot of room for improvement — this is true for other state-of-the-art systems as well.
 $$$$$ Interestingly, they find a degredation in performance.
 $$$$$ Our problem formulation and use of ILP are based on both (Roth and Yih, 2004) and (Barzilay and Lapata, 2006).
 $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.

Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.
Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ (2001) system, this is done by pairing each mention j with each preceding mention i.
Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC).
Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ Roth and Yih thus set this up as problem in which each task is performed separately; their output is used to assign costs associated with indicator variables in an objective function, which is then minimized subject to constraints that relate the two kinds of outputs.

We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ The features used for the anaphoricity classifier are quite simple.
We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ A complement assignment cost cC(i,j) = −log(1−pC) is associated with choosing not to establish a link.
We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.

Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). $$$$$ The ILP objective function incorporates the probabilities produced by both classifiers as weights on variables that indicate the ILP assignments for those tasks.
Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). $$$$$ The JOINT-ILP system demonstrates the benefit ILP’s ability to support joint task formulations.
Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). $$$$$ Ng (2004) obtains f-score improvements of 2.8-4.5% by tuning the anaphoricity threshold on held-out data.

We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ That is, it always picks the correct antecedent for an anaphor.
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ (2001) use “Closest-First” selection: that is, the process terminates as soon as an antecedent (i.e., a test instance with probability > .5) is found or the beginning of the text is reached.
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ This sensitivity is unsurprising, given that the tasks are codependent.
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.

 $$$$$ There are also improvements in recall over COREF-ILP for NPAPER and NWIRE.
 $$$$$ As mentioned in the introduction, coreference classifiers such as that presented in section 2 suffer from errors in which (a) they assign an antecedent to a non-anaphor mention or (b) they assign no antecedents to an anaphoric mention.
 $$$$$ With ILP, it is not necessary to carefully control the anaphoricity threshold.

Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). $$$$$ They include information regarding (1) the mention itself, such as the number of words and whether it is a pronoun, and (2) properties of the potential antecedent set, such as the number of preceding mentions and whether there is a previous mention with a matching string.
Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). $$$$$ We then give the details of our ILP formulations and evaluate their performance with respect to each other and the base classifier.
Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). $$$$$ Compared to COREF-PAIRWISE, JOINT-ILP dramatically improves recall with relatively small losses in precision, providing overall f-score gains of 5.3%, 4.9%, and 3.7% on the three datasets.

Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ There are also improvements in recall over COREF-ILP for NPAPER and NWIRE.
Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC).
Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ Soon et. al.

Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ This approach proceeds by first generating 54 candidate partitions, which are each generated by a different system.
Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ This is an extra degree of freedom that allows COREFILP to cast a wider net, with a consequent risk of capturing incorrect antecedents.
Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ This corpus is divided into three parts, each corresponding to a different genre: newspaper texts (NPAPER), newswire texts (NWIRE), and broadcasted news transcripts (BNEWS).
Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ We will also consider linguistic constraints (e.g., restrictions on pronouns) in order to improve precision.

Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ Its only errors are due to being unable to resolve mentions which were marked as nonanaphoric (by the imperfect anaphoricity classifier) when in fact they were anaphoric.
Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ Thus, highly improbable links are still heavily penalized and are not chosen as coreferential.
Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ The features used by the reranker are of two types: (i) partition-based features which are here simple functions of the local features, and (ii) method-based features which simply identify the coreference system used for generating the given partition.
Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling1 (Malouf, 2002).

By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ This work was supported by NSF grant IIS0535154.
By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ We omit details here for the sake of brevity — the ILP systems we employ here could be equally well applied to many different base classifiers using many different feature sets.
By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ They include information regarding (1) the mention itself, such as the number of words and whether it is a pronoun, and (2) properties of the potential antecedent set, such as the number of preceding mentions and whether there is a previous mention with a matching string.
By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ In what follows, M denotes the set of mentions in the document, and P the set of possible coreference links over these mentions (i.e., P = {hi, ji|hi, ji ∈ M × M and i < j}).
