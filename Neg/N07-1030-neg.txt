Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. $$$$$ A possible source of this difference is constraint (9), which ensures that mentions which are considered anaphoric must have at least one antecedent.
Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. $$$$$ To demonstrate the inherent limitations of cascading, we also give results for an oracle system, ORACLE-LINK, which assumes perfect linkage.
Denis and Baldridge (2007) report 67.1 F1 and 69.2 F1 on the English NWIRE and BNEWS respectively using true mention boundaries. $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.

It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ It produces significantly better f-scores by regaining some of the ground on precision lost by COREFILP.
It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ Even if we had a system that can pick the correct antecedents for all truly anaphoric mentions, it would have a maximum recall of roughly 70% for the different datasets.
It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ Table 2 summarizes the results for these different systems.
It is also similar to the MLN-JOINT (BF) model of Song et al (2012), but we enforce the single parent constraint at a deeper structural level, allowing us to treat non-anaphoricity symmetrically with coreference as in Denis and Baldridge (2007) and Stoyanov and Eisner (2012). $$$$$ This is in contrast with the usual clustering algorithms, in which a unique antecedent is typically picked for each anaphor (e.g., the most probable or the most recent).

Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ (2001) system, this is done by pairing each mention j with each preceding mention i.
Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.
Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ Model parameters are estimated using maximum entropy (Berger et al., 1996).
Denis and Baldridge (2007) adopted an Integer Linear Programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task. $$$$$ We test significant differences with paired t-tests (p < .05).

 $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
 $$$$$ There are two major drawbacks with most systems that make pairwise coreference decisions.
 $$$$$ The task for the anaphoricity determination component is the following: one wants to decide for each mention i in a document whether i is anaphoric or not.
 $$$$$ The features used for the anaphoricity classifier are quite simple.

Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ For example, we expect transitivity constraints over coreference pairs, as well as constraints on the entire partition (e.g., the number of entities in the document), to help considerably.
Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ Training instances for the coreference classifier are constructed based on pairs of mentions of the form hi, ji, where j and i are the descriptions for an anaphor and one of its candidate antecedents, respectively.
Extending Denis and Baldridge (2007) and Finkel and Manning (2008)'s work, we exploit loose transitivity constraints on coreference pairs. $$$$$ Most importantly, the different systems employed for generating the different partitions are all instances of the local classification approach, and they all use very similar features.

Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ The anaphoricity classifier has an average accuracy of 80.2% on the three ACE datasets (using a threshold of .5).
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ Thus, highly improbable links are still heavily penalized and are not chosen as coreferential.
Denis and Baldridge (2007) proposed an ILP formulation to find the optimal solution for the problem. $$$$$ The integer programming formulation we provide here has qualities which address both of these issues.

It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys. $$$$$ For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver.
It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys. $$$$$ This feature set is similar (though not equivalent) to that used by Ng and Cardie (2002a).
It is worth noting, in particular, that Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008) evaluate their approaches on true mentions extracted from the answer keys. $$$$$ Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around.

 $$$$$ Such models are well-suited for coreference, because they are able to handle many different, potentially overlapping learning features without making independence assumptions.
 $$$$$ Interestingly, they find a degredation in performance.
 $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.
 $$$$$ Thus, highly improbable links are still heavily penalized and are not chosen as coreferential.

 $$$$$ ILP solutions are also obtained very quickly for the objective functions and constraints we use.
 $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
 $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.
 $$$$$ They achieve higher performance by doing so; however, their setup uses the two classifiers in a cascade.

 $$$$$ We evaluate these systems on the datasets from the ACE corpus (Phase 2).
 $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
 $$$$$ That is, this task can be performed using a simple binary classifier with two outcomes: ANAPH and ANAPH.
 $$$$$ This requires labeling a set of named entities in a text with labels such as person and location, and identifying relations between them such as spouse of and work for.

Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ Since independence between decisions is an unwarranted assumption for the task, models that consider a more global context are likely to be more appropriate.
Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ This method operates by comparing the equivalence classes defined by the resolutions produced by the system with the gold standard classes: these are the two “models”.
Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.
Ng and Cardie (2002a) and Denis and Baldridge (2007) show that when used effectively, explicitly predicting anaphoricity can be helpful. $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.

We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around.
We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ Most importantly, the different systems employed for generating the different partitions are all instances of the local classification approach, and they all use very similar features.
We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ We would like to thank Ray Mooney, Rohit Kate, and the three anonymous reviewers for their comments.
We formulate our lexicon adaptation task using integer linear programming (ILP), which has been shown to bevery effective when solving problems with complex constraints (e.g., Roth and Yih (2004), Denis and Baldridge (2007)). $$$$$ As was just demonstrated, ILP provides a principled way to model dependencies between anaphoricity decisions and coreference decisions.

Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). $$$$$ To reduce such errors, Ng and Cardie (2002a) and Ng (2004) use an anaphoricity classifier –which has the sole task of saying whether or not any antecedents should be identified for each mention– as a filter for their coreference system.
Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). $$$$$ Although both are global approaches, CRFs and ILP have important differences.
Examples of such approaches have exploited techniques including integer linear programming (ILP) (Denis and Baldridge, 2007a), label propagation (Zhou and Kong, 2009), and minimum cuts (Ng, 2009). $$$$$ This is because our focus is on evaluating pairwise local approaches versus the global ILP approach rather than on building a full coreference resolution system.

We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ ILP solutions are also obtained very quickly for the objective functions and constraints we use.
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ This is because our focus is on evaluating pairwise local approaches versus the global ILP approach rather than on building a full coreference resolution system.
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ This requires labeling a set of named entities in a text with labels such as person and location, and identifying relations between them such as spouse of and work for.
We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. $$$$$ This work was supported by NSF grant IIS0535154.

 $$$$$ Roth and Yih thus set this up as problem in which each task is performed separately; their output is used to assign costs associated with indicator variables in an objective function, which is then minimized subject to constraints that relate the two kinds of outputs.
 $$$$$ The features used by the reranker are of two types: (i) partition-based features which are here simple functions of the local features, and (ii) method-based features which simply identify the coreference system used for generating the given partition.
 $$$$$ A complement assignment cost cC(i,j) = −log(1−pC) is associated with choosing not to establish a link.

Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). $$$$$ Each such pair is assigned either a label COREF (i.e. a positive instance) or a label ¬COREF (i.e. a negative instance) depending on whether or not the two mentions corefer.
Section 2 briefly summarizes the approach proposed by Denis and Baldridge (2007). $$$$$ In this paper, we propose an integer linear programming (ILP) formulation for coreference resolution which models anaphoricity and coreference as a joint task, such that each local model informs the other for the final assignments. joint ILP formulation provides score improvements of 3.7-5.3% over a base coreference classifier on the ACE datasets.

Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ Previous work on coreference using maximum entropy includes (Kehler, 1997; Morton, 1999; Morton, 2000).
Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ The second problem is that most coreference systems make each decision independently of previous ones in a greedy fashion (McCallum and Wellner, 2004).
Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ Recall is computed by trying to map the predicted chains onto the true chains, while precision is computed the other way around.
Denis and Baldridge (2007) defined the following object function for the joint anaphoricity and coreference determination problem. $$$$$ This score is slightly lower than the scores reported by Ng and Cardie (2002a) for another data set (MUC).

Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ There are two major drawbacks with most systems that make pairwise coreference decisions.
Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ That is, this task can be performed using a simple binary classifier with two outcomes: ANAPH and ANAPH.
Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
Non-referential it detection should not be a pre-processing step, but rather part of a globally optimal configuration, as was done for general noun phrase anaphoricity by Denis and Baldridge (2007). $$$$$ Compared to COREF-PAIRWISE, JOINT-ILP dramatically improves recall with relatively small losses in precision, providing overall f-score gains of 5.3%, 4.9%, and 3.7% on the three datasets.

Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ CRFs estimate a global model that directly uses the constraints of the domain.
Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ For solving the ILP problem, we use lp solve, an open-source linear programming solver which implements the simplex and the Branch-and-Bound methods.3 In practice, each test document is processed to define a distinct ILP problem that is then submitted to the solver.
Ranking models provide a theoretically more adequate and empirically better alternative approach to pronoun resolution than standard classification based approaches (Denis and Baldridge, 2007b). $$$$$ They achieve higher performance by doing so; however, their setup uses the two classifiers in a cascade.

By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ Standard pairwise coreference resolution systems are subject to errors resulting from their performing anaphora identification as an implicit part of coreference resolution.
By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ Thus, highly improbable links are still heavily penalized and are not chosen as coreferential.
By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ This is very much like coreference, where each partition corresponds to an entity in a discourse model.
By using joint inference for anaphoricity and coreference, Denis and Baldridge (2007a) avoid cascade-induced errors without the need to separately optimize the threshold. $$$$$ A possible source of this difference is constraint (9), which ensures that mentions which are considered anaphoric must have at least one antecedent.
