Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ A further goal of these experiments was to obtain one or two metrics which can be automatically computed, and which have been shown to significantly correlate with relevant human judgments.
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ We have used four different baseline quantitative metrics for evaluating our generator.
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ We have used four different baseline quantitative metrics for evaluating our generator.

More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ For the _final sentence we have contrasts between the string measures with tree measures being approximately equal.
More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ Note that if there are insertions and deletions, the number of operations may be larger than the number of tokens involved for either one of the two strings.

Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). $$$$$ We therefore divide by this score to assure that a perfect sentence gets a score of 1.
Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). $$$$$ We subtracted the mean score for each subject from each observed score and then divided this by standard deviation of the scores for that subject.
Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]); Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]); Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al 2000]). $$$$$ Finally, it is also plausible that longer sentences are more difficult to understand. so that length has a (small) negative coefficient.

The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.
The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ This was confirmed.
The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ The XTAG grammar analyzes these as being headed by the noun,rather-than,by.the copula, and we follow the XTAG analysis.

Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ The next two metrics are based on a syntactic representation of the sentence.
Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ This model was significant: F(3,19) = 6.62, p < 0.005.
Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.

In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.
In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ We pick the best path through the lattice resulting from the composition using the Viterbi algorithm, and this top ranking word sequence is the output of the LP Chooser and the generator.
In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ The metrics are useful to us as relative quantitative assessments of different models we experiment with; however, we do not pretend that these metrics in themselves have any validity.

Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. $$$$$ To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.
Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. $$$$$ In all these cases, stochastic methods provide an alternative to hand-crafted approaches to NLG.
Simple String Accuracy (SSA, Bangalore et al 2000) has been proposed as a baseline evaluation metric for natural language generation. $$$$$ The target sentence is on top, the generated sentence below.

Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. $$$$$ To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. $$$$$ To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. $$$$$ Consider the subphrase estimate for phase the second of the sentence in (2).
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms: Carroll et al (1999) for HPSG, Bangalore et al (2000) for LTAG, Moore (2002) for unification grammars, White and Baldridge (2003) for CCG. $$$$$ Finally. the LP Chooser chooses the most likely traversal of this lattice. given a linear language 'The sentence generated by this tree is a predicative noun construction.

Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.
Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ The first metric, simple accuracy, is the same string distance metric used for measuring speech recognition accuracy.
Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ We therefore divide by this score to assure that a perfect sentence gets a score of 1.
Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.

 $$$$$ This model was significant: F(3,19) = 6.62, p < 0.005.
 $$$$$ FERGUS, a realization module, follows Knight and Langkilde's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a lexicalized syntactic grammar.
 $$$$$ The test corpus is a randomly chosen subset of 100 sentences from the Section 20 of WSJ.

We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ The simple accuracy, generation accuracy, simple tree accuracy and generation tree accuracy for the two experiments are tabulated in Table 2.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ The Unraveler therefore uses the XTAG grammar of English (XTAG-Group, 1999) to produce a lattice of all possible linearizations that are compatible with the supertagged tree.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ We rank these word sequences in the order of their likelihood by composing the lattice with a finite-state machine representing a trigram language model.

 $$$$$ Here we summarize two experiments that we have performed that use different tree models.
 $$$$$ In Section 5 we discuss some, of the -many problematic issues related to the use of metrics and our metrics in particular, and discuss on-going work.

(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ Moreover, in many cases it is very important not to deviate from very specific output in generation (e.g., maritime weather reports), in which case hand-crafted grammars give excellent control.
(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ The input to the system is a dependency tree as shown in Figure 2.1 Note that the nodes are unordered and are labeled only with lexemes, not with any sort of syntactic annotations.2 The Tree Chooser uses a stochastic tree model to choose syntactic properties (expressed as trees in a Tree Adjoining Grammar) for the nodes in the input structure.
(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ This model was also significant: F(3,19) = 7.23. p < 0.005.

It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ Another reason for relaxing the quality of the output may be that not enough time is available to develop a full grammar. for a new target language in NLG.
It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ While the string-based metrics are very easy to apply, they have the disadvantage that they do not reflect the intuition that all token moves are not equally &quot;bad&quot;.
It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ The metric is summarized in Equation (1).
It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ (For a more detailed comparisons of different tree models, see (Bangalore and Rainbow, 2000).)

This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ The simple tree metric was designed to measure the quality of a sentence and it has a positive coefficient.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ This is particularly worrisome in our case, since in our evaluation scenario the generated sentence is a permutation of the tokens in the reference string.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ We now turn to model for quality.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ FERGUS, a realization module, follows Knight and Langkilde's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a lexicalized syntactic grammar.
