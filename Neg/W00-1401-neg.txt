Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ The results are shown in Table 3.
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ In Section 5 we discuss some, of the -many problematic issues related to the use of metrics and our metrics in particular, and discuss on-going work.
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ Certain generation applications may profit from the use of stochastic methods.
Su et al (1992), Alshawi et al (1998) and Bangalore et al (2000) employ string edit distance between reference and output sentences to gauge output quality for MT and generation. $$$$$ In cases where the XTAG grammar allows a daughter node to be attached at more than one place in the mother supertag (as is the case in our example for was and for; generally, such underspecification occurs with adjuncts and with arguments if their syntactic role is not specified), a disjunction of all these positions is assigned to the daughter node.

More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ FERGUS, a realization module, follows Knight and Langkilde's seminal work in using an n-gram language model, but we augment it with a tree-based stochastic model and a lexicalized syntactic grammar.
More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ The input to the system is a dependency tree as shown in Figure 2.1 Note that the nodes are unordered and are labeled only with lexemes, not with any sort of syntactic annotations.2 The Tree Chooser uses a stochastic tree model to choose syntactic properties (expressed as trees in a Tree Adjoining Grammar) for the nodes in the input structure.
More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ In developing stochastic methods, it is crucial to be able to quickly assess the relative merits of different approaches or models.
More on these and other metrics can be found in [Bangalore et al, 2000]. $$$$$ For many applications in natural language generation (NLG), the range of linguistic expressions that must be generated is quite restricted, and a grammar for a surface realization component can be fully specified by hand.

Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include $$$$$ As a result, the simple string accuracy metric may .be,:negative (though_ Bever -greater than 1, of course).
Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include $$$$$ (roughly 5This shows the importance of the alignment algorithm in the definition of these two metrics: had it. not aligned phase and cost. as a substitution (but each with an empty position in .the other!string-instead),. then time simple string accuracy would have 6 errors instead or 5, but the generation string accuracy would have 3 errors instead of 4. speaking) creating discontinuous constituents.
Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include $$$$$ As is the case for stochastic approaches in natural language understanding, the research and development itself requires an effective intrinsic metric in order to be able to evaluate progress.
Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include $$$$$ Certain generation applications may profit from the use of stochastic methods.

The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ (For a more detailed comparisons of different tree models, see (Bangalore and Rainbow, 2000).)
The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ In all these cases, stochastic methods provide an alternative to hand-crafted approaches to NLG.
The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ The simple accuracy, generation accuracy, simple tree accuracy and generation tree accuracy for the two experiments are tabulated in Table 2.
The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al (2000). $$$$$ Given the variance between subjects we first normalized the data.

Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ Specifically, the daughter nodes are ordered with respect to the head at each level of the derivation tree.
Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
Bangalore et al describe some of the quantitative measures that have been used in (Bangalore et al, 2000). $$$$$ Given the variance between subjects we first normalized the data.

In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ The experiment confirms that intrinsic metrics cannot replace human evaluation, but some correlate significantly with human judgments of quality and understandability and can be used for evaluation during development.
In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ In Section 5 we discuss some, of the -many problematic issues related to the use of metrics and our metrics in particular, and discuss on-going work.
In contrast, quantitative metrics for automatic evaluation of surface realisers have been developed (Bangalore et al, 2000) and they have been shown to correlate well with human judgement for quality and understandability. $$$$$ In these cases, evaluations of the generator that rely on human judgments (Lester and Porter, 1997) or on human annotation of the test corpora (Kukich, 1983) are quite sufficient.


Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms $$$$$ Consider the following example.
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms $$$$$ This is of course unrealistic for applications â€¢ see Section 5 for further remarks.
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms $$$$$ The Unraveler therefore uses the XTAG grammar of English (XTAG-Group, 1999) to produce a lattice of all possible linearizations that are compatible with the supertagged tree.
Kay (1996) identified parsing charts as such an architecture, which led to the development of various chart generation algorithms $$$$$ In this paper, we present several types of intrinsic (system internal) metrics which we have used for baseline quantitative assessment.

Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ The model is plotted in Figure 4, with the data point representing the removed outlier at the top of the diagram.
Similarly, the metrics proposed for text generation by (Bangalore et al, 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output. $$$$$ Instead, we follow work done in dialog systems (Walker et al., 1997) and attempt to find metrics which on the one hand can be computed easily but on the other hand correlate with empirically verified human judgments in qualitative categories such as readability.

 $$$$$ The Unraveler therefore uses the XTAG grammar of English (XTAG-Group, 1999) to produce a lattice of all possible linearizations that are compatible with the supertagged tree.
 $$$$$ The difference between the two strings is that the first scrambled string, but not the second, can be read off from the dependency tree for the sentence (as shown in Figure 2) without violation of projectivity, i.e., without.

We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ In all these cases, stochastic methods provide an alternative to hand-crafted approaches to NLG.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ An alignment algorithm using substitution, insertion and deletion of tokens as operations attempts to match the generated string with the reference string.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ Moreover, in many cases it is very important not to deviate from very specific output in generation (e.g., maritime weather reports), in which case hand-crafted grammars give excellent control.
We also used the version of string-edit distance described by Bangalore et al (2000) which normalises for length. $$$$$ Normalized quality = 1.2134*simple tree accuracy - 0.0839*substitutions - 0.0280 * length - 0.0689.

 $$$$$ This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.
 $$$$$ However, it would of course also be possible to use a grammar that allows for the copula-headed analysis.
 $$$$$ (As for the previously introduced metrics, the scores may be less than 0.)
 $$$$$ In our example sentence (2), we see that the insertion and deletion of no can be collapsed into one move.

(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ As is the case for stochastic approaches in natural language understanding, the research and development itself requires an effective intrinsic metric in order to be able to evaluate progress.
(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ We therefore use models that just use the simple tree accuracy and the number of substitutions as independent variables.
(Bangalore et al, 2000) finds this metric to correlate well with human judgments of understandability and quality. $$$$$ This quantitative assessment should then be augmented to a fuller evaluation that examines qualitative aspects.

It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ The structure of the paper is as follows.
It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ The input to the system is a dependency tree as shown in Figure 2.1 Note that the nodes are unordered and are labeled only with lexemes, not with any sort of syntactic annotations.2 The Tree Chooser uses a stochastic tree model to choose syntactic properties (expressed as trees in a Tree Adjoining Grammar) for the nodes in the input structure.
It is not appropriate to reward the mere presence (regardless of place in the string) of, say, by midnight (which is what some evaluation metrics are specifically designed to do, e.g. [Bangalore et al, 2000]). $$$$$ In Section 2. we briefly describe the architecture of FERGUS, and some of the modules.

This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ (Mel'auk, 1988)), so that a violation of projectivity is presumably a more severe error than a word order variation that does not violate projectivity.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ A bottom-up algorithm then constructs a lattice that encodes the strings represented by each level of the derivation tree.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ Each of these operations is assigned a cost value such that a substitution operation is cheaper than the combined cost of a deletion and an insertion operation.
This metric correlates significantly with human judgments and is better than Simple String Accuracy (Bangalore et al, 2000) for judging compression quality (Clarke and Lapata, 2006). $$$$$ To this end, we describe an experiment that tests correlation between the quantitative metrics and human qualitative judgment.
