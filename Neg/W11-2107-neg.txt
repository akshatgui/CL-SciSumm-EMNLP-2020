Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. $$$$$ In addition, the fragmentation penalties are generally less severe across languages.
Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. $$$$$ Any word with relative frequency of 10−3 or greater is added to the function word list.
Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. $$$$$ We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system.
Table 2 lists the BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011) scores of both systems. $$$$$ To evaluate a metric’s performance on a data set, we count the number of pairwise translation rankings preserved when translations are re-ranked by metric score.

To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics $$$$$ New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.
To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics $$$$$ To evaluate a metric’s performance on a data set, we count the number of pairwise translation rankings preserved when translations are re-ranked by metric score.
To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics $$$$$ Synonym: Match words if they share membership in any synonym set according to the WordNet (Miller and Fellbaum, 2007) database.
To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonly used evaluation metrics $$$$$ We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists.

For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). $$$$$ In addition, the fragmentation penalties are generally less severe across languages.
For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). $$$$$ This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.
For the current evaluation phase four automatic evaluation metrics have been employed, i.e. BLEU (Papineni et al, 2002), NIST (NIST 2002), Meteor (Denkowski and Lavie, 2011) and TER (Snover et al, 2006). $$$$$ We compare Meteor 1.3 results with those from version 1.2 with results shown in Table 6.

Inverted Automatic Scores $$$$$ The following two rules are particularly helpful: Consider the behavior of the Moses tokenizer and Meteor normalizers given a reference translation containing the phrase “U.S.-based organization”: Of these, only the Meteor 1.3 normalization allows metrics to match all of the following stylizations: While intended for Meteor evaluation, use of this normalizer is a suitable preprocessing step for other metrics to improve accuracy when reference sentences are stylistically different from hypotheses.
Inverted Automatic Scores $$$$$ This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.
Inverted Automatic Scores $$$$$ Calculate weighted precision and recall using matcher weights (wi...wn) and content-function word weight (δ): The parameterized harmonic mean of P and R (van Rijsbergen, 1979) is then calculated: To account for gaps and differences in word order, a fragmentation penalty is calculated using the total number of matched words (m, average over hypothesis and reference) and number of chunks (ch): The parameters α, β, γ, δ, and wi...wn are tuned to maximize correlation with human judgments.

We report two translation measures $$$$$ The 2009 and 2010 WMT shared evaluation data sets are made available as development data for WMT 2011.
We report two translation measures $$$$$ Commonly used metrics such as BLEU and earlier versions of Meteor make no distinction between content and function words.
We report two translation measures $$$$$ In addition, the fragmentation penalties are generally less severe across languages.
We report two translation measures $$$$$ This illustrates the differences between evaluation and tuning tasks.

System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). $$$$$ As listed in Table 7, these parameters are often skewed to emphasize the differences between system outputs.
System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). $$$$$ The 2009 and 2010 WMT shared evaluation data sets are made available as development data for WMT 2011.
System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). $$$$$ The 2009 and 2010 WMT shared evaluation data sets are made available as development data for WMT 2011.
System performance is evaluated on newstest 2011 using BLEU (uncased and cased) (Papineni et al, 2002), Meteor (Denkowski and Lavie, 2011), and TER (Snover et al, 2006). $$$$$ New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.

The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). $$$$$ All matches are generalized to phrase matches with a start position and phrase length in each sentence.
The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). $$$$$ New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.
The baseline results (non-factored model) under the standard evaluation metrics are shown in the first row of Table 3 in terms of BLEU (Papineni et al, 2002) and METEOR (Denkowski and Lavie, 2011). $$$$$ The versions of Meteor corresponding to the translation evaluation task submissions, (Ranking and Adequacy), are described in Sections 3 through 5 while the submission to the tunable metrics task, (Tuning), is described in Section 6.

The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ The source code and all resources for Meteor 1.3 and the version of Z-MERT with Meteor integration will be available for download from the Meteor website.
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ A standard SRI 5-gram language model (Stolke, 2002) is estimated from monolingual data.
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ The exact and paraphrase matchers support all five WMT languages while the stem matcher is limited to English, French, German, and Spanish and the synonym matcher is limited to English.
The quotient is lower under the automatic metrics Meteor (Version 1.3, (Denkowski and Lavie, 2011)), BLEU and TERp (Snover et al, 2009). $$$$$ The source code and all resources for Meteor 1.3 and the version of Z-MERT with Meteor integration will be available for download from the Meteor website.

The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. $$$$$ The results are fairly consistent across both test sets: the Tuning version of Meteor outperforms BLEU across all metrics while versions of Meteor that perform well on other tasks perform poorly in tuning.
The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. $$$$$ For a hypothesis-reference pair, the search space of possible alignments is constructed by identifying all possible matches between the two sentences according to the following matchers: Exact: Match words if their surface forms are identical.
The Meteor scoring tool (Denkowski and Lavie, 2011) for evaluating the output of statistical machine translation systems can be used to calculate the similarity of two sentences in the same language. $$$$$ In addition to improving accuracy, the reduction of phrase table sizes also reduces the load time and memory usage of the Meteor paraphrase matcher.

We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. $$$$$ Meteor 1.3 shows significantly higher correlation on both tune and test data for English, French, and Spanish while Czech and German demonstrate overfitting with higher correlation on tune data but lower on test data.
We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. $$$$$ The additional features in Meteor 1.3 allow for more balanced parameters that distribute responsibility for penalizing various types of erroneous translations.
We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. $$$$$ We also conduct a MT system tuning experiment on Urdu-English data to compare the effectiveness of using multiple versions of Meteor in minimum error rate training.
We evaluate translation quality using BLEU score (Papineni et al, 2002), both on the word and character level (with n= 4), as well as METEOR (Denkowski and Lavie, 2011) on the word level. $$$$$ Once matches are identified, the final alignment is resolved as the largest subset of all matches meeting the following criteria in order of importance: Given an alignment, the metric score is calculated as follows.

 $$$$$ We compare Meteor 1.3 results with those from version 1.2 with results shown in Table 6.
 $$$$$ Phrases are extracted using standard phrase-based heuristics (Koehn et al., 2003) and used to build a translation table and lexicalized reordering model.

We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. $$$$$ New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.
We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. $$$$$ The evaluation resources are modular, usable with any other evaluation metric or MT software.
We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. $$$$$ The Tuning version of Meteor is shown to outperform BLEU in minimum error rate training of a phrase-based system on small Urdu-English data and we believe that it will generalize well to other tuning scenarios.
We used BLEU 4 (Papineni et al, 2002), METEOR (v.1.3) (Denkowski and Lavie, 2011) to evaluate the texts at document level. $$$$$ For each of the matchers (mi), count the number of content and function words covered by matches of this type in the hypothesis (mi(hc), mi(hf)) and reference (mi(rc), mi(rf)).
