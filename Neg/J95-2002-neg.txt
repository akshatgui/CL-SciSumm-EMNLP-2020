This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. $$$$$ Thus, the present algorithm fills a gap in the existing array of algorithms for SCFGs, efficiently combining the functionalities and advantages of several previous approaches.
This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. $$$$$ Reverse completion.
This adaptation, related to that of (Stolcke 1995), involves reformulating the Earley algorithm to work with probabilistic recursive transition networks rather than with deterministic production rules. $$$$$ From now on, we will use &quot;derivation&quot; to imply a left-most derivation.

For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995). $$$$$ Rationale.
For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995). $$$$$ The probabilities of all rules with the same nonterminal X on the LHS must therefore sum to unity.
For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995). $$$$$ Both Nakagawa (1987) and Easeler (1988) use a nonprobabilistic Earley parser augmented with &quot;word match&quot; scoring.
For the Berkeley grammar, we use a probabilistic Earley parser modified by Levy to calculate exact prefix probabilities using the algorithm of Stolcke (1995). $$$$$ In this section we examine the computation of production count expectations required for the E-step.

Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). $$$$$ Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.
Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). $$$$$ The terminology used here is taken from Booth and Thompson (1973).
Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). $$$$$ Strings of mixed nonterminal and terminal symbols are written using lowercase Greek letters A, it, v. The empty string is denoted by E.
Efficient algorithms for its solution have been proposed by Jelinek and Lafferty (1991) and Stolcke (1995). $$$$$ Thanks are due Dan Jurafsky and Steve Omohundro for extensive discussions on the topics in this paper, and Fernando Pereira for helpful advice and pointers.

Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al (1999). $$$$$ In Section 4.5 we defined the probabilistic left-corner and unit-production matrices RL and Ru, respectively, to collapse recursions in the prediction and completion steps.
Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al (1999). $$$$$ In Section 4.5 we defined the probabilistic left-corner and unit-production matrices RL and Ru, respectively, to collapse recursions in the prediction and completion steps.
Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al (1999). $$$$$ We observe that the same property holds not only for S, but for all nonterminals, if the grammar has no useless terminals.
Nonetheless, the partition function can still be approximated to any degree of precision by iterative computation of the relation in (4), as done for instance by Stolcke (1995) and by Abney et al (1999). $$$$$ Jerry Feldman, Terry Regier, Jonathan Segal, Kevin Thompson, and the anonymous reviewers provided valuable comments for improving content and presentation.

Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. $$$$$ Thanks are due Dan Jurafsky and Steve Omohundro for extensive discussions on the topics in this paper, and Fernando Pereira for helpful advice and pointers.
Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. $$$$$ The forward and inner probabilities of the states thus created are those of the first state X —> .Y1 ... Y,_1Y,A, multiplied by factors that account for the implied Eexpansions.
Note that the algorithms for the computation of prefix probabilities by Jelinek and Lafferty (1991) and Stolcke (1995) do allow incrementality, which contributes to their practical usefulness for speech recognition. $$$$$ A detailed account of this technique is given in Stolcke (1993).

Two of these, P(X) and S(X) are just the prefix and suffix probability distributions for the symbol(Stolcke, 1995) $$$$$ All of the applications listed above involve (or could potentially make use of) one or more of the following standard tasks, compiled by Jelinek and Lafferty (1991)) The algorithm described in this article can compute solutions to all four of these problems in a single framework, with a number of additional advantages over previously presented isolated solutions.
Two of these, P(X) and S(X) are just the prefix and suffix probability distributions for the symbol(Stolcke, 1995) $$$$$ It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm.
Two of these, P(X) and S(X) are just the prefix and suffix probability distributions for the symbol(Stolcke, 1995) $$$$$ It does not require rewriting the grammar into normal form.
Two of these, P(X) and S(X) are just the prefix and suffix probability distributions for the symbol(Stolcke, 1995) $$$$$ States are derived from productions in the grammar.

Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. $$$$$ This grammar will cause the Earley parser to find all partial parses of substrings, effectively behaving like a bottom-up parser constructing the chart in left-to-right fashion.
Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. $$$$$ But that probability tends to 0 as n oo, and hence so must each entry in P. For the unit production matrix Pu a similar argument applies, since the length of a derivation is at least as long as it takes to terminate any initial unit production chain.
Stolcke (1995) summarizes extensively their approach to utilize probabilistic Earley parsing. $$$$$ For an SCFG G over an alphabet E, with start symbol S, we say that' a) G is proper iff for all nonterminals X the rule&quot; probabilities sum to unity, i.e., where P(S 4 x) is induced by the rule probabilities according to Definition 1(a). c) G has no useless nonterminals iff all nonterminals X appear in at least one derivation of some string x E E* with nonzero probability, i.e., P(S AX 4' x) > 0.

The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. $$$$$ The terminology used here is taken from Booth and Thompson (1973).
The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. $$$$$ For each complete state (move the dot over the current nonterminal).
The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. $$$$$ Because the current position (dot) also refers to the same input index in all states, all nonterminals Xi, X2, ..., X, have been expanded into the same substring of the input between ki and the current position.
The above task has some resemblance to probabilistic context-free grammar (PCFG) parsing for which efficient algorithms are available (Stolcke, 1995), but we note that our task of finding the most probable semantic derivation differs from PCFG parsing in two important ways. $$$$$ Therefore, the choice of the production X —> Apt is not part of the outer probability associated with a state kX A.A.

Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). $$$$$ The terminology used here is taken from Booth and Thompson (1973).
Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). $$$$$ For each state
Therefore, we use an Earley-style probabilistic parser, which outputs Viterbi parses (Stolcke, 1995). $$$$$ The standard computational technique for Viterbi parses is applicable here.

A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. $$$$$ Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.
A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. $$$$$ In Section 4.5 we defined the probabilistic left-corner and unit-production matrices RL and Ru, respectively, to collapse recursions in the prediction and completion steps.
A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. $$$$$ Before going into the details of computing outer probabilities, we describe their use in obtaining the expected rule counts needed for the E-step in grammar estimation.
A third approach is to calculate the forward probability (Stolcke, 1995) of the sentence using a PCFG. $$$$$ In this appendix we give a proof that the existence of these inverses is assured if the grammar is well-defined in the following three senses.

The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm. $$$$$ As explained in Section 4.5, one has to perform a single pass over the current state set, identifying all nonterminals Z occurring to the right of dots, and add states corresponding to all productions Y v that are reachable through the left-corner relation Z =L Y.
The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm. $$$$$ In Section 7 we summarize and draw some conclusions.
The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm. $$$$$ The recursive instance of the parser is passed any predicted states at that position, processes the input up to the matching right parenthesis, and hands complete states back to the invoking instance.
The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley's algorithm. $$$$$ Although clearly not a perfect model of natural language, stochastic context-free grammars (SCFGs) are superior to nonprobabilistic CFGs, with probability theory providing a sound theoretical basis for ranking and pruning of parses, as well as for integration with models for nonsyntactic aspects of language.

In some special cases only a linear solver is needed $$$$$ Briscoe and Carroll (1993) turn this incongruity into an advantage by using the LR parser as a probabilistic model in its own right, and show how LR probabilities can be extended to capture non—context-free contingencies.
In some special cases only a linear solver is needed $$$$$ A simple extension of the Earley chart allows finding partial parses of ungrammatical input.
In some special cases only a linear solver is needed $$$$$ It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm.

Monolingual parsing with unary productions is fairly straightforward (Stolcke,1995), however in the transductive setting these rules can licence infinite insertions in the target string. $$$$$ However, for fully parameterized grammars in CNF we can verify the scaling of the algorithm in terms of the number of nonterminals n, and verify that it has the same 0(n3) time and space requirements as the Inside/Outside (I/O) and LRI algorithms.
Monolingual parsing with unary productions is fairly straightforward (Stolcke,1995), however in the transductive setting these rules can licence infinite insertions in the target string. $$$$$ In Section 4.5 we defined the probabilistic left-corner and unit-production matrices RL and Ru, respectively, to collapse recursions in the prediction and completion steps.
Monolingual parsing with unary productions is fairly straightforward (Stolcke,1995), however in the transductive setting these rules can licence infinite insertions in the target string. $$$$$ A simple extension of the Earley chart allows finding partial parses of ungrammatical input.

An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). $$$$$ Because of the possible start indices each state set can contain 0(1) Earley states, giving 0(12) worst-case space complexity overall.
An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). $$$$$ One such dimension is whether the quantities entered into the parser chart are defined in a bottom-up (CYK) fashion, or whether left-to-right constraints are an inherent part of their definition.19 The probabilistic Earley parser shares the inherent left-to-right character of the LRI algorithm, and contrasts with the bottom-up I/O algorithm.
An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). $$$$$ Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input.
An Earley chart is used for keeping track of all derivations that are consistent with the input (Stolcke, 1995). $$$$$ The recurrence for RL can be conveniently written in matrix notation from which the closed-form solution is derived: An existence proof for RL is given in Appendix A.

This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) . $$$$$ In many applications ungrammatical input has to be dealt with in some way.
This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) . $$$$$ We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities.
This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) . $$$$$ One of the major alternative context-free parsing paradigms besides Earley's algorithm is LR parsing (Aho and Ullman 1972).
This problem has been studied by Jelinek and Lafferty (1991) and by Stolcke (1995) . $$$$$ Definition 1 The following quantities are defined relative to a SCFG G, a nonterminal X, and a string x over the alphabet E of G. where i, v2,. vk are strings of terminals and nonterminals, X -4 A is a production of G, and 1)2 is derived from vi by replacing one occurrence of X with A. b) The string probability P(X 4 x) (of x given X) is the sum of the probabilities of all left-most derivations X • • •= x producing x from X.' c) The sentence probability P(S x) (of x given G) is the string probability given the start symbol S of G. By definition, this is also the probability P(x I G) assigned to x by the grammar G. d) The prefix probability P(S 4L x) (of x given G) is the sum of the probabilities of all sentence strings having x as a prefix, In the following, we assume that the probabilities in a SCFG are proper and consistent as defined in Booth and Thompson (1973), and that the grammar contains no useless nonterminals (ones that can never appear in a derivation).

Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. $$$$$ Both of these approaches are restricted to grammars without unbounded ambiguities, which can arise from unit or null productions.
Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. $$$$$ The following is essentially a restatement of Lemma 2 in terms of forward and inner probabilities.
Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. $$$$$ States are derived from productions in the grammar.
Our solution to the problem of computing prefix probabilities is formulated in quite different terms from the solutions by Jelinek and Lafferty (1991) and by Stolcke (1995) for probabilistic context-free grammars. $$$$$ The inner probabilities, on the other hand, represent the probability of generating a substring of the input from a given nonterminal, using a particular production.

This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. $$$$$ B.3 Efficient Parsing with Large Sparse Grammars During work with a moderate-sized, application-specific natural language grammar taken from the BeRP speech system (Jurafsky, Wooters, Tajchman, Segal, Stolcke, Foster, and Morgan 1994) we had an opportunity to optimize our implementation of the algorithm.
This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. $$$$$ The completion pass can now be implemented as follows.
This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. $$$$$ Most probabilistic parsers are based on a generalization of bottom-up chart parsing, such as the CYK algorithm.
This contrasts with the techniques proposed by Jelinek and Lafferty (1991) and by Stolcke (1995), which are extensions of parsing algorithms for probabilistic context-free grammars, and require considerably more involved proofs of correctness. $$$$$ The following definitions are relative to a given SCFG G. As before, a matrix inversion can compute the relation Ru in closed form: The existence of Ru is shown in Appendix A.

An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al (1999), and Stolcke (1995). $$$$$ • • xi) = P(xo )/P(xo • .. xi).
An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al (1999), and Stolcke (1995). $$$$$ Unlike bottom-up parsers it also computes accurate prefix probabilities incrementally while scanning its input, along with the usual substring (inside) probabilities.
An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al (1999), and Stolcke (1995). $$$$$ The crucial notion introduced by Baker (1979) for this purpose is the &quot;outer probability&quot; of a nonterminal, or the joint probability that the nonterminal is generated with a given prefix and suffix of terminals.
An additional complication with our construction is that finding any of the values in (3) may involve solving a system of non-linear equations, similarly to the case of probabilistic context-free grammars; see again Abney et al (1999), and Stolcke (1995). $$$$$ Essentially the same method can be used in the Earley framework, after extending the definition of outer probabilities to apply to arbitrary Earley states.

These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). $$$$$ A state produced by prediction is called a predicted state.
These quantities can be computed to any degree of precision, as discussed for instance in (Booth and Thompson, 1973) and (Stolcke, 1995). $$$$$ Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input.

A probabilistic Earley parser can retrieve all possible derivations at (Stolcke, 1995). $$$$$ These restrictions ensure that all nonterminals define probability measures over strings; i.e., P(X x) is a proper distribution over x for all X.
A probabilistic Earley parser can retrieve all possible derivations at (Stolcke, 1995). $$$$$ The input alphabet is denoted by E. Substrings are identified by beginning and end positions x..1.
A probabilistic Earley parser can retrieve all possible derivations at (Stolcke, 1995). $$$$$ In this respect the Earley approach contrasts with both the CNF-oriented I/O and LRI algorithms.
A probabilistic Earley parser can retrieve all possible derivations at (Stolcke, 1995). $$$$$ All measurements were obtained on a Sun SPARCstation 2 with a CommonLisp/CLOS implementation of generic sparse matrices that was not particularly optimized for this task. matrix with rows indexed by nonterminals and columns indexed by terminals.
