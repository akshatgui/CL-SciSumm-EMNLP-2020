Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ Two-Pass Parsing To investigate the trade-off between precision and recall, we report results with a two-pass parsing strategy.
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ A key aim in natural language processing is to learn a mapping from natural language sentences to formal representations of their meaning.
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ Finally, there is work on using categorial grammars to solve other, related learning problems.

Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ The GeoQuery data is annotated with both lambda-calculus and variable-free meaning representations, which we have seen examples of throughout the paper.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ We compute translation scores for (word, constant) pairs that cooccur in examples in the training data.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ A CCG grammar consists of a language-specific lexicon, whose entries pair individual words and phrases with both syntactic and semantic information, and a universal set of combinatory rules that project that lexicon onto the sentences and meanings of the language via syntactic derivations.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).

Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ For parsing and parameter estimation, we use standard algorithms (Clark & Curran, 2007), as described below.
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.

Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ Kwiatkowski was supported by an EPRSC studentship.
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ Finally, Section 4.3 defines the full set of lexical entry pairs that can be created by splitting a lexical entry.

The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ For a more qualitative evaluation, Table 4 shows a selection of lexical items learned with high weights for the lambda-calculus meaning representations.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ Second, we include semantic features that are functions of the output logical expression z.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.

 $$$$$ For each pair (C, wi:j), NEW-LEX considers introducing a new lexical item wi:j `C, which allows for the possibility of a parse where the subtree rooted at C is replaced with this new entry.
 $$$$$ Features and initial feature weights are described in Section 7.
 $$$$$ We define the set of possible splits for a category X:h with syntax X and logical form h by enumerating the solution pairs (f, g) to the higher-order unification problems defined above and creating syntactic categories for the resulting expressions.

An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ The induced grammar is also typically highly ambiguous, producing a large number of possible analyses for each sentence.
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ For a more qualitative evaluation, Table 4 shows a selection of lexical items learned with high weights for the lambda-calculus meaning representations.
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.

For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ Skipping words can potentially increase recall, if the ignored word is an unknown function word that does not contribute semantic content.
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ First, new lexical items are induced for the training instance by splitting and merging nodes in the best correct parse, given the current parameters.
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ The space of possible lexical items supported by this splitting procedure is too large to explicitly enumerate.
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.

WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ For example, T(tex) = e and T(Ax.state(x)) = (e, t).
WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ Step 1: Updating the Lexicon In the lexical update step the algorithm first computes the best correct parse tree y* for the current training example and then uses y* as input to the procedure NEW-LEX, which determines which (if any) new lexical items to add to A. NEW-LEX begins by enumerating all pairs (C, wi:j), for i < j, where C is a category occurring at a node in y* and wi:j are the (two or more) words it spans.
WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ 4.

As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ The weights for the lexical features are initialized according to coocurrance statistics estimated with the Giza++ (Och & Ney, 2003) implementation of IBM Model 1.
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ As we will see, this splitting process is overly prolific for any single language and will yield many lexical items that do not generalize well.
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer & Collins (2005).
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z.

We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ The new syntactic category for g is determined based on its type, T(g).
We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ Kwiatkowski was supported by an EPRSC studentship.
We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ Another line of work has focused on recovering meaning representations that are not based on logic.
We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.

 $$$$$ We thank the reviewers for useful feedback.
 $$$$$ For example, in the left parse in Figure 1, there would be four pairs: one with the category C = NP\NP:Ax.border(x) and the phrase wi:j =“ye siniri vardir”, and one for each non-leaf node in the tree.
 $$$$$ Zettlemoyer & Collins (2005, 2007) developed CCG grammar induction techniques where lexical items are proposed according to a set of hand-engineered lexical templates.
 $$$$$ Our approach discriminates between analyses using a log-linear CCG parsing model, similar to those used in previous work (Clark & Curran, 2003, 2007), but differing in that the syntactic parses are treated as a hidden variable during training, following the approach of Zettlemoyer & Collins (2005, 2007).

In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ For both meaning representations the model learns significantly more multiword lexical items for the somewhat analytic Japanese than the agglutinative Turkish.
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ Kwiatkowski was supported by an EPRSC studentship.
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.

This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). $$$$$ Other algorithms have been designed to recover lambda-calculus representations.
This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). $$$$$ These entries are iteratively refined with a restricted higher-order unification procedure (Huet, 1975) that defines all possible ways to subdivide them, consistent with the requirement that each training sentence can still be parsed to yield its labeled meaning.
This set is generated with the functional mapping T:{t}= T (s, m), which is defined, following Kwiatkowski et al (2010), using only the CCG combinators and a mapping from semantic type to syntactic category (presented in in Section 4). $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).

Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ For each pair (C, wi:j), NEW-LEX considers introducing a new lexical item wi:j `C, which allows for the possibility of a parse where the subtree rooted at C is replaced with this new entry.
Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ To limit the number of possible splits, we enforce the following restrictions on the possible higherorder solutions that will be used during learning: Together, these three restrictions guarantee that the number of splits is, in the worst case, an Ndegree polynomial of the number of constants in h. The constraints were designed to increase the efficiency of the splitting algorithm without impacting performance on the development data.
Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.
Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ For both meaning representations the model learns significantly more multiword lexical items for the somewhat analytic Japanese than the agglutinative Turkish.

For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ To limit the number of possible splits, we enforce the following restrictions on the possible higherorder solutions that will be used during learning: Together, these three restrictions guarantee that the number of splits is, in the worst case, an Ndegree polynomial of the number of constants in h. The constraints were designed to increase the efficiency of the splitting algorithm without impacting performance on the development data.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ For example, given the lexicon above, the sentence New York borders Vermont can be parsed to produce: where each step in the parse is labeled with the combinatory rule (− > or − <) that was used.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.

Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ Skipping words can potentially increase recall, if the ignored word is an unknown function word that does not contribute semantic content.
Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ A CCG grammar consists of a language-specific lexicon, whose entries pair individual words and phrases with both syntactic and semantic information, and a universal set of combinatory rules that project that lexicon onto the sentences and meanings of the language via syntactic derivations.
Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ The process is driven by solving a higher-order unification problem that defines all of the ways of splitting the logical expression into two parts, as described in Section 4.1.

Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.
Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ Inputs: Training set {(xi, zi) : i = 1... n} where each example is a sentence xi paired with a logical form zi.
Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.

We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).
We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. $$$$$ Zettlemoyer was supported by a US NSF International Research Fellowship.
We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; ts Vb (Jonesetal., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al, 2010), which learns a CCGlexicon with semantic annotations; and hybrid tree (Lu et al, 2008), which learns a synchronous generative model over variable-free MRs and NL strings. $$$$$ We used the learning rate α0 = 1.0 and cooling rate c = 10−5 in all training scenarios, and ran the algorithm for T = 20 iterations.

The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z.
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ Such an approach would complement ideas for using high-order unification to model a wider range of language phenomena, such as VP ellipsis (Dalrymple et al., 1991).
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ It is possible to run the algorithm without the initial NP list; we include it to allow direct comparisons with previous approaches, which also included NP lists.
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ Although it would be simple enough to forbid vacuous variables in f and g, the number of solutions would still be exponential in the size of h. For example, when h contains a conjunction, such as h = Ax.city(x) n major(x) n in(x, tex), any subset of the expressions in the conjunction can be assigned to f (or g).
