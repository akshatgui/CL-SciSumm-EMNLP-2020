Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ For example, Wong & Mooney (2007) developed a variant of WASP (A-WASP) specifically designed for this alternate representation.
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ For example, the following two categories are often learned with high weight for the Japanese word “chiisai”: and are treated as distinct entries in the lexicon.
Following previous work (Kwiatkowski et al, 2010), we make use of a higher-order unification learning scheme that defines a space of CCG grammars consistent with the (sentence, logical form) training pairs. $$$$$ The local gradient of the individual parameter 0j associated with feature Oj and training instance (xi, zi) is given by: As with Eq.

Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ The next three subsections define the set of possible splits for any given lexical item.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data.
Kwiatkowski et al (2010) described an approach for language-independent learning that replaces the hand-specified templates with a higher-order-unification-based lexical induction method, but their approach does not scale well to challenging, unedited sentences. $$$$$ We thank the reviewers for useful feedback.

Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ There is less variation and complexity in the learned lexical items for the variable-free representation.
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ We thank the reviewers for useful feedback.
Our approach for learning factored PCCGs extends the work of Kwiatkowski et al (2010), as reviewed in Section 7. $$$$$ The full Geo880 dataset contains 880 (Englishsentence, logical-form) pairs, which we split into a development set of 600 pairs and a test set of 280 pairs, following Zettlemoyer & Collins (2005).

Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ Such an approach would complement ideas for using high-order unification to model a wider range of language phenomena, such as VP ellipsis (Dalrymple et al., 1991).
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.
Our Factored Unification Based Learning (FUBL) method extends the UBL algorithm (Kwiatkowski et al, 2010) to induce factored lexicons, while also simultanously estimating the parameters of a log linear CCG parsing model. $$$$$ The distributions p(y|x, z; B, A) and p(y, z|x; B, A) are defined by the log-linear model, as described in Section 3.3.

The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ The reason for generalizing to multiple languages is obvious.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ For both meaning representations the model learns significantly more multiword lexical items for the somewhat analytic Japanese than the agglutinative Turkish.
The overall approach is closely related to the UBL algorithm (Kwiatkowski et al, 2010), but includes extensions for updating the factored lexicon, as motivated in Section 6. $$$$$ 4.

 $$$$$ The previous section described how a splitting procedure can be used to break apart overly specific lexical items into smaller ones that may generalize better to unseen data.
 $$$$$ We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.
 $$$$$ We learn this function by inducing a probabilistic CCG (PCCG) grammar from a training set {(xZ, zz)|i = 1... n} containing example (sentence, logical-form) pairs such as (“New York borders Vermont”, next to(ny, vt)).
 $$$$$ These values were selected with cross validation on the Geo880 development set, described below.

An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ We represent the meaning of words and phrases using lambda-calculus expressions that can contain constants, quantifiers, logical connectors, and lambda abstractions.
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ Exploring the extent to which these representations are compatible with the logic-based learning approach we developed is an important area for future work.
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ The distributions p(y|x, z; B, A) and p(y, z|x; B, A) are defined by the log-linear model, as described in Section 3.3.
An area for future work is developing an automated way to produce this lexicon, perhaps by extending the recent work on automatic lexicon generation (Kwiatkowski et al2010) to the weakly supervised setting. $$$$$ We will present the approach in two parts.

For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ We thank the reviewers for useful feedback.
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ Recent work has addressed this problem by learning semantic parsers given sentences paired with logical meaning representations (Thompson & Mooney, 2002; Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006, 2007; Zettlemoyer & Collins, 2005, 2007; Lu et al., 2008).
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.
For evaluation, following from Kwiatkowski et al (2010), we reserve 280 sentences for test and train on the remaining 600. $$$$$ In addition to data like the above, this approach can also learn from examples such as: Sentence: hangi eyaletin texas ye siniri vardir Meaning: answer(state(borders(tex))) where the sentence is in Turkish and the meaning representation is a variable-free logical expression of the type that has been used in recent work (Kate et al., 2005; Kate & Mooney, 2006; Wong & Mooney, 2006; Lu et al., 2008).

WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ 3 can be calculated efficiently using the inside-outside algorithm with a CKY-style parsing algorithm.
WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ This paper has presented a method for inducing probabilistic CCGs from sentences paired with logical forms.
WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ One potential limitation is in the constraints we introduced to ensure the tractability of the higher-order unification procedure.
WASP (Wong and Mooney, 2006) and the hybrid tree (Lu et al, 2008) are chosen to represent tree transformation based approaches, and, while this comparison is our primary focus, we also report UBL-S (Kwiatkowski et al, 2010) as a non tree based top-performing system. $$$$$ The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z.

As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ Zettlemoyer was supported by a US NSF International Research Fellowship.
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ We thank the reviewers for useful feedback.
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.
As a starting point, we used the UBL system developed by Kwiatkowski et al (2010) to learn a semantic parser based on probabilistic Combinatory Categorial Grammar (PCCG). $$$$$ For future work, we are interested in exploring the generality of the approach while extending it to new understanding problems.

We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ However, if the learned lexical items are used in too many incorrect parses, the stochastic gradient updates will down weight them to the point where the lexical induction step can merge or re-split nodes in the trees that contain them.
We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ The learning process starts by postulating, for each sentence in the training data, a single multi-word lexical item pairing that sentence with its complete logical form.
We show that it outperforms a state-of-the-art semantic parser (Kwiatkowski et al 2010) when run with similar training conditions (i.e., neither system is given the corpus based initialization originally used by Kwiatkowski et al). $$$$$ The approach uses higher-order unification to define the space of possible grammars in a language- and representation-independent manner, paired with an algorithm that learns a probabilistic parsing model.

 $$$$$ For example, the forward (>) and These rules apply to build syntactic and semantic derivations under the control of the word order information encoded in the slash directions of the lexical entries.
 $$$$$ We thank the reviewers for useful feedback.
 $$$$$ Zettlemoyer was supported by a US NSF International Research Fellowship.
 $$$$$ The goal of our algorithm is to find a function f : x —* z that maps sentences x to logical expressions z.

In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ For example, T(tex) = e and T(Ax.state(x)) = (e, t).
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ We seed the lexical induction with a multi-word lexical item xi`S:zi for each training example (xi, zi), consisting of the entire sentence xi and its associated meaning representation zi.
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ The lexical induction process (Section 4) uses a restricted form of higher order unification along with the CCG combinatory rules to propose new entries for A.
In particular, our approach is closely related that of Kwiatkowski et al (2010) but, whereas that work required careful initialisation and multiple passes over the training data to learn a discriminative parsing model, here we learn a generative parsing model without either. $$$$$ To learn effectively, we will need to split overly specific entries of this type into pairs of new, smaller, entries that generalize better.

This set is generated with the functional mapping T $$$$$ For this representation, lexical items such as: can be used to construct the desired output.
This set is generated with the functional mapping T $$$$$ We find pairs of logical expressions (f, g) such that either f(g) = h or Ax.f(g(x)) = h. Solving these problems creates new expressions f and g that can be recombined according to the CCG combinators, as defined in Section 3.2, to produce h. In the unrestricted case, there can be infinitely many solution pairs (f, g) for a given expression h. For example, when h = tex and f = Ax.tex, the expression g can be anything.
This set is generated with the functional mapping T $$$$$ For example, Buszkowski & Penn (1990) describe a unification-based approach for grammar discovery from bracketed natural language sentences and Villavicencio (2002) developed an approach for modeling child language acquisition.
This set is generated with the functional mapping T $$$$$ This splitting process is used to expand the lexicon during learning.

Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.
Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ Since we initially assign positive weights to the parameters for new lexical items, the overall approach prefers splitting; trees with many lexical items will initially be much more likely.
Here we review the splitting procedure of Kwiatkowski et al (2010) that is used to generate CCG lexical items and describe how it is used by T to create a packed chart representation of all parses {t} that are consistent with s and at least one of the meaning representations in {m}. $$$$$ The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.

For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ Another line of work has focused on recovering meaning representations that are not based on logic.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ We will present the approach in two parts.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ The induced grammar consists of two components which the algorithm must learn: tion over the possible parses y, conditioned on the sentence x.
For comparison we use the UBL semantic parser of Kwiatkowski et al (2010) trained in a similar setting i.e., with no language specific initialisation. $$$$$ For example, one initial lexical item might be: Although these initial, sentential lexical items can parse the training data, they will not generalize well to unseen data.

Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ For present purposes a CCG grammar includes a lexicon A with entries like the following: where each lexical item w �- X : h has words w, a syntactic category X, and a logical form h expressed as a lambda-calculus expression.
Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ We thank the reviewers for useful feedback.
Kwiatkowski et al (2010) initialise lexical weights in their learning algorithm using corpus-wide alignment statistics across words and meaning elements. $$$$$ Set of NP lexical items ANP.

Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ Due to the small size of this dataset we use 10-fold cross validation for evaluation.
Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.
Figure 4 shows accuracy for our approach with and without guessing, for UBL when run over the training data once (UBL1) and for UBL when run over the training data 10 times (UBL10) as in Kwiatkowski et al (2010). $$$$$ In practice, UBL often learns entries with only a single slash, like those above, varying only in the direction, as required for the language.

We also compare the MT-based semantic parsers to several recently published ones $$$$$ The complete learning algorithm (Section 5) integrates this lexical induction with a parameter estimation scheme that learns 0.
We also compare the MT-based semantic parsers to several recently published ones $$$$$ This work was supported by the EU under IST Cognitive Systems grant IP FP6-2004-IST-4-27657 “Paco-Plus” and ERC Advanced Fellowship 249520 “GRAMPLUS” to Steedman.
We also compare the MT-based semantic parsers to several recently published ones $$$$$ We will present the approach in two parts.
We also compare the MT-based semantic parsers to several recently published ones $$$$$ First, new lexical items are induced for the training instance by splitting and merging nodes in the best correct parse, given the current parameters.

The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ Previous work has focused on a variety of different meaning representations.
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ Before presenting the details, we first review necessary background.
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.
The remaining system discussed in this paper, UBL (Kwiatkowski et al, 2010), leverages the fact that the MRL does not simply encode trees, but rather - calculus expressions. $$$$$ First, we include a set of lexical features: For each lexical item L E A, we include a feature OL that fires when L is used.
