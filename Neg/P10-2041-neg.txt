We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ As we might expect, reducing training data by random sampling always increases perplexity.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ Test set perplexity for each of these modifed language models is compared to that of the original version of the model in Table 2.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ Out of the 55566 test set tokens, the number of OOV tokens ranges from 418 (0.75%), for the smallest training set based on in-domain crossentropy scoring, to 20 (0.03%), for training on the full Gigaword corpus.

In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ If the candidate text segments vary greatly in length—e.g., if we partition N into sentences— this correlation can be a serious problem.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.

We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ This not only produces a language model better matched to the domain of interest (as measured in terms of perplexity on held-out in-domain data), but it reduces the computational resources needed to exploit a large amount of non-domain-specific data, since the resources needed to filter a large amount of data are much less (especially in terms of memory) than those required to build a language model from all the data.
We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ Let us imagine that our non-domainspecific corpus N contains an in-domain subcorpus NI, drawn from the same distribution as our in-domain corpus I.
We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ The comparisons implied by Figure 1, however, are only approximate, because each perplexity (even along the same curve) is computed with respect to a different vocabulary, resulting in a different out-of-vocabulary (OOV) rate.

One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ Selecting Gigaword sentences by their cross-entropy according to the Europarl-trained model is effective in reducing both test set perplexity and training corpus size, with an optimum perplexity of 124, obtained with a model built from 36% of the Gigaword corpus.
One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ However, we believe there is reason to be optimistic about this.
One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.

Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation.
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ To further increase the comparability of these Europarl and Gigaword language models, we restricted the vocabulary of both models to the tokens appearing at least twice in the Europarl training data, treating all other tokens as instances of <UNK>.
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ However, we believe there is reason to be optimistic about this.

In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ Let HI(s) be the per-word cross-entropy, according to a language model trained on I, of a text segment s drawn from N. Let HN(s) be the per-word cross-entropy of s according to a language model trained on a random sample of N. We partition N into text segments (e.g., sentences), and score the segments according to HI(s) − HN(s), selecting all text segments whose score is less than a threshold T. This method can be justified by reasoning simliar to that used to derive methods for training binary text classifiers without labeled negative examples (Denis et al., 2002; Elkin and Noto, 2008).
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ We compared our selection method to three other methods.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ We tested this in an experiment not described here in detail, and found it not to be significantly better as a selection criterion than random selection.

For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ Klakow (2000) estimates a unigram language model from the entire non-domain-specific corpus to be selected from, and scores each candidate text segment from that corpus by the change in the log likelihood of the in-domain data according to the unigram model, if that segment were removed from the corpus used to estimate the unigram model.
For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.

Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ However, we believe there is reason to be optimistic about this.
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.

This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ Hence, using sentence probability ratios or log probability differences as our scoring function would result in selecting disproportionately very short sentences.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ We then selected subsets of the Gigaword data corresponding to 8 cutoff points in the cross-entropy difference scores, and trained 4-gram models (again using absolute discounting with a discount of 0.7) on each of these subsets and on the full Gigaword corpus.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.

Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ It can be seen that adjusting the vocabulary in this way, so that all models are based on the same vocabulary, yields only very small changes in the measured test-set perplexity, and these differences are much smaller than the differences between the different selection methods, whichever way the vocabulary of the language models is determined.
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.

Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ This presents a problem, because in virtually any particular application the amount of in-domain data is limited.
Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.
Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.

Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ Those segments whose removal would decrease the log likelihood of the in-domain data more than some threshold are selected.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.

We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel. $$$$$ Lin et al. (1997) and Gao et al.
We consider three methods for extracting domain targeted parallel data from a general corpus: source side cross-entropy (Cross-Ent), source-side cross entropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (b Ml), which is novel. $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.

For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ Next, we scored all the Gigaword sentences by the cross-entropy according to the Europarl-trained model alone.
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ This gets us very close to working with the difference in cross-entropies, because HI(s)−HN(s) is just a length-normalized version of log(P(s|I)) − log(P(s|N)), with the sign reversed.
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ However, we believe there is reason to be optimistic about this.

However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ Out of the 55566 test set tokens, the number of OOV tokens ranges from 418 (0.75%), for the smallest training set based on in-domain crossentropy scoring, to 20 (0.03%), for training on the full Gigaword corpus.
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ Test set perplexity for each of these modifed language models is compared to that of the original version of the model in Table 2.
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ Our method is a fairly simple variant of scoring by perplexity according to an in-domain language model.

For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ Hence, If we could estimate all the probabilities in the right-hand side of this equation, we could use it to select text segments that have a high probability of being in NI.
For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.
For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.
For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ The comparisons implied by Figure 1, however, are only approximate, because each perplexity (even along the same curve) is computed with respect to a different vocabulary, resulting in a different out-of-vocabulary (OOV) rate.

We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ That leaves us only P(NI|N), to estimate, but we really don’t care what P(NI|N) is, because knowing that would still leave us wondering what threshold to set on P (NI|s, N).
We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ We estimated this effect on a 1000-sentence sample of our experimental data described below, and found the correlation between sentence log probability difference and sentence length to be r = −0.92, while the cross-entropy difference was almost uncorrelated with sentence length (r = 0.04).
We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.

These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation.

We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ Hence, using sentence probability ratios or log probability differences as our scoring function would result in selecting disproportionately very short sentences.
We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.

It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ This presents a problem, because in virtually any particular application the amount of in-domain data is limited.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ Hence, If we could estimate all the probabilities in the right-hand side of this equation, we could use it to select text segments that have a high probability of being in NI.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ This presents a problem, because in virtually any particular application the amount of in-domain data is limited.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ Thus it has become standard practice to combine in-domain data with other data, either by combining N-gram counts from in-domain and other data (usually weighting the counts in some way), or building separate language models from different data sources, interpolating the language model probabilities either linearly or log-linearly.
