We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ We estimated this effect on a 1000-sentence sample of our experimental data described below, and found the correlation between sentence log probability difference and sentence length to be r = âˆ’0.92, while the cross-entropy difference was almost uncorrelated with sentence length (r = 0.04).
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ As a baseline, we trained language models on random subsets of the Gigaword corpus of approximately equal size to the data sets produced by the cutoffs we selected for the cross-entropy difference scores.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ The second previous approach does not explicitly make use of an in-domain language model, but is still applicable to our scenario.
We find that the best selection technique is the recently proposed cross-entropy difference method (Moore and Lewis, 2010). $$$$$ However, we believe there is reason to be optimistic about this.

In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.
In cross-entropy difference selection, a sentence's score is the in-domain cross-entropy minus the background cross-entropy (Moore and Lewis, 2010).This technique has been used to supplement European parliamentary text (48M words) with newswire data (3.4B words) (Moore and Lewis, 2010). $$$$$ With this tokenization, the sizes of our data sets in terms of sentences and tokens are shown in Table 1.

We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ It can be seen that adjusting the vocabulary in this way, so that all models are based on the same vocabulary, yields only very small changes in the measured test-set perplexity, and these differences are much smaller than the differences between the different selection methods, whichever way the vocabulary of the language models is determined.
We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ This not only produces a language model better matched to the domain of interest (as measured in terms of perplexity on held-out in-domain data), but it reduces the computational resources needed to exploit a large amount of non-domain-specific data, since the resources needed to filter a large amount of data are much less (especially in terms of memory) than those required to build a language model from all the data.
We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
We found that even for our small amount of in-domain data, the recently proposed cross-entropy difference method was consistently the best (Moore and Lewis, 2010). $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.

One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ Hence, using sentence probability ratios or log probability differences as our scoring function would result in selecting disproportionately very short sentences.
One of the most widely used sentence-selection approaches is that of Moore and Lewis (2010). $$$$$ Our method is a fairly simple variant of scoring by perplexity according to an in-domain language model.

Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ The candidate text segments with perplexity less than some threshold are selected.
Moore and Lewis (2010) test their method by partitioning the in-domain data into training data and test data, both of which are disjoint from the general-domain data. $$$$$ This study is preliminary, however, in that we have not yet shown improved end-to-end task performance applying this approach, such as improved BLEU scores in a machine translation task.

In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ The discounted probability mass at the unigram level was added to the probability of <UNK>.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
In Moore and Lewis (2010), the authors compare several approaches to selecting data for LMand Axelrod et al (2011) extend their ideas and apply them to MT. $$$$$ The test set perplexity for the language model trained on the full Gigaword corpus is 135.

For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.
For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).
For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). $$$$$ As we noted above, this is equivalent to the indomain perplexity scoring method used by Lin et al. (1997) and Gao et al.

Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ However, we believe there is reason to be optimistic about this.
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ This presents a problem, because in virtually any particular application the amount of in-domain data is limited.
Moore and Lewis (2010) propose a method for filtering large quantities of out-of-domain language model training data by comparing the cross-entropy of an in-domain language model and an out-of-domain language model trained on a random sampling of the data. $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.

This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ Next, we scored all the Gigaword sentences by the cross-entropy according to the Europarl-trained model alone.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ As a baseline, we trained language models on random subsets of the Gigaword corpus of approximately equal size to the data sets produced by the cutoffs we selected for the cross-entropy difference scores.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ As a baseline, we trained language models on random subsets of the Gigaword corpus of approximately equal size to the data sets produced by the cutoffs we selected for the cross-entropy difference scores.
This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010). $$$$$ We tested this in an experiment not described here in detail, and found it not to be significantly better as a selection criterion than random selection.

Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ As we might expect, reducing training data by random sampling always increases perplexity.
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
Another perplexity-based approach is that taken by Moore and Lewis (2010), where they use the cross-entropy difference as a ranking function rather than just cross-entropy. $$$$$ Those segments whose removal would decrease the log likelihood of the in-domain data more than some threshold are selected.

Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.
Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ For the nondomain-specific corpus, we used the LDC English Gigaword Third Edition (LDC Catalog No.
Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ However, we believe there is reason to be optimistic about this.
Difference Moore and Lewis (2010) also start with a language model LMI over the in-domain corpus, but then further construct a language model LMO of similar size over the general-domain corpus. $$$$$ The normal practice when using multiple languages models in machine translation seems to be to train models on as much data as feasible from each source, and to depend on feature weight optimization to down-weight the impact of data that is less well-matched to the translation application.

Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ However, we believe there is reason to be optimistic about this.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ In the modified language models, the unigram model based on the selected training set is smoothed by absolute discounting, and backed-off to an unsmoothed unigram model based on the full Gigaword corpus.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ Those segments whose removal would decrease the log likelihood of the in-domain data more than some threshold are selected.
Again, the vocabulary of the language model trained on a subset of the general domain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.

We consider three methods for extracting domain targeted parallel data from a general corpus $$$$$ The discounted probability mass at the unigram level was added to the probability of <UNK>.
We consider three methods for extracting domain targeted parallel data from a general corpus $$$$$ We then selected subsets of the Gigaword data corresponding to 8 cutoff points in the cross-entropy difference scores, and trained 4-gram models (again using absolute discounting with a discount of 0.7) on each of these subsets and on the full Gigaword corpus.
We consider three methods for extracting domain targeted parallel data from a general corpus $$$$$ Selecting Gigaword sentences by their cross-entropy according to the Europarl-trained model is effective in reducing both test set perplexity and training corpus size, with an optimum perplexity of 124, obtained with a model built from 36% of the Gigaword corpus.

For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ Test set perplexity for each of these modifed language models is compared to that of the original version of the model in Table 2.
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ It can be seen that adjusting the vocabulary in this way, so that all models are based on the same vocabulary, yields only very small changes in the measured test-set perplexity, and these differences are much smaller than the differences between the different selection methods, whichever way the vocabulary of the language models is determined.
For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.

However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation.
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ The cross-entropy difference selection method introduced here seems to produce language models that are both a better match to texts in a restricted domain, and require less data for training, than any of the other data selection methods tested.
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ However, we believe there is reason to be optimistic about this.
However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis,2010). $$$$$ OOV tokens in the test data are excluded from the perplexity computation, so the perplexity measurements are not strictly comparable.

For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ Hence, If we could estimate all the probabilities in the right-hand side of this equation, we could use it to select text segments that have a high probability of being in NI.
For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ This produces language models that are normalized over the same vocabulary as a model trained on the full Gigaword corpus; thus the test set has the same OOVs for each model.
For the 109 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). $$$$$ Let us imagine that our non-domainspecific corpus N contains an in-domain subcorpus NI, drawn from the same distribution as our in-domain corpus I.

We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ Next, we scored all the Gigaword sentences by the cross-entropy according to the Europarl-trained model alone.
We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.
We employ the data selection method of (Axelrod et al, 2011), which builds upon (Moore and Lewis, 2010). $$$$$ Log-linear interpolation is particularly popular in statistical machine translation (e.g., Brants et al., 2007), because the interpolation weights can easily be discriminatively trained to optimize an end-to-end translation objective function (such as BLEU) by making the log probability according to each language model a separate feature function in the overall translation model.

These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ Perplexity and cross-entropy are monotonically related, since the perplexity of a string s according to a model M is simply bHmW, where HM(s) is the cross-entropy of s according to M and b is the base with respect to which the cross-entropy is measured (e.g., bits or nats).
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ (2002) both used a method similar to ours, in which the metric used to score text segments is their perplexity according to the in-domain language model.
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
These are related, but our work focuses on machine-translated text. The closest to our approach is the method proposed by Moore and Lewis (2010). $$$$$ Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.

We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ If we consider only the training sets that appear to produce the lowest perplexity for each selection method, however, the spread of OOV counts is much narrower, ranging 53 (0.10%) for best training set based on crossentropy difference scoring, to 20 (0.03%), for random selection.
We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ For all four selection methods, plots of test set perplexity vs. the number of training data tokens selected are displayed in Figure 1.
We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
We compare our method with the method of (Moore and Lewis, 2010) (Cross-Entropy). $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.

It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ When a language model trained on non-domain-specific data is used in a statistical translation model as a separate feature function (as is often the case), lower perplexity on indomain target language test data derived from reference translations corresponds directly to assigning higher language model feature scores to those reference translations, which should in turn lead to translation system output that matches reference translations better.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ This presents a problem, because in virtually any particular application the amount of in-domain data is limited.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ However, we believe there is reason to be optimistic about this.
It seems to be a universal truth that LM performance can always be improved by using more training data (Brants et al., 2007), but only if the training data is reasonably well-matched with the desired output (Moore and Lewis, 2010). $$$$$ We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
