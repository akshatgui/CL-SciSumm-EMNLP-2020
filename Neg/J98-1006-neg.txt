Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. $$$$$ We are also grateful to Paul Bagyenda, Ben Johnson-Laird and Joshua Schecter.
Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. $$$$$ Test results are compared with those from manually tagged training examples.
Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. $$$$$ 1R19528983 and by the Defense Advanced Research Projects Agency, Grant No.
Leacock et al (1998), Agirre and Lopezde Lacalle (2004), and Mihalcea and Moldovan (1999) propose a set of methods for automatic harvesting of web data for the purposes of creating sense annotated corpora. $$$$$ We describe a statistical classifier that combines topical context with local cues to identify a word sense.

The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content. $$$$$ The classifier is used to disambiguate a noun, a verb, and an adjective.
The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content. $$$$$ N00014-91-1634.
The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content. $$$$$ We are indebted to the other members of the WordNet group who have provided advice and technical support: Christiane Fellbaum, Shari Landes, and Randee Tengi.
The following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (Leacocket al, 1998)), three measures based on information content. $$$$$ In the experiments of Section 3.2, these were estimated from the testing materials.

Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). $$$$$ Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). $$$$$ Do they depend on the syntactic category of the target?
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given definition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacocketal., 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). $$$$$ We have drawn on these two traditions, using corpus-based co-occurrence and the lexical knowledge base that is embodied in the WordNet lexicon.

Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). $$$$$ Examples of the different meanings of one thousand common, polysemous, open-class English words are being manually tagged.
Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). $$$$$ We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.
Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). $$$$$ Percentage accounted for by most frequent sense = 80%. the difference between the probability of this sense and that of the second highest is relatively small, the classifier's choice is often incorrect.
Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al, 1998). $$$$$ We describe a statistical classifier that combines topical context with local cues to identify a word sense.

Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ The existence of nontopical senses also limits the applicability of the &quot;one sense per discourse&quot; generalization of Gale, Church, and Yarowsky (1992b), who observed that, within a document, a repeated word is almost always used in the same sense.
Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ Test results are compared with those from manually tagged training examples.
Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ With these same training and testing data, TLC performed at 73% accuracy.
Inspired by the work of (Leacock et al, 1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ The local window width of ±2 was selected after pilot testing on the semantically tagged Brown corpus.

It was first suggested by Leacock et al (1998). $$$$$ The classifier is used to disambiguate a noun, a verb, and an adjective.
It was first suggested by Leacock et al (1998). $$$$$ The classifier is used to disambiguate a noun, a verb, and an adjective.
It was first suggested by Leacock et al (1998). $$$$$ To summarize, local context was more reliable than topical context as an indicator of sense for this verb and this adjective, but slightly less reliable for this noun.

Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. $$$$$ Positions j = —2, —1, 1,2 are used.
Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. $$$$$ Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. $$$$$ Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
Leacock et al (1998) attempted to exclude irrelevant or spurious examples by using only monosemous relatives in WordNet. $$$$$ One approach to this problem is to look for a word that appears in many more topical domains than its total number of senses.

Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. $$$$$ Test results are compared with those from manually tagged training examples.
Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. $$$$$ Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. $$$$$ When the threshold is set to maximize precision, the results are highly reliable and can be used to support an interactive application, such as machine-assisted translation, with the goal of reducing the amount of interaction.
Our approach is somewhat similar to the WordNet based approach of Leacock et al (1998) in that it acquires relatives of a target word from WordNetand extracts co-occurrence frequencies of the relatives from a raw corpus, but our system uses poly semous as well as monosemous relatives. $$$$$ Since the frequencies of the monosemous relatives do not correlate with the frequencies of the senses, prior probabilities must be estimated for classifiers that use them.

Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. $$$$$ We describe a statistical classifier that combines topical context with local cues to identify a word sense.
Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. $$$$$ When the threshold is set to maximize precision, the results are highly reliable and can be used to support an interactive application, such as machine-assisted translation, with the goal of reducing the amount of interaction.
Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. $$$$$ This approach to smoothing has yielded consistently better performance than relying on the Good-Turing values alone. tuation.
Other notable unsupervised and semi-supervised approaches are those of McCarthy et al (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. $$$$$ Our evidence indicates that local context is superior to topical context as an indicator of word sense when using a statistical classifier.

Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ Our evidence indicates that local context is superior to topical context as an indicator of word sense when using a statistical classifier.
Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ The benefits of adding topical to local context alone depend on syntactic category as well as on the characteristics of the individual word.
Inspired by the work of Leacock et al (1998), TSWEB was constructed using monosemous relatives from WN (synonyms ,hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the salient words with distinctive frequency using TFIDF. $$$$$ We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.

Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. $$$$$ A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.
Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. $$$$$ The results of this effort will be a useful resource for training statistical classifiers, but what about the next thousand polysemous words, and the next?
Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. $$$$$ Corpus-based approaches to word sense identification have flexibility and generality but suffer from a knowledge acquisition bottleneck.
Others, such as Leacock et al (1998) and Agirre and Martnez (2004b), used information from WordNet to construct queries which were used to retrieve training examples. $$$$$ The values in the table are based on decisions made on all test examples.

In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. $$$$$ Finally we are grateful to the three anonymous CL reviewers for their comments and advice.
In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. $$$$$ Obtaining training materials for statistical methods is costly and timeconsuming—it is a &quot;knowledge acquisition bottleneck&quot; (Gale, Church, and Yarowsky 1992a).
In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. $$$$$ We have presented the components of a system for acquiring unsupervised training materials that can be used with any statistical classifier.
In (Leacock et al, 1998), they used Bayesian approach for sense disambiguation of three ambiguous words. $$$$$ Senses whose contexts greatly overlap can be identified with a simple cosine correlation.

Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy. $$$$$ They can also be estimated from a small manually tagged sample, such as the parts of the Brown corpus that have been tagged with senses in WordNet.
Another method, by (Leacock et al, 1998), normalizes path distance based on the depth of hierarchy. $$$$$ We are indebted to the other members of the WordNet group who have provided advice and technical support: Christiane Fellbaum, Shari Landes, and Randee Tengi.

Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). $$$$$ This material is based upon work supported in part by the National Science Foundation under NSF Award No.
Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). $$$$$ TLC is trained with automatically extracted examples, its performance is compared with that obtained from manually tagged training materials.
Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). $$$$$ A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.
Various word-to-word similarity measures where applied, including distributional similarity (such as (Lin, 1998)), web-based co-occurrence statistics and WordNet based similarity measures (such as (Leacock et al, 1998)). $$$$$ Test results are compared with those from manually tagged training examples.

Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. $$$$$ We thank Scott Wayland, Tim Allison and Jill Hollifield for tagging the serve and hard corpora.
Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. $$$$$ We thank Scott Wayland, Tim Allison and Jill Hollifield for tagging the serve and hard corpora.
Many supervised learning algorithms have been applied for WSD, ex. Bayesian learning (Leacock et al., 1998), exemplar based learning (Ng and Lee, 1996), decision list (Yarowsky, 2000), neural network (Towel and Voorheest, 1998), maximum entropy method (Dang et al., 2002), etc. $$$$$ For a polysemous word, locate the monosemous relatives for each of its senses in WordNet and extract examples containing these relatives from a large corpus.

The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. $$$$$ We are indebted to the other members of the WordNet group who have provided advice and technical support: Christiane Fellbaum, Shari Landes, and Randee Tengi.
The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. $$$$$ The results of this effort will be a useful resource for training statistical classifiers, but what about the next thousand polysemous words, and the next?
The simple path measure computes the similarity between a pair of nodes in WordNet as the reciprocal of the number of edges in the shortest path between them, the LChmea sure (Leacock et al, 1998) also uses information about the length of the shortest path between a pair of nodes. $$$$$ We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.

To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). $$$$$ Using topical context alone, TLC performs no worse than other topical classifiers.
To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). $$$$$ A knowledge base in the form of WordNet's lexical relations is used to automatically locate training examples in a general text corpus.
To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). $$$$$ An impressive array of statistical methods have been developed for word sense identification.
To our knowledge, the methods of auto-acquiring sense-labeled instances include using parallel corpora like Gale et al. (1992) and Ng et al. (2003), extracting by monosemous relative of WordNet like Leacock et al. (1998), Mihalcea and Moldovan (1999), Agirre and Martínez (2004), Martínez et al. (2006) and PengYuan et al. (2008). $$$$$ We show how knowledge-based techniques can be used to open the bottleneck by automatically locating training corpora.

The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). $$$$$ Section 3 describes WordNet's lexical relations and the role that monosemous &quot;relatives&quot; of polysemous words can play in creating unsupervised training materials.
The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). $$$$$ In part, this is due to the rather high probability of the most frequent sense for this set.
The method we applied is based on the monosemous relatives of the target words (Leacock et al, 1998), and we studied some parameters that affect the quality of the acquired corpus, such as the distribution of the number of training instances per each word sense (bias), and the type of features used for disambiguation (local vs. topical). $$$$$ Finally, in combined mode, the set of cues contains all four types.

In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. $$$$$ Test results are compared with those from manually tagged training examples.
In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. $$$$$ N00014-91-1634.
In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. $$$$$ We have observed that when Classifier performance on three senses of the adjective hard.
In (Leacock et al, 1998), the method to obtain sense-tagged examples using monosemous relatives is presented. $$$$$ We thank Scott Wayland, Tim Allison and Jill Hollifield for tagging the serve and hard corpora.

This method is inspired in (Leacock et al, 1998). $$$$$ The results of combining the two types of context to disambiguate a noun (line), a verb (serve), and an adjective (hard) are presented.
This method is inspired in (Leacock et al, 1998). $$$$$ Senses whose contexts greatly overlap can be identified with a simple cosine correlation.
