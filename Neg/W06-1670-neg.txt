Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). $$$$$ In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.
Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). $$$$$ The model outperforms remarkably the best known baseline, the first sense heuristic – to the best of our knowledge, for the first time on the most realistic “all words” evaluation setting.
Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). $$$$$ Segond et al. (1997) are possibly the first to have applied an HMM tagger to semantic disambiguation.
Supersense tagging A WordNet-based supersense tagger (Ciaramita and Altun, 2006). $$$$$ Interestingly, to make the method more tractable, they also used the supersense tagset and estimated the model on Semcor.

 $$$$$ More needs to be understood concerning sources of information, and processes, that affect word sense selection in context.
 $$$$$ We use the perceptron algorithm for sequence tagging (Collins, 2002).
 $$$$$ 80s (Carreras et al., 2002; Florian et al., 2003), while Bio-NER accuracy ranges between the low 70s and 80s, depending on the data-set used for training/evaluation (Dingare et al., 2005).
 $$$$$ In this paper we presented a novel approach to broad-coverage word sense disambiguation and information extraction.

Columns 1 - 3 were predicted using the tagger of Ciaramita and Altun (2006). $$$$$ The 10 synsets are mapped to 6 supersenses: “artifact”, “quantity”, “shape”, “state”, “plant”, and “act”.
Columns 1 - 3 were predicted using the tagger of Ciaramita and Altun (2006). $$$$$ Our goal is to simplify the disambiguation task, for both nouns and verbs, to a level at which it can be approached as any other tagging problem, and can be solved with state of the art methods.
Columns 1 - 3 were predicted using the tagger of Ciaramita and Altun (2006). $$$$$ A sequential dependency model might not be the most accurate at capturing the grammatical dependencies between these elements.
Columns 1 - 3 were predicted using the tagger of Ciaramita and Altun (2006). $$$$$ Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.

grained distinctions of WN (Hearst and Schutze,1993) (Peters et al, 1998) (Mihalcea and Moldovan, 2001) (Agirre et al, 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997) (Ciaramita and Johnson, 2003) (Villarejo et al, 2005) (Curran, 2005) (Ciaramita and Altun, 2006). $$$$$ The Senseval all-words task evaluates the performance of WSD systems on all open class words in complete documents.
grained distinctions of WN (Hearst and Schutze,1993) (Peters et al, 1998) (Mihalcea and Moldovan, 2001) (Agirre et al, 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997) (Ciaramita and Johnson, 2003) (Villarejo et al, 2005) (Curran, 2005) (Ciaramita and Altun, 2006). $$$$$ Algorithm 1).
grained distinctions of WN (Hearst and Schutze,1993) (Peters et al, 1998) (Mihalcea and Moldovan, 2001) (Agirre et al, 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997) (Ciaramita and Johnson, 2003) (Villarejo et al, 2005) (Curran, 2005) (Ciaramita and Altun, 2006). $$$$$ Sequential models are common in NER, POS tagging, shallow parsing, etc..

In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). $$$$$ “wsj 1695”, and a fiction excerpt, “cl 23”, from the unannotated portion of the Brown corpus.
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). $$$$$ Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc.
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). $$$$$ Table 1.
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006). $$$$$ This property makes it very efficient which is a desirable feature in a task dealing with a large tagset such as ours.

Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). $$$$$ We define a tagset based on Wordnet’s lexicographers classes, or supersenses (Ciaramita and Johnson, 2003), cf.
Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). $$$$$ Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known “first-sense” baseline.
Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). $$$$$ Finally, word shape features (5) are regular expression-like transformation in which each character c of a string s is substituted with X if c is uppercase, if lowercase, c is substituted with x, if c is a digit it is substituted with d and left as it is otherwise.
Wherever applicable, we explore different syntactic and semantic representations of the textual content, e.g., extracting the dependency-based representation of the text or generalizing words to their WordNet supersenses (WNSS) (Ciaramita and Altun, 2006). $$$$$ The scalability of statistical NER allowed researchers to apply it successfully on large collections of newswire text, in several languages, and biomedical literature.

Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tag set. $$$$$ (Rosario and Hearst, 2004)).
Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tag set. $$$$$ Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.
Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tag set. $$$$$ We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model.
Sentences were annotated with WNSS categories, using the tagger of Ciaramita and Altun (2006), which annotates text with a 46-label tag set. $$$$$ The supersense label “noun.Tops” refers to 45 synsets which lie at the very top of the Wordnet noun hierarchy.

We used the implementation available from http $$$$$ Wordnet (Fellbaum, 1998) is a broad-coverage machine-readable dictionary which includes 11,306 verbs mapped to 13,508 word senses, called synsets, and 114,648 common and proper nouns mapped to 79,689 synsets.
We used the implementation available from http $$$$$ Section 5 reports on experimental settings and results.
We used the implementation available from http $$$$$ The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs.
We used the implementation available from http $$$$$ 80s (Carreras et al., 2002; Florian et al., 2003), while Bio-NER accuracy ranges between the low 70s and 80s, depending on the data-set used for training/evaluation (Dingare et al., 2005).

This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). $$$$$ However, in (Segond et al., 1997) the tagset is used differently, by defining equivalence classes of words with the same set of senses.
This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). $$$$$ Word sense disambiguation (WSD) is the task of deciding the intended sense for ambiguous words in context.
This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). $$$$$ Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known “first-sense” baseline.
This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). $$$$$ Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.

This annotation was performed automatically using the SuperSense Tagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses. $$$$$ Table 1.
This annotation was performed automatically using the SuperSense Tagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses. $$$$$ Research.
This annotation was performed automatically using the SuperSense Tagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses. $$$$$ The Senseval all-words task evaluates the performance of WSD systems on all open class words in complete documents.
This annotation was performed automatically using the SuperSense Tagger (Ciaramita and Altun, 2006) and includes 1183 named-entities and WordNet Super-Senses. $$$$$ As far as applications are concerned, it has been shown that supersense information can support supervised WSD, by providing a partial disambiguation step (Ciaramita et al., 2003).

We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). $$$$$ Second, classes, although fairly general, are easily recognizable and not too abstract or vague.
We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). $$$$$ Furthermore, since the noun tags include the standard named entity detection classes – person, location, organization, time, etc.
We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). $$$$$ This indicates that sense granularity is only one of the problems in WSD.
We recommend mate-tools (Bjorkelund et al, 2009) and SuperSenseTagger (Ciaramita and Altun, 2006). $$$$$ The supersense tagger improves mostly on precision, while also improving on recall.

We used Ciaramita and Altun's Su per Sense Tagger (Ciaramita and Altun, 2006) to tag the supersenses. $$$$$ The supersense tagger was trained on the Semcor datasets SEM and SEMv.
We used Ciaramita and Altun's Su per Sense Tagger (Ciaramita and Altun, 2006) to tag the supersenses. $$$$$ Named entity recognition (NER) is the most studied information extraction (IE) task.
We used Ciaramita and Altun's Su per Sense Tagger (Ciaramita and Altun, 2006) to tag the supersenses. $$$$$ Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation.

This is the coarse lexicographic category label, elsewhere denoted supersense (Ciaramita and Altun, 2006), which is the terminology we use. $$$$$ Table 1.
This is the coarse lexicographic category label, elsewhere denoted supersense (Ciaramita and Altun, 2006), which is the terminology we use. $$$$$ Interestingly, to make the method more tractable, they also used the supersense tagset and estimated the model on Semcor.

we use the out puts of SuperSense Tagger (Ciaramita and Altun,2006), which is optimised for assigning the super senses described above, and can outperform a WNF style baseline on at least some datasets. $$$$$ However, in (Segond et al., 1997) the tagset is used differently, by defining equivalence classes of words with the same set of senses.
we use the out puts of SuperSense Tagger (Ciaramita and Altun,2006), which is optimised for assigning the super senses described above, and can outperform a WNF style baseline on at least some datasets. $$$$$ The first sense baseline is the supersense of the most frequent synset for a word, according to Wordnet’s sense ranking.
we use the out puts of SuperSense Tagger (Ciaramita and Altun,2006), which is optimised for assigning the super senses described above, and can outperform a WNF style baseline on at least some datasets. $$$$$ Hence, supersensedisambiguated words are also, at least partially, synset-disambiguated.

We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger. $$$$$ Genia (Ohta et al., 2002), for example, is an ontology of 46 classes – with annotated 2The supersense category “group” is rather a superordinate of “organization” and has wider scope. corpus – designed for supporting information extraction in the molecular biology domain.
We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger. $$$$$ We believe that supersense tagging has the potential to be useful, in combination with other sources of information such as part of speech, domain-specific NER models, chunking or shallow parsing, in tasks such as question answering and information extraction and retrieval, where large amounts of text need to be processed.
We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger. $$$$$ – the tagger, as a by-product, returns extended named entity information.
We use SuperSenseTagger (Ciaramita and Altun,2006) as our NER tagger. $$$$$ As a by-product, this task includes and extends NER.

SUPERSENSE LEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. $$$$$ Semcor is divided in three parts: “brown1” and “brown2”, here referred to as “SEM”, in which nouns, verbs, adjectives and adverbs are annotated.
SUPERSENSE LEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. $$$$$ This vector is given by Each individual feature φi typically represents a morphological, contextual, or syntactic property, or also the inter-dependence of consecutive labels.
SUPERSENSE LEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. $$$$$ Our goal is to learn a function from input vectors, the observations from labeled data, to response variables, the supersense labels.
SUPERSENSE LEARNER brings together under one system the features previously used in the SENSELEARNER (Mihalcea and Csomai, 2005) and the SUPERSENSE (Ciaramita and Altun, 2006) all-words word sense disambiguation systems. $$$$$ One possible drawback is that senses which one might want to keep separate, e.g., the most common sense box/container (1), can be collapsed with others.

A detailed description of the features used and the tagger can be foundin (Ciaramita and Altun, 2006). $$$$$ To this extent, we cast the supersense tagging problem as a sequence labeling task and train a discriminative Hidden Markov Model (HMM), based on that of Collins (2002), on the manually annotated Semcor corpus (Miller et al., 1993).
A detailed description of the features used and the tagger can be foundin (Ciaramita and Altun, 2006). $$$$$ We believe that supersense tagging has the potential to be useful, in combination with other sources of information such as part of speech, domain-specific NER models, chunking or shallow parsing, in tasks such as question answering and information extraction and retrieval, where large amounts of text need to be processed.
A detailed description of the features used and the tagger can be foundin (Ciaramita and Altun, 2006). $$$$$ However, in (Segond et al., 1997) the tagset is used differently, by defining equivalence classes of words with the same set of senses.
A detailed description of the features used and the tagger can be foundin (Ciaramita and Altun, 2006). $$$$$ For each observed word xi in the data � extracts the following features: described below.

In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). $$$$$ Hence, supersensedisambiguated words are also, at least partially, synset-disambiguated.
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). $$$$$ Table 1.
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). $$$$$ In fact, the other noun categories on which the tagger performs poorly in SE3 are “group” and “location” (baseline 52.10 tagger 44.72 and baseline 47.62% tagger 47.54% F-score).
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al, 1997), (Ciaramita and Johnson, 2003), (Villarejo et al, 2005), (Curran, 2005) and (Ciaramita and Altun, 2006). $$$$$ Notwithstanding much research, the benefits of disambiguated lexical information for language processing are still mostly speculative.

WSD are those reported by (Ciaramita and Altun, 2006). $$$$$ It is also possible that this kind of shallow semantic information can help building more sophisticated linguistic analysis as in full syntactic parsing and semantic role labeling.
WSD are those reported by (Ciaramita and Altun, 2006). $$$$$ Sequential models are common in NER, POS tagging, shallow parsing, etc..
WSD are those reported by (Ciaramita and Altun, 2006). $$$$$ The tagger considerably outperformed the most competitive baseline on both Semcor and Senseval data.

Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us 468. $$$$$ This indicates that sense granularity is only one of the problems in WSD.
Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us 468. $$$$$ In this paper we approach word sense disambiguation and information extraction as a unified tagging problem.
Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us 468. $$$$$ Segond et al. (1997) are possibly the first to have applied an HMM tagger to semantic disambiguation.
