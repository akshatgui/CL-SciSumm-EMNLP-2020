For comparison, we also computed two baselines $$$$$ However, the difference between their performance and that of the Maximum Entropy approach of Chieu and Ng (2003) is not significant.
For comparison, we also computed two baselines $$$$$ Tjong Kim Sang is financed by IWT STWW as a researcher in the ATraNoS project.

 $$$$$ Eleven teams have incorporated information other than the training data in their system.
 $$$$$ More recently, there have been other system development competitions which dealt with different languages (IREX and CoNLL-2002).
 $$$$$ A baseline rate was computed for the English and the German test sets.
 $$$$$ Five participating groups have applied system combination.

In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). $$$$$ They employed a wide variety of machine learning techniques as well as system combination.
In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.
In addition to the conceptual simplicity of this approach, it also seems to perform better experimentally (Tjong Kim Sang and De Meulder, 2003). $$$$$ The performance of the system of Chieu et al. (2003) was not significantly different from the best performance for English and the method of Klein et al.

The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). $$$$$ All data files contain one word per line with empty lines representing sentence boundaries.
The first source is the CoNLL 2003 shared task date (Tjong Kim Sang and De Meulder, 2003) and the second source is the 2004 NIST Automatic Content Extraction (Weischedel, 2004). $$$$$ A named entity is correct only if it is an exact match of the corresponding entity in the data file.

Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). $$$$$ The learning methods were trained with the training data.
Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). $$$$$ The shared task of CoNLL-2003 concerns language-independent named entity recognition.
Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.

Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). $$$$$ The performance of the system of Chieu et al. (2003) was not significantly different from the best performance for English and the method of Klein et al.
Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). $$$$$ The Message Understanding Conferences (MUC) have offered developers the opportunity to evaluate systems for English on the same data in a competition.
Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.
Supervised NE Tagging has been studied extensively over the past decade (Bikel et al 1999, Baluja et. al. 1999, Tjong Kim Sang and De Meulder 2003). $$$$$ (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.

The annotation is distributed in the standard column based BIO format applied for e.g. CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004) data, among other established datasets. $$$$$ Three systems used Maximum Entropy Models in isolation (Bender et al., 2003; Chieu and Ng, 2003; Curran and Clark, 2003).

The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. $$$$$ This tagging scheme is the IOB scheme originally put forward by Ramshaw and Marcus (1995).
The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. $$$$$ Named entity recognition is an important task of information extraction systems.
The IOB2 strategy, which is very popular, having been used in public challenges such as those of CoNLL (Tjong Kim Sang and De Meulder, 2003) and JNLPBA (Kim et al, 2004), has been found to be indeed the best of all established tagging strategies. $$$$$ The data contains entities of four types: persons (PER), organizations (ORG), locations (LOC) and miscellaneous names (MISC).

Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. $$$$$ Sixteen systems have processed English and German named entity data.
Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. $$$$$ Munro et al. (2003) employed both voting and bagging for combining classifiers.
Named Entities predicted with the Maximum Entropy based tagger of Chieu and Ng (2003). The tagger follows the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003), and thus is not developed with WSJ data. $$$$$ The shared task of CoNLL-2002 dealt with named entity recognition for Spanish and Dutch (Tjong Kim Sang, 2002).

We benchmark the performance of our baseline MaxEnt classifier using the feature set from Section 5.1 (MaxEnt-A henceforth) on the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003), the de-facto standard for evaluating coarse-grained NERC systems. $$$$$ The Message Understanding Conferences (MUC) have offered developers the opportunity to evaluate systems for English on the same data in a competition.

This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ The optimal set of systems was determined by performing a bidirectional hill-climbing search (Caruana and Freitag, 1994) with beam size 9, starting from zero features.
This scheme was initially introduced in CoNLL's (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work. $$$$$ When a named entity is embedded in another named entity, usually only the top level entity has been annotated.

Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). $$$$$ Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al., 2003; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003).
Entity tagging has been thoroughly addressed by many statistical machine learning techniques, obtaining greater than 90% F1 on many datasets (Tjong Kim Sang and De Meulder, 2003). $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.

We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). $$$$$ The performance of the system of Chieu et al. (2003) was not significantly different from the best performance for English and the method of Klein et al.
We consider the problem of named-entity recognition (NER) and use the English data from the CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). $$$$$ (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German.

The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.
The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). $$$$$ They have used the data for developing a named-entity recognition system that includes a machine learning component.
The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). $$$$$ Munro et al. (2003) employed both voting and bagging for combining classifiers.
The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). $$$$$ Named entities are phrases that contain the names of persons, organizations and locations.

Finaly, combining models has been a successful way of achieving good results, such as those of Florian et al (2003) who had the top performance in the named entity recognition shared task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). $$$$$ It was produced by a system which only identified entities which had a unique class in the training data.
Finaly, combining models has been a successful way of achieving good results, such as those of Florian et al (2003) who had the top performance in the named entity recognition shared task of CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003). $$$$$ The shared task organizers were especially interested in approaches that made use of resources other than the supplied training data, for example gazetteers and unannotated data.

Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). $$$$$ The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization (Florian et al., 2003).
Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). $$$$$ The shared task organizers were especially interested in approaches that made use of resources other than the supplied training data, for example gazetteers and unannotated data.
Tokenisation and sentence splitting is followed by part-of speech tagging with the Maximum Entropy Markov Model (MEMM) tagger developed by Curran and Clark (2003) (here after referred to as C&C) for the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003), trained on the MedPost data (Smith et al, 2004). $$$$$ An overview of some of the types of features chosen by the shared task participants, can be found in Table 3.

All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). $$$$$ We created two basic language-specific tokenizers for this shared task.
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). $$$$$ Named entities are phrases that contain the names of persons, organizations and locations.
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sangand De Meulder, 2003). $$$$$ Table 2 contains an overview of the number of named entities in each data file.

language newspaper domain (English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)). $$$$$ Four of them have obtained error reductions of 15% or more for English and one has managed this for German.
language newspaper domain (English data set of the CoNLL-2003 shared task (Tjong Kim Sang and De Meulder, 2003)). $$$$$ However, in the CoNLL-2002 shared task we found out that choice of features is at least as important.

 $$$$$ De Meulder is supported by a BOF grant supplied by the University of Antwerp.
 $$$$$ Four of them have obtained error reductions of 15% or more for English and one has managed this for German.
 $$$$$ The performance of the system of Chieu et al. (2003) was not significantly different from the best performance for English and the method of Klein et al.

The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ Transformation-based learning (Florian et al., 2003), Support Vector Machines (Mayfield et al., 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each.
The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ Sixteen systems have processed English and German named entity data.
The fourth type, called miscellaneous, was introduced in the CoNLL NER tasks in 2002 (Tjong Kim Sang, 2002) and 2003 (Tjong Kim Sang and De Meulder, 2003), and includes proper names falling outside the three classic types. $$$$$ The resources used by these systems, gazetteers and externally trained named entity systems, still require a lot of manual work.
