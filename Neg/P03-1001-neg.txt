(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ They are “quick and dirty” implementations meant to extract a large proportion of the patterns in a text, acknowledging that some bad examples may leak through.
(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ Williams found
(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.

We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ Williams found
We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ The authors would like to thank Miruna Ticrea for her valuable help with training the classifier.
We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ Using patterns involving the phrase “such as”, she reports finding only 46 relations in 20M of New York Times text.
We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ “president George Bush”) are very productive and occur 40 times more often than patterns employed by Hearst (1992).

Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ For the purposes of learning a filter, these patterns were treated as illegitimate.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ Mann reports extracting 200,000 concept-instance pairs from 1GB of Associated Press text, only 60% of which were found to be legitimate descriptions.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ Most newspaper texts, for example, do not remain accessible on the Web for more than a few weeks.

Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ Berland and Charniak (1999) extract “part-of” relations between lexical items in text, achieving only 55% accuracy with their method.
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ In order to address the Recall problem, we extend the list of patterns used for extraction to take advantage of appositions.
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ This suggests that over half of the questions that TextMap got wrong could have benefited from information in the concept-instance pairs.

Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ “Who is ...” questions, against a state of the art web-based Question Answering system.
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ Based on this graph we choose to set the threshold at 0.9.
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ Further, while Information Retrieval techniques are relatively successful at managing the vast quantity of text available on the Web, the exactness required of Question Answering systems makes them too slow and impractical for ordinary users.
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ ).

 $$$$$ First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem).
 $$$$$ And finally, because it is designed to answer any type of question, not just “Who is...“ questions, TextMap is not as precise as the extraction technique.
 $$$$$ As the threshold is raised, precision increases while recall decreases.

We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ While this paper examines only this type of relation, the techniques we propose are easily extensible to other question types.
We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ Second, only a small fraction of the information that the patterns yield is reliable (the Precision problem).
We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ The approximately 15GB of newspaper text (described above) was passed through the regular expression patterns and filtered through their appropriate learned classifier.

This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.
This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ 5 Answers that unequivocally identify an instance’s celebrity (e.g., “Jennifer Capriati is a tennis star”) are marked correct.
This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ Mann (2002) notes that concept-instance relationships are often expressed by a syntactic pattern in which a proper noun follows immediately after a common noun.

The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ We believe that generating and maintaining information repositories will advance many aspects of Natural Language Processing.
The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.
The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ Even for “Who is ...” questions multiple answers need to be integrated before final output is presented.

The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ Mann (2002) notes that concept-instance relationships are often expressed by a syntactic pattern in which a proper noun follows immediately after a common noun.
The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.
The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.

Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ The authors would like to thank Miruna Ticrea for her valuable help with training the classifier.
Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ Further, following Banko and Brill (2001), we increase our yield by increasing the amount of data used by an order of magnitude over previously published work.
Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ This suggests that over half of the questions that TextMap got wrong could have benefited from information in the concept-instance pairs.
Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ The authors would like to thank Miruna Ticrea for her valuable help with training the classifier.

To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ A large number of questions were collected over the period of a few months from www.askJeeves.com.
To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ Answers for both systems are then classified by hand into three categories based upon their information content.
To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ And finally, because it is designed to answer any type of question, not just “Who is...“ questions, TextMap is not as precise as the extraction technique.
To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ Results of this comparison are presented in Figure 3.

This confirms the results of Fleischman et al (2003) $$$$$ First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem).
This confirms the results of Fleischman et al (2003) $$$$$ Hearst (1992) examined extracting hyponym data by taking advantage of lexical patterns in text.
This confirms the results of Fleischman et al (2003) $$$$$ Each pattern would have a machine-learned filter in order to insure high precision output relations.
This confirms the results of Fleischman et al (2003) $$$$$ “president George Bush”) are very productive and occur 40 times more often than patterns employed by Hearst (1992).

In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ If all pairs appear with equal frequency, a selection is made at random.
In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ Recent work in Question Answering has focused on web-based systems that answers using simple lexicosyntactic patterns.
In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ Berland and Charniak (1999) extract “part-of” relations between lexical items in text, achieving only 55% accuracy with their method.

After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ Mann (2002) notes that concept-instance relationships are often expressed by a syntactic pattern in which a proper noun follows immediately after a common noun.
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ Such patterns (e.g.
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem).
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ The approximately 15GB of newspaper text (described above) was passed through the regular expression patterns and filtered through their appropriate learned classifier.

In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ These studies indicate two distinct problems associated with using patterns to extract semantic information from text.
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ Finally, Mann (2002) describes a method for extracting instances from text that takes advantage of part of speech patterns involving proper nouns.
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ Williams found
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ The Web, while nearly infinite in content, is not a complete repository of useful information.

Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ 5 Answers that unequivocally identify an instance’s celebrity (e.g., “Jennifer Capriati is a tennis star”) are marked correct.
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ In samples of approximately 5000 pairs, 79% of the APOS extracted relations were legitimate, and only 45% of the CN/PN extracted relations were legitimate.
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ Finally, while the look-up of extracted pairs took approximately ten seconds for all 100 questions, TextMap took approximately 9 hours.
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ “Who is ...” questions, against a state of the art web-based Question Answering system.

The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ The authors would like to thank Miruna Ticrea for her valuable help with training the classifier.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ The information repository approach to Question Answering offers possibilities of increased speed and accuracy for current systems.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ Recent work in Question Answering has focused on web-based systems that answers using simple lexicosyntactic patterns.
