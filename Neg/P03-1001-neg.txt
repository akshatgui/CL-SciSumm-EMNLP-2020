(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ The output of this process is approximately 2,000,000 concept-instance pairs.
(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ These repositories would be automatically filled by a system that continuously watches various online news sources, scouring them for useful information.
(Fleischman et al., 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions. $$$$$ This difference represents a time speed up of three orders of magnitude.

We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ Evaluations are conducted using a set of “Who is ...” questions collected over the period of a few months from the commercial question-based search engine www.askJeeves.com.
We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ Each pattern would have a machine-learned filter in order to insure high precision output relations.
We also made use of the person-name/instance pairs automatically extracted by Fleischman et al (2003). $$$$$ Second, only a small fraction of the information that the patterns yield is reliable (the Precision problem).

Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ 93% of these were judged legitimate concept-instance pairs.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions.
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem).
Mann (2002) and Fleischman et al (2003) used part of speech patterns to extract a subset of hyponym relations involving proper nouns. $$$$$ The Web, while nearly infinite in content, is not a complete repository of useful information.

Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ Finally, in order to address the Precision problem, we use machine learning techniques to filter the output of the part of speech patterns, thus purifying the extracted instances.
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ Based on this graph we choose to set the threshold at 0.9.
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ In order to facilitate further research, we have made the extracted pairs described here publicly available at www.isi.edu/~fleisch/instances.txt.gz.
Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al (2003). $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.

Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ Results indicate that the extracted relations answer 25% more questions correctly and do so three orders of magnitude faster than the state of the art system.
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ 100 questions of the form “Who is x” were randomly selected from this set.
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ In addition to the CN/PN pattern of Mann (2002), we extracted syntactic appositions (APOS).
Following Fleischman et al (2003), we select the 50 definition questions from the TREC2003 (Voorhees 2003) question set. $$$$$ We present an alternative strategy in which patterns are used to extract highly precise relational information offline, creating a data repository that is used to efficiently answer questions.

 $$$$$ News (.5GB), the AP newswire (~2GB), the Los Angeles Times (~.5GB), the New York Times (~2GB), Reuters (~.8GB), the Wall Street Journal (~1.2GB), and various online news websites (~.7GB).
 $$$$$ Our approach follows closely from Mann (2002).
 $$$$$ Second, only a small fraction of the information that the patterns yield is reliable (the Precision problem).
 $$$$$ Our strategy is to collect a large sample of newspaper text (15GB) and use multiple part of speech patterns to extract the semantic relations.

We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ ).
We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ Williams found
We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ Part of speech patterns were generated to take advantage of two syntactic constructions that often indicate concept-instance relationships: common noun/proper noun constructions (CN/PN) and appositions (APOS).
We compared our system with the concepts in WordNet and Fleischman et al's instance/concept relations (Fleischman et al 2003). $$$$$ The simple look-up of extracted conceptinstance pairs generated 8% more partially correct answers and 25% more entirely correct answers than TextMap.

This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ “Who is ...” questions, against a state of the art web-based Question Answering system.
This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ Table 4 shows the list of features used to describe each concept-instance pair for training the CN/PN filter.
This approach is similar in spirit to the work reported by Fleischman et al (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. $$$$$ “Who is ...” questions, against a state of the art web-based Question Answering system.

The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ In order to facilitate further research, we have made the extracted pairs described here publicly available at www.isi.edu/~fleisch/instances.txt.gz.
The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ A sample of 100 concept-instance pairs was randomly selected from the 2,000,000 extracted pairs and hand annotated.
The precision of the extracted information can be improved significantly by using machine learning methods to filter out noise (Fleischman et al, 2003). $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.

The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ Under these same conditions, but applied to a held out test set of 500 examples extracted with the APOS pattern, the filter has a precision of .95 and a recall of .92.
The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ Table 2 shows the regular expression used to extract appositions and examples of extracted patterns.
The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ Based on this graph we choose to set the threshold at 0.9.
The recall problem is usually addressed by increasing the amount of text data for extraction (taking larger collections (Fleischman et al, 2003)) or by developing more surface patterns (Soubbotin and Soubbotin, 2002). $$$$$ These studies indicate two distinct problems associated with using patterns to extract semantic information from text.

Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ News (.5GB), the AP newswire (~2GB), the Los Angeles Times (~.5GB), the New York Times (~2GB), Reuters (~.8GB), the Wall Street Journal (~1.2GB), and various online news websites (~.7GB).
Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ First, the patterns yield only a small amount of the information that may be present in a text (the Recall problem).
Fleischman et al (2003) focus on the precision of the information extracted using simple part-of-speech patterns. $$$$$ “president George Bush”) are very productive and occur 40 times more often than patterns employed by Hearst (1992).

To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ Such patterns (e.g.
To get a clear picture of the impact of using different information extraction methods for the offline construction of knowledge bases, similarly to (Fleischman et al, 2003), we focused only on questions about persons, taken from the TREC8 through TREC 2003 question sets. $$$$$ Mann reports extracting 200,000 concept-instance pairs from 1GB of Associated Press text, only 60% of which were found to be legitimate descriptions.

This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. $$$$$ We imagine huge data warehouses where each repository contains relations, such as birthplace-of, location-of, creator-of, etc.
This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. $$$$$ We enhance these techniques by increasing the yield and precision of the relations that we extract.
This confirms the results of Fleischman et al (2003): shallow methods may benefit significantly from the post-processing. $$$$$ We would also like to thank Andrew Philpot for his work on integrating instances into the Omega Ontology, and Daniel Marcu whose comments and ideas were invaluable.

In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.
In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ However, instead of relying on costly structured databases and pain stakingly generated wrappers, repositories are automatically filled with information from many different patterns.
In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ Thus, a probability cutoff is set such that only positive classifications that exceed this cutoff are actually classified as legitimate.
In our future work we plan to investigate the effect of more sophisticated and, probably, more accurate filtering methods (Fleischman et al, 2003) on the QA results. $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.

After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ Berland and Charniak (1999) extract “part-of” relations between lexical items in text, achieving only 55% accuracy with their method.
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ In addition to the CN/PN pattern of Mann (2002), we extracted syntactic appositions (APOS).
After that, several million instances of people, locations, and other facts were added (Fleischman et al, 2003). $$$$$ We extract approximately 2,000,000 concept-instance relations from newspaper text using syntactic patterns and machine-learned filters (e.g., “president Bill Clinton” and “Bill Clinton, president of the USA,”).

In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ Part of speech patterns were generated to take advantage of two syntactic constructions that often indicate concept-instance relationships: common noun/proper noun constructions (CN/PN) and appositions (APOS).
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ State of the Art Extraction Answer Mark Answer Mark Who is Nadia U.S. P Romanian C Comaneci? citizen Gymnast Who is Lilian News I French P Thuram? page defender Who is the mayor Anthony C no answer I of Wash., D.C.?
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ And finally, because it is designed to answer any type of question, not just “Who is...“ questions, TextMap is not as precise as the extraction technique.
In particular, we use the name/instance lists described by (Fleischman et al., 2003) and available on Fleischman's web page to generate features between names and nominals (this list contains noU pairs mined from pI GBs of news data). $$$$$ Thus, a probability cutoff is set such that only positive classifications that exceed this cutoff are actually classified as legitimate.

Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ Mann (2002) notes that concept-instance relationships are often expressed by a syntactic pattern in which a proper noun follows immediately after a common noun.
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ The text was cleaned of HTML (when necessary), word and sentence segmented, and part of speech tagged using Brill’s tagger (Brill, 1994).
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ We evaluate our strategy on a challenging subset of questions, i.e.
Fleischman et al (2003) describe a dataset of concept-instance pairs extracted automatically from a very large corpus of newspaper articles. $$$$$ Their uses in 7 An important addition to this system would be the inclusion of time/date stamp and data source information.

The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ Such patterns (e.g.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ Applying the Decision Tree algorithm with Bagging, using the pre-determined threshold, to the held out test set of 500 examples extracted with the CN/PN pattern yields a precision of .95 and a recall of .718.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ “Who is ...” questions, against a state of the art web-based Question Answering system.
The goal of this study has been to automatically extract a large set of hyponymy relations, which play a critical role in many NLP applications, such as Q&A systems (Fleischman et al, 2003). $$$$$ Finally, Mann (2002) describes a method for extracting instances from text that takes advantage of part of speech patterns involving proper nouns.
