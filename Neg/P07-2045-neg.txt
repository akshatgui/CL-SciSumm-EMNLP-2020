This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). $$$$$ Instead of passing along the one-best output of the recognizer, a network of different word choices may be examined by the machine translation system.
This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). $$$$$ The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors.
This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). $$$$$ Moses implements an efficient representation of the phrase translation table.
This could be addressed by the so-called factored phrase-based model as implemented in the Moses decoder (Koehn et al, 2007). $$$$$ This creates a factored representation of each word, Figure 2.

For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Moses also integrates confusion network decoding, which allows the translation of ambiguous input.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006).

Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). $$$$$ To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder.
Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). $$$$$ This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006).
Subsequently, we extracted the bi-lingual phrase table from the aligned corpora using the Moses toolkit (Koehn et al., 2007). $$$$$ This new direction in research opens up many possibilities and issues that require further research and experimentation.

Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). $$$$$ Its key properare a tree for source words and demand i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder.
Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). $$$$$ It consists of all the components needed to preprocess data, train the language models and the translation models.
Finally, we extract the semantic phrase table from the augmented aligned corpora using the Moses toolkit (Koehn et al, 2007). $$$$$ It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step.

Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). $$$$$ The toolkit has been hosted and developed under sourceforge.net since inception.
Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). $$$$$ In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.
Bidirectional lexical scores for all rules with lexical items, calculated from a unigram lexicon over Viterbi-aligned word pairs as in the Moses decoder (Koehn et al, 2007). $$$$$ Efficient data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware.

The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). $$$$$ We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). $$$$$ This paper has presented a suite of open-source tools which we believe will be of value to the MT research community.
The first two baselines are standard systems using PBMT or Hiero trained using Moses (Koehn et al, 2007). $$$$$ In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: It was developed in C++ for efficiency and followed modular, object-oriented design.

Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. $$$$$ Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder.
Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. $$$$$ This new direction in research opens up many possibilities and issues that require further research and experimentation.
Our baseline system is phrase-based Moses (Koehn et al, 2007) with feature weights trained using MERT. $$$$$ In factored translation models, the surface forms may be augmented with different factors, such as POS tags or lemma.

We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
We evaluate MAXFORCE for HIERO over two CHEN corpora, IWSLT09 and FBIS, and compare the performance with vanilla n-best MERT (Och, 2003) from Moses (Koehn et al, 2007), Hypergraph MERT (Kumar et al, 2009), and PRO (Hopkins and May, 2011) from cdec. $$$$$ Namely, our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models.

For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). $$$$$ This uses the tools contained in Moses and requires minimal changes to set up and customize.
For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). $$$$$ This new direction in research opens up many possibilities and issues that require further research and experimentation.
For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). $$$$$ Every factor on the target language can have its own language model.
For our experiments we use the phrase-based machine translation techniques described in (Koehn, 2004) and (Koehn et al, 2007), integrating our models within a log-linear framework (Och and Ney, 2002). $$$$$ Machine translation input currently takes the form of simple sequences of words.

Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. $$$$$ We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. $$$$$ Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. $$$$$ In order to unify the experimental stages, a utility has been developed to run repeatable experiments.
Each instance of the decoder is a standard phrase based machine translation decoder that operates according to the same principles as the publicly available PHARAOH (Koehn, 2004) and MOSES (Koehn et al, 2007) SMT decoders. $$$$$ Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses.

(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. $$$$$ Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding.
(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. $$$$$ It consists of all the components needed to preprocess data, train the language models and the translation models.
(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. $$$$$ However, until now, most work in this field has been carried out on proprietary and in-house research systems.
(Koehn et al, 2007) was also included in the scores for partial hypothesis during the decoding. $$$$$ Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al. 2006).

An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).
An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). $$$$$ Moses has an active research community and has reached over 1000 downloads as of 1st March 2007.
An automatic extraction of bilingual MWEs is carried out by Ren et al (2009), using a log likelihood ratio based hierarchical reducing algorithm to investigate the usefulness of bilingual MWEs in SMT by integrating bilingual MWEs into the Moses decoder (Koehn et al, 2007). $$$$$ This paper has presented a suite of open-source tools which we believe will be of value to the MT research community.

The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). $$$$$ Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). $$$$$ These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps.
The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). $$$$$ Moses has an active research community and has reached over 1000 downloads as of 1st March 2007.
The effectiveness of the MWE-aligned and chunk aligned parallel corpus is demonstrated by using the standard log-linear PB-SMT model as our baseline system: GIZA++ implementation of IBM word alignment model 4, phrase-extraction heuristics described in (Koehn et al, 2003), minimum-error-rate training (Och, 2003) on a held-out development set, target language model trained using SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing (Kneser and Ney, 1995) and the Moses decoder (Koehn et al, 2007). $$$$$ Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al. 2006) and (Koehn and Hoang 2007).

We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. $$$$$ This enables, for instance, the tighter integration of speech recognition and machine translation.
We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. $$$$$ By providing a free and complete toolkit, we hope that this will stimulate the development of the field.
We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. $$$$$ It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004).
We then trained the Moses phrase-based system (Koehn et al, 2007) on the segmented and marked text. $$$$$ This input type has been used successfully for speech to text translation (Shen et al. 2006).

In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. $$$$$ We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. $$$$$ For example, we can decompose translating from surface forms to surface forms and lemma, as shown in Figure 3. je ach√®te you a un a une vert cat 178 Figure 3.
In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. $$$$$ Finally, translation of spoken language is prone to speech recognition errors, which can possibly corrupt the syntax and the meaning of the input.
In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al, 2007), 2008 version. $$$$$ However, until now, most work in this field has been carried out on proprietary and in-house research systems.

Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. $$$$$ Instead of passing along the one-best output of the recognizer, a network of different word choices may be examined by the machine translation system.
Backward 2-gram and 3-gram source and target log probabilities: as proposed by Duchateau et al (2002) Log probability of target segments on 5-gram MT-output-based LM: using MOSES (Koehn et al, 2007) trained on the provided parallel corpus, we translated the English side of this corpus into Spanish, assuming that the MT output contains mistakes. $$$$$ For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems.

When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. $$$$$ Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data.
When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. $$$$$ Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input.
When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. $$$$$ This new direction in research opens up many possibilities and issues that require further research and experimentation.
When an ambiguous connective is explicitly translated by another connective, the incorrect rendering of its sense can lead to erroneous translations, as in the second and third examples in Table 1, which are translated by the Moses SMT decoder (Koehn et al., 2007) trained on the Europarl corpus. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.

We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. $$$$$ We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. $$$$$ Phrase-based statistical machine translation (Koehn et al. 2003) has emerged as the dominant paradigm in machine translation research.
We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. $$$$$ However, until now, most work in this field has been carried out on proprietary and in-house research systems.
We used two decoders, Matrax (Simardetal., 2005) and Moses (Koehn et al, 2007), both standard statistical phrase based decoders. $$$$$ The main online presence is at http://www.statmt.org/moses/ where many sources of information about the project can be found.

In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. $$$$$ The other large data resource for statistical machine translation is the language model.
In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. $$$$$ We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. $$$$$ Almost unlimited text resources can be collected from the Internet and used as training data for language modeling.
In this paper we describe the speed improvements to the Moses decoder (Koehn et al,2007), as well as a novel framework to specify reordering constraints with XML markup, which we tested with punctuation-based constraints. $$$$$ This new direction in research opens up many possibilities and issues that require further research and experimentation.

on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag. $$$$$ This may encourage more syntactically correct output.
on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al, 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al, 2007) and estimating the Maximum Likelihood probabilities of errors for each part-of-speech tag. $$$$$ This input type has been used successfully for speech to text translation (Shen et al. 2006).
