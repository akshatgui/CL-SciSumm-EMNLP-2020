Because of these characteristics, Chinese has a rather different set of salient ambiguities from the perspective of statistical parsing (Levy and Manning, 2003). $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
Because of these characteristics, Chinese has a rather different set of salient ambiguities from the perspective of statistical parsing (Levy and Manning, 2003). $$$$$ The second difference, distinction of adjunction and complementation levels, has been made only for VP (Figure 2), consistent with the headedness issues described in Section 1.1.
Because of these characteristics, Chinese has a rather different set of salient ambiguities from the perspective of statistical parsing (Levy and Manning, 2003). $$$$$ Rules and UnRu are the number of rule types and unary rule types respectively; BF is average Branching Factor and UnTok is percentage unary of local tree tokens.
Because of these characteristics, Chinese has a rather different set of salient ambiguities from the perspective of statistical parsing (Levy and Manning, 2003). $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.

We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. $$$$$ In practice, we found that the former hurt performance whereas the latter helped somewhat.
We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. $$$$$ Interestingly, coordination scope ambiguity is recognized as perhaps the most recalcitrant problem in ETB parsing, while many cases of N/V ambiguity are particularly difficult points of linguistic analysis for Chinese, as discussed in Section 4.2.
We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.
We use the SVM-Light Toolkit version 6.02 (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser version 1.6 (Levy and Manning, 2003) as the constituent parser and the constituent-to-dependency converter. $$$$$ In particular, linguistic generalizations corresponding to category refinements are easily implemented via category-splitting in the PCFG model, without concern for affecting the dependency model.

To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the devset and test set. $$$$$ In addition to simplifying the parameterization of the parsing model and maintaining exactness, this 2WSJ-small is a randomly selected tenth of the full English Wall Street Journal corpus. model offers the prospect of increased flexibility in tuning the individual parse models.
To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the devset and test set. $$$$$ Significantly, this means that attachment ambiguity among a verb's complements, a major source of parsing ambiguity in English, is rare in Chinese.
To run the DE classifiers, we use the Stanford Chinese parser (Levy and Manning, 2003) to parse the Chinese side of the MT training data, the devset and test set. $$$$$ We found, however, that this development set was uncharacteristic of the corpus as a whole and not ideal for development.

In this work, we use the Stanford Parser (Levy and Manning 2003). $$$$$ Parsing in this model involves combining two independent parses: one of a non-lexicalized, maximum likelihoodestimated (MLE) PCFG model and another of a constituent-free dependency parse.
In this work, we use the Stanford Parser (Levy and Manning 2003). $$$$$ The indications for the utility of parent annotation in CTB parsing are mixed.
In this work, we use the Stanford Parser (Levy and Manning 2003). $$$$$ We use the factored parsing model of (Klein and Manning, 2002).
In this work, we use the Stanford Parser (Levy and Manning 2003). $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.

Levy and Manning (2003) used a factored model that combines an unlexicalized PCFG model with a dependency model. $$$$$ Simply splitting punctuation along the lines of English, combined with PCFG markovization and the introduction of a dependency model factor, reduced the LP/LR split to 1.1%.
Levy and Manning (2003) used a factored model that combines an unlexicalized PCFG model with a dependency model. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
Levy and Manning (2003) used a factored model that combines an unlexicalized PCFG model with a dependency model. $$$$$ Due to the highly articulated structure of prenominal modifiers, it seems difficult to address this problem directly; one measure we found somewhat successful is to mark IP daughters of prenominal modification.

While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. $$$$$ It seems that (i) is a difficult problem; in some cases, certain &quot;discourse-level&quot; adverbs such as however and )E,42t_ A./especially prefer IP modification and are thus strong indicators of high attachment.
While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. $$$$$ 4For unknown words we estimated P(wordltag) based on the first character of the word. foreign investment icalism (Sag and Wasow, 1999).
While it is uncommon to offer an error analysis for probabilistic parsing, Levy and Manning (2003) argue that a careful error classification can reveal possible improvements. $$$$$ The first difference, projection of phrasal categories, is particularly prominent within NPs: CTB adjective-noun modification, for example, is always at the level of ADJP and NP, whereas in English it can be a direct rewrite of NP to JJ and NN tags (Figure 1).

Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. $$$$$ We found that first-order markovization was superior to zero-order, second-order, and unmarkovized PCFGs for all levels of ancestor annotation, and that within first-order markovization parent annotation was slightly superior to no annotation, with grandparent annotation decidedly worse.
Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. $$$$$ In addition to further PCFG refinements, tuning the dependency model may lead to improved performance.
Verb mistagging is also a problem for other languages: Levy and Manning (2003) describe a similar problem in Chinese for noun/verb ambiguity. $$$$$ Since NP is right-headed while VP and IP are leftheaded, an improved dependency model may be the best place to address at least one of the key problems we have identified for CTB parsing.

The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). $$$$$ We found that head-dependent distances in the CTB are larger than in the ETB, consistent with the greater degree of center-embedding resulting from the mixed headedness of Chinese, and suggesting that a dependency model developed for English may not be optimal for Chinese.
The closest previous work is the detailed manual analysis performed by Levy and Manning (2003). $$$$$ Whereas ETB annotation strongly reflects late 1970s mainstream transformational grammar, CTB annotation draws primarily on Government-Binding (GB) theory from the 1980s.

 $$$$$ In both languages, there are borderline cases, but they are handled differently by the respective treebanks.
 $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
 $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
 $$$$$ We interpret these results as indicating that we have unlocked a heretofore undiscovered space of independence-assumption refinements for CTB parsing, suggesting that there is still considerable room for improvement in CTB parsing even with a small (90,000-word) training set; a parser-combining model such as that proposed in (Henderson and Brill, 1999), for example, might be effective here.

 $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
 $$$$$ This is an encouraging result for the use of detailed error analysis followed by focused treestructure enhancements to improved parser performance.
 $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.

See Levy and Manning (2003) for a similar discussion of Chinese and the Penn Chinese Treebank. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
See Levy and Manning (2003) for a similar discussion of Chinese and the Penn Chinese Treebank. $$$$$ This type of parse ambiguity is grounded in the semantic ambiguity of compound noun interpretation.
See Levy and Manning (2003) for a similar discussion of Chinese and the Penn Chinese Treebank. $$$$$ As shown in Table 4, previous work on CTB parsing consistently achieved higher results on precision than on recall.

We use the SVM-Light Toolkit (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser (Levy and Manning, 2003) as the parser and the constituent-to-dependency converter. $$$$$ We found, however, that this development set was uncharacteristic of the corpus as a whole and not ideal for development.
We use the SVM-Light Toolkit (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser (Levy and Manning, 2003) as the parser and the constituent-to-dependency converter. $$$$$ In adapting this parsing model to Chinese, we have retained unchanged the dependency model developed for English; the model backs off to tags, and backoff parameters remain the same.3 In all cases, test input to the parser was segmented but untagged.4 Our focus in parser development has been to refine the PCFG model via stepwise refinements informed by major observed ambiguity classes.
We use the SVM-Light Toolkit (Joachims, 1999) for the implementation of SVM, and use the Stanford Parser (Levy and Manning, 2003) as the parser and the constituent-to-dependency converter. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.

Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). $$$$$ False high scopings can be reduced by marking NP conjuncts.
Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). $$$$$ We use the factored parsing model of (Klein and Manning, 2002).
Noun/verb mis-taggings are a frequent error case for PCFG parsing on PCTB data, compounded in Chinese by the lack of function words and morphology (Levy and Manning, 2003). $$$$$ In principle, any contextual information could be used, but in practice two types are most heavily relied on: (i) information highly local to the enhanced node; and (ii) a unique preterminal/terminal pair identified as the head of the node.

It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). $$$$$ The trends we obtained are different enough from previous work to merit discussion.
It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
It should be noted that it is straightforward to simultaneously do POS tagging and constituent parsing, as POS tags can be regarded as non-terminals in the constituent structure (Levy and Manning, 2003). $$$$$ These practices have correlates in contemporary linguistic theory as principles of locality and lex3An algorithm to determine the head daughter of every non-terminal node is necessary for the dependency model and for grammar markovization (Collins, 1999), and since the CTB and ETB have different grammars, we did write a simple headfinder for the CTB grammar.

Levy and Manning (2003) established that properties of Chinese such as noun/verb ambiguity contribute to the difficulty of Chinese parsing. $$$$$ We found that first-order markovization was superior to zero-order, second-order, and unmarkovized PCFGs for all levels of ancestor annotation, and that within first-order markovization parent annotation was slightly superior to no annotation, with grandparent annotation decidedly worse.
Levy and Manning (2003) established that properties of Chinese such as noun/verb ambiguity contribute to the difficulty of Chinese parsing. $$$$$ Of the simplest local-context enrichment strategies, the one that has proven effective on a systematic basis involves parent annotation; (Johnson, 1998) showed that when uniformly applied, it considerably improved WSJ Treebank parsing.
Levy and Manning (2003) established that properties of Chinese such as noun/verb ambiguity contribute to the difficulty of Chinese parsing. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.

greatly affects parsing accuracy (Levy and Manning, 2003). $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.
greatly affects parsing accuracy (Levy and Manning, 2003). $$$$$ In principle, any contextual information could be used, but in practice two types are most heavily relied on: (i) information highly local to the enhanced node; and (ii) a unique preterminal/terminal pair identified as the head of the node.
greatly affects parsing accuracy (Levy and Manning, 2003). $$$$$ In addition to simplifying the parameterization of the parsing model and maintaining exactness, this 2WSJ-small is a randomly selected tenth of the full English Wall Street Journal corpus. model offers the prospect of increased flexibility in tuning the individual parse models.
greatly affects parsing accuracy (Levy and Manning, 2003). $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.

We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al, 2009). $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.
We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al, 2009). $$$$$ In the CTB, BA heads a VP and always has a unique sister IP; but that IP essentially always rewrites as NP VP.
We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al, 2009). $$$$$ We suspect that this is due to the low branching factor in the CTB, which increases the potential reward from the parser's perspective for picking flatter structures.
We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al, 2009). $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.

Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. $$$$$ Starred examples are correct in corpus; alternates are parse errors. string speculator Richard Denthese structures are typically bracketed flat in the ETB, underspecifying the semantic relations relative to the CTB.
Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. $$$$$ The indications for the utility of parent annotation in CTB parsing are mixed.
Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
Adapting unlexicalized parsers appears to be equally difficult: Levy and Manning (2003) adapt the unlexicalized parser of Klein and Manning (2003) to Chinese, but even after significant efforts on choosing category splits, only modest performance gains are reported. $$$$$ Parsing in this model involves combining two independent parses: one of a non-lexicalized, maximum likelihoodestimated (MLE) PCFG model and another of a constituent-free dependency parse.

As pointed out in (Levyand Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese. $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.
As pointed out in (Levyand Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese. $$$$$ To begin development, we tested the interaction of complete parent and/or grandparent annotation with PCFG markovization (see (Collins, 1999; Charniak, 2000) for discussion).
As pointed out in (Levyand Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese. $$$$$ We are grateful to Dan Klein for valuable input, and for the parser implementation used here.
As pointed out in (Levyand Manning, 2003), there are many linguistic differences between Chinese and English, as well as structural differences between their corresponding tree banks, and some of these make it a harder task to parse Chinese. $$$$$ We illustrate that each of these refinements can effectively be viewed as an amendment to the independence assumptions made by a simple PCFG.

First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words. $$$$$ We found that these ambiguities were most effectively dealt with by marking root IPs as well as those in certain sister contexts.
First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words. $$$$$ This paper is based on research supported by the Advanced Research and Development Activity (ARDA)'s Advanced Question Answering for Intelligence (AQUAINT) Program.
First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words. $$$$$ In addition to simplifying the parameterization of the parsing model and maintaining exactness, this 2WSJ-small is a randomly selected tenth of the full English Wall Street Journal corpus. model offers the prospect of increased flexibility in tuning the individual parse models.
First of all, we adopt the head finding rules for Chinese used in (Levy and Manning, 2003), and this affects sieve 4, 6 and 7 which are all take advantage of the head words. $$$$$ 4.1 Analysis by error type and PCFG-enrichment fixes Multilevel VP adjunction errors (Figure 5) are common in models without parent annotation, although even with parent annotation the presence of VP coordination would give multilevel VP adjunction nonzero probability.
