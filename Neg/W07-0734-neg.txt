Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). $$$$$ an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.
Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). $$$$$ In order to construct instances of METEOR for Spanish, French and German, we created new languagespecific “stemming” modules.
Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007). $$$$$ The stability of the optimized parameters on different data sets remains to be investigated for languages other than English.

We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007).
We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ Various researchers have noted, however, various weaknesses in the metric.
We evaluate using BLEU4 (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ Tests for statistical significance using bootstrap sampling indicate that the differences in correlation levels are all significant at the 95% level.

The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. $$$$$ an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.
The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. $$$$$ METEOR evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation.
The translation accuracy reported in Table 3, as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007), also shows significant improvement and approaches the quality achieved using gold standard data. $$$$$ This alignment is incrementally produced by a sequence of word-mapping modules.

For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007).
For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and
For the translation, a multi-stack phrase-based decoder was used. For the evaluation of translation quality, we applied standard automatic metrics, i.e., BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ We have recently expanded the implementation of METEOR to support evaluation of translations in Spanish, French and German, in addition to English.

We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ The stability of the optimized parameters on different data sets remains to be investigated for languages other than English.
We report case-insensitive scores on version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ For English, we used the NIST 2003 Arabic-toEnglish MT evaluation data for training and the NIST 2004 Arabic-to-English evaluation data for testing.

The translation quality is measured by three MT evaluation metrics $$$$$ We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007).
The translation quality is measured by three MT evaluation metrics $$$$$ The latest release of METEOR is freely available on our website at: http://www.cs.cmu.edu/~alavie/METEOR/
The translation quality is measured by three MT evaluation metrics $$$$$ Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.
The translation quality is measured by three MT evaluation metrics $$$$$ We are currently exploring broadening the set of features used in METEOR to include syntax-based features and alternative notions of synonymy.

We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ This paper recaps the technical details underlying the metric and describes recent improvements in the metric.
We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ We are currently exploring broadening the set of features used in METEOR to include syntax-based features and alternative notions of synonymy.
We report case-insensitive scores for version 0.6 of METEOR (Lavie and Agarwal, 2007) with all modules enabled, version 1.04 of IBM-style BLEU (Papineni et al, 2002), and version 5 of TER (Snover et al, 2006). $$$$$ We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007).

For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ We use the freely available Perl implementation packages for Porter stemmers for the three languages (Humphrey, 2007).
For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The “exact” module maps two words if they are exactly the same.
For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ Furthermore, several parameters within the metric have been optimized on language-specific training data.
For evaluation of the given task, we have incorporated evaluation techniques based on current evaluation techniques used in machine translation, BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The “porter stem” module maps two words if they are the same after they are stemmed using the Porter stemmer.

We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ The resources used for stemming and synonymy detection for English are the Porter Stemmer (Porter, 2001) and English WordNet (Miller and Fellbaum, 2007).
We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ Furthermore, several parameters within the metric have been optimized on language-specific training data.
We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ In this paper we described newly developed language-specific instances of the METEOR metric and the process of optimizing metric parameters for different human measures of translation quality and for different languages.

In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1 $$$$$ METEOR evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation.
In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1 $$$$$ The second main language-specific issue which required adaptation is the tuning of the three parameters within METEOR , described in section 4.
In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1 $$$$$ In this paper we described newly developed language-specific instances of the METEOR metric and the process of optimizing metric parameters for different human measures of translation quality and for different languages.
In this case, we ID Description1-4 n-gram precisions against pseudo references (1? n? 4) 5-6 PER and WER 7-8 precision, recall, fragmentation from METEOR (Lavie and Agarwal, 2007) 9-12 precisions and recalls of nonconsecutive bigrams with a gap size of m (1? m? 2) 13-14 longest common subsequences 15-19 n-gram precision against a target corpus (1? n? 5) Table 1 $$$$$ The order in which the modules are run reflects wordmatching preferences.

The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. $$$$$ The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and
The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. $$$$$ Various researchers have noted, however, various weaknesses in the metric.
The translation accuracy as measured by BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) also shows improvement over baseline and approaches gold standard quality. $$$$$ Previous publications on METEOR (Lavie et al., 2004; Banerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.

This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences. $$$$$ The word-matching component within the English version of METEOR uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference.
This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences. $$$$$ Most notably, BLEU does not produce very reliable sentence-level scores.
This problem is similar to the task of automatic translation output evaluation and so we use METEOR (Lavie and Agarwal,2007), an automatic MT evaluation metric for com paring two sentences. $$$$$ The word-matching component within the English version of METEOR uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference.

The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). $$$$$ This paper recaps the technical details underlying the metric and describes recent improvements in the metric.
The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). $$$$$ The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and
The free parameters can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). $$$$$ an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.

Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). $$$$$ This paper recaps the technical details underlying the metric and describes recent improvements in the metric.
Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). $$$$$ METEOR , as well as several other proposed metrics such as GTM (Melamed et al., 2003), TER (Snover et al., 2006) and CDER (Leusch et al., 2006) aim to address some of these weaknesses.
Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). $$$$$ The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and
Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). $$$$$ Furthermore, several parameters within the metric have been optimized on language-specific training data.

Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics $$$$$ Another interesting observation is that the value of y is higher for fluency optimization.
Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics $$$$$ Our evaluations demonstrate that parameter tuning improves correlation with human judgments.
Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics $$$$$ Finally, the METEOR score for the alignment between the two strings is calculated as: score = (1 − Pen) · Fmean In all previous versions of METEOR , the values of the three parameters mentioned above were set to be: a = 0.9, 0 = 3.0 and y = 0.5, based on experimentation performed in early 2004.
Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics $$$$$ This alignment is incrementally produced by a sequence of word-mapping modules.

In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). $$$$$ an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more used It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop.
In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). $$$$$ An alignment is mapping between words, such that every word in each string maps to at most one word in the other string.
In an MT evaluation setting, sense clusters have been integrated into an Mt evaluation metric (METEOR) (Lavie and Agarwal, 2007) and brought about an increase of the metric's correlation with human judgments of translation quality in different languages (Apidianaki and He, 2010). $$$$$ Various researchers have noted, however, various weaknesses in the metric.

In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. $$$$$ The original version of METEOR (Banerjee and Lavie, 2005) has instantiated values for three parameters in the metric: one for controlling the relative weight of precision and recall in computing the Fmean score (a); one governing the shape of the penalty as a function of fragmentation (0) and one for the relative weight assigned to the fragmentation penalty (y).
In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. $$$$$ In this paper we described newly developed language-specific instances of the METEOR metric and the process of optimizing metric parameters for different human measures of translation quality and for different languages.
In particular, our framework might be useful with translation metrics such as TER (Snover et al, 2006) or METEOR (Lavie and Agarwal, 2007). In contrast to a phrase-based SMT system, a syntax based SMT system (e.g. Zollmann and Venugopal (2006)) can generate a hypergraph that represents a generalized translation lattice with word sand hidden tree structures. $$$$$ We then compute a parameterized harmonic mean of P and R (van Rijsbergen, 1979): Precision, recall and Fmean are based on singleword matches.

The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. $$$$$ Evaluating an MT system using such automatic metrics is much faster, easier and cheaper compared to human evaluations, which require trained bilingual evaluators.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. $$$$$ The word-matching component within the English version of METEOR uses stemming and synonymy modules in constructing a word-to-word alignment between translation and reference.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. $$$$$ We are investigating the possibility of developing new synonymy modules for the various languages based on alternative methods, which could then be used in place of WordNet.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavieand Agarwal, 2007) 12. $$$$$ Various researchers have noted, however, various weaknesses in the metric.

Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows $$$$$ Most notably, BLEU does not produce very reliable sentence-level scores.
Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows $$$$$ METEOR versions for Spanish, French and German therefore currently include only “exact” and “stemming” matching modules.
Moreover, the overall BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007) scores, as well as numbers of exact string matches (as measured against to the original sentences in the CCG bank) are higher for the hyper tagger-seeded realizer than for the preexisting realizer. This paper is structured as follows $$$$$ For all three languages, the parameters that were found to be optimal were quite different than those that were found for English, and using the language-specific optimal parameters results in significant gains in Pearson correlation levels with human judgments on the training data (compared with those obtained using the English optimal parameters)'.

Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. $$$$$ Parameter adaptation is also an issue in the newly created METEOR instances for other languages.
Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. $$$$$ The default ordering is to first apply the “exact” mapping module, followed by “porter stemming” and then “WN synonymy”.
Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. $$$$$ When evaluating a set of parameters on test data, we compute segment-level correlation with human judgments for each of the systems in the test set and then report the mean over all systems.
Instead of NIST scores, other MT evaluation scores can be plugged into this formula, such as METEOR (Lavie and Agarwal, 2007) for languages for which paraphrase data is available. $$$$$ Automatic metrics are useful for comparing the performance of different systems on a common translation task, and can be applied on a frequent and ongoing basis during MT system development.
