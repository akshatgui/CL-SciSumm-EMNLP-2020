Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ The decision whether a corpus is good enough for publication should be based on more than the agreement score—specifically, an important consideration is an independent evaluation of the results that are based on the corpus.
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ Special thanks to Klaus Krippendorff for an extremely detailed review of an earlier version of this article.
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.

Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ (Notice that this definition of observed agreement is equivalent to the mean of the two-coder observed agreement values from Section 2.4 for all coder pairs.)
Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.

Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. $$$$$ Similarly mediocre results for intercoder agreement between naive coders were reported in the subsequent editions of SENSEVAL.

For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Since the mid 1990s, increasing effort has gone into putting semantics and discourse research on the same empirical footing as other areas of computational linguistics (CL).
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Observed agreement Ao is easy to compute, and is the same for all three coefficients—the proportion of items on which the two coders agree.
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ A coded corpus is thus akin to the result of a scientific experiment, and it can only be considered valid if it is reproducible—that is, if the same coded results can be replicated in an independent coding exercise.
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Krippendorff also discusses the criteria to be used in the selection of the sample, from the minimum number of units (obtained using a formula from Bloch and Kraemer [1989], reported in Krippendorff [2004a, page 239]), to how to make the sample representative of the data population (each category should occur in the sample often enough to yield at least five chance agreements), to how to ensure the reliability of the instructions (the sample should contain examples of all the values for the categories).

For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ During this period, however, a number of questions have also been raised about K and similar coefficients—some already in Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the way the coefficient is computed (e.g., whether it is really applicable when more than two coders are used), to debates about which levels of agreement can be considered ‘acceptable’ (Di Eugenio 2000; Craggs and McGee Wood 2005), to the realization that K is not appropriate for all types of agreement (Poesio and Vieira 1998; Marcu, Romera, and Amorrortu 1999; Di Eugenio 2000; Stevenson and Gaizauskas 2000).
For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ We introduce a uniform notation, which we hope will make the relations between the different coefficients of agreement clearer.
For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ If two coders randomly classify utterances in a uniform manner using the scheme of Table 1, we would expect an equal number of items to fall in each of the four cells in the table, and therefore pure chance will cause the coders to agree on half of the items (the two cells on the diagonal: 14 + 14).

It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ This is not to imply that multiple coders increase reliability: The variance of the individual coders’ distributions can be just as large with many coders as with few coders, but its effect on the value of κ decreases as the number of coders grows, and becomes more similar to random noise.
It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ This is a measure of the coders’ ability to agree on the rare category.
It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ The best estimate of this distribution is ˆP(k), the observed proportion of items assigned to category k by both coders.
It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ (Just as in real life, the fact that witnesses to an event disagree with each other makes it difficult for third parties to know what actually happened.)

Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ A rigorous methodology for reliability testing does not, in our opinion, exclude the use of expert coders, and here we feel there may be a motivated difference between the fields of content analysis and CL.
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ Reliability is thus a prerequisite for demonstrating the validity of the coding scheme—that is, to show that the coding scheme captures the “truth” of the phenomenon being studied, in case this matters: If the annotators are not consistent then either some of them are wrong or else the annotation scheme is inappropriate for the data.
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ This distance metric is used in calculating observed and expected disagreement, and αU itself.
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ Since the mid 1990s, increasing effort has gone into putting semantics and discourse research on the same empirical footing as other areas of computational linguistics (CL).

Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ As an intuitive example, think of a person who consults two analysts when deciding whether to buy or sell certain stocks.
Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ comments and discussion.
Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ The last decade has already seen considerable improvement, from the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al. 2004) and OntoNotes (Hovy et al.

As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ Therefore, again, unweighted measures, and in particular K, tend to be used for measuring inter-coder agreement.
As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ Reliability, or reproducibility of the coding, is reduced by all disagreements—both random and systematic.
As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.
As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ When the coefficient is used to assess reliability, a single-distribution coefficient like π or α should be used; this is indeed already the practice in CL, because Siegel and Castellan’s K is identical with (multi-)π.

Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.
Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ We begin with a quick recap of the goals of agreement studies, inspired by Krippendorff (2004a, Section 11.1).
Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ Carlson, Marcu, and Okurowski (2003) reported very high agreement over the identification of the boundaries of discourse units, the building blocks of their annotation of rhetorical structure.
Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ If two coders randomly classify utterances in a uniform manner using the three categories in the second scheme, they would only agree on a third of the items (19 + 19 + 19).

Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ This allows us to make the following observations about the relationship between π and κ.
Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ (Just as in real life, the fact that witnesses to an event disagree with each other makes it difficult for third parties to know what actually happened.)
Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ The purpose of this article has been to expose the reader to the mathematics of chancecorrected coefficients of agreement as well as the current state of the art of using these coefficients in CL.

The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ However, a better practice is to use generalized versions of the coefficients.
The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ This “blurriness” of boundaries, combined with the prevalence effects discussed in Section 3.2, also explains the fact that topic annotation efforts which were only concerned with roughly dividing a text into segments (Passonneau and Litman 1993; Carletta et al. 1997; Hearst 1997; Reynar 1998; Ries 2002) generally report lower agreement than the studies whose goal is to identify smaller discourse units.
The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ A coded corpus is thus akin to the result of a scientific experiment, and it can only be considered valid if it is reproducible—that is, if the same coded results can be replicated in an independent coding exercise.

A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ This is a measure of the coders’ ability to agree on the rare category.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ Finally, we use n with a subscript to indicate the number of judgments of a given type: The simplest measure of agreement between two coders is percentage of agreement or observed agreement, defined for example by Scott (1955, page 323) as “the percentage of judgments on which the two analysts agree when coding the same data independently.” This is the number of items on which the coders agree divided by the total number of items.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ First we find how much agreement is expected by chance: Let us call this value Ae.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ • P(·) is reserved for the probability of a variable, and ˆP(·) is an estimate of such probability from observed data.

In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ This can be expressed as the sum of the squares of the differences between all of the judgments pairs without regard to items, again scaled by the appropriate factor.
In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ This confusion matrix reports the results of an experiment where two coders classify a set of utterances into three categories.
In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ We agree with the view that K and α are more appropriate, as they abstract away from the bias of specific coders.
In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.

However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ For example, with a tagset like the one in Table 9, the cases in which the coders used the label ‘Group 1’ would be uniformly “distributed down” and added in equal measure to the number of cases in which the coders assigned each of the four WordNet labels.
However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ This is a measure of the coders’ ability to agree on the rare category.
However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.

Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ This is a measure of the coders’ ability to agree on the rare category.
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ More precisely, and looking ahead to the following discussion, observed agreement is the arithmetic mean of the agreement value agri for all items i ∈ I, defined as follows: For example, let us assume a very simple annotation scheme for dialogue acts in information-seeking dialogues which makes a binary distinction between the categories statement and info-request, as in the DAMSL dialogue act scheme (Allen and Core 1997).
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ That said, we doubt that a single cutoff point is appropriate for all purposes.
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ We think there are at least two types of coding schemes in which this is the case: (i) hierarchical tagsets and (ii) set-valued interpretations such as those proposed for anaphora.

Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ Fleiss (1971) therefore uses a different type of table which lists each item with the number of judgments it received for each category; Siegel and Castellan (1988) use a similar table, which Di Eugenio and Glass (2004) call an agreement table.
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ Two coders classify 100 utterances according to this scheme as shown in Table 1.
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ With more than two coders, the observed agreement Ao can no longer be defined as the percentage of items on which there is agreement, because inevitably there will be items on which some coders agree and others disagree.

 $$$$$ Furthermore, they argue, the magnitude of d(c1, c2) should be proportional to the distance between the functions in the hierarchy.
 $$$$$ This is a measure of the coders’ ability to agree on the rare category.
 $$$$$ There is also a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments.

Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ Special thanks to Klaus Krippendorff for an extremely detailed review of an earlier version of this article.
Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ comments and discussion.
Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ Consider the reference of the underlined pronoun it in the following dialogue excerpt (TRAINS 1991 [Gross, Allen, and Traum 1993], dialogue d91-3.2).8 pick up oranges Some of the coders in a study we carried out (Poesio and Artstein 2005) indicated the noun phrase engine E2 as antecedent for the second it in utterance 3.1, whereas others indicated the immediately preceding pronoun, which they had previously marked as having engine E2 as antecedent.

We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.
We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.
We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ comments and discussion.
