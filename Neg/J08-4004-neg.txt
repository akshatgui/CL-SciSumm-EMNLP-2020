Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ We therefore feel that if a threshold needs to be set, 0.8 is a good value.
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ As mentioned in Section 2.4, this difference has been the subject of much debate (Fleiss 1975; Krippendorff 1978, 2004b; Byrt, Bishop, and Carlin 1993; Zwick 1988; Hsu and Field 2003; Di Eugenio and Glass 2004; Craggs and McGee Wood 2005).
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.
Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. $$$$$ Di Eugenio raised the issue of the effect of skewed distributions on the value of K and pointed out that the original x developed by Cohen is based on very different assumptions about coder bias from the K of Siegel and Castellan (1988), which is typically used in CL.

Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ Finally, Passonneau has been advocating the use of Krippendorff’s α (Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and disjoint categories, including anaphoric annotation, wordsense tagging, and summarization (Passonneau 2004, 2006; Nenkova and Passonneau 2004; Passonneau, Habash, and Rambow 2006).
Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ We feel that inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators.
Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ In other words, provided enough coders are used, it should not matter whether a single-distribution or individual-distribution coefficient is used.
Near misses occur frequently in segmentation, although manual coders often agree upon the bulk of where segment lie, they frequently disagree upon the exact position of boundaries (Artstein and Poesio, 2008, p. 40). $$$$$ Then instead of reporting two coefficients, as suggested by Di Eugenio and Glass (2004), the appropriate coefficient should be chosen based on the task (not on the observed differences between coder marginals).

Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.
Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Fournier and Inkpen (2012, p. 156-157) adapted four inter-coder agreement formulations provided by Artstein and Poesio (2008) to use S to award partial credit for near misses, but because S produces cosmetically high agreement values they grossly overestimate agreement. $$$$$ Di Eugenio and Glass pointed out that the choice of calculating chance agreement by using individual coder marginals (x) or pooled distributions (K) can lead to reliability values falling on different sides of the accepted 0.67 threshold, and recommended reporting both values.

For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ We use this notation to rewrite the formula for the within-level variance.
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Since the mid 1990s, increasing effort has gone into putting semantics and discourse research on the same empirical footing as other areas of computational linguistics (CL).
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Therefore, the choice of coefficient should not depend on the magnitude of the divergence between the coders, but rather on the desired interpretation of chance agreement.
For more details on terminology issues, we refer to the introduction of (Artstein and Poesio, 2008). $$$$$ The method proposed in the paper has, however, problematic properties when used to measure intercoder agreement.

For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ Krippendorff (1980, 2004a) offers distance metrics suitable for nominal, interval, ordinal, and ratio scales.
For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ This is a measure of the coders’ ability to agree on the rare category.
For more information on the terminology issue, refer to the introduction of (Artstein and Poesio, 2008). $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.

It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ Special thanks to Klaus Krippendorff for an extremely detailed review of an earlier version of this article.
It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ Additional limitations are noted by Hsu and Field (2003).
It is k = 0.72 on the full set, which is considered acceptable according to Artstein and Poesio (2008). $$$$$ In both studies, coders were allowed to mark each communicative function independently: That is, they were allowed to choose for each utterance one of the Statement tags (or possibly none), one of the Influencing-Addressee-Future-Action tags, and so forth—and agreement was evaluated separately for each dimension using (unweighted) K. Core and Allen found values of K ranging from 0.76 for answer to 0.42 for agreement to 0.15 for Committing-Speaker-Future-Action.

Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006).
Only the agreement on verbs is slightly below the acceptability threshold of 0.67 (Artstein and Poesio, 2008). $$$$$ However, a null hypothesis of chance agreement is not very interesting, and demonstrating that agreement is significantly better than chance is not enough to establish reliability.

Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ Our own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).
Given the aims of our classification, and of STaRS.sys in general, we choose to evaluate our coding scheme by asking to a group of non experts to label a subset of the non-normalized Kremer et al's (2008) norms and measuring the inter-coder agreement between them (Artstein and Poesio, 2008), adhering to the Krippendorff's (2004, 2008) recommendations. $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.

As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.
As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ In this article, a survey of coefficients of agreement and their use in CL, we have three main goals.
As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.
As pointed out by Artstein and Poesio (2008), agreement doesn't ensure validity. $$$$$ Observed agreement enters in the computation of all the measures of agreement we consider, but on its own it does not yield values that can be compared across studies, because some agreement is due to chance, and the amount of chance agreement is affected by two factors that vary from one study to the other.

Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.
Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ Because both expected and observed disagreement are twice the respective variances, the coefficient α retains the same form when expressed with the disagreement values.
Based on Cohen's seminal work (Cohen, 1968), Artstein and Poesio (2008) suggest the measure in (4), where k is calculated as the weighted difference between observed and expected disagreement. $$$$$ In the experiment mentioned previously (Poesio and Artstein 2005) we used 18 coders to test α and K under a variety of conditions.

Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.
Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997).
Our k values generally fall within the range that Landis and Koch (1977) deem "moderate agreement", but below the .8 cut-off tentatively suggested by Artstein and Poesio (2008). $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.

The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ For example, Di Eugenio and Glass (2004, page 99) say that κ suffers from what they call the bias problem, described as “the paradox that κCo [our κ] increases as the coders become less similar.” Similar reservations about the use of κ have been noted by Brennan and Prediger (1981) and Zwick (1988).
The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ Special thanks to Klaus Krippendorff for an extremely detailed review of an earlier version of this article.
The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.
The individual Kappa scores largely fall into the range that Landis and Koch (1977) regard as substantial agreement, while three labels are above the more strict .8 threshold for reliable annotations (Artstein and Poesio, 2008). $$$$$ comments and discussion.

A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff’s alpha as well as Scott’s pi and Cohen’s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks—but that their use makes the interpretation of the value of the coefficient even harder.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.
A comprehensive overview of methods for measuring the inter-annotator agreement in various areas of computational linguistics was given in Artstein and Poesio (2008). $$$$$ Krippendorff’s αU is not applicable to all CL tasks.

In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.
In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ Special thanks to Klaus Krippendorff for an extremely detailed review of an earlier version of this article.
In addition to percentage agreement, we measured Cohen's k (Artstein and Poesio, 2008) between all 3 possible annotator pairings. $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.

However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ In case 2, the coders disagree on three times as many utterances, but K is higher than in the first case because expected agreement is substantially lower (Ae = 0.53).
However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997).
However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ particular coder as Rare, and calculate what proportion of those items were labeled Rare by the other coder.
However, the interpretation of k is still under discussion (Artstein and Poesio, 2008). $$$$$ But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997).

Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ Krippendorff’s recommendations were developed for the field of content analysis, where coding is used to draw conclusions from the texts.
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ Teufel, Carletta, and Moens (1999), who studied agreement on the identification of argumentative zones, found high reliability (K = 0.81) for their three main zones (own, other, background), although lower for the whole scheme (K = 0.71).
Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures, see Artstein and Poesio (2008), it is usually assumed that Cohen's kappa figures over 60 are good while those over 75 are excellent (Fleiss, 1971). $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.

Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
Although mean k scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan's (1988) k has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555-556). $$$$$ We are also extremely grateful to the British Library in London, which made accessible to us virtually every paper we needed for this research.

 $$$$$ Unlike significance values which report a probability (that an observed effect is due to chance), agreement coefficients report a magnitude, and it is less clear how to interpret such magnitudes.
 $$$$$ A rigorous methodology for reliability testing does not, in our opinion, exclude the use of expert coders, and here we feel there may be a motivated difference between the fields of content analysis and CL.
 $$$$$ This is because Ao and Ae are not independent: Both are drawn from the same set of observations.
 $$$$$ For instance, the dimension of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts Open-option (used to mark suggestions) and Directive, both of which bring into consideration a future action to be performed by the addressee.

Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ All three coefficients assume independence of the two coders—that is, that the chance of c1 and c2 agreeing on any given category k is the product of the chance of each of them assigning an item to that category: P(k|c1) · P(k|c2).4 Expected agreement is then the probability of c1 and c2 agreeing on any category, that is, the sum of the product over all categories: The difference between S, π, and κ lies in the assumptions leading to the calculation of P(k|ci), the chance that coder ci will assign an arbitrary item to category k (Zwick 1988; Hsu and Field 2003).
Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ But suppose we want to refine the simple binary coding scheme by introducing a new category, check, as in the MapTask coding scheme (Carletta et al. 1997).
Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ Previous studies of discourse deixis annotation showed that these are extremely difficult judgments to make (Eckert and Strube 2000; Navarretta 2000; Byron 2002), except perhaps for identifying the type of object (Poesio and Modjeska 2005), so we simplified the task by only requiring our participants to identify the boundaries of the area of text in which the antecedent was introduced.
Artstein and Poesio (2008) note that most of a coder's judgements are non-boundaries. $$$$$ 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks (e.g., V´eronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and McGee Wood 2004; Mieskes and Strube 2006).

We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ For every (ordered) pair of distinct values ka, kb ∈ K there are nikanikb pairs of judgments of item i, whereas for non-distinct values there are nika(nika − 1) pairs.
We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ More precisely, and looking ahead to the following discussion, observed agreement is the arithmetic mean of the agreement value agri for all items i ∈ I, defined as follows: For example, let us assume a very simple annotation scheme for dialogue acts in information-seeking dialogues which makes a binary distinction between the categories statement and info-request, as in the DAMSL dialogue act scheme (Allen and Core 1997).
We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ Confusion also arises from the use of the letter P, which is used in the literature with at least three distinct interpretations, namely “proportion,” “percent,” and “probability.” We will use the following notation uniformly throughout the article. respectively.
We calculate agreement (Apia) as pairwise mean S (scaled by each item's size) to enable agreement to quantify near misses leniently, and chance agreement (Apie) can be calculated as in Artstein and Poesio (2008). $$$$$ This article is a survey of methods for measuring agreement among corpus annotators.
