Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ This test set and all test sets, unless noted otherwise are Section-23 of PropBank.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ Fig ure 1 shows the syntax tree representation along with the argument labels for an example structure extracted from the PropBank corpus.Most of the experiments in this paper, unless speci fied otherwise, are performed on the July 2002 release of PropBank.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ Table 17 shows coverage for the same features onthe AQUAINT test set.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ Features Arguments non-Arguments (%) (%) Predicate, Path 87.60 2.91 Predicate, Head Word 48.90 26.55 Cluster, Path 96.31 4.99 Cluster, Head Word 83.85 60.14 Path 99.13 15.15 Head Word 93.02 90.59 Table 16: Feature Coverage on PropBank test set using parser trained on PropBank training set.

 $$$$$ Classes Task Hand-corrected parses P R F1 A (%) (%) (%) ALL Id. 95.2 92.5 93.8 ARGs Classification - - - 91.0 Id. + Classification 88.9 84.6 86.7 CORE Id. 96.2 93.0 94.6 ARGs Classification - - - 93.9 Id. + Classification 90.5 87.4 88.9 Table 6: Best system performance on all tasks using hand-corrected parses.
 $$$$$ In analyzing the performance of the system, it is useful to estimate the relative contribution of the various featuresets used.
 $$$$$ and neithercontain a named entity, but the former is ARGM LOC, whereas the latter is ARGM-TMP.
 $$$$$ However, removal of predicate information hurts performance significantly, so does the removal of a family of features, eg., all phrase types, or the head word (HW), first word (FW) and last word (LW)information.

Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ Owing to time limitations, we could not get the results on the argument iden tification task and the combined argument identification and classification task using automatic parses.
Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ Argument Classification ? Given constituents known torepresent arguments of a predicate, assign the appropri ate argument labels to them.Argument Identification and Classification ? A combina tion of the above two tasks.
Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ There is a significant drop in the precision and recallnumbers for the AQUAINT test set (compared to the pre cision and recall numbers for the PropBank test set which were 84% and 75% respectively).
Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ is a NON-NULL node, since it does correspond to a semantic argument ? ARGM-TMP.

 $$$$$ ity of being NULL.
 $$$$$ We will also report some final best performance numbers on this corpus.
 $$$$$ In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.
 $$$$$ In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.

We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ There is a significant drop in the precision and recallnumbers for the AQUAINT test set (compared to the pre cision and recall numbers for the PropBank test set which were 84% and 75% respectively).
We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ However, removal of predicate information hurts performance significantly, so does the removal of a family of features, eg., all phrase types, or the head word (HW), first word (FW) and last word (LW)information.
We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ + Classification 75.2 63.3 68.7 Table 15: Performance on the AQUAINT test set.
We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ Classes Task P R F1 A (%) (%) (%) ALL Id. 90.9 89.8 90.4 ARGs Classification - - - 87.9 Id. + Classification 83.3 78.5 80.8 CORE Id. 94.7 90.1 92.3 ARGs Classification - - - 91.4 Id. + Classification 88.4 84.1 86.2 Table 1: Baseline performance on all three tasks using hand-corrected parses.

Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ We lemmatized the predicate using the XTAG morphology database5 (Daniel et al, 1992).
Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ For example, in Figure 1, the path from ARG0 ? ?He?
Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ Table 1 shows the baseline performance numbers on the three tasks mentioned earlier; these results are based onsyntactic features computed from hand-corrected Tree Bank (hence LDC hand-corrected) parses.For the argument identification and the combined iden tification and classification tasks, we report the precision (P), recall (R) and the F14 scores, and for the argument classification task we report the classification accuracy (A).
Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ In analyzing the performance of the system, it is useful to estimate the relative contribution of the various featuresets used.

We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ Key aspects of our results include significant improvement via an SVM classifier, improvement from new features and a series of analytic experiments on the contributions of the features.
We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ We would like to thank Ralph Weischedel and Scott Miller ofBBN Inc. for letting us use their named entity tagger ? Iden tiFinder; Martha Palmer for providing us with the PropBank data, Valerie Krugler for tagging the AQUAINT test set with PropBank arguments, and all the anonymous reviewers for their helpful comments.
We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ ity of being NULL.
We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ + Classification 75.2 63.3 68.7 Table 15: Performance on the AQUAINT test set.

For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ Thisseems to be logical considering the fact that the ADJUNC TIVE ARGUMENTS are not linguistically constrained inany way as to their position in the sequence of argu ments, or even the quantity.
For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ The best system is trained by first filtering the mostlikely nulls using the best NULL vs NON-NULL classifier trained using all the features whose argument identi fication F1 score is marked in bold in Table 4, and thentraining a ONE vs ALL classifier using the data remain ing after performing the filtering and using the features that contribute positively to the classification task ? ones whose accuracies are marked in bold in Table 4.
For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ The problem of shallow semantic parsing can be viewed as three different tasks.Argument Identification ? This is the process of identi fying parsed constituents in the sentence that represent semantic arguments of a given predicate.
For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.

To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ In this ?This research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grant IS-9978025 paper, we report on a series of experiments exploring this approach.
To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ This indicates that thesyntactic parser performance directly influences the argu ment boundary identification performance.
To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ Wecould not provide the numbers for argument identifica tion performance upon removal of the path feature since that made the SVM training prohibitively slow, indicating that the SVM had a very hard time separating the NULL class from the NON-NULL class.
To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ We show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the AQUAINT corpus.

Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ The second ?C&R System II?
Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ is a NULL node because it does not correspond to a semantic argument.
Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ In real-word applications, the system will have to extract features from an automatically generated parse.

 $$$$$ Task P R F1 A (%) (%) (%)ALL Id. 75.8 71.4 73.5 ARGs Classification - - - 83.8Id.
 $$$$$ Voice ? Whether the predicate is realized as an ac tive or passive construction.
 $$$$$ Table 6 shows the performance of this system.
 $$$$$ Voice ? Whether the predicate is realized as an ac tive or passive construction.

 $$$$$ Table 8 shows the same information as in previous Tables 6 and7, but generated using the new data.
 $$$$$ Key aspects of our results include significant improvement via an SVM classifier, improvement from new features and a series of analytic experiments on the contributions of the features.
 $$$$$ Predicate ? The predicate itself is used as a feature.
 $$$$$ is a NON-NULL node, since it does correspond to a semantic argument ? ARGM-TMP.

 $$$$$ Each node in the parse tree can be classified as eitherone that represents a semantic argument (i.e., a NONNULL node) or one that does not represent any seman tic argument (i.e., a NULL node).
 $$$$$ We also analyzed the transferability of the features to a new text source.
 $$$$$ Owing to the Feb 2004 release of much more and com pletely adjudicated PropBank data, we have a chance to5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph 1.5.tar.gz Classes Task Automatic parses P R F1 A (%) (%) (%) ALL Id. 89.3 82.9 86.0 ARGs Classification - - - 90.0 Id. + Classification 84.0 75.3 79.4 CORE Id. 92.0 83.3 87.4 ARGs Classification - - - 90.5 Id. + Classification 86.4 78.4 82.2 Table 7: Performance degradation when using automatic parses instead of hand-corrected ones.
 $$$$$ path is the most salient feature.

We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ Position ? This is a binary feature identifying whether the phrase is before or after the predicate.?
We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ In addition to these CORE ARGUMENTS, additional ADJUNCTIVE ARGUMENTS, referred to asARGMs are also marked.
We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ The system as described above might label two con stituents NON-NULL even if they overlap in words.
We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ When pre sented with a sentence, a parser should, for each predicatein the sentence, identify and label the predicate?s seman tic arguments.

They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ Table 6 shows the performance of this system.
They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ These features were named enti ties, head word part of speech and verb clusters.
They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ The remaining OVA classifiers are trained on the nodes passed by thefilter (approximately 20% of the total), resulting in a con siderable savings in training time.
They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ report our performance numbers on this data set.

For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ The clustering algorithm uses a database of verb-direct-object rela tions extracted by Lin (1998).
For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ Classes Task Hand-corrected parses P R F1 A (%) (%) (%) ALL Id. 95.2 92.5 93.8 ARGs Classification - - - 91.0 Id. + Classification 88.9 84.6 86.7 CORE Id. 96.2 93.0 94.6 ARGs Classification - - - 93.9 Id. + Classification 90.5 87.4 88.9 Table 6: Best system performance on all tasks using hand-corrected parses.
For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ ALL ARGs Task P R F1 A (%) (%) (%) HAND Id. 96.2 95.8 96.0 Classification - - - 93.0 Id. + Classification 89.9 89.0 89.4 AUTOMATIC Classification - - - 90.1 Table 8: Best system performance on all tasks using hand-corrected parses using the latest PropBank data.
For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ Dynamic class context ? In the task of argument.

As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ About 99% of the predicates inthe AQUAINT test set were seen in the PropBank train ing set.
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ Addingfeatures that are generalizations of the more specific features seemed to help.
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ In the July 2002 release, thetraining set comprises about 51,000 sentences, instantiat ing about 132,000 arguments, and the test set comprises 2,700 sentences instantiating about 7,000 arguments.
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ Classes Task Hand-corrected parses P R F1 A (%) (%) (%) ALL Id. 95.2 92.5 93.8 ARGs Classification - - - 91.0 Id. + Classification 88.9 84.6 86.7 CORE Id. 96.2 93.0 94.6 ARGs Classification - - - 93.9 Id. + Classification 90.5 87.4 88.9 Table 6: Best system performance on all tasks using hand-corrected parses.

A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ Classes Task P R F1 A (%) (%) (%) ALL Id. 90.9 89.8 90.4 ARGs Classification - - - 87.9 Id. + Classification 83.3 78.5 80.8 CORE Id. 94.7 90.1 92.3 ARGs Classification - - - 91.4 Id. + Classification 88.4 84.1 86.2 Table 1: Baseline performance on all three tasks using hand-corrected parses.
A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ Table 1 shows the baseline performance numbers on the three tasks mentioned earlier; these results are based onsyntactic features computed from hand-corrected Tree Bank (hence LDC hand-corrected) parses.For the argument identification and the combined iden tification and classification tasks, we report the precision (P), recall (R) and the F14 scores, and for the argument classification task we report the classification accuracy (A).
A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ ALL ARGs Task P R F1 A (%) (%) (%) HAND Id. 96.2 95.8 96.0 Classification - - - 93.0 Id. + Classification 89.9 89.0 89.4 AUTOMATIC Classification - - - 90.1 Table 8: Best system performance on all tasks using hand-corrected parses using the latest PropBank data.
A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.

A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ The problem of shallow semantic parsing can be viewed as three different tasks.Argument Identification ? This is the process of identi fying parsed constituents in the sentence that represent semantic arguments of a given predicate.
A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ Like previous work, our parser is based on a supervised machine learning approach.
A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ The problem of shallow semantic parsing can be viewed as three different tasks.Argument Identification ? This is the process of identi fying parsed constituents in the sentence that represent semantic arguments of a given predicate.
A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ For the initial experiments, we adopted the approachdescribed by Gildea and Jurafsky (2002) (G&J) and evaluated a series of modifications to improve its performance.

 $$$$$ A sigmoid function is fitted to the raw scores to convert the scores to probabilities as described by (Platt, 2000).
 $$$$$ Table 8 shows the same information as in previous Tables 6 and7, but generated using the new data.
 $$$$$ Owing to the Feb 2004 release of much more and com pletely adjudicated PropBank data, we have a chance to5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph 1.5.tar.gz Classes Task Automatic parses P R F1 A (%) (%) (%) ALL Id. 89.3 82.9 86.0 ARGs Classification - - - 90.0 Id. + Classification 84.0 75.3 79.4 CORE Id. 92.0 83.3 87.4 ARGs Classification - - - 90.5 Id. + Classification 86.4 78.4 82.2 Table 7: Performance degradation when using automatic parses instead of hand-corrected ones.
 $$$$$ Absence of this information can be potentially confusing to the learning mechanism.
