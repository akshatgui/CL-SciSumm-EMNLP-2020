Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ This is calculated using a head word table described by (Magerman, 1994) and modified by (Collins, 1999, Appendix.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ Classes Task Hand-corrected parses P R F1 A (%) (%) (%) ALL Id. 95.2 92.5 93.8 ARGs Classification - - - 91.0 Id. + Classification 88.9 84.6 86.7 CORE Id. 96.2 93.0 94.6 ARGs Classification - - - 93.9 Id. + Classification 90.5 87.4 88.9 Table 6: Best system performance on all tasks using hand-corrected parses.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ We tested several new features.
Note that the result here is not comparable with the best in this domain (Pradhan et al., 2004) where the full parse tree is assumed given. $$$$$ A sigmoid function is fitted to the raw scores to convert the scores to probabilities as described by (Platt, 2000).

 $$$$$ The clustering algorithm uses a database of verb-direct-object rela tions extracted by Lin (1998).
 $$$$$ Table 1 shows the baseline performance numbers on the three tasks mentioned earlier; these results are based onsyntactic features computed from hand-corrected Tree Bank (hence LDC hand-corrected) parses.For the argument identification and the combined iden tification and classification tasks, we report the precision (P), recall (R) and the F14 scores, and for the argument classification task we report the classification accuracy (A).
 $$$$$ classifiers, one of which is the NULL-NON-NULL classifier.With this strategy only one classifier (NULL vs NON NULL) has to be trained on all of the data.
 $$$$$ 13.3 Argument Identification (NULL vs NON-NULL).

Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ Task P R F1 A (%) (%) (%)ALL Id. 75.8 71.4 73.5 ARGs Classification - - - 83.8Id.
Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ Table 6 shows the performance of this system.
Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ Owing to the Feb 2004 release of much more and com pletely adjudicated PropBank data, we have a chance to5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph 1.5.tar.gz Classes Task Automatic parses P R F1 A (%) (%) (%) ALL Id. 89.3 82.9 86.0 ARGs Classification - - - 90.0 Id. + Classification 84.0 75.3 79.4 CORE Id. 92.0 83.3 87.4 ARGs Classification - - - 90.5 Id. + Classification 86.4 78.4 82.2 Table 7: Performance degradation when using automatic parses instead of hand-corrected ones.
Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al, 2004). $$$$$ To evaluate this scenario, we used the Charniak parser (Chaniak, 2001) to generate parses for PropBank training and test data.

 $$$$$ is a NULL node because it does not correspond to a semantic argument.
 $$$$$ Since SVMs are binary classifiers, we have to con vert the multi-class problem into a number of binary-class problems.
 $$$$$ The tables show featurecoverage for constituents that were Arguments and con stituents that were NULL.
 $$$$$ PropBank was constructed by assigning semantic arguments to constituents of the hand-corrected TreeBank parses.

We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ which uses some additional features.
We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ is a NON-NULL node, since it does correspond to a semantic argument ? ARGM-TMP.
We compared our system to the freely available Assert system (Pradhan et al, 2004). $$$$$ The remaining training data is used to train OVA.

Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ We call this ?SurdeanuSystem I.?
Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ For example, in the tree of Figure 1, the node IN that encompasses ?for?
Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ A binary NULL vs NON-NULL classifier is trained on the entire dataset.
Because ASSERT uses a parser, and because PropBank was built by labeling the nodes of a hand-annotated parse tree, per node accuracy is usually reported in papers such as (Pradhan et al, 2004). $$$$$ This is calculated using a head word table described by (Magerman, 1994) and modified by (Collins, 1999, Appendix.

We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ Classes System Hand Automatic P R F1 P R F1 ALL SVM 89 85 87 84 75 79 ARGs G&H System I 76 68 72 71 63 67 G&P 71 64 67 58 50 54 CORE SVM System 90 87 89 86 78 82 ARGs G&H System I 82 79 80 76 73 75 C&R System II - - - 65 75 70 Table 14: Identification and classification
We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ We have described an algorithm which significantly im proves the state-of-the-art in shallow semantic parsing.
We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ to the predicate talked, is represented with the string NP?S?VP?VBD.
We measured the argument classification accuracy of our network, assuming the correct segmentation is given to our system, as in (Pradhan et al, 2004), by post-processing our per-word tags to form a majority vote over each segment. $$$$$ We have described an algorithm which significantly im proves the state-of-the-art in shallow semantic parsing.

For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ Also, being more specific they might not transfer well across domains.
For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ ity of being NULL.
For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ While training the language model, we can either use the actual pred icate to estimate the transition probabilities in and out of the predicate, or we can perform a joint estimationover all the predicates.
For these reasons, we use a semantic role labeler (Pradhan et al, 2004) to provide and delimit the text spans that contain the semantic arguments of a predicate. $$$$$ The lower part of the table shows the per formance of some feature combinations by themselves.Table 10 shows the feature salience on the task of ar gument identification.

To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ To evaluate this scenario, we used the Charniak parser (Chaniak, 2001) to generate parses for PropBank training and test data.
To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ We would like to thank Ralph Weischedel and Scott Miller ofBBN Inc. for letting us use their named entity tagger ? Iden tiFinder; Martha Palmer for providing us with the PropBank data, Valerie Krugler for tagging the AQUAINT test set with PropBank arguments, and all the anonymous reviewers for their helpful comments.
To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ This indicates that thesyntactic parser performance directly influences the argu ment boundary identification performance.
To compute the semantic roles for the source trees, we use an in-house max-ent classifier with features following Xue and Palmer (2004) and Pradhan et al (2004). $$$$$ classifiers, one of which is the NULL-NON-NULL classifier.With this strategy only one classifier (NULL vs NON NULL) has to be trained on all of the data.

Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ Classes Task Hand-corrected parses P R F1 A (%) (%) (%) ALL Id. 95.2 92.5 93.8 ARGs Classification - - - 91.0 Id. + Classification 88.9 84.6 86.7 CORE Id. 96.2 93.0 94.6 ARGs Classification - - - 93.9 Id. + Classification 90.5 87.4 88.9 Table 6: Best system performance on all tasks using hand-corrected parses.
Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ We have described an algorithm which significantly im proves the state-of-the-art in shallow semantic parsing.
Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ ALL ARGs Task P R F1 A (%) (%) (%) HAND Id. 96.2 95.8 96.0 Classification - - - 93.0 Id. + Classification 89.9 89.0 89.4 AUTOMATIC Classification - - - 90.1 Table 8: Best system performance on all tasks using hand-corrected parses using the latest PropBank data.
Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al, 2004), decision trees (Surdeanu et al, 2003), and log-linear models (Xue and Palmer, 2004). $$$$$ For example, in the tree of Figure 1, the node IN that encompasses ?for?

 $$$$$ Table 1 shows the baseline performance numbers on the three tasks mentioned earlier; these results are based onsyntactic features computed from hand-corrected Tree Bank (hence LDC hand-corrected) parses.For the argument identification and the combined iden tification and classification tasks, we report the precision (P), recall (R) and the F14 scores, and for the argument classification task we report the classification accuracy (A).
 $$$$$ All the nodes are classified directly as NULL or one of the arguments using the classifiertrained in step 2 above.
 $$$$$ Table 13 compares the argument classification accuracies of various systems, and at various levels of classification granularity, and parse accuracy.
 $$$$$ Owing to time limitations, we could not get the results on the argument iden tification task and the combined argument identification and classification task using automatic parses.

 $$$$$ We lemmatized the predicate using the XTAG morphology database5 (Daniel et al, 1992).
 $$$$$ However, we found that there was an improve ment in the CORE ARGUMENT accuracy on the combined task of identifying and assigning semantic arguments, given hand-corrected parses, whereas the accuracy of the ADJUNCTIVE ARGUMENTS slightly deteriorated.
 $$$$$ Like previous work, our parser is based on a supervised machine learning approach.

 $$$$$ Features P R F1 (%) (%) All 95.2 92.5 93.8 All except HW 95.1 92.3 93.7 All except Predicate 94.5 91.9 93.2 Table 10: Performance of various feature combinations on the task of argument identification
 $$$$$ S hhhh (((( NP PRP He ARG0 VP hhhh (((( VBD talked predicate PP hhh ((( IN for NULL NP hhhhh ((((( about 20 minutes ARGM ? TMPFigure 1: Syntax tree for a sentence illustrating the Prop Bank tags.
 $$$$$ + Classification 65.2 61.5 63.3 CORE Id. 88.4 74.4 80.8 ARGs Classification - - - 84.0Id.
 $$$$$ This is a problem since overlapping arguments are not allowedin PropBank.

We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ Like previous work, our parser is based on a supervised machine learning approach.
We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ In this paper, we propose a machine learning algorithm for shallow semantic parsing, extend ing the work of Gildea and Jurafsky (2002),Surdeanu et al (2003) and others.
We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ We first convert the raw SVM scores to probabilities using a sigmoid function.
We adopted the ASSERT English SRL labeler (Pradhan et al, 2004), which was trained on PropBank data using SVM classifier. $$$$$ We show perfor mance improvements through a number of newfeatures and measure their ability to general ize to a new test set drawn from the AQUAINT corpus.

They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ We lemmatized the predicate using the XTAG morphology database5 (Daniel et al, 1992).
They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ We also analyzed the transferability of the features to a new text source.
They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ Our al gorithm is based on Support Vector Machineswhich we show give an improvement in performance over earlier classifiers.
They use ASSERT (Pradhan et al, 2004), a publicly available shallow semantic parser trained on PropBank, to generate predicate-argument structures which subsequently form the basis of comparison between question and answer sentences. $$$$$ The best system is trained by first filtering the mostlikely nulls using the best NULL vs NON-NULL classifier trained using all the features whose argument identi fication F1 score is marked in bold in Table 4, and thentraining a ONE vs ALL classifier using the data remain ing after performing the filtering and using the features that contribute positively to the classification task ? ones whose accuracies are marked in bold in Table 4.

For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ In real-word applications, the system will have to extract features from an automatically generated parse.
For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ Neither method showed any improvement.
For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ Head Word ? The syntactic head of the phrase.
For semantic analysis, we used the ASSERT toolkit (Pradhan et al, 2004) that produces shallow semantic parses using the PropBank conventions. $$$$$ We have described an algorithm which significantly im proves the state-of-the-art in shallow semantic parsing.

As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ Our al gorithm is based on Support Vector Machineswhich we show give an improvement in performance over earlier classifiers.
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ Constituent relative features ? These are nine fea-.
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ In the experiments reported here, we first re placed their statistical classification algorithm with one that uses Support Vector Machines and then added to theexisting feature set.
As mentioned by (Pradhan et al, 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. $$$$$ About 99% of the predicates inthe AQUAINT test set were seen in the PropBank train ing set.

A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ The tables show featurecoverage for constituents that were Arguments and con stituents that were NULL.
A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ There is a significant drop in the precision and recallnumbers for the AQUAINT test set (compared to the pre cision and recall numbers for the PropBank test set which were 84% and 75% respectively).
A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ One important observation we canmake here is that the path feature is the most salient fea ture in the task of argument identification, whereas it is the least salient in the task of argument classification.
A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al, 2004) is that by identifying relations as part of a frame, you have identified a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. $$$$$ We lemmatized the predicate using the XTAG morphology database5 (Daniel et al, 1992).

A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ In real-word applications, the system will have to extract features from an automatically generated parse.
A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ For example, in Figure 1, the path from ARG0 ? ?He?
A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ CCG is a form of dependencygrammar and is hoped to capture long distance relationships better than a phrase structure grammar.
A pair of sentences is first fed to a syntactic parser (Charniak, 2000) and then passed to a semantic role labeler (ASSERT; (Pradhan et al, 2004)), to label predicate argument tuples. $$$$$ The remaining training data is used to train OVA.

 $$$$$ To simplify the search, we allowed only NULL assignments to nodeshaving a NULL likelihood above a threshold.
 $$$$$ To evaluate this scenario, we used the Charniak parser (Chaniak, 2001) to generate parses for PropBank training and test data.
 $$$$$ Since SVMs are binary classifiers, we have to con vert the multi-class problem into a number of binary-class problems.
 $$$$$ Partial Path ? For the argument identification task,.
