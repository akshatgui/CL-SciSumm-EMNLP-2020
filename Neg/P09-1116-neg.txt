We tried two kinds of indicator features $$$$$ Our NER system achieves the best current result on the widely used CoNLL benchmark.
We tried two kinds of indicator features $$$$$ Named entity recognition (NER) is one of the first steps in many applications of information extraction, information retrieval, question answering and other applications of NLP.
We tried two kinds of indicator features $$$$$ When we extract features from multiple clusterings, the selection of the top-N clusters is done separately for each clustering.

Lin and Wu (2009) further explored a two-stage cluster-based approach $$$$$ We did not observe a similar phenomenon with our CRF.
Lin and Wu (2009) further explored a two-stage cluster-based approach $$$$$ The performance of our baseline system is rather mediocre because it has far fewer feature functions than the more competitive systems.
Lin and Wu (2009) further explored a two-stage cluster-based approach $$$$$ Our query categorization system is on par with the best system in KDDCUP 2005, which, unlike ours, involved a great deal of knowledge engineering effort.

Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ The algorithm is quadratic in the number of elements to be clustered.
Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ Since the cosine measure of two unit length vectors is simply their dot product, when searching for the closest centroid to an element, we only care about features in the centroids that are in common with the element.

Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ The last column lists the numbers of phrases we used when running the clustering with that corpus.
Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ Over the past decade, supervised learning algorithms have gained widespread acceptance in natural language processing (NLP).

 $$$$$ Table 6 shows the scores of each of the three human labelers when each of them is evaluated against the other two.
 $$$$$ Phrasal clusters obtained from the LDC corpus give the same level of improvement as word clusters from the web corpus that is 20 times larger.
 $$$$$ The named entity problem in Section 3 and the query classification problem in Section 4 exemplify the two scenarios.
 $$$$$ In addition to word-clusters, we also use phraseclusters as features.

Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ We demonstrated the power and generality of this approach on two very different applications: named entity recognition and query classification.
Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ In the literature, contexts have been defined as subject and object relations involving the word (Hindle, 1990), as the documents containing the word (Deerwester et al, 1990), or as search engine snippets for the word as a query (Sahami and Heilman, 2006).
Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ There are two main scenarios that motivate semi-supervised learning.

We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ Here, bow indicates the use of bag-of-words features; WN refers to word clusters of size N; and PN refers to phrase clusters of size N. All the clusters are soft clusters created with the web corpus using 3word context windows.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ Out of context, natural language words are often ambiguous.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ In this paper, we present a semi-supervised learning algorithm that goes a step further.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ The disambiguation power of phrases is also evidenced by the improvements of phrase-based machine translation systems (Koehn et. al., 2003) over word-based ones.

Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ 2001) is one of the most competitive NER algorithms.
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ This technique has also been used with Brown clustering (Miller et. al.

Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We demonstrated the power and generality of this approach on two very different applications: named entity recognition and query classification.

Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ With 3word context windows, the cluster is about language learning and translation.
Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ When training the classifier for a class with p positive examples out of a total of n examples, we change the objective function to: With this modification, the total weight of the positive and negative examples become equal.

 $$$$$ When creating the word clusters, we do not rely on a predefined list.
 $$$$$ The positions of the result pages in the directory hierarchies as well as the words in the pages are used to classify the queries.
 $$$$$ The baseline system uses only the words in the queries as features (the bag-of-words representation), treating the query classification problem as a typical text categorization problem.
 $$$$$ One would never have guessed that it is a company name based on the clusters containing Odds and Land.

We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ Ando and Zhang (2005) defined an objective function that combines the original problem on the labeled data with a set of auxiliary problems on unlabeled data.
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ The goal of query classification is to determine to which ones of a predefined set of classes a query belongs.
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ We define the contexts of a phrase to be small, fixed-sized windows centered on occurrences of the phrase in a large corpus.
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ Although K-Means is generally described as a hard clustering algorithm (each element belongs to at most one cluster), it can produce soft clustering simply by assigning an element to all clusters whose similarity to the element is greater than a threshold.

Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We demonstrated the power and generality of this approach on two very different applications: named entity recognition and query classification.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ Another advantage is that it can be applied to a wider range of domains and problems.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ Uszkoreit and Brants (2008) proposed a distributed clustering algorithm with a similar objective function as the Brown algorithm.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ For example, in the named entity recognition task, categorical clusters are more successful, whereas in query categorization, the topical clusters are much more beneficial.

K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ Our system achieved the best current result on the CoNLL NER data set.
K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ However, they require clusters to be strictly hierarchical, whereas we do not.
K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ The CoNLL data set consists of news articles from Reuters1.

Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ The training set consists of 111 example queries, each of which belongs to up to 5 of the 67 categories.
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ If a phrase belonging to cluster c is found at positions b to e (inclusive), we add the following features to the CRF classifier: where B (before), A (after), S (start), M (middle), and E (end) denote a position in the input sequence relative to the phrase belonging to cluster c. We treat the cluster membership as binary.
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ We demonstrate the advantages of phrase-based clusters over word-based ones with experimental results from two distinct application domains: named entity recognition and query classification.
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ In the two-stage cluster-based approaches such as ours, clustering is mostly decoupled from the supervised learning problem.

We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. $$$$$ To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification.

In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ We demonstrated the power and generality of this approach on two very different applications: named entity recognition and query classification.
In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ In fact, only 12% of the words in the 800 test queries are found in the training examples.
In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ The goal of query classification is to determine to which ones of a predefined set of classes a query belongs.

Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ This technique has also been used with Brown clustering (Miller et. al.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ Furthermore, when summing up a large number of features vectors, numerical underflow becomes a potential problem.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ Our NER system achieves the best current result on the widely used CoNLL benchmark.

As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Since the clusters are obtained without any labeled data, they may not correspond directly to concepts that are useful for decision making in the problem domain.
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ When creating the word clusters, we do not rely on a predefined list.
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Our query categorization system is on par with the best system in KDDCUP 2005, which, unlike ours, involved a great deal of knowledge engineering effort.
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Our query categorization system is on par with the best system in KDDCUP 2005, which, unlike ours, involved a great deal of knowledge engineering effort.

 $$$$$ Since topical information is much more relevant to query classification than categorical information, we use clusters created with 3-word context windows.
 $$$$$ The performance of such learning-based solutions thus crucially depends on the informativeness of the features.
 $$$$$ Since the query classes are not mutually exclusive, we treat the query classification task as 67 binary classification problems.
