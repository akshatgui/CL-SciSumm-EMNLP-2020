We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). $$$$$ The training set consists of 111 example queries, each of which belongs to up to 5 of the 67 categories.
We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). $$$$$ Our NER system achieves the best current result on the widely used CoNLL benchmark.
We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). $$$$$ However, they require clusters to be strictly hierarchical, whereas we do not.
We tried two kinds of indicator features: Bucket features: For both parent and child vectors in a potential dependency, we fire one indicator feature per dimension of each embedding. A similar effect, when changing distributional context window sizes, was found by Lin and Wu (2009). $$$$$ Since the centroids of clusters are assumed to be constant within each iteration, the assignment of elements to clusters (Step ii) can be done totally independently.

Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. $$$$$ We impose an upper limit on the number of instances of each phrase when constructing its feature vector.
Lin and Wu (2009) further explored a two-stage cluster-based approach: first clustering phrases and then relying on a supervised learner to identify useful clusters and assign proper weights to cluster features. $$$$$ However, they require clusters to be strictly hierarchical, whereas we do not.

Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts.
Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ The cluster features are the cross product of the unigram/bigram labels and the attributes.
Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ Our results show that phrase clusters offer significant improvements over word clusters.
Our phrase pair clustering approach is similar inspirit to the work of Lin and Wu (2009), who use K means to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. $$$$$ We demonstrated the power and generality of this approach on two very different applications: named entity recognition and query classification.

Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ To demonstrate the power and generality of this approach, we apply the method in two very different applications: named entity recognition and query classification.
Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ The second consists of various news texts from LDC: English Gigaword, the Tipster corpus and Reuters RCV1.
Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type's contexts by averaging the contexts of all its phrase instances. $$$$$ This approach is taken by (Miller et. al.

 $$$$$ For each of these clusters, we sum the clusterâ€™s similarity to all the phrases in the query and select the top-N as features for the logistic regression classifier (N=150 in our experiments).
 $$$$$ 2004), (Wong and Ng 2007), (Suzuki and Isozaki 2008), and (Koo et. al., 2008), as well as this paper.
 $$$$$ The best F-score of 90.90, which is about 1 point higher than the previous best result, is obtained with a combination of clusters.

Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ One advantage of the two-stage approach is that the same clusterings may be used for different problems or different components of the same system.
Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ Another is to further boost the performance of a supervised classifier that is already trained with a large amount of supervised data.
Our clustering approach is related to Lin and Wu's work (Lin and Wu, 2009). $$$$$ Furthermore, when summing up a large number of features vectors, numerical underflow becomes a potential problem.

We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ Table 5 shows three example queries and their classes.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ An important advantage of not needing a POS tagger as a preprocessor is that the system is much easier to adapt to other languages, since training a tagger often requires a larger amount of more extensively annotated data than the training data for NER.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ Our system does not need this information to achieve its peak performance.
We check the match of not just the top 1 cluster ids, but also farther down in the 20 sized lists because, as discussed in Lin and Wu (2009), the soft cluster assignments often reveal different senses of a word. $$$$$ Our results show that phrase clusters offer significant improvements over word clusters.

Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ It is able to cluster tens of thousands of words, but is not scalable enough to deal with tens of millions of phrases.
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ With larger windows, the clusters tend to be more topical, whereas smaller windows result in categorical clusters.
Lin and Wu (2009) report an F1 score of 90.90 on the original split of the CoNLL data. $$$$$ Our results show that phrase clusters offer significant improvements over word clusters.

Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We present a simple and scalable algorithm for clustering tens of millions of phrases and use the resulting clusters as features in discriminative classifiers.

Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.
Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ Our system achieved the best current result on the CoNLL NER data set.
Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. $$$$$ We presented a simple and scalable algorithm to cluster tens of millions of phrases and we used the resulting clusters as features in discriminative classifiers.

 $$$$$ Table 7 contains the evaluation results of various configurations of our system.
 $$$$$ An important advantage of not needing a POS tagger as a preprocessor is that the system is much easier to adapt to other languages, since training a tagger often requires a larger amount of more extensively annotated data than the training data for NER.

We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ One advantage of the two-stage approach is that the same clusterings may be used for different problems or different components of the same system.
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ The participating systems were evaluated by their average F-scores (F1) and average precision (P) over these three sets of answer keys for the 800 selected queries.
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ We can easily use multiple clusterings in feature extraction.
We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and for NER Lin and Wu (2009). $$$$$ Another advantage is that it can be applied to a wider range of domains and problems.

Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ 2004, Koo, et. al.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ Suzuki and Isozaki (2008), on the other hand, used the automatically labeled corpus to train HMMs.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ Our system does not need this information to achieve its peak performance.
Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. $$$$$ We suspect that such features make the optimization of the objective function much more difficult.

K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ They manually constructed a mapping from the query classes to hierarchical directories such as Google Directory3 or Open Directory Project4.
K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ They have become the workhorse in almost all sub-areas and components of NLP, including part-ofspeech tagging, chunking, named entity recognition and parsing.
K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ The algorithm is quadratic in the number of elements to be clustered.
K-Means clustering algorithm described in Lin and Wu (2009). $$$$$ Our query categorization system is on par with the best system in KDDCUP 2005, which, unlike ours, involved a great deal of knowledge engineering effort.

Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ Named entity recognition (NER) is one of the first steps in many applications of information extraction, information retrieval, question answering and other applications of NLP.
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ We therefore expect it to produce mostly categorical clusters.
Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional context of that word. $$$$$ The features are the words (tokens) in the window.

We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. $$$$$ Our results show that phrase clusters offer significant improvements over word clusters.
We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. $$$$$ The training set has 203,621 tokens and the development and test set have 51,362 and 46,435 tokens, respectively.
We follow Lin and Wu (2009) in applying various thresholds during K-Means, such as a frequency threshold for the initial vocabulary, a total count threshold for the feature vectors, and a threshold for PMI scores. $$$$$ Phrases are much less so because the words in a phrase provide contexts for one another.

In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ These meanings are reflected in the top clusters assignments for Whistler in Table 2 (window size = 3).
In addition to the features described in Lin and Wu (2009), we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source-language (Spanish or Japanese in our experiments). $$$$$ Even though the phrases include single token words, we create word clusters with the same clustering algorithm as well.

Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ However, one can rely on a discriminative classifier to establish the connection by assigning proper weights to the cluster features.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ Our NER system achieves the best current result on the widely used CoNLL benchmark.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ Our query categorization system is on par with the best system in KDDCUP 2005, which, unlike ours, involved a great deal of knowledge engineering effort.
Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. $$$$$ We demonstrated the power and generality of this approach on two very different applications: named entity recognition and query classification.

As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Our results show that phrase clusters offer significant improvements over word clusters.
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Over the past decade, supervised learning algorithms have gained widespread acceptance in natural language processing (NLP).
As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. $$$$$ Our query classifier is on par with the best system in KDDCUP 2005 without resorting to labor intensive knowledge engineering efforts.

 $$$$$ The algorithm fits nicely into the MapReduce paradigm for parallel programming (Dean and Ghemawat, 2004).
 $$$$$ In this paper, we present a semi-supervised learning algorithm that goes a step further.
 $$$$$ The context feature vector of a phrase is constructed by first aggregating the frequency counts of the words in the context windows of different instances of the phrase.
 $$$$$ Our query categorization system is on par with the best system in KDDCUP 2005, which, unlike ours, involved a great deal of knowledge engineering effort.
