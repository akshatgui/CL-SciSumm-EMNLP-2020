(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ However, due to the lack of strong assumption of transitivity, the selection procedure is in fact a greedy search.
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ By contrast, our approach adopts a twin-candidate learning model.
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ While the filter guarantees the qualification of the candidates, it removes too many positive candidates, and thus the recall suffers.
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).

Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ By contrast, our approach adopts a twin-candidate learning model.
Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ We started with the introduction of the singlecandidate model adopted by most supervised machine learning approaches.
Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ (3) Our approach makes use of more indicative features, such as Appositive, Name Alias, String-matching, etc.
Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected.

 $$$$$ Section 2 briefly describes the single-candidate model and analyzes its limitation.
 $$$$$ In our future work, we intend to adopt a looser filter together with an anaphoricity determination module (Bean and Riloff, 1999; Ng and Cardie, 2002b).
 $$$$$ Furthermore, we would like to incorporate more syntactic features into our feature set, such as grammatical role or syntactic parallelism.

Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ For a NP under consideration, all of its preceding NPs could be the antecedent candidates.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ (2) The antecedent identification in Connolly et al.’s approach is to apply the classifier to successive pairs of candidates, each time retaining the better candidate.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ After training, a classifier is ready to resolve the NPs1 encountered in a new document.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ In the MUC-6 data set, for example, the immediate antecedents of 95% pronominal anaphors can be found within the above distance.

learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ These features are effective especially for non-pronoun resolution.
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ One limitation of this model, however, is that it only considers the relationships between a NP encountered and one of its candidates at a time during its training and testing procedures.
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ During training, we create the candidate set for each anaphor with the following filtering algorithm: Features describing the two candidates During resolution, we filter the candidates for each encountered pronoun in the same way as during training.
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ In fact, the twin-candidate model in itself can identify the qualification of a candidate.

 $$$$$ Consider an anaphor ana and its candidate set candidate_set, {C1, C2, ..., Ck}, where Cj is closer to ana than Ci if j > i.
 $$$$$ The classifier is then used to determine the preference between any two candidates of an anaphor encountered in a new document.
 $$$$$ Let inst(ci , c0, ana) have the same class label as inst(ci , ana) , that is, inst(ci , c0, ana) is positive if Ci is the antecedent of ana; or negative if not.
 $$$$$ A feature vector is specified for each training or testing instance.

The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ Moreover, many of the preceding NPs are irrelevant or even invalid with regard to the anaphor.
The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ Traditionally, supervised machine learning approaches adopt the singlecandidate model.
The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ However, due to the lack of strong assumption of transitivity, the selection procedure is in fact a greedy search.

A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ However, due to the lack of strong assumption of transitivity, the selection procedure is in fact a greedy search.
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ Based on the feature vectors generated for each anaphor encountered in the training data set, a classifier can be trained using a certain machine learning algorithm, such as C4.5, RIPPER, etc.
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ As shown in Table 3, our approach resolves non-pronominal anaphors with the recall of 51.3 (39.7) and the precision of 90.4 (87.6) for MUC-6 (MUC-7).
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.

 $$$$$ The candidate filtering algorithm during resolution is as follows:
 $$$$$ The differences between our approach and theirs are: (1) In Connolly et al.’s approach, all the preceding NPs of an anaphor are taken as the antecedent candidates, whereas in our approach we use candidate filters to eliminate invalid or irrelevant candidates.
 $$$$$ The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the singlecandidate model.

 $$$$$ So far, various algorithms have been proposed to determine the preference relationship between two candidates.
 $$$$$ We argued that the confidence values returned by the single-candidate classifier are not reliable to be used as ranking criterion for antecedent candidates.
 $$$$$ The main reason is the absence of candidate filtering strategy in their approach (this is why the recall equals to the precision in the tables).

 $$$$$ (2) The antecedent identification in Connolly et al.’s approach is to apply the classifier to successive pairs of candidates, each time retaining the better candidate.
 $$$$$ These data noises may hamper the training of a goodperformanced classifier, and also damage the accuracy of the antecedent selection: too many comparisons are made between incorrect candidates.
 $$$$$ In such a model, a classifier is trained based on the instances formed by an anaphor and a pair of its antecedent candidates.
 $$$$$ Consider an anaphor ana and its candidate set candidate_set, {C1, C2, ..., Ck}, where Cj is closer to ana than Ci if j > i.

However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ That is to say, determining whether Ci is coreferential to ana by T1 in the single-candidate model equals to determining whether Ci is better than C0 w.r.t ana by T2 in the twin-candidate model.
However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ Consequently, the confidence values are unreliable to represent the true competition criterion for the candidates.
However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ In this paper we have proposed a competition learning approach to coreference resolution.
However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ Consequently, the confidence values cannot accurately represent the true competition criterion for the candidates.

Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ In fact, the twin-candidate model in itself can identify the qualification of a candidate.
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ The antecedent of an anaphor is identified using the algorithm shown in Figure 1.
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ Similar to those in the singlecandidate model, the features may describe the lexical, syntactic, semantic and positional relationships of an anaphor and any one of its candidates.

 $$$$$ Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.
 $$$$$ Input: ana: the anaphor under consideration candidate_set: the set of antecedent candidates of ana, {C1, C2,...,Ck} for i = 1 to K do Score[ i ] = 0; for i = K downto 2 do for j = i – 1 downto 1 do if CR( inst( ci , cj, ana )) = = positive then While the realization and the structure of the twincandidate model are significantly different from the single-candidate model, the single-candidate model in fact can be regarded as a special case of the twin-candidate model.
 $$$$$ One problem of the single-candidate model, however, is that it only takes into account the relationships between an anaphor and one individual candidate at a time, and overlooks the preference relationship between candidates.
 $$$$$ As shown in Table 2, our approach outperforms Ng and Cardie’s singlecandidate based approach by 3.7 and 5.4 in Fmeasure for MUC-6 and MUC-7, respectively.

These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ The recall measures the number of correctly resolved anaphors over the total anaphors in the MUC test data set, and the precision measures the number of correct anaphors over the total resolved anaphors.
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ Then some algorithms are applied to choose one of the remaining candidates, if any, as the antecedent.
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ In these tables we focus on the abilities of different approaches in resolving an anaphor to its antecedent correctly.
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ Traditionally, supervised machine learning approaches adopt the singlecandidate model.

 $$$$$ Consequently, we can safely draw the conclusion that the twin-candidate model is more powerful than the single-candidate model in characterizing the relationships among an anaphor and its candidates.
 $$$$$ In the above text segment, the antecedent candidate set of the pronoun “them?” consists of six candidates highlighted in Italics.
 $$$$$ “If some countries2 try to block China TO accession, that will not be popular and will fail to win the support of other countries3” she said.
 $$$$$ Based on the feature vectors generated for each anaphor encountered in the training data set, a classifier can be trained using a certain machine learning algorithm, such as C4.5, RIPPER, etc.

 $$$$$ Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.
 $$$$$ One limitation of this model, however, is that it only considers the relationships between a NP encountered and one of its candidates at a time during its training and testing procedures.
 $$$$$ After training, a classifier is ready to resolve the NPs1 encountered in a new document.
 $$$$$ Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.

On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ The F-measure F=2*RP/(R+P) is the harmonic mean of precision and recall.
On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ In this paper we propose a competition learning approach to coreference resolution.
On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ In our future work, we intend to adopt a looser filter together with an anaphoricity determination module (Bean and Riloff, 1999; Ng and Cardie, 2002b).

In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ In the experiment we compared our approach with the following research works: Among them, S-List, a version of centering algorithm, uses well-defined heuristic rules to rank the antecedent candidates; Ng and Cardie’s approach employs the standard single-candidate model and “Best-First” rule to select the antecedent; Connolly et al.’s approach also adopts the twin-candidate model, but their approach lacks of candidate filtering strategy and uses greedy linear search to select the antecedent (See “Related work” for details).
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ For a NP under consideration, all of its preceding NPs could be the antecedent candidates.
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ In particular, our approach obtains significant improvement (21.1 for MUC-6, and 13.1 for MUC-7) over Connolly et al.’s twin-candidate based approach.
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ Suppose positive_set is the set of candidates that occur in the coreferential chain of ana, and negative_set is the set of candidates not in the chain, that is, negative_set = candidate_set - positive_set.

Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).
Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ By contrast, our approach adopts a twin-candidate learning model.
Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ By contrast, our approach evaluates a candidate according to the times it wins over the other competitors.
