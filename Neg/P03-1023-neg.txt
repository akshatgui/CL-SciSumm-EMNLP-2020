(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ While the classification in the single-candidate model can find its interpretation in the twincandidate model, it is not true vice versa.
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ The layout of this paper is as follows.
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ The reason may be that in non-pronoun resolution, the coreference of an anaphor and its candidate is usually determined only by some strongly indicative features such as alias, apposition, string-matching, etc (this explains why we obtain a high precision but a low recall in non-pronoun resolution).
(2) Givenananaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al, 2003). $$$$$ Thus, we can safely adopt the single-candidate classifier as our candidate filter.

Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ Given the feature vector of a test instance inst(ci, cj , ana ) (i > j), the classifier returns the positive class indicating that Ci is preferred to Cj as the antecedent of ana; or negative indicating that Cj is preferred.
Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ Currently, we employ the single-candidate classifier to filter the candidate set during resolution.
Each instance is represented by 33 lexical, grammatical, semantic, and 540 positional features that have been employed by high performing resolvers such as Ng and Cardie (2002) and Yang et al (2003), as described below. $$$$$ Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.

 $$$$$ Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model.
 $$$$$ As shown in Table 2, our approach outperforms Ng and Cardie’s singlecandidate based approach by 3.7 and 5.4 in Fmeasure for MUC-6 and MUC-7, respectively.
 $$$$$ This nevertheless contradicts the ranking rules.
 $$$$$ Here we could take C0 as a “standard candidate”.

Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ Alternatively, we presented a twin-candidate model that learns the competition criterion for antecedent candidates directly.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ Alternatively, we presented a twin-candidate model that learns the competition criterion for antecedent candidates directly.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ Comparatively this algorithm could lead to a better solution.
Yang et al (2005) made use of nonanaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and thus equipped it with the nonanaphoricity determination capability. $$$$$ That is, we only consider the NPs in the current and the preceding 2 sentences.

learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ By contrast, our approach evaluates a candidate according to the times it wins over the other competitors.
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ The confidence value reflects the probability that the candidate is coreferential to the NP in the overall distribution2, but not the conditional probability when the candidate is concurrent with other competitors.
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ The confidence value reflects the probability that the candidate is coreferential to the NP in the overall distribution2, but not the conditional probability when the candidate is concurrent with other competitors.
learning method proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ The differences between our approach and theirs are: (1) In Connolly et al.’s approach, all the preceding NPs of an anaphor are taken as the antecedent candidates, whereas in our approach we use candidate filters to eliminate invalid or irrelevant candidates.

 $$$$$ The recall measures the number of correctly resolved anaphors over the total anaphors in the MUC test data set, and the precision measures the number of correct anaphors over the total resolved anaphors.
 $$$$$ Consequently, we can safely draw the conclusion that the twin-candidate model is more powerful than the single-candidate model in characterizing the relationships among an anaphor and its candidates.
 $$$$$ In our future work, we intend to adopt a looser filter together with an anaphoricity determination module (Bean and Riloff, 1999; Ng and Cardie, 2002b).
 $$$$$ The named entity recognition component recognizes various types of MUC-style named entities, i.e., organization, location, person, date, time, money and percentage.

The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ T1 and T2 would assign the same class label for the test instances inst(ci, ana ) and inst(ci, c0, ana ) , respectively.
The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ By contrast, our approach adopts a twin-candidate learning model.
The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ Given the feature vector of a test instance inst(ci, cj , ana ) (i > j), the classifier returns the positive class indicating that Ci is preferred to Cj as the antecedent of ana; or negative indicating that Cj is preferred.
The model in (Yang et al., 2003) expands the conditioning scope by including a competing candidate. $$$$$ The main reason is the absence of candidate filtering strategy in their approach (this is why the recall equals to the precision in the tables).

A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ Let CF2 and CF3 denote the class value of a leaf node “F2 = 1” and “F3 = 1”, respectively.
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ For a NP under consideration, all of its preceding NPs could be the antecedent candidates.
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ (2) The antecedent identification in Connolly et al.’s approach is to apply the classifier to successive pairs of candidates, each time retaining the better candidate.
A first approach towards group-wise classifiers is the twin-candidate model (Yang et al, 2003). $$$$$ These features are effective especially for non-pronoun resolution.

 $$$$$ The reason may be that in non-pronoun resolution, the coreference of an anaphor and its candidate is usually determined only by some strongly indicative features such as alias, apposition, string-matching, etc (this explains why we obtain a high precision but a low recall in non-pronoun resolution).
 $$$$$ The set of training instances based on ana, inst_set, is defined as follows: From the above definition, an instance is formed by an anaphor, one positive candidate and one negative candidate.
 $$$$$ Traditionally, supervised machine learning approaches adopt the singlecandidate model.

 $$$$$ (2) The antecedent identification in Connolly et al.’s approach is to apply the classifier to successive pairs of candidates, each time retaining the better candidate.
 $$$$$ While the filter guarantees the qualification of the candidates, it removes too many positive candidates, and thus the recall suffers.
 $$$$$ These data noises may hamper the training of a goodperformanced classifier, and also damage the accuracy of the antecedent selection: too many comparisons are made between incorrect candidates.
 $$$$$ We can compare every candidate with a virtual “standard candidate”, C0.

 $$$$$ Coreference resolution is the process of linking together multiple expressions of a given entity.
 $$$$$ For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task.
 $$$$$ Given the feature vector of a test instance inst(ci, cj , ana ) (i > j), the classifier returns the positive class indicating that Ci is preferred to Cj as the antecedent of ana; or negative indicating that Cj is preferred.

However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ The recall measures the number of correctly resolved anaphors over the total anaphors in the MUC test data set, and the precision measures the number of correct anaphors over the total resolved anaphors.
However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ However, due to the lack of strong assumption of transitivity, the selection procedure is in fact a greedy search.
However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin candidate (Yang et al, 2003) that focusses on the preference relationships among candidates. $$$$$ Besides, compared with Strube’s S-list algorithm, our approach also achieves gains in the F-measure by 3.2 (MUC-6), and 1.6 (MUC-7).

Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ Different from Table 2 and 3, here we focus on whether a coreferential chain could be correctly identified.
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ Suppose positive_set is the set of candidates that occur in the coreferential chain of ana, and negative_set is the set of candidates not in the chain, that is, negative_set = candidate_set - positive_set.
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ To illustrate this problem, just suppose a data set where an instance could be described with four exclusive features: F1, F2, F3 and F4.
Yang et al (2003) proposed an alternative twin candidate model for anaphora resolution task. $$$$$ A feature vector is specified for each training or testing instance.

 $$$$$ The reason may be that in non-pronoun resolution, the coreference of an anaphor and its candidate is usually determined only by some strongly indicative features such as alias, apposition, string-matching, etc (this explains why we obtain a high precision but a low recall in non-pronoun resolution).
 $$$$$ Input: ana: the anaphor under consideration candidate_set: the set of antecedent candidates of ana, {C1, C2,...,Ck} for i = 1 to K do Score[ i ] = 0; for i = K downto 2 do for j = i – 1 downto 1 do if CR( inst( ci , cj, ana )) = = positive then While the realization and the structure of the twincandidate model are significantly different from the single-candidate model, the single-candidate model in fact can be regarded as a special case of the twin-candidate model.
 $$$$$ Suppose positive_set is the set of candidates that occur in the coreferential chain of ana, and negative_set is the set of candidates not in the chain, that is, negative_set = candidate_set - positive_set.
 $$$$$ During training, we create the candidate set for each anaphor with the following filtering algorithm: Features describing the two candidates During resolution, we filter the candidates for each encountered pronoun in the same way as during training.

These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ Then some algorithms are applied to choose one of the remaining candidates, if any, as the antecedent.
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ In fact, the twin-candidate model in itself can identify the qualification of a candidate.
These results not only affirm the claim by Yang et al (2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution. $$$$$ In fact, the twin-candidate model in itself can identify the qualification of a candidate.

 $$$$$ 0.5) are discarded.
 $$$$$ For a NP under consideration, all of its preceding NPs could be the antecedent candidates.
 $$$$$ Therefore, most of the positive candidates are coreferential to the anaphors even though they are not the “best”.

 $$$$$ In this paper we have proposed a competition learning approach to coreference resolution.
 $$$$$ Currently, we employ the single-candidate classifier to filter the candidate set during resolution.
 $$$$$ See the following example: Any design to link China's accession to the WTO with the missile tests1 was doomed to failure.
 $$$$$ A training instance inst(ci, cj , ana) is labeled as positive if Ci ∈ positive-set and Cj ∈ negative-set; or negative if Ci ∈ negative-set and Cj ∈ positiveset.

On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates.
On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ It is labeled as positive or negative based on whether or not the candidate is tagged in the same coreferential chain of the anaphor.
On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ In this paper we have proposed a competition learning approach to coreference resolution.
On the MUC6 data set, for example, the best published MUC score using extracted CEs is approximately 71 (Yang et al, 2003), while multiple systems have produced MUC scores of approximately 85 when using annotated CEs (e.g. Luo et al (2004), McCallum and Wellner (2004)). $$$$$ Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution.

In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ For testing, we utilize the 30 standard test documents from MUC-6 and the 20 standard test documents from MUC-7.
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ Section 5 reports and discusses the experimental results.
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ Input: ana: the anaphor under consideration candidate_set: the set of antecedent candidates of ana, {C1, C2,...,Ck} for i = 1 to K do Score[ i ] = 0; for i = K downto 2 do for j = i – 1 downto 1 do if CR( inst( ci , cj, ana )) = = positive then While the realization and the structure of the twincandidate model are significantly different from the single-candidate model, the single-candidate model in fact can be regarded as a special case of the twin-candidate model.
In some sense, this is a natural extension of the twin-candidate learning approach proposed in Yang et al (2003), which explicitly models the competition between two antecedent candidates. $$$$$ See the following example: Any design to link China's accession to the WTO with the missile tests1 was doomed to failure.

Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ Let inst(ci , c0, ana) have the same class label as inst(ci , ana) , that is, inst(ci , c0, ana) is positive if Ci is the antecedent of ana; or negative if not.
Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution.
Yang et al (2005) made use of non-anaphors to create a special class of training instances in the twin-candidate model (Yang et al 2003) and improved the performance by 2.9 and 1.6 to 67.3 and 67.2 in F1-measure on the MUC-6 and MUC-7corpora, respectively. $$$$$ A similar twin-candidate model was adopted in the anaphoric resolution system by Connolly et al. (1997).
