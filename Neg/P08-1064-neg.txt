This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al, 2008a). $$$$$ When translating a span, if the usable rule is an initial rule, then the tree sequence on the target side of the rule is a candidate translation (lines 4-5).
This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al, 2008a). $$$$$ 1, where r1 is a tree-to-tree rule and r2 is a tree sequence-to-tree sequence rule.
This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al, 2008a). $$$$$ Finally, we would also like to study unsupervised learningbased bilingual parsing for SMT.
This method generates all possible tree fragments rooted by each node in the source parse tree or forest, and then matches all the generated tree fragments against the source parts (left hand side) of translation rules to extract the useful rules (Zhang et al, 2008a). $$$$$ 1, where r1 is a tree-to-tree rule and r2 is a tree sequence-to-tree sequence rule.

Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ 3 are used to map the two source tree sequences to two target tree sequences, which are then combined to generate a target parse tree.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ In addition, word alignment is a hard constraint in our rule extraction.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ Finally, a target translation is yielded from the target tree.
Works that apply the TTT model include Gildea (2003) and Zhang et al (2008). $$$$$ The experimental results on the NIST MT2005 Chinese-English translation task demonstrate the effectiveness of the proposed model.

These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction. $$$$$ The proposed model adopts tree sequence1 as the basic translation unit and utilizes tree sequence alignments to model the translation process.
These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction. $$$$$ It is designed to combine the strengths of phrase-based and syntax-based methods.
These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction. $$$$$ If leaf nodes of TS(f jj2 ) and TS(e1) .
These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al (2006) integrates bilingual phrase tables as separate TTS templates, Zhang et al (2008) uses an algorithm to convert leaves in a parse tree to phrases be fore rule extraction. $$$$$ A tree sequence translation rule r is a pair of aligned tree sequences r =< TS f j , two tree sequences, satisfying the following condition: `d (i, j) E A : i1 < i < i2 H j1 < j < j2 .

In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008). $$$$$ We go beyond the single sub-tree mapping model to propose a tree sequence alignment-based translation model.
In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008). $$$$$ mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase.
In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008). $$$$$ 6 studies the impact on performance by setting different maximal tree number (d) in a rule.
In particular, I will investigate settings that incorporate non syntactic phrases, using methods similar to Liu et al (2006) and Zhang et al (2008). $$$$$ This allows our model not only to handle structure reordering by tree node operations in a larger span, but also to capture non-syntactic phrases, which circumvents previous syntactic constraints, thus giving our model more expressive power.

For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ We then derive abstract rules from initial rules by removing one or more of its sub initial rules.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ Many of them are redundant, which make decoding very slow.
For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al (2008). $$$$$ The experimental results are reported in Section 6.

Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). $$$$$ However, in order to extract long-distance reordering rules, we also generate those initial rules with more than seven lexical words for abstract rules extraction only (not used in decoding).
Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). $$$$$ Furthermore, the translation probabilities of the bilingual phrases and other tree-to-string rules are not compatible since they are estimated independently, thus having different parameter spaces.
Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). $$$$$ Phrase-based modeling method (Koehn et al., 2003; Och and Ney, 2004a) is a simple, but powerful mechanism to machine translation since it can model local reorderings and translations of multiword expressions well.

Synchronous tree-sequence substitution grammar (STSSG) al lows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al, 2008). $$$$$ We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing.
Synchronous tree-sequence substitution grammar (STSSG) al lows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al, 2008). $$$$$ Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span.
Synchronous tree-sequence substitution grammar (STSSG) al lows either side of a rule to comprise a sequence of trees instead of a single tree (Zhang et al, 2008). $$$$$ Then we study the model’s expressive ability by comparing the contributions made by different kinds of rules, including strict tree sequence rules, non-syntactic phrase rules, structure reordering rules and discontinuous tured by the two syntax-based models through tree node operations.

Finally, STSSG, which have been derived from rational tree relations (Raoult, 1997), have been discussed by Zhang et al (2008a), Zhang et al (2008b), and Sun et al (2009). $$$$$ Many of them are redundant, which make decoding very slow.
Finally, STSSG, which have been derived from rational tree relations (Raoult, 1997), have been discussed by Zhang et al (2008a), Zhang et al (2008b), and Sun et al (2009). $$$$$ For example, most function words and headwords can be kept in abstract rules as features.
Finally, STSSG, which have been derived from rational tree relations (Raoult, 1997), have been discussed by Zhang et al (2008a), Zhang et al (2008b), and Sun et al (2009). $$$$$ The probability of each derivationθ is given as the product of the probabilities of all the rules p(ri ) used in the derivation (here we assume that Eq.
Finally, STSSG, which have been derived from rational tree relations (Raoult, 1997), have been discussed by Zhang et al (2008a), Zhang et al (2008b), and Sun et al (2009). $$$$$ 13: output the hypothesis with the highest score in h[1, J] as the final best translation The decoder is a span-based beam search together with a function for mapping the source derivations to the target ones.

However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality. $$$$$ 13: output the hypothesis with the highest score in h[1, J] as the final best translation The decoder is a span-based beam search together with a function for mapping the source derivations to the target ones.
However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality. $$$$$ Obviously, tree sequence rules are more powerful than phrases or tree rules as they can capture all phrases (including both syntactic and non-syntactic phrases) with syntactic structure information and allow any tree node operations in a longer span.
However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality. $$$$$ This gives our model stronger expressive power than other reported models.
However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expressivity gained from non-contiguous rules greatly improves the translation quality. $$$$$ Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span.

A model that is even more powerful than LMBOT is the non-contiguous version of STSSG (synchronous tree-sequence substitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which allows sequences of trees on both sides of rules [see also (Raoult, 1997)]. $$$$$ The rest of this paper is organized as follows: Section 2 reviews previous work.
A model that is even more powerful than LMBOT is the non-contiguous version of STSSG (synchronous tree-sequence substitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which allows sequences of trees on both sides of rules [see also (Raoult, 1997)]. $$$$$ It translates each span iteratively from small one to large one (lines 1-2).
A model that is even more powerful than LMBOT is the non-contiguous version of STSSG (synchronous tree-sequence substitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which allows sequences of trees on both sides of rules [see also (Raoult, 1997)]. $$$$$ It is straightforward to extract initial rules.
A model that is even more powerful than LMBOT is the non-contiguous version of STSSG (synchronous tree-sequence substitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which allows sequences of trees on both sides of rules [see also (Raoult, 1997)]. $$$$$ The experimental results on the NIST MT2005 Chinese-English translation task demonstrate the effectiveness of the proposed model.

Chiang (2005) and Graehl et al (2008) argue that STSG have sufficient expressive power for syntax based machine translation, but Zhang et al (2008a) show that the additional expressive power of tree sequences helps the translation process. $$$$$ However, neither source side syntactic knowledge nor reordering model is further explored. the solution shows effective empirically, it only utilizes the source side syntactic phrases of the input parse tree during decoding.
Chiang (2005) and Graehl et al (2008) argue that STSG have sufficient expressive power for syntax based machine translation, but Zhang et al (2008a) show that the additional expressive power of tree sequences helps the translation process. $$$$$ 2) Internal structures and large span (due to h increasing) are also useful as attested by the gain of 0.86 (26.14-25.28) Blue score when the value of h increases from 2 to 4.
Chiang (2005) and Graehl et al (2008) argue that STSG have sufficient expressive power for syntax based machine translation, but Zhang et al (2008a) show that the additional expressive power of tree sequences helps the translation process. $$$$$ If leaf nodes of TS(f jj2 ) and TS(e1) .
Chiang (2005) and Graehl et al (2008) argue that STSG have sufficient expressive power for syntax based machine translation, but Zhang et al (2008a) show that the additional expressive power of tree sequences helps the translation process. $$$$$ We use seven basic features that are analogous to the commonly used features in phrase-based systems (Koehn, 2004): 1) bidirectional rule mapping probabilities; 2) bidirectional lexical rule translation probabilities; 3) the target language model; 4) the number of rules used and 5) the number of target words.

Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). $$$$$ The experimental results are reported in Section 6.
Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al, 2008). $$$$$ Our model is implemented under log-linear framework (Och and Ney, 2002).

 $$$$$ Rules are extracted from word-aligned, bi-parsed sentence pairs < T (fJ ), T (e; ), A > , which are classified into two categories: 2) Extracting abstract rules from extracted initial rules with the help of sub initial rules.
 $$$$$ For the MER training (Och, 2003), we modified Koehn’s MER trainer (Koehn, 2004) for our tree sequence-based system.
 $$$$$ We conducted Chinese-to-English translation experiments.
 $$$$$ If no rules can be used to generate a complete target parse tree, the decoder just outputs whatever have phrase rules2.

Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping. $$$$$ It is straightforward to extract initial rules.
Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping. $$$$$ Compared with previous models, it not only captures non-syntactic phrases and discontinuous phrases with linguistically structured features, but also supports multi-level structure reordering of tree typology with larger span.
Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping. $$$$$ When translating a span, if the usable rule is an initial rule, then the tree sequence on the target side of the rule is a candidate translation (lines 4-5).
Similar to the definition of tree sequence used in a single parse tree defined in Liu et al (2007) and Zhang et al (2008a), a tree sequence in a forest also refers to an ordered sub-tree sequence that covers a continuous phrase without overlapping. $$$$$ Zhang et al. (2007b) present a STSG-based tree-to-tree translation model.

 $$$$$ Liu et al. (2006) propose a tree-to-string model.
 $$$$$ Finally, we conclude our work in Section 7.
 $$$$$ However, it cannot handle long-distance reorderings properly and does not exploit discontinuous phrases and linguistically syntactic structure features (Quirk and Menezes, 2006).
 $$$$$ This gives our model stronger expressive power than other reported models.

Among them, Zhang et al (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs, and find them useless via real syntax-based translation systems. $$$$$ The rest of this paper is organized as follows: Section 2 reviews previous work.
Among them, Zhang et al (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs, and find them useless via real syntax-based translation systems. $$$$$ mzhang@i2r.a-star.edu.sg hfjiang@mtlab.hit.edu.cn tancl@comp.nus.edu.sg aaiti@i2r.a-star.edu.sg lisheng@hit.edu.cn hli@i2r.a-star.edu.sg Abstract This paper presents a translation model that is based on tree sequence alignment, where a tree sequence refers to a single sequence of subtrees that covers a phrase.
Among them, Zhang et al (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs, and find them useless via real syntax-based translation systems. $$$$$ Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.
Among them, Zhang et al (2008a) acquire the non-contiguous phrasal rules from the contiguous tree sequence pairs, and find them useless via real syntax-based translation systems. $$$$$ This strategy can guarantee that when translating the current span, all spans smaller than the current one have already been translated before if they are translatable (line 7).

In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a). $$$$$ For the SCFG/STSG and our proposed model, we used the same settings except for the parameters d and h (d =1 and h = 2 for the SCFG; d =1 and h = 6 for the STSG; d = 4 and h = 6 for our model).
In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a). $$$$$ Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.
In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a). $$$$$ This is mainly due to data sparseness issue when d >3.
In our opinion, the non-contiguous phrasal rules themselves may not play a trivial role, as reported in Zhang et al (2008a). $$$$$ Our study also finds that in our model the tree sequence rules are very useful since they can model non-syntactic phrases and reorderings with rich linguistic structure features while discontinuous phrases and tree sequence rules with more than three sub-trees have less impact on performance.

tree-to-tree translation model based on tree sequence alignment (Zhang et al 2008a) without losing of generality to most syntactic tree based models. $$$$$ Experiment results on the NIST MT-2005 ChineseEnglish translation task show that our method significantly outperforms Moses (Koehn et al., 2007), a state-of-the-art phrase-based SMT system, and other linguistically syntax-based methods, such as SCFG-based and STSG-based methods (Zhang et al., 2007).

By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al (2008a). $$$$$ Experimental results on the NIST MT-2005 Chinese-English translation task show that our method statistically significantly outperforms the baseline systems.
By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al (2008a). $$$$$ 13: output the hypothesis with the highest score in h[1, J] as the final best translation The decoder is a span-based beam search together with a function for mapping the source derivations to the target ones.
By means of the Initial rules, we derive the Abstract rules similarly as in Zhang et al (2008a). $$$$$ Table 4 reports the numbers of these two kinds of rules.

ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a). $$$$$ It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts.
ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a). $$$$$ 2) Even if only two-layer sub-trees (lower line) are allowed, our method still outperforms STSG and Moses when d>1.
ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a). $$$$$ Cowan et al. (2006) propose a featurebased discriminative model for target language syntactic structures prediction, given a source parse tree.
ncPR refers to non-contiguous phrasal rules derived from contiguous tree sequence pairs with at least one non-terminal leaf node between two lexicalized leaf nodes (i.e., all non-contiguous rules in STSSG defined as in Zhang et al (2008a). $$$$$ It automatically learns aligned tree sequence pairs with mapping probabilities from word-aligned biparsed parallel texts.
