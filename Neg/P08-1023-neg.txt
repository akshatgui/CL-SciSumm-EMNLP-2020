However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ The corresponding BLEU score of Pharaoh (Koehn, 2004) is 0.2182 on this dataset.
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ Then rule r2 grabs the B`ushisubtree and transliterate it Similarly, rule r3 shown in Figure 1 is applied to the VP subtree, which swaps the two NPBs, yielding the situation in (d).
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ Again, there are two steps, parsing and decoding.

In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. $$$$$ Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length.
In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. $$$$$ First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept ofpackedforest. which results in two unfinished subtrees in (c).
In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. $$$$$ In this case the Chinese sentence is translated into Shown in Figure 3(a), these two parse trees can be represented as a single forest by sharing common subtrees such as NPB0,1 and VPB3,6.

Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ Part of this work was done while L. H. was visiting CAS/ICT.
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004).
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps.

Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives.
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ For 1-best search, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) which approximately intersects the translation forest with the LM.
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ This work can thus be viewed as a compromise between string-based and tree-based paradigms, with a good trade-off between speed and accuarcy.

In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ We would also like to thank Chris Quirk for inspirations, Yang Liu for help with rule extraction, Mark Johnson for posing the question of virtual ∞-best list, and the anonymous reviewers for suggestions.
In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ This work can thus be viewed as a compromise between string-based and tree-based paradigms, with a good trade-off between speed and accuarcy.
In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ Basically, just as the depthfirst traversal procedure in tree-based decoding (Figure 2), we visit in top-down order each node v in the

An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ Forest provides a compact data-structure for efficient handling of exponentially many tree structures, and is shown to be a promising direction with state-of-the-art translation results and reasonable decoding speed.
An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ Part of this work was done while L. H. was visiting CAS/ICT.
An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ Each hyperedge e E E is a pair (tails(e), head(e)), where head(e) E V is the consequent node in the deductive step, and tails(e) E V * is the list of antecedent nodes.
An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ This rule is particularly interesting since it has multiple levels on the source side, which has more expressive power than synchronous context-free grammars where rules are flat.

At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ Among syntax-based translation models, the which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart.
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation.
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best.
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.

Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ Finally, from step (d) we apply rules r4 and r5 which perform phrasal translations for the two remaining subtrees, respectively, and get the Chinese translation in (e).
Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ 2006AA010108 (H. M and Q. L.), and by NSF ITR EIA-0205456 (L. H.).
Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ More formally, a forest is a pair (V, E), where V is the set of nodes, and E the set of hyperedges.

For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ A similar formalism appears in another form in (Liu et al., 2006).
For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept ofpackedforest. which results in two unfinished subtrees in (c).
For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ Current tree-based systems perform translation in two separate steps: parsing and decoding.

At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ This work can thus be viewed as a compromise between string-based and tree-based paradigms, with a good trade-off between speed and accuarcy.
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ Forest provides a compact data-structure for efficient handling of exponentially many tree structures, and is shown to be a promising direction with state-of-the-art translation results and reasonable decoding speed.
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3).
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3).

 $$$$$ Current tree-based systems perform translation in two separate steps: parsing and decoding.
 $$$$$ This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.
 $$$$$ For example, which covers three parse hyperedges, while nodes in gray do not pattern-match any rule (although they are involved in the matching of other nodes, where they match interior nodes of the source-side tree fragments in a rule).

Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ Our training corpus consists of 31,011 sentence pairs with 0.8M Chinese words and 0.9M English words.
Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ We would also like to thank Chris Quirk for inspirations, Yang Liu for help with rule extraction, Mark Johnson for posing the question of virtual ∞-best list, and the anonymous reviewers for suggestions.
Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length.

For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ We a that translates a packed forest of exponentially many parses, which encodes many more alternatives standard lists.
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse.
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ We would also like to thank Chris Quirk for inspirations, Yang Liu for help with rule extraction, Mark Johnson for posing the question of virtual ∞-best list, and the anonymous reviewers for suggestions.
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ Figure 5 shows that, the 1-best parse is still preferred 25% of the time among 30-best trees, and 23% of the time by the forest decoder.

We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ These phrases are called syntactic phrases which are consistent with syntactic constituents (Chiang, 2005), and have been shown to be helpful in tree-based systems (Galley et al., 2006; Liu et al., 2006).
We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ Using bilingual phrases further improves the BLEU score by 3.1% points, which is 2.1% points higher than the respective 1-best baseline.
We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ A similar formalism appears in another form in (Liu et al., 2006).

Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ A similar formalism appears in another form in (Liu et al., 2006).
Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ An +LM item of node v has the form (va*b), where a and b are the target-language boundary words.
Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a sequence of translation steps) d* that converts source tree T into a target-language string among all possible derivations D: We will now proceed with a running example translating from Chinese to English: “Bush held a talk2 with Sharon1” Figure 2 shows how this process works.

Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ The subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”).
Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ For a given sentence w1:l = w1 ... wl, each node v E V is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions i through j (that is, wi+1 ... wj).
Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ The subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”).
Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ This procedure is summarized in Pseudocode 1.

However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ Then the decoder searches for the best derivation on the translation forest and outputs the target string (Section 3.3).
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ Among syntax-based translation models, the which takes as input a parse tree of the source sentence, is a promising direction being faster and simpler than its string-based counterpart.
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ We use the standard minimum error-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on the dev set.
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ Syntax-based machine translation has witnessed promising improvements in recent years.

The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ First, at the root node, we apply rule r1 preserving top-level word-order between English and Chinese, (Liu et al., 2007) was a misnomer which actually refers to a set of several unrelated subtrees over disjoint spans, and should not be confused with the standard concept ofpackedforest. which results in two unfinished subtrees in (c).
The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ Such a forest has a structure of a hypergraph (Klein and Manning, 2001; Huang and Chiang, 2005), where items like NP0,3 are called nodes, and deductive steps like (*) correspond to hyperedges.

Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ 2006AA010108 (H. M and Q. L.), and by NSF ITR EIA-0205456 (L. H.).
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ This scheme can be easily extended to work with a general n-gram by storing n − 1 words at both ends (Chiang, 2007).
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ We can thus construct a translation hyperedge from match(r, v) to v for each node v and rule r. In addition, we also need to keep track of the target string s(r) specified by rule r, which includes target-language terminals and variables.

For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ This rule is particularly interesting since it has multiple levels on the source side, which has more expressive power than synchronous context-free grammars where rules are flat.
For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.
For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ushiand Sh¯al´ong) are combined to form a coordinated NP which functions as the subject of the sentence.
