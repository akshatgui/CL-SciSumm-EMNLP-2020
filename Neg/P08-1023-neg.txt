However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ The Chinese sentence (a) is first parsed into tree (b), which will be converted into an English string in 5 steps.
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ Forest provides a compact data-structure for efficient handling of exponentially many tree structures, and is shown to be a promising direction with state-of-the-art translation results and reasonable decoding speed.
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ This result is also 0.8 points higher than decoding with 30-best parses, and takes even less time.
However, they are promising because the search space of translations is much larger than the typical N-best list (Mi et al, 2008). $$$$$ The only extra term in forest-based decoding is P(t  |Hp) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext.

In addition, Mi et al (2008) have proposed a method for forest-to-string (F2S) translation using packed forests to encode many possible sentence interpretations. $$$$$ 2006AA010108 (H. M and Q. L.), and by NSF ITR EIA-0205456 (L. H.).

Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ For future work, we would like to use packed forests not only in decoding, but also for translation rule extraction during training.
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ This procedure is summarized in Pseudocode 1.
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ However, this time we work on a finer-grained forest, called translation+LMforest, resulting from the intersection of the translation forest and the LM, with its nodes being the +LM items during cube pruning.
Nor do we try to expand the space where rules can apply by propagating uncertainty from the parser in building input forests, as in (Mi et al, 2008), but we build ambiguity into the translation rule. $$$$$ These rules are in the reverse direction of the original string-to-tree transducer rules defined by Galley et al. (2004).

Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ This scheme can be seen as a compromise between the string-based and treebased methods, while combining the advantages of both: decoding is still fast, yet does not commit to a single parse.
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ Compared with their string-based counterparts, treebased systems offer some attractive features: they are much faster in decoding (linear time vs. cubic time, see (Huang et al., 2006)), do not require a binary-branching grammar as in string-based models (Zhang et al., 2006), and can have separate grammars for parsing and translation, say, a context-free grammar for the former and a tree substitution grammar for the latter (Huang et al., 2006).
Thus, high quality parsers are unavailable for many source languages of interest. Parse forests can be used to mitigate the accuracy problem, allowing the decoder to choose from many alternative parses, (Mi et al, 2008). $$$$$ Syntax-based machine translation has witnessed promising improvements in recent years.

In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ Part of this work was done while L. H. was visiting CAS/ICT.
In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ This confirms the fact that we need exponentially large kbest lists with the explosion of alternatives, whereas a forest can encode these information compactly.
In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ The only extra term in forest-based decoding is P(t  |Hp) denoting the source side parsing probability of the current translation rule r in the parse forest, which is the product of probabilities of each parse hyperedge ep covered in the pattern-match of t against Hp (which can be recorded at conversion time): Our experiments are on Chinese-to-English translation, and we use the Chinese parser of Xiong et al. (2005) to parse the source side of the bitext.
In terms of formal similarity, Mi et al (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1 best parsing errors (as such, all derivations yield the same source string). $$$$$ However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006).

An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ Part of this work was done while L. H. was visiting CAS/ICT.
An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ The subtranslations of the matched variable nodes will be substituted for the variables in s(r) to get a complete translation for node v. So a translation hyperedge e is a triple (tails(e), head(e), s) where s is the target string from the rule, for example, e3 = ((NPB2,3, NPB5,6), VP1,6, “held x2 with x1”).
An example derivation of tree-to-string translation (much simplified from Mi et al (2008)). $$$$$ Although this new forest is prohibitively large, Algorithm 3 is very efficient with minimal overhead on top of 1-best.

At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ We instead propose a new approach, forest-based translation (Section 3), where the decoder translates a packed forest of exponentially many parses,1 which compactly encodes many more alternatives than k-best parses.
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation.
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ Part of this work was done while L. H. was visiting CAS/ICT.
At decoding time, we again parse the input sentences into trees, and convert them into translation forest by rule pattern matching (Mi et al, 2008). $$$$$ Forest provides a compact data-structure for efficient handling of exponentially many tree structures, and is shown to be a promising direction with state-of-the-art translation results and reasonable decoding speed.

Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ The parse tree for the preposition case is shown in Figure 2(b) as the 1-best parse, while for the conjunction case, the two proper nouns (B`ushiand Sh¯al´ong) are combined to form a coordinated NP which functions as the subject of the sentence.
Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ For example, s(r3) = “held x2 with x1”.
Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ Note that our rule extraction is still done on 1-best parses, while decoding is on k-best parses or packed forests.
Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al 2008). $$$$$ The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.

For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ We evaluate the translation quality using the case-sensitive BLEU-4 metric (Papineni et al., 2002).
For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ 2006AA010108 (H. M and Q. L.), and by NSF ITR EIA-0205456 (L. H.).
For example, Quirk et al (2005) use features involving phrases and source side dependency trees and Mi et al (2008) use features from a forest of parses of the source sentence. $$$$$ For future work, we would like to use packed forests not only in decoding, but also for translation rule extraction during training.

At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ The two LMs have distinct weights tuned by minimum error rate training.
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ However, despite these advantages, current tree-based systems suffer from a major drawback: they only use the 1best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors (Quirk and Corston-Oliver, 2006).
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ Figure 5 shows that, the 1-best parse is still preferred 25% of the time among 30-best trees, and 23% of the time by the forest decoder.
At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule pattern matching (Mi et al, 2008). $$$$$ A parser first parses the source language input into a 1-best tree T, and the decoder then searches for the best derivation (a sequence of translation steps) d* that converts source tree T into a target-language string among all possible derivations D: We will now proceed with a running example translating from Chinese to English: “Bush held a talk2 with Sharon1” Figure 2 shows how this process works.

 $$$$$ However, a k-best list, with its limited scope, often has too few variations and too many redundancies; for example, a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 25 < 50 < 26), and many subtrees are repeated across different parses (Huang, 2008).
 $$$$$ We use the pruning algorithm of (Jonathan Graehl, p.c.

Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ For example, (VP held * Sharon) is an +LM item with its translation starting with “held” and ending with “Sharon”.
Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.
Mi et al (2008) applied statistical machine translation to a source language parse forest, rather than to the 1-best parse. $$$$$ We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation.

For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Billot and Lang, 1989).
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ The decoder performs two tasks on the translation forest: 1-best search with integrated language model (LM), and k-best search with LM to be used in minimum error rate training.
For example, Mi et al (2008) achieved a 3.1-point improvement in BLEU score (Papineni et al, 2002) by including bilingual syntactic phrases in their forest-based system. $$$$$ This confirms the fact that we need exponentially large kbest lists with the explosion of alternatives, whereas a forest can encode these information compactly.

We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ Large-scale experiments show an absolute improvement of 1.7 BLEU points over the 1-best baseline.
We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation.
We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ This kbest list postpones some disambiguation to the decoder, which may recover from parsing errors by getting a better translation from a non 1-best parse.
We implemented the forest-to-string decoder described in (Mi et al, 2008) that makes use of forest based translation rules (Mi and Huang, 2008) as the baseline system for translating English HPSG forests into Japanese sentences. $$$$$ A similar formalism appears in another form in (Liu et al., 2006).

Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.
Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length.
Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.
Mi et al (2008) give a detailed description of the two-step decoding process. $$$$$ Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length.

Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ Each hyperedge e E E is a pair (tails(e), head(e)), where head(e) E V is the consequent node in the deductive step, and tails(e) E V * is the list of antecedent nodes.
Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ To increase the coverage of the rule set, we also introduce a default translation hyperedge for each parse hyperedge by monotonically translating each tail node, so that we can always at least get a complete translation in the end.
Given a source parse forest and an STSG grammar G, we first apply the conversion algorithm proposed by Mi et al (2008) to produce a translation forest. $$$$$ We have presented a novel forest-based translation approach which uses a packed forest rather than the 1-best parse tree (or k-best parse trees) to direct the translation.

However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ Using more than one parse tree apparently improves the BLEU score, but at the cost of much slower decoding, since each of the top-k trees has to be decoded individually although they share many common subtrees.
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ For k-best search after getting 1-best derivation, we use the lazy Algorithm 3 of Huang and Chiang (2005) that works backwards from the root node, incrementally computing the second, third, through the kth best alternatives.
However, when a pack forest encodes over 1M parses per sentence, the improvements are less significant, which echoes the results in (Mi et al, 2008). $$$$$ More formally, a (tree-to-string) translation rule (Huang et al., 2006) is a tuple (t, s, 0), where t is the source-side tree, whose internal nodes are labeled by nonterminal symbols in N, and whose frontier nodes are labeled by source-side terminals in E or variables from a set X = {x1, x2, ...1; s E (X U A)* is the target-side string where A is the target language terminal set; and 0 is a mapping from X to nonterminals in N. Each variable xi E X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set.

The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ In the former, a (modified) parser will parse the input sentence and output a packed forest (Section 3.1) rather than just the 1-best tree.
The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ More formally, a (tree-to-string) translation rule (Huang et al., 2006) is a tuple (t, s, 0), where t is the source-side tree, whose internal nodes are labeled by nonterminal symbols in N, and whose frontier nodes are labeled by source-side terminals in E or variables from a set X = {x1, x2, ...1; s E (X U A)* is the target-side string where A is the target language terminal set; and 0 is a mapping from X to nonterminals in N. Each variable xi E X occurs exactly once in t and exactly once in s. We denote R to be the translation rule set.
The first direct use of packed forest is proposed by Mi et al (2008). $$$$$ The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.

Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ Again, there are two steps, parsing and decoding.
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ However, current tree-based systems suffer from a major drawback: they only use the 1-best parse to direct the translation, which potentially introduces translation mistakes due to parsing errors.
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ Then rule r2 grabs the B`ushisubtree and transliterate it Similarly, rule r3 shown in Figure 1 is applied to the VP subtree, which swaps the two NPBs, yielding the situation in (d).
Following Mi et al (2008), we first convert the lattice-forest into lattice translation forest with the conversion algorithm proposed by Mi et al (2008), and then the decoder finds the best derivation on the lattice translation forest. $$$$$ We would also like to thank Chris Quirk for inspirations, Yang Liu for help with rule extraction, Mark Johnson for posing the question of virtual ∞-best list, and the anonymous reviewers for suggestions.

For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ Figure 4 compares forest decoding with decoding on k-best trees in terms of speed and quality.
For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ Then rule r2 grabs the B`ushisubtree and transliterate it Similarly, rule r3 shown in Figure 1 is applied to the VP subtree, which swaps the two NPBs, yielding the situation in (d).
For more detail, we refer to the algorithms of Mi et al (2008). $$$$$ Longer sentences will also aggravate this situation as the number of parses grows exponentially with the sentence length.
