Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). $$$$$ used for training and section 23 for testing (Collins,1999; Charniak, 2000).

In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ Conversely, we see that Model 2 outper forms Model 1 with the larger label set B, which is consistent with the hypothesis that part-of-speech features become redundant as dependency labels get more informative.
In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ The data has been converted to dependency trees using head rules (Magerman, 1995; Collins, 1996).
In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ First of all, the part-of-speech tagger used for preprocessing in our experiments has a loweraccuracy than the one used by Yamada and Mat sumoto (2003) (96.1% vs. 97.1%).

It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ words that are analyzed as such (Yamada and Matsumoto, 2003).
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ One important reason seems to be thatdependency parsing offers a good compromise be tween the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.

Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ 4 Experiments.
Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ Given an input string W , the parser is initial ized to ?nil,W, ??2 and terminates when it reaches a configuration ?S,nil, A?

To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme.
To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.
To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ In thefuture, we therefore want to perform more experi ments with genuine dependency treebanks like the Prague Dependency Treebank (Hajic, 1998) and the Danish Dependency Treebank (Kromann, 2003).

Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. $$$$$ NL.POS   ? NL.DEP . . .
Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. $$$$$ The memory-based classifiers used in the experiments have been constructed using theTilburg Memory-Based Learner (TiMBL) (Daelemans et al, 2003).
Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. $$$$$ Even though it is possible to use SVMfor multi-class classification, this can get cumber some when the number of classes is large.

For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ there is no arc (w, r, n) ? A, extend A with (t, r?, n) and push n onto the stack, giving the configuration ?n|t|S,I,A?{(t, r?, n)}?.
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ Thus, whereas a complete dependency structure provides a fully disambiguated analysisof a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed determin istically with reasonable accuracy.

We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ And although the structural ac curacy falls short of the best available parsers, the labeling accuracy appears to be competitive.The most important weakness is the limited ac curacy in identifying the root node of a sentence, especially for longer sentences.

 $$$$$ The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
 $$$$$ This permits us to make exact com parisons with the parser of Yamada and Matsumoto (2003), but also the parsers of Collins (1997) and Charniak (2000), which are evaluated on the same data set in Yamada and Matsumoto (2003).One problem that we had to face is that the standard conversion of phrase structure trees to de pendency trees gives unlabeled dependency trees, whereas our parser requires labeled trees.
 $$$$$ NL.POS   ? NL.DEP . . .

The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ This means that the depen dency graph given at termination is well-formed if and only if it is connected (Nivre, 2003).
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ We use the following metrics for evaluation: 1.
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ The problem becomes even more visible when we consider the dependency and root accuracy for sentences of different lengths, as shown in Table 3.

This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ Secondly, since 5The information in the first three rows is taken directly from Yamada and Matsumoto (2003).our parser makes crucial use of dependency type in formation in predicting the next action of the parser, it is very likely that it suffers from the lack of realdependency labels in the converted treebank.
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ A dependency graph for a string of words W = w1?
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.

Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). $$$$$ dependency types (arc labels).
Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). $$$$$ acyclic, projective and connected.

Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ Another difference is that Yamada and Matsumoto use support vector machines (Vapnik, 1995), whilewe instead rely on memory-based learning (Daele mans, 1999).
Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ The problem becomes even more visible when we consider the dependency and root accuracy for sentences of different lengths, as shown in Table 3.
Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ Given an arbitrary configuration of the parser, there are four possible transitions to the next configuration (where t is the token on top of the stack, n is the next input token, w is any word, and r, r?

Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ However, there are also important differences between the twoapproaches.
Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ A dependency graph for a string of words W = w1?
Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ (Since in a dependencygraph the set of nodes is given by the input tokens, only the arcs need to be represented explicitly.)
Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ The parsing methodology investigated here haspreviously been applied to Swedish, where promis ing results were obtained with a relatively smalltreebank (approximately 5000 sentences for train ing), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al, 2004).1 However, since there are no comparable resultsavailable for Swedish, it is difficult to assess the significance of these findings, which is one of the reasons why we want to apply the method to a bench mark corpus such as the the Penn Treebank, even though the annotation in this corpus is not ideal for labeled dependency parsing.The paper is structured as follows.

We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme.
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto.
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ Experimental results are reported in sec tion 4, and conclusions are stated in section 5.

That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ In this way, our approach can be seen as a form ofhistory-based parsing (Black et al, 1992; Mager man, 1995).
That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ Indi rect support for this assumption can be gained fromprevious experiments with Swedish data, where al most the same accuracy (85% unlabeled attachment score) has been achieved with a treebank whichis much smaller but which contains proper depen dency annotation (Nivre et al, 2004).

The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ The input string W is accepted if the de pendency graph D = (W,A) given at termination is well-formed; otherwise W is rejected.
The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English,the Penn Treebank (Marcus et al, 1993), is anno tated primarily with constituent analysis.
The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ In thefuture, we therefore want to perform more experi ments with genuine dependency treebanks like the Prague Dependency Treebank (Hajic, 1998) and the Danish Dependency Treebank (Kromann, 2003).
The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.

Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ This paper has explored the application of a data driven dependency parser to English text, using data from the Penn Treebank.
Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ Figure 1 shows a converted dependency tree using the B labels; in the corresponding tree with G labels NP-SBJ would be replaced by SBJ, ADVP and VP by DEP.

Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. $$$$$ Parser configurations are rep resented by triples ?S, I,A?, where S is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph.
Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. $$$$$ Thus, whereas a complete dependency structure provides a fully disambiguated analysisof a sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed determin istically with reasonable accuracy.

There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English,the Penn Treebank (Marcus et al, 1993), is anno tated primarily with constituent analysis.
