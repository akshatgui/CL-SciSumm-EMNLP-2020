Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). $$$$$ (All differences are significant beyond the .01 level; McNemar?s test.)
Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). $$$$$ Figure 2 illustrates the features that are used to define parser states in the present study.
Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al (2005). $$$$$ The fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over sup port vector machines, since we require a multi-class classifier.

In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ This can be compared with a labeled attachment score of 84.4% for Model 2 with our B set, which is of about the same size as the set used by Buchholz, although the labels are not the same.
In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ 2.
In this paper we fill a gap in the CCG literature by developing a shift reduce parser for CCG. Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.

It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ Although this is not a very interesting explanation, it undoubtedly accounts for part of the difference.
It was extended to labeled dependency parsing by Nivre et al (2004) (for Swedish) and Nivre and Scholz (2004) (for English). $$$$$ The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).

Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ , rm} be the set of permissible.
Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme.
Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ , rm} be the set of permissible.
Memory-based learning (MBL), which is based on the idea that learning is the simple storage of experiences in memory and that solving a new problem is achieved by reusing solutions from similar previously solved problems (Daelemans and Van den Bosch, 2005), has been used primarily by Nivre et al (2004), Nivre and Scholz (2004), and Sagae and Lavie (2005). $$$$$ The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).

To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ Root accuracy (RA): The proportion of root.
To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ Parser configurations are rep resented by triples ?S, I,A?, where S is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph.
To constrain our reorderings, we first produce a parse tree, using a dependency parser similar to that of Nivre and Scholz (2004). $$$$$ Another important issue to investigate further is the influence of different kinds of arc labels, and in particular labels that are based on a proper dependency grammar.

Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. $$$$$ And although the structural ac curacy falls short of the best available parsers, the labeling accuracy appears to be competitive.The most important weakness is the limited ac curacy in identifying the root node of a sentence, especially for longer sentences.

For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ First of all, whereas Yamada and Matsumoto employs a strict bottom-up algorithm (es sentially shift-reduce parsing) with multiple passes over the input, the present parser uses the algorithmproposed in Nivre (2003), which combines bottom up and top-down processing in a single pass in order to achieve incrementality.
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004). $$$$$ For English, the interest in dependency parsing has been weaker than for other languages.

We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ The problem becomes even more visible when we consider the dependency and root accuracy for sentences of different lengths, as shown in Table 3.
We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ To some extent, this can probably be explained by the strong tradition of constituent analysis in Anglo-American linguistics, but this trend has been reinforced by the fact that the major treebank of American English,the Penn Treebank (Marcus et al, 1993), is anno tated primarily with constituent analysis.
We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ The best overall accuracy ob tained for identifying both the correct head and the correct arc label is 86.0%, when restricted to grammatical role labels (7 labels), and 84.4% for the maximum set (50 labels).
We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser. $$$$$ To con struct deterministic parsers based on this system,we use classifiers trained on treebank data in or der to predict the next transition (and dependency type) given the current configuration of the parser.

 $$$$$ The data has been converted to dependency trees using head rules (Magerman, 1995; Collins, 1996).
 $$$$$ Even though it is possible to use SVMfor multi-class classification, this can get cumber some when the number of classes is large.
 $$$$$ sentences whose unlabeled dependency structure is completely correct (Yamada and Mat sumoto, 2003).

The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ Moreover, the deterministic dependency parser of Yamada and Matsumoto (2003), when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000).
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ portion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al, 2004).
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ used for training and section 23 for testing (Collins,1999; Charniak, 2000).
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz,2004) has been shown to offer state-of-the-art accuracy (Nivre et al, 2006) with high efficiency due to a greedy search strategy. $$$$$ Parser configurations are rep resented by triples ?S, I,A?, where S is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph.

This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ Given an input string W , the parser is initial ized to ?nil,W, ??2 and terminates when it reaches a configuration ?S,nil, A?
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ dependency types (arc labels).
This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. $$$$$ To con struct deterministic parsers based on this system,we use classifiers trained on treebank data in or der to predict the next transition (and dependency type) given the current configuration of the parser.

Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). $$$$$ However, there are also important differences between the twoapproaches.
Nivre's parser has been tested for Swedish (Nivre et al, 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al (2005), while McDonald? s parser has been applied to English (McDonald et al, 2005a), Czech (McDonald et al, 2005b) and, very recently, Danish (McDonald and Pereira, 2006). $$$$$ The learning algorithm used is the IB1 algorithm (Aha et al, 1991) with k = 5, i.e. classification basedon 5 nearest neighbors.4 Distances are measured us ing the modified value difference metric (MVDM) (Stanfill and Waltz, 1986; Cost and Salzberg, 1993) for instances with a frequency of at least 3 (andthe simple overlap metric otherwise), and classifica tion is based on distance weighted class voting with inverse distance weighting (Dudani, 1976).

Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ We believe that there are mainly three reasons for this.
Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ This paper presents a deterministic dependency parser based on memory-based learning, which parses English text in linear time.
Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. We will refer to this as bottom-up-trees. $$$$$ Finally, we use a lookahead of three tokens, considering only their parts-of-speech.

Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.
Johansson and Nugues (2006) describe a non-deterministic implementation to the dependency parser outlined by Nivre and Scholz (2004), where they apply an n-best beam search strategy. For a highly constrained unification-based formalism like HPSG, a deterministic parsing strategy could frequently lead to parse failures. $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.

We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ In formal terms, we define dependency graphs as follows: 1.
We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. $$$$$ However, there are also important differences between the twoapproaches.

That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ (All differences are significant beyond the .01 level; McNemar?s test.)
That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ This also means that the time complexity of the algorithm used here is linearin the size of the input, while the algorithm of Ya mada and Matsumoto is quadratic in the worst case.
That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. $$$$$ Parser configurations are rep resented by triples ?S, I,A?, where S is the stack (represented as a list), I is the list of (remaining) input tokens, and A is the (current) arc relation for the dependency graph.

The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ Unlikemost previous systems, the parser produces la beled dependency graphs, using as arc labels a combination of bracket labels and grammaticalrole labels taken from the Penn Treebank II annotation scheme.
The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ The conversion of the Penn Tree bank to dependency trees has been performed using head rules kindly provided by Hiroyasu Yamada and Yuji Matsumoto.
The dependency structure for Thai is more flexible than some languages like Japanese (Sekine et al, 2000), Turkish (Eryigit and Oflazer, 2006), while it is close to Chinese (Cheng et al, 2005) and English (Nivre and Scholz, 2004). $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.

Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ For English, the interest in dependency parsing has been weaker than for other languages.
Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ Let R = {r1, . . .
Compared with greedy local-search (Nivre and Scholz, 2004), the use of a beam allows the parser to explore a larger search space and delay difficult ambiguity-resolving decisions by considering multiple items in parallel. $$$$$ This gives a total of 7 labels in the G set and 50 labels in the B set.

Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. $$$$$ It is note worthy that our parser has lower root accuracy than dependency accuracy, whereas the inverse holds for all the other parsers.
Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. $$$$$ When trainedand evaluated on the Wall Street Journal sec tion of the Penn Treebank, the parser achieves a maximum attachment score of 87.1%.

There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ This can be compared with a labeled attachment score of 84.4% for Model 2 with our B set, which is of about the same size as the set used by Buchholz, although the labels are not the same.
There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ 2.
There are dependency parsers that operate orders of magnitude faster, by exploiting the fact that accurate dependency parsing can be achieved by using a shift-reduce linear-time process which makes a single decision at each point in the parsing process (Nivre and Scholz, 2004). In this paper we focus on the Combinatory Categorial Grammar (CCG) parser of Clark and Curran (2007). $$$$$ dependency types (arc labels).
