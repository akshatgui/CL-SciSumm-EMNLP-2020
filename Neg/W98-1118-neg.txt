 $$$$$ We are not sure why MENEProteus was hurt more badly by the evaluationtime switch from aviation disaster articles to missue/rocket launch articles, but suspect that it may have been due to Identifinder's greater quantity and quality of training data.
 $$$$$ We would hypothesize that, given sufficient training data, any handcoded system would benefit from having its output passed to MENE as a final step.
 $$$$$ After having trained the features of an M.E. model and assigned the proper weight (a values) to each of the features, decoding (i.e.
 $$$$$ By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decision.

A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. $$$$$ These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms.
A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. $$$$$ The 11 binary history-views used by MENE's binary features are very similar to those used in BBN's Nymble/Identifinder system (Bikel et al., 1997) with two exceptions: b, when the (history,future) space on which feature b activates must be a subset of the space for feature a, it can be shown that the M.E. model will yield the same results whether a and b are included as features or if (a — b) and b are features.
A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. $$$$$ Given a tokenization of a test corpus and a set of n (for MUC-7, n = 7) tags which define the name categories of the task at hand, the problem of named entity recognition can be reduced to the problem of assigning one of 4n + 1 tags to each token.
A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al 1997), Maximum Entropy methods (Borthwick et al 1998, Chieu and Ng 2002), Decision Trees (Sekine et al 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al 2002), Agent-based Approach (Ye et al 2002) and Support Vector Machines. $$$$$ Note that these history views generally hold information about a limited window around the current token.

Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003). $$$$$ The 11 binary history-views used by MENE's binary features are very similar to those used in BBN's Nymble/Identifinder system (Bikel et al., 1997) with two exceptions: b, when the (history,future) space on which feature b activates must be a subset of the space for feature a, it can be shown that the M.E. model will yield the same results whether a and b are included as features or if (a — b) and b are features.
Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003). $$$$$ In addition, subsequent to the evaluation, the University of Manitoba (Lin, 1998) and IsoQuest, Inc. (Krupka and Hausman, 1998) shared with us the outputs of their systems on our training corpora as well as on various test corpora.
Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003). $$$$$ MENE currently has no direct ability to learn compound features or &quot;patterns&quot;—the &quot;history&quot; side of a lexical feature activates based on only a single word, for instance.
Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997), named entity recognition (Borthwick et al, 1998), partial parsing (Inui and Inui, 2000), word sense disambiguation (Florian and Yarowsky, 2002) and question answering (Chu-Carroll et al, 2003). $$$$$ BBN used 790,000 words of training data to our 321,000.

 $$$$$ BBN used 790,000 words of training data to our 321,000.
 $$$$$ Named entity recognition is one of the simplest of the common message understanding tasks.
 $$$$$ By working within the framework of maximum entropy theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions.

For instance, maximum entropy may be used when a high diversity of knowledge sources are to be taken into account (Borthwick et al, 1998). $$$$$ For instance, an assignment of [person-start, location_end] to two consecutive tokens would be invalid.
For instance, maximum entropy may be used when a high diversity of knowledge sources are to be taken into account (Borthwick et al, 1998). $$$$$ It seems reasonable that adding an ability to handle fully general compound features (i.e. feature A fires if features B and C both fire) would improve system performance based on this limited experiment.
For instance, maximum entropy may be used when a high diversity of knowledge sources are to be taken into account (Borthwick et al, 1998). $$$$$ For instance, an assignment of [person-start, location_end] to two consecutive tokens would be invalid.

It is straightforward to see that this problem may be resolved using dynamic programming, as did Borthwick et al (1998). $$$$$ These knowledge sources include capitalization features, lexical features, features indicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-word terms.
It is straightforward to see that this problem may be resolved using dynamic programming, as did Borthwick et al (1998). $$$$$ If the current token is denoted as wo, then our model only holds information about tokens w1 for all history views except the lexical ones.
It is straightforward to see that this problem may be resolved using dynamic programming, as did Borthwick et al (1998). $$$$$ Nevertheless, we believe that we have already demonstrated some very useful results.
