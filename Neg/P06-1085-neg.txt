Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ Moreover, the segmentation accuracy of the NGS unigram, bigram, and trigram models hardly differ, suggesting that contextual dependencies are irrelevant to word segmentation.
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.

 $$$$$ Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.

USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation. $$$$$ M. Sun, D. Shen, and B. Tsou.
USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation. $$$$$ 2005.
USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation. $$$$$ Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar.
USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) for word segmentation. $$$$$ A Bayesian interpretation of interpolated kneser-ney.

We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ We have presented a method of inference using Gibbs sampling, which is guaranteed to converge to the posterior distribution over possible segmentations of a corpus.
We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ Due to search, the performance of the bigram NGS model is not much different from that of the unigram model.
We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ The base distribution shared by all bigrams is given by P1, which can be viewed as a unigram backoff where the unigram probabilities are learned from the bigram table labels.

 $$$$$ To prevent memorization, we could restrict our hypothesis space to models with fewer parameters than the number of utterances in the data.
 $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
 $$$$$ Our approach to word segmentation allows us to investigate questions that could not be addressed satisfactorily in earlier work.

Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ We have shown that the search algorithms used with previous models of word segmentation do not achieve their objectives, which has led to misleading results.
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ Second, the factorization into four separate steps makes it theoretically possible to modify each step independently in order to investigate the effects of the various modeling assumptions.
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ However, the true maximum likelihood solution is not competitive, since it contains no utterance-internal word boundaries.

We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ We present two such models incorporating unigram and bigram word dependencies, respectively.
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ The first term is the probability of generating w from the cache (i.e., sitting at an occupied table), and the second term is the probability of generating it anew (sitting at an unoccupied table).
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ 1998.
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ In Proceedings of COLING-ACL.

 $$$$$ We are not the first to propose explicit probabilistic models of word segmentation.
 $$$$$ In this paper, we present an alternative framework for word segmentation based on the Dirichlet process, a distribution used in nonparametric Bayesian statistics.
 $$$$$ Best solutions under each model are bold. ties under each model of various segmentations of the corpus.

We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ However, the segmentations produced by both these methods depend crucially on properties of the search procedures they employ.
We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ A. Venkataraman.

 $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
 $$$$$ In this paper, we present an alternative framework for word segmentation based on the Dirichlet process, a distribution used in nonparametric Bayesian statistics.
 $$$$$ A. Venkataraman.
 $$$$$ Gibbs sampling is an iterative procedure in which variables are repeatedly sampled from their conditional posterior distribution given the current values of all other variables in the model.

 $$$$$ We also show that previous probabilistic models rely crucially on suboptimal search procedures.
 $$$$$ Venkataraman uses standard unigram, bigram, and trigram language models in three versions of his system, which we refer to as n-gram Segmentation (NGS).
 $$$$$ Our Gibbs sampler considers a single possible boundary point at a time, so each sample is from a set of two hypotheses, h1 and h2.
 $$$$$ A more general and mathematically satisfactory solution is to assume a nonuniform prior, assigning higher probability to hypotheses with fewer parameters.

 $$$$$ This is in fact the route taken by Brent in his MBDP model, as we shall see in the following section.
 $$$$$ Hierarchical Dirichlet processes.
 $$$$$ Second, the factorization into four separate steps makes it theoretically possible to modify each step independently in order to investigate the effects of the various modeling assumptions.
 $$$$$ After observing w−i, the HDP grammar is as shown in Figure 4, where h−i = (w−i, z−i); t$, tE∗, and twi are the total number of tables (across all words) labeled with $, non-$, and wi, respectively; t = t$ + tE∗ is the total number of tables; and n(wi−1,wi) is the number of occurrences of the bigram (wi−1, wi).

 $$$$$ Figure 3 shows the effects of varying of p# and α0.3 Lower values of p# cause longer words, which tends to improve recall (and thus F-score) in the lexicon, but decrease token accuracy.
 $$$$$ We experimented with different values of α0 and α1, keeping p# = .5 throughout.
 $$$$$ Y. Teh.
 $$$$$ Due to search, the performance of the bigram NGS model is not much different from that of the unigram model.

We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.
We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ In particular, previous work suggested that the use of word-to-word dependencies has little effect on word segmentation.

 $$$$$ We know, however, that hypotheses that memorize the input data are unlikely to generalize to unseen data, and are therefore poor solutions.
 $$$$$ Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar.
 $$$$$ Technical Report TRA2/06, National University of Singapore, School of Computing.

Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ This model can be used to find the highest probability segmentation hypothesis h given the data d by using Bayes’ rule: NGS assumes a uniform prior P(h) over hypotheses, so its goal is to find the solution that maximizes the likelihood P(djh).
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ M. Sun, D. Shen, and B. Tsou.
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ The algorithms we use to search for likely segmentations do differ, but so long as the segmentations they produce are close to optimal we can be confident that any differences in the segmentations reflect differences in the probabilistic models, i.e., in the kinds of dependencies between words.

Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ Our approach to word segmentation allows us to investigate questions that could not be addressed satisfactorily in earlier work.
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ Y. Teh, M. Jordan, M. Beal, and D. Blei.
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ We have presented a method of inference using Gibbs sampling, which is guaranteed to converge to the posterior distribution over possible segmentations of a corpus.
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.

Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ While all three models perform better on the artificial corpus, the improvements of the DP model are by far the most striking.
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Journal of Memory and Language, 35:606–621.

While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ 2005.
While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ In Table 1(a), we compare the results of our system to those of MBDP and NGS.4 Although our system has higher lexicon accuracy than the others, its token accuracy is much worse.
While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ We have argued that this is necessary to avoid memorization, and indeed the unsegmented corpus is not the optimal solution under this model, as we will show in Section 3.
While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ We also show that previous probabilistic models rely crucially on suboptimal search procedures.

It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ 2005.
It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ Some results of these experiments are plotted in Figure 5.
It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ To prevent memorization, we could restrict our hypothesis space to models with fewer parameters than the number of utterances in the data.
It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ The use of the Dirichlet process as the basis of our approach yields sparse solutions and allows us the flexibility to modify individual components of the models.
