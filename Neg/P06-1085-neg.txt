Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ 1996.
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ In particular, previous work suggested that the use of word-to-word dependencies has little effect on word segmentation.
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ We have shown that the search algorithms used with previous models of word segmentation do not achieve their objectives, which has led to misleading results.
Similarly, Goldwater et al (2006) use a hierarchical Dirichlet model in combination with morph bigram probabilities. $$$$$ We also show that previous probabilistic models rely crucially on suboptimal search procedures.

 $$$$$ Since our research aims to investigate the effects of different modeling assumptions on lexical acquisition, we develop in the following sections a far more flexible model that also incorporates a preference for sparse solutions.
 $$$$$ We also show that previous probabilistic models rely crucially on suboptimal search procedures.
 $$$$$ Two successful word segmentation systems based on explicit probabilistic models are those of Brent (1999) and Venkataraman (2001).
 $$$$$ While methods based on local statistics are quite successful, here we focus on approaches based on explicit probabilistic models.

USM $$$$$ The use of the Dirichlet process as the basis of our approach yields sparse solutions and allows us the flexibility to modify individual components of the models.
USM $$$$$ We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
USM $$$$$ In Figure 5: Word (F) and lexicon (LF) F-score (a) as a function of α0, with α1 = 10 and (b) as a function of α1, with α0 = 1000. in bold.
USM $$$$$ To test this hypothesis, we extended our model to incorporate bigram dependencies using a hierarchical Dirichlet process (HDP) (Teh et al., 2005).

We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ The results of our unigram experiments suggested that word segmentation could be improved by taking into account dependencies between words.
We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ We provide some intuition for the roles of α0 and P0 below.
We applied this model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al (2006) (9790 sentences) and the Academia Sinica (AS) corpus from the first SIGHAN Chinese word segmentation bakeoff (we used the first 100K sentences). $$$$$ Table 2 shows the probabilider each model of the true solution, the solution with no utterance-internal boundaries, and the solutions found by each algorithm.

 $$$$$ We present two such models incorporating unigram and bigram word dependencies, respectively.
 $$$$$ After observing w−i, the HDP grammar is as shown in Figure 4, where h−i = (w−i, z−i); t$, tE∗, and twi are the total number of tables (across all words) labeled with $, non-$, and wi, respectively; t = t$ + tE∗ is the total number of tables; and n(wi−1,wi) is the number of occurrences of the bigram (wi−1, wi).
 $$$$$ In our case, we define a bigram model by assuming each word has a different distribution over the words that follow it, but all these distributions are linked.
 $$$$$ Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.

Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ Formulating an explicit probabilistic model permits us to cleanly separate assumptions about the input and properties of likely segmentations from details of algorithms used to find such solutions.
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ Intuitively, the NGS model considers the unsegmented solution to be optimal because it ranks all hypotheses equally probable a priori.
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ Despite their rather different generative structure, the MBDP and NGS segmentation accuracies are very similar.
Unsupervised monolingual segmentation has been studied as a model of language acquisition (Goldwater et al, 2006), and as model of learning morphology in European languages (Goldsmith, 2001). $$$$$ We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.

We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ The use of the Dirichlet process as the basis of our approach yields sparse solutions and allows us the flexibility to modify individual components of the models.
We start at a random derivation of the corpus, and at every iteration resample a derivation by amending the current one through local changes made at the node level, in the style of Goldwater et al (2006). $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.

 $$$$$ This model is an instance of the two-stage modeling framework described by Goldwater et al. (2006), with P0 as the generator and the CRP as the adaptor.
 $$$$$ Overall, these results strongly support our hypothesis that modeling bigram dependencies is important for accurate word segmentation.
 $$$$$ Since h1 and h2 are the same except for a few rules, this is straightforward.
 $$$$$ With appropriate parameter settings, both lexicon and token accuracy are higher than in the unigram model (dramatically so, for tokens), and there is no longer a negative correlation between the two.

We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ After observing w−i, the HDP grammar is as shown in Figure 4, where h−i = (w−i, z−i); t$, tE∗, and twi are the total number of tables (across all words) labeled with $, non-$, and wi, respectively; t = t$ + tE∗ is the total number of tables; and n(wi−1,wi) is the number of occurrences of the bigram (wi−1, wi).
We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ We then present the MBDP model, which uses a non-uniform prior but is difficult to extend beyond the unigram case.
We evaluated the f-score of the recovered word constituents (Goldwater et al, 2006b). $$$$$ We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.

 $$$$$ We also show that previous probabilistic models rely crucially on suboptimal search procedures.
 $$$$$ Venkataraman uses standard unigram, bigram, and trigram language models in three versions of his system, which we refer to as n-gram Segmentation (NGS).
 $$$$$ Although the DP model makes the distribution G explicit, we never deal with G directly.
 $$$$$ In Table 1(a), we compare the results of our system to those of MBDP and NGS.4 Although our system has higher lexicon accuracy than the others, its token accuracy is much worse.

 $$$$$ In the next section, we describe MBDP and NGS in detail.
 $$$$$ Our model can be viewed intuitively as a cache model: each word in the corpus is either retrieved from a cache or generated anew.
 $$$$$ Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.

 $$$$$ We have presented a method of inference using Gibbs sampling, which is guaranteed to converge to the posterior distribution over possible segmentations of a corpus.
 $$$$$ Overall, these results strongly support our hypothesis that modeling bigram dependencies is important for accurate word segmentation.
 $$$$$ In this section, we focus on the very different probabilistic models underlying the two systems.
 $$$$$ However, the true maximum likelihood solution is not competitive, since it contains no utterance-internal word boundaries.

 $$$$$ Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.
 $$$$$ The results of our unigram experiments suggested that word segmentation could be improved by taking into account dependencies between words.
 $$$$$ Specifically, this paper demonstrates the importance of contextual dependencies for word segmentation by comparing two probabilistic models that differ only in that the first assumes that the probability of a word is independent of its local context, while the second incorporates bigram dependencies between adjacent words.
 $$$$$ We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.

We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ By integrating over them, we get a distribution over bigram frequencies that can be understood in terms of the CRP.
We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ Our experiments indicate instead that bigram dependencies can be crucial for avoiding under-segmentation of frequent collocations.
We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ We experimented with different values of α0 and α1, keeping p# = .5 throughout.
We then investigated adaptor grammars that incorporate one additional kind of information, and found that modeling collocations provides the greatest improvement in word segmentation accuracy, resulting in a model that seems to capture many of the same inter word dependencies as the bigram model of Goldwater et al (2006b). $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.

 $$$$$ Brent’s ModelBased Dynamic Programming (MBDP) system assumes a unigram word distribution.
 $$$$$ We present two such models incorporating unigram and bigram word dependencies, respectively.
 $$$$$ In the next section, we describe MBDP and NGS in detail.

Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ In particular, a solution with additional word boundaries must have 1 − p$ > 0, which means it wastes probability mass modeling unseen data (which can now be generated by concatenating observed utterances together).
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ We have argued that this is necessary to avoid memorization, and indeed the unsegmented corpus is not the optimal solution under this model, as we will show in Section 3.
Goldwater et al (2006) introduced two nonparametric Bayesian models of word segmentation, which are discussed in more detail in (Goldwater et al, 2009). $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).

Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ Technical Report TRA2/06, National University of Singapore, School of Computing.
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ Our approach is similar to previous n-gram models using hierarchical Pitman-Yor processes (Goldwater et al., 2006; Teh, 2006).
Goldwater et al (2006) and Goldwater et al (2009) demonstrated the importance of contextual dependencies for word segmentation, and proposed a bigram model in order to capture some of these. $$$$$ The best values of α0 are much larger than in the unigram model, presumably because all unique word types must be generated via P0, but in the bigram model there is an additional level of discounting (the unigram process) before reaching P0.

Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ This is in fact the route taken by Brent in his MBDP model, as we shall see in the following section.
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ While all three models perform better on the artificial corpus, the improvements of the DP model are by far the most striking.
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Now, each word type w is associated with its own restaurant, which represents the distribution over words that follow w. Different restaurants are not completely independent, however: the labels on the tables in the restaurants are all chosen from a common base distribution, which is another CRP.
Goldwater et al (2006) used hierarchical Dirichlet processes (HDP) to induce contextual word models. $$$$$ Section 4 extends that model to incorporate bigram dependencies, and Section 5 concludes the paper.

While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ Since h1 and h2 are the same except for a few rules, this is straightforward.
While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ The NGS and MBDP systems are similar in some ways: both are designed to find utterance boundaries in a corpus of phonemically transcribed utterances, with known utterance boundaries.
While there is no reason why these methods cannot be used to learn the syntax and semantics of human languages, much of the work to date has focused on lower-level learning problems such as morphological structure learning (Goldwater et al, 2006b) and word segmentation, where the learner is given unsegmented broad-phonemic utterance transcriptions and has to identify the word boundaries (Goldwater et al, 2006a; Goldwater et al, 2007). $$$$$ Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.

It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ In particular, the fourth step uses a uniform distribution, which creates a unigram constraint that cannot easily be changed.
It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ Why don’t the MBDP and NGS unigram models exhibit these problems?
It confirmed the importance of modeling contextual dependencies above the word level for word segmentation (Goldwater et al, 2006a). $$$$$ Incorporating these dependencies into our model greatly improved segmentation accuracy, and led to better performance than previous approaches on all measures.
