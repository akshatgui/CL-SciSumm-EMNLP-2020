In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ Indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward.
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ Any mention which corresponds to an appositive node has its set of possible antecedents limited to its parent.
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent.
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ By and large the word pairs extracted in this way are correct (in particular we now have coverage for over two-thirds of the head pair recall errors from Table 1.)

In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ However, it is rich in important ways which we argue are marginalized in recent coreference work.

For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions). $$$$$ On the MUC6-TEST dataset, our system outpersion made by the system.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions). $$$$$ Instead, we note that rich syntactic and semantic processing vastly reduces the need to rely on discourse effects or evidence reconciliation for reference resolution.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions). $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions). $$$$$ We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al. (2007) and Bengston and Roth (2008).

 $$$$$ Finally, of the antecedents which remain after rich syntactic and semantic filtering, reference is chosen to minimize tree distance.
 $$$$$ In particular, we assume a three-step process.
 $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.

All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ On the MUC6-TEST dataset, our system outpersion made by the system.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Instead, we note that rich syntactic and semantic processing vastly reduces the need to rely on discourse effects or evidence reconciliation for reference resolution.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Each row is a mention type and the column the predicted mention type antecedent.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Interestingly, error analysis from our final system shows that its failures are far more often due to syntactic failures (e.g. parsing mistakes) and semantic failures (e.g. missing knowledge) than failure to model discourse phenomena or appropriately weigh conflicting evidence.

We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ In this section we develop our system and report developmental results on ACE2004-ROTHDEV (see Section 2.1); we report pairwise F1 figures here, but report on many more evaluation metrics in Section 4.
We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).
We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).

Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ On the MUC6-TEST dataset, our system outpersion made by the system.
Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.

We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ The majority of errors are made in the NOMINAL category. forms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008).
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ We first evaluate our model on the ACE2004-CULOTTA-TEST dataset used in the state-of-the-art systems from Culotta et al. (2007) and Bengston and Roth (2008).
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ We present formal experimental results here (see Table 2).
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ Mapping Mentions to Parse Nodes: In order to use the syntactic position of mentions to determine anaphoricity, we must associate each mention in the document with a parse tree node.

The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ Typically, these errors involve a combination of missing syntactic and semantic information.
The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which Bengston and Roth (2008) does.

Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ Each row is a mention type and the column the predicted mention type antecedent.
Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).
Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ In order to harvest compatible head pairs, we utilize our BLIPP and WIKI data sets (see Section 2), and for each noun (proper or common) and pronoun, we assign a maximal NP mention node for each nominal head as in Section 3.1.1; we then annotate appositive and predicate-nominative NPs as in Section 3.1.3.

For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ The problem is that these highly-reliable syntactic configurations are too sparse and cannot capture all the entity information present.
For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ Overall, we conclude that our system outperforms state-of-the-art unsupervised systems12 and is in the range of the state-of-the art systems of Culotta et al. (2007) and Bengston and Roth (2008).
For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ We present a simple approach which completely modularizes these three aspects.

 $$$$$ Table 3 shows the number of pairwise errors made on MUC6-TEST dataset by mention type; note these errors are not equally weighted in the final evaluations because of the transitive closure taken at the end.
 $$$$$ These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data.
 $$$$$ Semantic constraints like selectional compatibility (e.g. a spokesperson can announce things) and subsumption (e.g.
 $$$$$ In particular, we assume a three-step process.

This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ Our best b3 result of 79.0 is broadly in the range of these results.
This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet, as well as domain-specific features (Culotta et al., 2007).
This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.

 $$$$$ Indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward.
 $$$$$ This yields 58.5 pairwise F1 (see SEMCOMPAT in Table 2) as well as similar improvements across other metrics.
 $$$$$ The problem is that these highly-reliable syntactic configurations are too sparse and cannot capture all the entity information present.
 $$$$$ In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus.

The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ On the MUC6-TEST dataset, our system outpersion made by the system.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ Indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.

 $$$$$ In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent.
 $$$$$ In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent.
 $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.
 $$$$$ However, given a semantically compatible mention head pair, say AOL and company, one might expect to observe a reliable appositive or predicative-nominative construction involving these mentions somewhere in a large corpus.

We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. $$$$$ The majority of errors are made in the NOMINAL category. forms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures.11 Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008).
We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. $$$$$ However, it is rich in important ways which we argue are marginalized in recent coreference work.
We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. $$$$$ Our best b3 result of 79.0 is broadly in the range of these results.
We start by preprocessing all the news in the news collections with a standard NLP pipeline: tokenization and sentence boundary detection (Gillick, 2009), part-of-speech tagging, dependency parsing (Nivre, 2006), coreference resolution (Haghighi and Klein, 2009) and entity linking based on Wikipedia and Freebase. $$$$$ This also includes errors of definiteness in nominals (e.g. the people in the room and Chinese people).

ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Of course, a system which is rich in all axes will find some advantage over any simplified approach.
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ The result of this error analysis is given in Table 4; note that a single error may be attributed to more than one cause.
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).

We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ In particular, we most frequently say a nominal is NULL when it has an antecedent; this is typically due to not having the necessary semantic knowledge to link a nominal to a prior expression.
We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ The closest (in tree distance, see Section 3.1.1) compatible mention is The Israelis, which is correct particular, this fixes examples such as those in Figure 1 where the true antecedent has many embedded mentions between itself and the pronoun.
We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ Since the NER tagger typically does not label non-proper NP heads, we have no NER compatibility information for nominals.

F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features.
F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).
