In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
In general, coreference errors in state-of-the art systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009). $$$$$ These decisions are made from compatibility lists extracted from unlabeled data sources such as newswire and web data.

In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ We first focus on fixing the pronoun antecedent choices.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ Los Angeles, California) appears to be an appositive and incorrectly allows our system to have angeles as an antecedent for california.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ Indeed, we suspect that further improving the syntactic and semantic modules in our system may produce greater error reductions than any other route forward.
In addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. $$$$$ Second, a self-contained semantic module evaluates the semantic compatibility of headwords and individual names.

For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting $$$$$ In this work, we break from the standard view.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting $$$$$ Ideally, we would like to have information about the entity type of each referential NP, however this information is not easily obtainable.
For r, we use a deterministic entity assignment Zr, similar to the Haghighi and Klein (2009)'s SYN-CONSTR setting $$$$$ In order to get a more thorough view of the cause of pairwise errors, we examined 20 random errors made in aligning each mention type to an antecedent.

 $$$$$ We categorized the errors as follows: we incorrectly aligned a pronoun to a mention with which it is not semantically compatible (e.g. he aligned to board). mentions with the same head are always compatible.
 $$$$$ We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which Bengston and Roth (2008) does.
 $$$$$ Here, we implement person, number and entity type agreement.6 A number feature is assigned to each mention deterministically based on the head and its POS tag.
 $$$$$ We limit the paths extracted in this way in several ways: paths are only allowed to go between adjacent sentences and have a length of at most 10.

All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Of course, a system which is rich in all axes will find some advantage over any simplified approach.
All systems except Haghighi and Klein (2009) and current work are fully supervised. $$$$$ Some of these paths can be ruled in or out by deterministic but conservative syntactic constraints.

We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).
We also compared to the strong deterministic system of Haghighi and Klein (2009). $$$$$ In particular citystate constructions (e.g.

Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.
Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ The result of this error analysis is given in Table 4; note that a single error may be attributed to more than one cause.
Haghighi and Klein (2009) reports on true mentions; here, we report performance on automatically detected mentions. $$$$$ For instance consider the example in Figure 1, the mention America is closest to its in flat mention distance, but syntactically Nintendo ofAmerica holds a more prominent syntactic position relative to the pronoun which, as Hobbs (1977) argues, is key to discourse salience.

We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ Despite our efforts in Section 3 to add syntactic and semantic information to our system, the largest source of error is still a combination of missing semantic information or annotated syntactic structure rather than the lack of discourse or salience modeling.
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ Of course, a system which is rich in all axes will find some advantage over any simplified approach.
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ We present a simple approach which completely modularizes these three aspects.
We compared our output to the deterministic system of Haghighi and Klein (2009). $$$$$ Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.

The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ We present a simple approach which completely modularizes these three aspects.
The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).
The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ In this work, we break from the standard view.
The news have been processed with a tokenizer, a sentence splitter (Gillick and Favre, 2009), a part-of-speech tagger and dependency parser (Nivre, 2006), a co-reference resolution module (Haghighi and Klein, 2009) and an entity linker based on Wikipedia and Freebase (Milneand Witten, 2008). $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.

Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ This also includes errors of definiteness in nominals (e.g. the people in the room and Chinese people).
Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ Finally, we offer our approach as a very strong, yet easy to implement, baseline.
Specifically, when searching for an antecedent for mk, its candidate antecedents are visited in an order determined by their positions in the associated parse tree (Haghighi and Klein, 2009). $$$$$ We present a simple approach which completely modularizes these three aspects.

For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ In this work we use the following data sets: Development: (see Section 3) We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior:
For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ We present a simple approach which completely modularizes these three aspects.
For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ We categorized the errors as follows: we incorrectly aligned a pronoun to a mention with which it is not semantically compatible (e.g. he aligned to board). mentions with the same head are always compatible.
For Chinese, we handle role appositives as introduced by Haghighi and Klein (2009) analogously. $$$$$ Table 3 shows the number of pairwise errors made on MUC6-TEST dataset by mention type; note these errors are not equally weighted in the final evaluations because of the transitive closure taken at the end.

 $$$$$ There are three basic mention types: proper (Barack Obama), nominal (president), and pronominal (he).1 For comparison to previous work, we evaluate in the setting where mention boundaries are given at test time; however our system can easily annotate reference on all noun phrase nodes in a parse tree (see Section 3.1.1).
 $$$$$ Despite our efforts in Section 3 to add syntactic and semantic information to our system, the largest source of error is still a combination of missing semantic information or annotated syntactic structure rather than the lack of discourse or salience modeling.
 $$$$$ Of course, a system which is rich in all axes will find some advantage over any simplified approach.
 $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.

This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.
This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ The pronoun them is closest to the site mention, but has an incompatible number feature with it.
This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ While much research (Ng and Cardie, 2002; Culotta et al., 2007; Haghighi and Klein, 2007; Poon and Domingos, 2008; Finkel and Manning, 2008) has explored how to reconcile pairwise decisions to form coherent clusters, we simply take the transitive closure of our pairwise decision (as in Ng and Cardie (2002) and Bengston and Roth (2008)) which can and does cause system errors.
This was what (Haghighi and Klein, 2009) did and we did this in training with the REUTERS corpus (Hasler et al, 2006) in which syntactic roles are annotated. $$$$$ Across metrics, the syntactic constraints and semantic compatibility components contribute most to the overall final result.

 $$$$$ Each row is a mention type and the column the predicted mention type antecedent.
 $$$$$ Both of these systems were supervised systems discriminatively trained to maximize b3 and used features from many different structured resources including WordNet, as well as domain-specific features (Culotta et al., 2007).
 $$$$$ SON, but its can be an ORGANIZATION or LOCATION).
 $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.

The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ We should note that in our work we use neither the gold mention types (we do not model pre-nominals separately) nor do we use the gold NER tags which Bengston and Roth (2008) does.
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ We present formal experimental results here (see Table 2).
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ In this work we use the following data sets: Development: (see Section 3) We will present evaluations on multiple coreference resolution metrics, as no single one is clearly superior:
The other systems we compare to and outperform are the perceptron-based Reconcile system of Stoyanov et al (2009), the strong deterministic system of Haghighi and Klein (2009), and the cluster-ranking model of Rahman and Ng (2009). $$$$$ For instance consider the example in Figure 1, the mention America is closest to its in flat mention distance, but syntactically Nintendo ofAmerica holds a more prominent syntactic position relative to the pronoun which, as Hobbs (1977) argues, is key to discourse salience.

 $$$$$ Typically, these errors involve a combination of missing syntactic and semantic information.
 $$$$$ Primary contributions include (1) the presentation of a simpleto-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).
 $$$$$ We present formal experimental results here (see Table 2).

We start by preprocessing all the news in the news collections with a standard NLP pipeline $$$$$ Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.
We start by preprocessing all the news in the news collections with a standard NLP pipeline $$$$$ In particular, we assume a three-step process.
We start by preprocessing all the news in the news collections with a standard NLP pipeline $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.

ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features.
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ In coreference resolution, we are given a document which consists of a set of mentions; each mention is a phrase in the document (typically an NP) and we are asked to cluster mentions according to the underlying referent entity.
ARKref is a syntactically rich, rule-based within-document coreference system very similar to (the syntactic components of) Haghighi and Klein (2009). $$$$$ Our approach is not intended as an argument against the more complex, discourse-focused approaches that typify recent work.

We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ Our best b3 result of 79.0 is broadly in the range of these results.
We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ We present formal experimental results here (see Table 2).
We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ We make no claim that learning to reconcile disparate features in a joint model offers no benefit, only that it must not be pursued to the exclusion of rich, nonreference analysis.
We implemented an algorithm for the task described above which was inspired by the work of Haghighi and Klein (2009). $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.

F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ Coreference systems are driven by syntactic, semantic, and discourse constraints.
F1-scores could range between 39.8 and 67.3 for various methods and test sets (Haghighi and Klein, 2009). $$$$$ Nonetheless, our coreference system, despite being relatively simple and having no tunable parameters or complexity beyond the non-reference complexity of its component modules, manages to outperform state-of-the-art unsupervised coreference resolution and be broadly comparable to state-of-the-art supervised systems.
