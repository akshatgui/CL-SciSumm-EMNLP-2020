Recently, a new dataset including "Unknown" pairs has been used in the "Cross-Lingual Textual Entailment for Content Synchronization" task at SemEval-2012 (Negri et al, 2012). $$$$$ Cardinalities are computed in different ways, considering tokens in T1 and T2, their IDF, and their similarity (computed with edit-distance) UAlacant [pivoting, multi-class] (Espl`a-Gomis et al., 2012) exploits translations obtained from Google Translate, Microsoft Bing translator, and the Apertium open-source MT platform (Forcada et al., 2011).8 Then, a multi-class SVM classifier is used to take entailment decisions using information about overlapping sub-segments as features.
Recently, a new dataset including "Unknown" pairs has been used in the "Cross-Lingual Textual Entailment for Content Synchronization" task at SemEval-2012 (Negri et al, 2012). $$$$$ Compared with “forward” entailment, these judgments are in fact less scattered across the entire length diff range (i.e. less intermingled with the other classes).
Recently, a new dataset including "Unknown" pairs has been used in the "Cross-Lingual Textual Entailment for Content Synchronization" task at SemEval-2012 (Negri et al, 2012). $$$$$ Despite the novelty of the problem and the difficulty to capture multi-directional entailment relations across languages, the first round of the Crosslingual Textual Entailment for Content Synchronization task organized within SemEval-2012 was a successful experience.
Recently, a new dataset including "Unknown" pairs has been used in the "Cross-Lingual Textual Entailment for Content Synchronization" task at SemEval-2012 (Negri et al, 2012). $$$$$ The metric used for systems’ ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set.

 $$$$$ In the other runs, the same features are used for multi-class classification.
 $$$$$ The interest shown by participants was encouraging: 10 teams submitted a total of 92 runs for all the language pairs proposed.
 $$$$$ Although contradiction is relevant from an application-oriented perspective, contradictory pairs are not present in the dataset created for the first round of the task.
 $$$$$ The authors would also like to acknowledge Giovanni Moretti from CELCT for evaluation scripts and technical assistance, and the volunteer translators that contributed to the creation of the dataset:

For a comprehensive description of the task see (Negri et al, 2012). $$$$$ The cross-lingual textual entailment task (Mehdad et al., 2010) addresses textual entailment (TE) recognition (Dagan and Glickman, 2004) under the new dimension of cross-linguality, and within the new challenging application scenario of content synchronization.
For a comprehensive description of the task see (Negri et al, 2012). $$$$$ Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated.
For a comprehensive description of the task see (Negri et al, 2012). $$$$$ However, mainly due to the absence of cross-lingual textual entailment (CLTE) recognition
For a comprehensive description of the task see (Negri et al, 2012). $$$$$ For each language combination, two baselines considering the length difference between T1 and T2 have been calculated (besides the trivial 0.25 accuracy score obtained by assigning each test pair in the balanced dataset to one of the four classes): judgments returned by the two classifiers are composed into a single multi-directional judgment (“YES-YES”=“bidirectional”, “YESNO”=“forward”, “NO-YES”=“backward”, “NO-NO”=“no entailment”); Both the baselines have been calculated with the LIBSVM package (Chang and Lin, 2011), using a linear kernel with default parameters.

Readers can refer to M. Negri et al 2012.s., for more detailed introduction. $$$$$ For the other language pairs the results are lower, with only 3 out of 8 participants above the two baselines in all datasets.
Readers can refer to M. Negri et al 2012.s., for more detailed introduction. $$$$$ Then, entailment decisions are taken combining directional relatedness scores between words in both directions (Perini, 2011).
Readers can refer to M. Negri et al 2012.s., for more detailed introduction. $$$$$ Similarity measures (e.g.
Readers can refer to M. Negri et al 2012.s., for more detailed introduction. $$$$$ Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments (see Figure 1 for Spanish/English examples of each judgment): In this task, both T1 and T2 are assumed to be true statements.

Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al, 2012). $$$$$ Besides the frequent recourse to MT tools, other resources used by participants include: on-line dictionaries for the translation of single words, word alignment tools, part-of-speech taggers, NP chunkers, named entity recognizers, stemmers, stopwords lists, and Wikipedia as an external multilingual corpus.
Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al, 2012). $$$$$ Regarding the DE-EN dataset, pivoting methods might be penalized by the lower quality of MT output when German T1s are translated into English.
Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al, 2012). $$$$$ We report on the training and test data used for evaluation, the process of their creation, the participating systems (10 teams, 92 runs), the approaches adopted and the results achieved.
Spanish data sets provided in the task 8 of SemEval 2012 (Negri et al, 2012). $$$$$ The task was designed to promote research on semantic inference over texts written in different languages, targeting at the same time a real application scenario.

The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment. $$$$$ The metric used for systems’ ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set.
The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment. $$$$$ The authors would also like to acknowledge Giovanni Moretti from CELCT for evaluation scripts and technical assistance, and the volunteer translators that contributed to the creation of the dataset:
The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment. $$$$$ This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).
The SemEval-2012 CLTE task (Negri et al, 2012) asks participants to judge entailment pairs in four language combinations, defining four target entailment relations, for ward, backward, bidirectional and no entailment. $$$$$ Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated.

Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al, 2012). $$$$$ This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).
Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al, 2012). $$$$$ Thanks to the contiguity between CLTE, TE and SMT, the proposed task provides an interesting scenario to approach the issues outlined above from different perspectives, and large room for mutual improvement.
Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al, 2012). $$$$$ The authors would also like to acknowledge Giovanni Moretti from CELCT for evaluation scripts and technical assistance, and the volunteer translators that contributed to the creation of the dataset:
Cross-Lingual Text Entailment (CLTE), besides introducing the extra dimension of cross-linguality, also requires to determine the exact direction of the entailment relation, to provide content synchronization (Negri et al, 2012). $$$$$ Our ambition, for the future editions of the CLTE task, is to further consolidate the bridge between the semantics and MT communities.

In this paper we have presented the DirRelCond3 systems that participated at the CLTE task (Negri et al., 2012) from SemEval-2012. $$$$$ The interest shown by participants was encouraging: 10 teams submitted a total of 92 runs for all the language pairs proposed.
In this paper we have presented the DirRelCond3 systems that participated at the CLTE task (Negri et al., 2012) from SemEval-2012. $$$$$ The metric used for systems’ ranking is accuracy over the whole test set, i.e. the number of correct judgments out of the total number of judgments in the test set.
In this paper we have presented the DirRelCond3 systems that participated at the CLTE task (Negri et al., 2012) from SemEval-2012. $$$$$ Cross-linguality represents a dimension of the TE recognition problem that has been so far only partially investigated.
