We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ Themodel learns parameter values for each of these fea tures, allowing it to learn to penalise these rules to the correct extent.
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ 3.2 Additional Rules of Type-Raising.
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ The approach of Zettlemoyer and Collins (2005) was presented in section 2.4.
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ In this paper we describe a learning algorithm that retains the advantages of using a detailed grammar, but is highly effective in dealing with phenomenaseen in spontaneous natural language, as exempli fied by the ATIS domain.

Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ Number of training iterations, T . Definitions: GENLEX(x, z) takes as input a sentence x anda logical form z and returns a set of lexical items as de scribed in section 2.4.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ In particular, it allows us to leverage the considerable body of work on semantics within these formalisms, for example see Carpenter (1997).
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ with learned costs.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ The approach of Zettlemoyer and Collins (2005) was presented in section 2.4.

In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ In addition, we report results for the method on the Geo880 domain.
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ The approach in volves perceptron training of a model with hidden variables.
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ = argmaxy?GEN(xi;?)

This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ However it has the addi tional twist of also performing grammar induction(lexical learning) in an online manner.
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ 6Rules of this type are non-standard in the sense that theyviolate Steedman?s Principle of Consistency (2000); this princi ple states that rules must be consistent with the slash direction of the principal category.
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ Recent work (Mooney, 2007; He and Young, 2006;Zettlemoyer and Collins, 2005) has developed learn ing algorithms for the problem of mapping sentences to underlying semantic representations.
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ Precision Recall F1 Full Online Method 87.26 74.44 80.35 Without control features 70.33 42.45 52.95 Without relaxed word order 82.81 63.98 72.19 Without word insertion 77.31 56.94 65.58 Table 4: Exact-match accuracy on the ATIS development setfor the full algorithm and restricted versions of it.

 $$$$$ Output: Lexicon ? together with parameters w. Figure 4: An online learning algorithm.
 $$$$$ These additional combinators are natural extensions of the forward appli cation, forward composition, and type-raising rulesseen in CCG.
 $$$$$ 6Rules of this type are non-standard in the sense that theyviolate Steedman?s Principle of Consistency (2000); this princi ple states that rules must be consistent with the slash direction of the principal category.
 $$$$$ with learned costs.

For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ w ? f(xi, y) . ? Define ?i to be the set of lexical entries in y?.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ The resulting sys tem achieved significant accuracy improvements in both the ATIS and Geo880 domains.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ flight number(x, 257) b) show me information on american airlines from fort worth texas to philadelphia ?x.airline(x, american airlines)?
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ The hypothesis is parsed again with the new lexicon, andan update to the parameters w is made if the result ing parse does not have the correct logical form.

We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ Precision Recall F1 Single-Pass Parsing 95.49 83.2 88.93 Two-Pass Parsing 91.63 86.07 88.76 ZC05 96.25 79.29 86.95 Table 3: Exact-match accuracy on the Geo880 test set.
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ day number(x, 22) ? during(x, night)?
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned.of techniques have been considered including ap proaches based on machine translation techniques (Papineni et al, 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques(Miller et al, 1996; Ge and Mooney, 2006), tech niques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al, 2005), andideas from string kernels and support vector ma chines (Kate and Mooney, 2006; Nguyen et al, 2006).

For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ We also present a new, online algorithm for inducing a weighted CCG.
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ Watkinson and Manandhar (1999) present an unsupervised approach for learning CCG lexiconsthat does not represent the semantics of the training sentences.
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ In addition to the application and compositionrules, we will also make use of type raising and co ordination combinators.

Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ In practice, optimization on developmentdata led to a positive value for ?, and negative val ues for ? and ?.
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ Thisapproach has been integrated with a speech recog nizer and shown to be robust to recognition errors (He and Young, 2006).There is also related work in the CCG litera ture.
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ Clark and Curran (2003) present a method forlearning the parameters of a log-linear CCG pars ing model from fully annotated normal?form parse trees.
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ Thesesentences exhibit characteristics which present significant challenges to the approach of ZC05.

Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ Luke Zettlemoyer was funded by a Microsoft graduateresearch fellowship and Michael Collins was sup ported by the National Science Foundation under grants 0347631 and DMS-0434222.
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ The fourth row reports experiments without the type-raising combinators presented in section 3.2.tences with the types of phenomena seen in spontaneous, unedited natural language.
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ w ? f(xi, y) . ? If L(y?) 6= zi : ? Set w = w + f(xi, y?)
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ There has been a significant amount of previous work on learning to map sentences to under lying semantic representations.

We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ The resulting sys tem achieved significant accuracy improvements in both the ATIS and Geo880 domains.
We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ A second set of new combinators are the relaxed functional composition rules: A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B) B\C : g A/B : f ? A\C : ?x.f(g(x)) (.

See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ However it has the addi tional twist of also performing grammar induction(lexical learning) in an online manner.
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ The analysis would be as follows: American Airlines from New York N/N N\N ?f.?x.f(x) ? airline(x, aa) ?f.?x.f(x) ? from(x, new york) TN N ?x.from(x, new york) > N ?x.airline(x, aa) ? from(x, new york)The new rule effectively allows the preposi tional phrase from New York to type-shift to an entry with syntactic type N and semantics ?x.from(x, new york), representing the set of all things from New York.7 We introduce a single additional feature which counts the number of times this rule is used.
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ The category con tains both syntactic and semantic information.
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ Second, the lexicon has entries for some function words such as wh-words, and determiners.9

 $$$$$ f(xi, y?)
 $$$$$ 6Rules of this type are non-standard in the sense that theyviolate Steedman?s Principle of Consistency (2000); this princi ple states that rules must be consistent with the slash direction of the principal category.
 $$$$$ The approach in volves perceptron training of a model with hidden variables.
 $$$$$ The output from the learning algo rithm is a combinatory categorial grammar (CCG),together with parameters that define a log-linear distribution over parses under the grammar.

The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ n. Each xi isa sentence, and each zi is a corresponding lambda expression.
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ Bos et al (2004) present an al gorithm that learns CCG lexicons with semantics but requires fully?specified CCG derivations in thetraining data.
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ f(xi, y?)
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ f(xi, y?)

The systems that we compared with are $$$$$ This example demonstrates the use of the relaxed application and composition rules, as well as the new type-raising rules.
The systems that we compared with are $$$$$ Again, we address this 7Note that we do not analyze this prepositional phrase as having the semantics ?x.flight(x) ? from(x, new york)?although in principle this is possible?as the flight(x) predi cate is not necessarily implied by this utterance.
The systems that we compared with are $$$$$ flight number(x, 257) b) show me information on american airlines from fort worth texas to philadelphia ?x.airline(x, american airlines)?

We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ The first new combinators we consider are the relaxed functional application rules: A\B : f B : g ? A : f(g) (&) B : g A/B : f ? A : f(g) (.)
We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ from(x, fort worth) ? to(x, philadelphia) c) okay that one?s great too now we?re going to go on april twenty second dallas to washington the latest nighttime departure one way argmax(?x.flight(x) ? from(x, dallas)?

We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ In addition, there is related work that focuses on modeling child language learning.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ The simplest such rules are the functional application rules: A/B : f B : g ? A : f(g) (>) B : g A\B : f ? A : f(g) (<) The first rule states that a category with syntactic type A/B can be combined with a category to the right of syntactic type B to create a new category of type A. It also states that the new semantics will be formed by applying the function f tothe expression g. The second rule handles argu ments to the left.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ The use of a detailed grammatical formalism such as CCG has the advantage that it allows a system tohandle quite complex semantic effects, such as co ordination or scoping phenomena.

 $$$$$ from(x, atlanta) ? to(x, denver)?
 $$$$$ The function GENLEX is defined through a set of rules?see figure 2?that consider the expression z, and generate a set of categories that may help in building the target semantics z. An exhaustive setof lexical entries is then generated by taking all categories generated by the GENLEX rules, and pair ing themwith all possible sub-strings of the sentencex.
 $$$$$ ?i . Step 3: (Update parameters) ? Let y?

For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ To create the annotations, wecreated a script that maps the original SQL annotations provided with the data to lambda-calculus ex pressions.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ GENLEX(xi, zi) . ? Let y?
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ The improved performance comes from aslight drop in precision which is offset by a large in crease in recall.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ with learned costs.

Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ GENLEX(xi, zi) . ? Let y?
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ An initial lexicon ?0.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ We introduce similar features which track disjunction as opposed to conjunction.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ We also present a new, online algorithm for inducing a weighted CCG.
