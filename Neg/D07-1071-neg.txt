We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ airline(x, delta air lines) ? flight(x)?
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).
We used the dataset introduced in Zettlemoyer and Collins (2007) and automatically converted their lambda-calculus expressions to attribute-value pairs following the conventions adopted by Liang et al (2009). $$$$$ As a final point, to see how these rules can interact in practice, see figure 3.

Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ Another approach, which we may consider in the future, would be to annotate a small subset of the training examples with full CCG derivations, from which these frequently occurring entries could be learned.of techniques have been considered including ap proaches based on machine translation techniques (Papineni et al, 1997; Ramaswamy and Kleindienst, 2000; Wong and Mooney, 2006), parsing techniques(Miller et al, 1996; Ge and Mooney, 2006), tech niques that use inductive logic programming (Zelle and Mooney, 1996; Thompson and Mooney, 2002; Tang and Mooney, 2000; Kate et al, 2005), andideas from string kernels and support vector ma chines (Kate and Mooney, 2006; Nguyen et al, 2006).
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ For example, the first entry states that the word flightscan have the category N : ?x.flight(x).

In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ The third row presents results without the combinators from section 3.1 that relax word order.
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ Villavicencio (2001) describes an approach that learns a categorial grammar with syntactic and semantic information.Both of these approaches use sentences from child directed speech, which differ significantly from the natural language interface queries we consider.
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ from(x, atlanta) ? to(x, denver)?
In order to provide an initial starting point, we initialize the weight vector using a similar procedure to the one used in (Zettlemoyer and Collins, 2007) to set weights for three features and a bias term. $$$$$ The final out 3For example, features which count the number of lexical entries of a particular type, or features that count the number of applications of a particular CCG combinator.4In our experiments we use a parsing algorithm that is simi lar to a CKY-style parser with dynamic programming.

This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ with learned costs.
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ airline(x, delta air lines) ? flight(x)?
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ In our experiments we have used the same split into training and test data as He and Young (2006), ensur ing that our results are directly comparable.
This can create problems for applications based on CCG, e.g. for the induction of stochastic CCGs from text annotated with logical forms (Zettlemoyerand Collins, 2007), where spreading probability mass over equivalent derivations should be avoided. $$$$$ In addition to the application and compositionrules, we will also make use of type raising and co ordination combinators.

 $$$$$ In general, the seman tic entries for words in the lexicon can consist of anylambda-calculus expression.
 $$$$$ A second set of new combinators are the relaxed functional composition rules: A\B : f B/C : g ? A/C : ?x.f(g(x)) (& B) B\C : g A/B : f ? A\C : ?x.f(g(x)) (.
 $$$$$ airline(x, delta air lines) ? flight(x)?

For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ We will write x to de note a sentence, and y to denote a CCG parse for asentence.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ In ZC05 it is assumed that training examples do not include additional information, for example parse trees or a) on may four atlanta to denver delta flight 257 ?x.month(x,may) ? day number(x, fourth)?
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ Thus GENLEX is not required, and in particular parsing the example with the large set of entries generated by GENLEX is not required.

We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ n} where each xi is a sentence, each zi is a logical form.
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ We then describe a set of type-raising rules which allow the parser to copewith telegraphic input (in particular, missing func tion words).
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ The analysis would be as follows: American Airlines from New York N/N N\N ?f.?x.f(x) ? airline(x, aa) ?f.?x.f(x) ? from(x, new york) TN N ?x.from(x, new york) > N ?x.airline(x, aa) ? from(x, new york)The new rule effectively allows the preposi tional phrase from New York to type-shift to an entry with syntactic type N and semantics ?x.from(x, new york), representing the set of all things from New York.7 We introduce a single additional feature which counts the number of times this rule is used.
We evaluated GUSP on end-to-end question answering using the ATIS dataset for semantic parsing (Zettlemoyer and Collins, 2007). $$$$$ A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?

For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ He and Young (2005) describe an algorithm that learns a probabilisticpush-down automaton that models hierarchical de pendencies but can still be trained on a data set that does not have full treebank-style annotations.
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ For example, take the fragment to washington the latest flight.
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ The func tional composition rules effectively allow the latest to take scope over flight and to washington, in spite of the fact that the latest appears between the twoother sub-strings.
For example, Zettlemoyer and Collins (2007) used a predicate from (f, c) to signify that flight f starts from city c. $$$$$ 6.3 Results.

Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ We introduce similar features which track disjunction as opposed to conjunction.
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ Thisapproach has been integrated with a speech recog nizer and shown to be robust to recognition errors (He and Young, 2006).There is also related work in the CCG litera ture.
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ The main focus of our experiments is on the ATIS travel planning domain.
Our starting point is the work done by Zettlemoyer and Collins on parsing using relaxed CCG grammars (Zettlemoyer and Collins, 2007) (ZC07). $$$$$ 6Rules of this type are non-standard in the sense that theyviolate Steedman?s Principle of Consistency (2000); this princi ple states that rules must be consistent with the slash direction of the principal category.

Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ We then describe a set of type-raising rules which allow the parser to copewith telegraphic input (in particular, missing func tion words).
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ For ex 678 ample, the sentences have quite flexible word order, and include telegraphic language where some words are effectively omitted.
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ The use of a detailed grammatical formalism such as CCG has the advantage that it allows a system tohandle quite complex semantic effects, such as co ordination or scoping phenomena.
Practically, the grammar relaxation is done via the introduction of non-standard CCG rules (Zettlemoyer and Collins, 2007). $$$$$ Results for the approach on ATIS data show 86% F-measure in recovering fully correct semantic analyses and 95.9% F-measure by a partial-match criterion, a more than 5% improvement over the 90.3% partial-match figure reported by He and Young (2006).

We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ The approach in volves perceptron training of a model with hidden variables.
We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ The output from the algorithm is a pair (w,?)
We use the set developed by Zettlemoyer and Collins (2007), which includes features that are sensitive to lexical choices and the structure of the logical form that is constructed. $$$$$ = argmaxy?GEN(xi;?)

See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ We consider the problem of learning toparse sentences to lambda-calculus repre sentations of their underlying semantics and present an algorithm that learns a weighted combinatory categorial grammar (CCG).
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ The resulting sys tem achieved significant accuracy improvements in both the ATIS and Geo880 domains.
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ We first define the features used and then de scribe a new online learning algorithm for the task.
See the discussion by Zettlemoyer and Collins (2007) (ZC07) for the full details. $$$$$ The new algorithm took less than 4 hours, compared to over 12 hours for the ZC05 algorithm.

 $$$$$ Acknowledgements Wewould like to thank Yulan He and Steve Young for their help with obtaining the ATIS data set.
 $$$$$ It generates categories that are used to learn lexical entries for semantically vacuous sentence prefixes such as the phrase show me information on in the example in figure 1(b).
 $$$$$ The sec ond row reports results of the approach without the featuresdescribed in section 3 that control the use of the new combi nators.
 $$$$$ The sec ond row reports results of the approach without the featuresdescribed in section 3 that control the use of the new combi nators.

The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ w ? f(xi, y) . ? If L(y?) = zi, go to the next example.
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ The algorithm in ZC05 embeds GENLEX within an overall learning approach that simultaneously selects a small subset of all entriesgenerated by GENLEX and estimates parameter val uesw.
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ This method will form the basis for our approach, and will be one of the baseline models for the experimental comparisons.The input to the ZC05 algorithm is a set of train ing examples (xi, zi) for i = 1 . . .
The later work of Zettlemoyer and Collins (2007), also uses hand crafted rules. $$$$$ A simple strategy to alle viate this problem is as follows.

The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ Acknowledgements Wewould like to thank Yulan He and Steve Young for their help with obtaining the ATIS data set.
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ with learned costs.
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ 2.1 Semantics.
The systems that we compared with are: The SYN0, SYN20 and GOLDSYN systems by Ge and Mooney (2009), the system SCISSOR by Ge and Mooney (2005), an SVM based system KRIPS by Kate and Mooney (2006), a synchronous grammar based system WASP by Wong and Mooney (2007), the CCG based system by Zettlemoyer and Collins (2007) and the work by Lu et al (2008). $$$$$ We can generalize CCGs to weighted, or probabilis tic, models as follows.

We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ In this sense it is related to the algorithmof Liang et al (2006).
We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ We will define w ? Rd to be a parameter vector.
We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ The third row presents results without the combinators from section 3.1 that relax word order.
We make use of the standard application, composition and coordination combinators, as well as type-shifting rules introduced by Zettlemoyer and Collins (2007) to model spontaneous, unedited text. $$$$$ The cost of this deletion operation is optimizedon development data.

We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ However it has the addi tional twist of also performing grammar induction(lexical learning) in an online manner.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ We will define w ? Rd to be a parameter vector.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ We also present a new, online algorithm for inducing a weighted CCG.
We follow the setup of Zettlemoyerand Collins (2007) where possible, including feature design, initialization of the semantic parser, and evaluation metrics, as reviewed below. $$$$$ The simplest approach to the task is to train the parser and directly apply it to test sentences.

 $$$$$ In Step 3, a simple perceptron update (Collins, 2002) is performed.
 $$$$$ For example, the first entry states that the word flightscan have the category N : ?x.flight(x).
 $$$$$ However, the learning algorithm in our approach can learn weights for the new rules, effectively allowing the model to learn touse them only in appropriate contexts; in the exper iments we show that the rules are highly effective additions when used within a weighted CCG.
 $$$$$ In Step 3, a simple perceptron update (Collins, 2002) is performed.

For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ 2.4 Zettlemoyer and Collins 2005.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ The fourth row reports experiments without the type-raising combinators presented in section 3.2.tences with the types of phenomena seen in spontaneous, unedited natural language.
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ Step 2: (Lexical generation) ? Set ? = ? ?
For example, Zettlemoyer and Collins (2007) learn a mapping from textual queries to a logical form. $$$$$ The new algorithm makes use of perceptron 683 Inputs: Training examples {(xi, zi) : i = 1 . . .

Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ A key idea is to introduce non-standard CCG combinators that relax certain parts of the grammar?for example allowing flexible word order, or insertion of lexical items?
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ Precision Recall F1 Full Online Method 87.26 74.44 80.35 Without control features 70.33 42.45 52.95 Without relaxed word order 82.81 63.98 72.19 Without word insertion 77.31 56.94 65.58 Table 4: Exact-match accuracy on the ATIS development setfor the full algorithm and restricted versions of it.
Following Zettlemoyer and Collins (2007), we trained on 4,962 scenarios and tested on ATIS NOV93 which contains 448 examples. $$$$$ In contrast, our ap proach is integrated into a learning framework.
