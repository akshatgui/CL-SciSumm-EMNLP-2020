We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ Given two (or more) “views” (as described in Blum and Mitchell (1998)) of a classification task, co-training can be informally described as follows: The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s).
We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ Both taggers were initialised with either 500 or 50 seed sentences, and agreement-based co-training was applied, using a cache size of 500 sentences.
We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output.

paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ In each case, co-training was used successfully to bootstrap a model from only a small amount of labelled data and a much larger pool of unlabelled data.
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ They prove that the two classifiers have low generalisation error if they agree on unlabelled data.
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.

Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ The results are shown in Table 4.
Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ Self-training shows only a slight improvement for C&C1 while naive co-training performance is always worse.
Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ We only perform the experiments in English for convenience.

Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ Using unlabelled data, we are able to improve TNT from 81.3% to 86.0%, whilst C&C shows a much more dramatic improvement of 73.2% to 85.9%.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ We also show that co-training can still benefit both taggers when the performance of one tagger is initially much better than the other.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ In fact, both taggers are sufficiently different for co-training to be effective.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003).

Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ For certain sizes of newly labelled data, this simple approach is just as effective as the agreement-based method.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ The advantage of ME models over the Markov model used by TNT is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information TNT uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective.

Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ Previous co-training approaches have typically used the score assigned by the model as an indicator of the reliability of a newly labelled example.
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ Since the output of both taggers is noisy, there is a question of which newly labelled examples to add to the training set.
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ This finding is in accord with earlier work on bootstrapping taggers using EM (Elworthy, 1994; Merialdo, 1994).

Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ The model used by TNT is a standard tagging Markov model, consisting of emission probabilities, and transition probabilities based on trigrams of tags.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ The current tagger is the one being retrained, while the other tagger is kept static.

Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data.
Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for computer support.

Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Manually tagged data for English exists in large quantities, which means that there is no need to create taggers from small amounts of labelled material.
Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption.
Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.

Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ In remainder of the paper we present a practical method for co-training POS taggers, and investigate the extent to which example selection based on the work of Dasgupta et al. and Abney can be effective.
Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ If TNT is being trained on the output of C&C, then the most recent version of C&C is used to measure agreement (and vice versa); so we first attempt to improve one tagger, then the other, rather than both at the same time.
Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ The co-training process uses the selection method for selecting sentences from the cache (which has been labelled by one of the taggers).
Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold.

For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. $$$$$ Our experiments can also be seen as a vehicle for exploring aspects of cotraining.
For self-training we use the definition by (Clark et al., 2003): a tagger that is retrained on its own labeled cache on each round. $$$$$ This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output.

Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).
Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set.
Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.

According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.
According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ We also showed that naive co-training was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data.
According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ Second, ME models are used to define the conditional probabilities of a tag given some context.
According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ Blum and Mitchell (1998) derive PAClike guarantees on learning by assuming that the two views are individually sufficient for classification and the two views are conditionally independent given the class.

In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ Given two (or more) “views” (as described in Blum and Mitchell (1998)) of a classification task, co-training can be informally described as follows: The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s).
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ We also showed that naive co-training was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data.
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data.

Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ An interesting question would be to determine the minimum number of manually labelled examples that need to be used to seed the system before we can achieve comparable results as using all available manually labelled sentences.
Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.

(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. $$$$$ For the purposes of this paper, a labelled example is a tagged sentence.
(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. $$$$$ Examination of the selected subsets showed a preference for a large proportion of the cache.
(Clark et al, 2003) provide a different definition: self-training is performed using a tagger that is retrained on its own labeled cache on each round. $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.

However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ Whether this robustness is a property of the tagging problem or our approach is left for future work.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ Co-training using this much larger amount of unlabelled material did improve our previously mentioned results, but not by a large margin.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ Whether this robustness is a property of the tagging problem or our approach is left for future work.

Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ TNT does improve using self-training, from 81.4% to 82.2%, but C&C is unaffected.
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ They prove that the two classifiers have low generalisation error if they agree on unlabelled data.
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ They prove that the two classifiers have low generalisation error if they agree on unlabelled data.
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ The model used by TNT is a standard tagging Markov model, consisting of emission probabilities, and transition probabilities based on trigrams of tags.

Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ There has been some previous work on boostrapping POS taggers (e.g., Zavrel and Daelemans (2000) and Cucerzan and Yarowsky (2002)), but to our knowledge no previous work on co-training POS taggers.
Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.
Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
