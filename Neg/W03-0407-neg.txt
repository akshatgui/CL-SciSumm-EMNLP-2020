We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ The opposite behaviour has been observed in other applications of co-training (Pierce and Cardie, 2001).
We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
We do not run self-training for POS tagging as it has been shown unuseful for this application (Clark et al 2003). $$$$$ Since the output of the taggers is noisy, there is a question of which newly labelled examples to add to the training set.

paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ Abney (2002) argues that the Blum and Mitchell independence assumption is very restrictive and typically violated in the data, and so proposes a weaker independence assumption, for which the Dasgupta et al. (2002) results still hold.
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data.
paradigm in which a small amount of labeled data is available (see, e.g., Clark et al (2003)). $$$$$ We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for computer support.

Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ The additional data should be accurate, but also useful, providing the tagger with new information.
Indeed, (Clark et al, 2003) applied self training to POS-tagging with poor results, and (Charniak, 1997) applied it to a generative statistical PCFG parser trained on a large seed set (40K sentences), without any gain in performance. $$$$$ Whether this robustness is a property of the tagging problem or our approach is left for future work.

Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ Abney also presents a greedy algorithm that maximises agreement on unlabelled data, which produces comparable results to Collins and Singer (1999) on their named entity classification task.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al (2007) who use co-training of two diverse POS taggers. $$$$$ For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)).

Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ The additional data should be accurate, but also useful, providing the tagger with new information.
Clark et al (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increases. $$$$$ Our agreement-based co-training results support the theoretical arguments of Abney (2002) and Dasgupta et al. (2002), that directly maximising the agreement rates between the two taggers reduces generalisation error.

Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ We also show that co-training can still benefit both taggers when the performance of one tagger is initially much better than the other.
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ Given two (or more) “views” (as described in Blum and Mitchell (1998)) of a classification task, co-training can be informally described as follows: The intuition behind the algorithm is that each classifier is providing extra, informative labelled data for the other classifier(s).
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ We have also investigated whether co-training can improve the taggers already trained on large amounts of manually annotated data.
Clark et al (2003) applies self-training to POS-tagging and reports the same outcomes. $$$$$ This work has grown out of many fruitful discussions with the 2002 JHU Summer Workshop team that worked on weakly supervised bootstrapping of statistical parsers.

Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ Manually tagged data for English exists in large quantities, which means that there is no need to create taggers from small amounts of labelled material.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ We have shown that co-training is an effective technique for bootstrapping POS taggers trained on small amounts of labelled data.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ Since the taggers share many common features, one might think they are not different enough for effective co-training to be possible.
Interestingly, previous works did not succeed in improving POS tagging performance using self-training (Clark et al, 2003). $$$$$ We leave these experiments to future work, but note that there is a large computational cost associated with such experiments.

Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ Our results show that agreement-based co-training can significantly improve tagging performance for small seed datasets.
Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ The advantage of ME models over the Markov model used by TNT is that arbitrary features can easily be included in the context; so as well as considering the target word and the previous two tags (which is the information TNT uses), the ME models also consider the words either side of the target word and, for unknown and infrequent words, various properties of the string of the target word.
Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ Co-training (Blum and Mitchell, 1998), and several variants of co-training, have been applied to a number of NLP problems, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001) and statistical parsing (Sarkar, 2001; Steedman et al., 2003).
Clark et al (2003) were unable to improve the accuracy of POS tagging using self-training. $$$$$ We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for computer support.

Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ The C&C tagger differs in a number of ways from TNT.
Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ For the parameter settings used in the previous experiments, agreement-based co-training required the taggers to be re-trained 10 to 100 times more often then naive co-training.
Co-training has been applied to a number of NLP applications, including POS-tagging (Clark et al, 2003), parsing (Sarkar, 2001), word sense disambiguation (Mihalcea, 2004), and base noun phrase detection (Pierce and Cardie, 2001). $$$$$ This finding is in accord with earlier work on bootstrapping taggers using EM (Elworthy, 1994; Merialdo, 1994).

Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ However, we find that simply re-training on all the newly labelled data can, in some cases, yield comparable results to agreement-based co-training, with only a fraction of the computational cost.
Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.
Reference resolution (Ng and Cardie 2003), POS tagging (Clark et al, 2003), and parsing (McClosky et al, 2006) were shown to be benefited from self-training. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.

For self-training we use the definition by (Clark et al., 2003) $$$$$ We see that naive co-training improves as the cache size increases.
For self-training we use the definition by (Clark et al., 2003) $$$$$ Our agreement-based co-training results support the theoretical arguments of Abney (2002) and Dasgupta et al. (2002), that directly maximising the agreement rates between the two taggers reduces generalisation error.
For self-training we use the definition by (Clark et al., 2003) $$$$$ Our results show that, when using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy.

Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ We also showed that naive co-training was unable to improve the performance of the taggers when they had already been trained on large amounts of manually annotated data.
Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption.
Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for computer support.
Steedman et al (2003) and Clark et al (2003) present co-training procedures for parsers and taggers respectively, which are effective when only very little labeled data is available. $$$$$ This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.

According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data.
According to Clark et al (2003), self-training is a procedure in which a tagger is retrained on its own labeled cache at each round. $$$$$ By examining the subsets chosen from the labelled cache at each round, we also observed that a large proportion of the cache was being selected for both taggers.

In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ There are advantages to agreement-based co-training, however.
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data.
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ One experiment used naive co-training, with 50 seed sentences and a cache of size 500.
In natural language learning, co-training was applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), and others, and was generally found to bring improvement over the case when no additional unlabeled data are used. $$$$$ First, it uses a conditional model of a tag sequence given a string, rather than a joint model.

Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ Goldman and Zhou (2000) show that, if the newly labelled examples used for re-training are selected carefully, co-training can still be successful even when the views used by the classifiers do not satisfy the independence assumption.
Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ Using unlabelled data, we are able to improve TNT from 81.3% to 86.0%, whilst C&C shows a much more dramatic improvement of 73.2% to 85.9%.
Moreover, (Clark et al, 2003) show that a naive co-training process that does not explicitly seek to maximize agreement on unlabelled data can lead to similar performance, at a much lower computational cost. $$$$$ The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagger MXPOST (Ratnaparkhi, 1996).

(Clark et al, 2003) provide a different definition $$$$$ Our agreement-based co-training results support the theoretical arguments of Abney (2002) and Dasgupta et al. (2002), that directly maximising the agreement rates between the two taggers reduces generalisation error.
(Clark et al, 2003) provide a different definition $$$$$ Further results show that this form of co-training considerably outperforms self-training.
(Clark et al, 2003) provide a different definition $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.

However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ Dasgupta et al. (2002) provide a theoretical basis for this approach by providing a PAC-like analysis, using the same independence assumption adopted by Blum and Mitchell.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for computer support.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ The additional data should be accurate, but also useful, providing the tagger with new information.
However, as theoretically shown in (Abney, 2002), and then empirically in (Clark et al,2003), co-training still works under a weaker independence assumption, and the results we obtain concur with these previous observations. $$$$$ We would like to thank the anonymous reviewers for their helpful comments, and also Iain Rae for computer support.

Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ This led us to propose a naive co-training approach, which significantly reduced the computational cost without a significant performance penalty.
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ This paper investigates booststrapping part-ofspeech taggers using co-training, in which two taggers are iteratively re-trained on each other’s output.
Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). $$$$$ We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers: the Markov model TNT tagger (Brants, 2000) and the maximum entropy C&C tagger (Curran and Clark, 2003).

Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ The first author was supported by EPSRC grant GR/M96889, and the second author by a Commonwealth scholarship and a Sydney University Travelling scholarship.
Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ The two POS taggers used in the experiments are TNT, a publicly available Markov model tagger (Brants, 2000), and a reimplementation of the maximum entropy (ME) tagger MXPOST (Ratnaparkhi, 1996).
Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ Collins and Singer (1999) present a variant of the Blum and Mitchell algorithm, which directly maximises an objective function that is based on the level of agreement between the classifiers on unlabelled data.
Following Clark et al (2003), we applied self training as described in Algorithm 1, with the perceptron as the supervised learner. $$$$$ The performance of the bootstrapped taggers is still a long way behind a tagger trained on a large amount of manually annotated data.
