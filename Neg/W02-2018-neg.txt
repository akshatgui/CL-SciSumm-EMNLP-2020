Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). $$$$$ In this paper, we have described experiments comparing the performance of a number of different algorithms for estimating the parameters of a conditional ME model.
Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). $$$$$ The results show that variants of iterative scaling, the algorithms which are most widely used in the literature, perform quite poorly when compared to general function optimization algorithms such as conjugate gradient and variable metric methods.
Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). $$$$$ To compare the algorithms described in §2, we applied the implementation outlined in the previous section to four training data sets (described in Table 1) drawn from the domain of natural language processing.
Although IIS is a useful tool for estimating log linear models, we have since moved-on to estimating models using limited-memory variable-metric methods (Malouf, 2002). $$$$$ Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices.

We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. $$$$$ A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.
We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. $$$$$ However, the flexibility of ME models is not without cost.
We feed both correct and incorrect parses licensed by the grammar to the TADM toolkit (Malouf, 2002), and learn a maximum entropy model. $$$$$ Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices.

Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). $$$$$ The results show that variants of iterative scaling, the algorithms which are most widely used in the literature, perform quite poorly when compared to general function optimization algorithms such as conjugate gradient and variable metric methods.
Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). $$$$$ A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.
Specifically, we estimate parameters with the limited memory variable metric algorithm implemented in the Toolkit for Advanced Discriminative Modeling (Malouf, 2002). $$$$$ However, statistical natural language processing involves non-trivial numeric computations.

We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights. $$$$$ To avoid this slowed convergence and the need for a correction feature, Della Pietra et al. (1997) propose an Improved Iterative Scaling (IIS) algorithm, whose update rule is the solution to the equation: The gradient of the log likelihood function, or the Ep[f] = ∑w,x p(w)q(k)(x|w)f(x)exp(M(x)δ(k)) vector of its first derivatives with respect to the parameter θ is: Since the likelihood function (2) is concave over the parameter space, it has a global maximum where the gradient is zero.
We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights. $$$$$ For these experiments, we judged that convergence was reached when the relative change in the loglikelihood between iterations fell below a predetermined threshold.
We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights. $$$$$ First, in order to assure a consistent comparison, we need to use the same stopping rule for each algorithm.
We use the limited memory variable metric algorithm (Malouf, 2002) to determine the weights. $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.

 $$$$$ This intuition underlies conjugate gradient methods, which choose a search direction which is a linear combination of the steepest ascent direction and the previous search direction.
 $$$$$ And, more specifically, for the NLP classification tasks considered, the limited memory variable metric algorithm of Benson and Mor´e (2001) outperforms the other choices by a substantial margin.
 $$$$$ The ‘summary’ dataset is part of a sentence extraction task (Osborne, to appear), and the ‘shallow’ dataset is drawn from a text chunking application (Osborne, 2002).

Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage. $$$$$ In this case, the training data is very sparse.
Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage. $$$$$ While theoretically equivalent, they use slighly different update rules and thus show different numeric properties.
Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage. $$$$$ This conclusion has obvious consequences for the field.
Parameter estimation was performed with the Limited Memory Variable Metric algorithm (Malouf, 2002) implemented in the Megampackage. $$$$$ At each step, we adjust an estimate of the parameters θ(k) to a new estimate θ(k+1) based on the divergence between the estimated probability distribution q(k) and the empirical distribution p. We continue until successive improvements fail to yield a sufficiently large decrease in the divergence.

 $$$$$ In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al., 1996; Ratnaparkhi, 1998; Johnson et al., 1999).
 $$$$$ However, the richness of the representations is not without cost.

 $$$$$ A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.
 $$$$$ While the log-likelihood function for ME models in (2) is twice differentiable, for large scale problems the evaluation of the Hessian matrix is computationally impractical, and Newton’s method is not competitive with iterative scaling or first order methods.
 $$$$$ As these results show, natural language processing can take great advantage of the algorithms and software libraries developed by and for more quantitatively oriented engineering and computational sciences.
 $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.

In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). $$$$$ Therefore, in order to evaluate the performance of the optimization techniques sketched in previous section when applied to the problem of parameter estimation, we need to compare the performance of actual implementations on realistic data sets (Dolan and Mor´e, 2002).
In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). $$$$$ In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods.

because this method seems substantially faster than comparable methods (Malouf, 2002). $$$$$ For such cases, we can apply limited memory variable metric methods, which implicitly approximate the Hessian matrix in the vicinity of the current estimate of θ(k) using the previous m values of y(k) and δ(k).
because this method seems substantially faster than comparable methods (Malouf, 2002). $$$$$ For such cases, we can apply limited memory variable metric methods, which implicitly approximate the Hessian matrix in the vicinity of the current estimate of θ(k) using the previous m values of y(k) and δ(k).
because this method seems substantially faster than comparable methods (Malouf, 2002). $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.
because this method seems substantially faster than comparable methods (Malouf, 2002). $$$$$ These improvements take advantage of a model’s structure to simplify the evaluation of the denominator in (1).

We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. $$$$$ The research of Dr. Malouf has been made possible by a fellowship of the Royal Netherlands Academy of Arts and Sciences and by the NWO PIONIER project Algorithms for Linguistic Processing.
We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.
We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. $$$$$ One way of looking at the problem with steepest ascent is that it considers the same search directions many times.
We use the TADM open-source package (Malouf, 2002) for training the models, using its limited-memory variable metric as the optimization method and experimentally determine the optimal convergence threshold and variance of the prior. $$$$$ Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters.

Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. $$$$$ Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.
Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. $$$$$ Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing.
Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. $$$$$ Thus, the use of such optimizations is independent of the choice of parameter estimation method.
Recent improvements on the original incremental feature selection (IFS) algorithm, such as Malouf (2002) and Zhou et al (2003), greatly speed up the feature selection process. $$$$$ Unfortunately, simply setting G(θ) = 0 and solving for θ does not yield a closed form solution, so we proceed iteratively.

For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). $$$$$ However, while the steps taken on each iteration are in a very narrow sense locally optimal, the global convergence rate of steepest ascent is very poor.
For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). $$$$$ More dramatically, both iterative scaling methods perform very poorly on the ‘shallow’ dataset.
For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). $$$$$ And, since the parameters of individual models can be estimated quite quickly, this will further open up the possibility for more sophisticated model and feature selection techniques which compare large numbers of alternative model specifications.
For parameter estimation of the disambiguation model, in all reported experiments we use the TADM2 toolkit (toolkit for advanced discriminative training), with a Gaussian prior and the (default) limited memory variable metric estimation technique (Malouf, 2002). $$$$$ As these results show, natural language processing can take great advantage of the algorithms and software libraries developed by and for more quantitatively oriented engineering and computational sciences.

Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.
Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). $$$$$ A leading advantage of ME models is their flexibility: they allow stochastic rule systems to be augmented with additional syntactic, semantic, and pragmatic features.
Given the HPSG tree bank as training data, the model parameters are estimated so as to maximize the log-likelihood of the training data (Malouf, 2002). $$$$$ This intuition underlies conjugate gradient methods, which choose a search direction which is a linear combination of the steepest ascent direction and the previous search direction.

For model estimation we use the TADM3 software (Malouf, 2002). $$$$$ Iterative scaling algorithms have a long tradition in statistics and are still widely used for analysis of contingency tables.
For model estimation we use the TADM3 software (Malouf, 2002). $$$$$ First, in order to assure a consistent comparison, we need to use the same stopping rule for each algorithm.
For model estimation we use the TADM3 software (Malouf, 2002). $$$$$ This conclusion has obvious consequences for the field.
For model estimation we use the TADM3 software (Malouf, 2002). $$$$$ While theoretically equivalent, they use slighly different update rules and thus show different numeric properties.

There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). $$$$$ Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.
There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). $$$$$ Even modest ME models can require considerable computational resources and very large quantities of annotated training data in order to accurately estimate the model’s parameters.
There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002). $$$$$ As a basis for the implementation, we have used PETSc (the “Portable, Extensible Toolkit for Scientific Computation”), a software library designed to ease development of programs which solve large systems of partial differential equations (Balay et al., 2001; Balay et al., 1997; Balay et al., 2002).

For parameter estimation, we use the open source TADM system (Malouf, 2002). $$$$$ Thanks also to Stephen Clark, Andreas Eisele, Detlef Prescher, Miles Osborne, and Gertjan van Noord for helpful comments and test data.
For parameter estimation, we use the open source TADM system (Malouf, 2002). $$$$$ This leads to a characteristic “zig-zag” ascent, with convergence slowing as the maximum is approached.
For parameter estimation, we use the open source TADM system (Malouf, 2002). $$$$$ Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices.
For parameter estimation, we use the open source TADM system (Malouf, 2002). $$$$$ In this paper, we have described experiments comparing the performance of a number of different algorithms for estimating the parameters of a conditional ME model.

Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM $$$$$ In addition, there is a larger lesson to be drawn from these results.
Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM $$$$$ For the ‘summary’ dataset, however, they differ by up to two orders of magnitude.
Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM $$$$$ They do not depend on evaluation of the gradient of the log-likelihood function, which, depending on the distribution, could be prohibitively expensive.

To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). $$$$$ We can adapt GIS to estimate the model parameters θ rather than the model probabilities q, yielding the update rule: The step size, and thus the rate of convergence, depends on the constant C: the larger the value of C, the smaller the step size.
To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). $$$$$ However, the richness of the representations is not without cost.
To maximize the above function, we use a limited memory variable method (Benson and More, 2002) that is implemented in the TAO package (Benson et al., 2002) and has been shown to be very effective in various natural language processing tasks (Malouf, 2002). $$$$$ In the case of ME models, however, the vector of expected values required by iterative scaling essentially is the gradient G. Thus, it makes sense to consider methods which use the gradient directly.

For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed. $$$$$ Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limitedmemory variable metric algorithm outperformed the other choices.
For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed. $$$$$ In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.
For example, Malouf (2002) reports a matrix of non-zeroes that has 55 million entries for a shallow parsing experiment where 260,000 features were employed. $$$$$ Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing.
