Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). $$$$$ For example, the baseline feature set includes indicators for word-to-word and tag-to-tag interactions between the head and modifier of a dependency.
Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). $$$$$ In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model.
Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al, 2008). $$$$$ Table 4 gives accuracy results on the PDT 1.0 test set for our unlabeled parsers.

Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. $$$$$ In natural language parsing, lexical information is seen as crucial to resolving ambiguous relationships, yet lexicalized statistics are sparse and difficult to estimate directly.
Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. $$$$$ We are thus relying on the ability of discriminative learning methods to identify and exploit informative features while remaining agnostic as to the origin of such features.
Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. $$$$$ Second, note that the parsers using cluster-based feature sets consistently outperform the models using the baseline features, regardless of model order or label usage.
Additional templates we include are the relative position (Bj ?orkelund et al, 2009), geneological relationship, distance (Zhao et al, 2009), and binned distance (Koo et al, 2008) between two words in the path. $$$$$ Following Miller et al. (2004), we use prefixes of the Brown cluster hierarchy to produce clusterings of varying granularity.

In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). $$$$$ For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). $$$$$ Michael Collins was funded by NSF grants 0347631 and DMS-0434222.
In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). $$$$$ We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.
In this work, we analyze a simple technique of using word clusters generated from unlabeled text, which has been shown to improve performance of dependency parsing (Koo et al, 2008), Chinese word segmentation (Liang, 2005) and NER (Miller et al, 2004). $$$$$ Many thanks also to Percy Liang for providing his implementation of the Brown algorithm, and Ryan McDonald for his assistance with the experimental setup.

We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. $$$$$ Specifically, for any feature that is predicated on a word form, we eliminate this feature if the word in question is not one of the top-N most frequent words in the corpus.
We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. $$$$$ Michael Collins was funded by NSF grants 0347631 and DMS-0434222.
We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al (2008) for dependency parsing. $$$$$ Alternately, one could design clustering algorithms that cluster entire head-modifier arcs rather than individual words.

Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. $$$$$ In our experiments, we employed two different feature sets: a baseline feature set which draws upon “normal” information sources such as word forms and parts of speech, and a cluster-based feature set that also uses information derived from the Brown cluster hierarchy.
Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. $$$$$ There are some clear trends in the results of Table 2.
Koo et al (2008) have proposed to use word clusters as features to improve graph-based statistical dependency parsing for English and Czech. $$$$$ In this paper, we have presented a simple but effective semi-supervised learning approach and demonstrated that it achieves substantial improvement over a competitive baseline in two broad-coverage dependency parsing tasks.

The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). $$$$$ When combining the effects of model order and cluster-based features, the reductions in the amount of supervised data required are even larger.
The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). $$$$$ We present a simple and effective semisupervised method for training dependency parsers.
The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). $$$$$ Some examples of cluster-based features are given in Table 1.
The authors report 97.70% of accuracy and 90.01% for unseen data. We use the Brown et al (1992) hard clustering algorithm, which has proven useful for various NLP tasks such as dependency parsing (Koo et al, 2008) and named entity recognition (Liang, 2005). $$$$$ We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3).

This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. $$$$$ In Table 7, we show the development-set performance of second-order parsers as the threshold for lexical feature elimination (see Section 3.2) is varied.
This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. $$$$$ To demonstrate the effectiveness of our approach, we conduct experiments in dependency parsing, which has been the focus of much recent research—e.g., see work in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007).
This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al, 2008) and transfer (Hwa et al, 2005) systems for prediction of linguistic structure. $$$$$ To select the number of iterations of perceptron training, we performed up to 30 iterations and chose the iteration which optimized accuracy on the development set.

We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). $$$$$ We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3).
We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). $$$$$ First, we use a large unannotated corpus to define word clusters, and then we use that clustering to construct a new cluster-based feature mapping for a discriminative learner.
We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). $$$$$ Full bit strings,3 which we used as substitutes for word forms.
We observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al, 2008). $$$$$ First, performance increases with the order of the parser: edge-factored models (dep1 and MD1) have the lowest performance, adding sibling relationships (MD2) increases performance, and adding grandparent relationships (dep2) yields even better accuracies.

A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier. $$$$$ For the second-order parsing experiments, we used the Carreras (2007) parser.
A simple method for using unlabeled data in discriminative dependency parsing was provided in (Koo et al, 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency tree bank was assigned a cluster identifier. $$$$$ However, their approach depends on the usage of a high-quality parse reranker, whereas the method described here simply augments the features of an existing parser.

In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. $$$$$ Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional training data for the parser; this self-training appraoch was shown to be quite effective in practice.
In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. $$$$$ Setting aside the development of new clustering algorithms, a final area for future work is the extension of our method to new domains, such as conversational text or other languages, and new NLP problems, such as machine translation.
In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. $$$$$ For all of the experiments in this paper, we used the Liang (2005) implementation of the Brown algorithm to obtain the necessary word clusters.
In this work, following (Koo et al, 2008), we use word cluster identifiers as the source of an additional set of features. $$$$$ To begin, recall that the Brown clustering algorithm is based on a bigram language model.

The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates. $$$$$ Within this tree, each word is uniquely identified by its path from the root, and this path can be compactly represented with a bit string, as in Figure 2.
The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates. $$$$$ We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.
The reader is directed to (Koo et al, 2008) for the list of cluster-based feature templates. $$$$$ The English experiments were performed on the Penn Treebank (Marcus et al., 1993), using a standard set of head-selection rules (Yamada and Matsumoto, 2003) to convert the phrase structure syntax of the Treebank to a dependency tree representation.6 We split the Treebank into a training set (Sections 2–21), a development set (Section 22), and several test sets (Sections 0,7 1, 23, and 24).

Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). $$$$$ We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.
Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). $$$$$ For all of the experiments in this paper, we used the Liang (2005) implementation of the Brown algorithm to obtain the necessary word clusters.
Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). $$$$$ To begin, recall that the Brown clustering algorithm is based on a bigram language model.
Our first word representation is exactly the same as the one used in (Koo et al, 2008) where words are clustered using the Brown algorithm (Brown et al, 1992). $$$$$ Xavier Carreras was supported by the Catalan Ministry of Innovation, Universities and Enterprise, and a grant from NTT, Agmt.

In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. $$$$$ It is therefore attractive to consider intermediate entities which exist at a coarser level than the words themselves, yet capture the information necessary to resolve the relevant ambiguities.
In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. $$$$$ The algorithm then repeatedly merges the pair of clusters which causes the smallest decrease in the likelihood of the text corpus, according to a class-based bigram language model defined on the word clusters.
In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. $$$$$ We present a simple and effective semisupervised method for training dependency parsers.
In our experiments we use the clusters obtained in (Koo et al, 2008), but were unable to match the accuracy reported there, perhaps due to additional features used in their implementation not described in the paper. $$$$$ Setting aside the development of new clustering algorithms, a final area for future work is the extension of our method to new domains, such as conversational text or other languages, and new NLP problems, such as machine translation.

Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. $$$$$ Our first-order baseline feature set is similar to the feature set of McDonald et al. (2005a), and consists of indicator functions for combinations of words and parts of speech for the head and modifier of each dependency, as well as certain contextual tokens.1 Our second-order baseline features are the same as those of Carreras (2007) and include indicators for triples of part of speech tags for sibling interactions and grandparent interactions, as well as additional bigram features based on pairs of words involved these higher-order interactions.
Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. $$$$$ To begin, recall that the Brown clustering algorithm is based on a bigram language model.
Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. $$$$$ We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.
Terry Koo was kind enough to share the source code for the (Koo et al, 2008) paper with us, and we plan to incorporate all the features in our future work. $$$$$ Another idea would be to integrate the clustering algorithm into the training algorithm in a limited fashion.

Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008). $$$$$ Specifically, for any feature that is predicated on a word form, we eliminate this feature if the word in question is not one of the top-N most frequent words in the corpus.
Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008). $$$$$ We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.
Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008). $$$$$ Terry Koo was funded by NSF grant DMS-0434222 and a grant from NTT, Agmt.
Moreover, we introduce two extensions related to dependency parsing: The first extension is to combine SS-SCMs with an other semi-supervised approach, described in (Koo et al, 2008). $$$$$ In the simplest case, these parts are the dependency arcs themselves, yielding a first-order or “edge-factored” dependency parsing model.

In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. $$$$$ For example, after training an initial parser, one could parse a large amount of unlabeled text and use those parses to improve the quality of the clusters.
In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. $$$$$ We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.
In particular, Koo et al (2008) describe a semi-supervised approach that makes use of cluster features induced from unlabeled data, and gives state-of-the-art results on the widely used dependency parsing test collections: the Penn Treebank (PTB) for English and the Prague Dependency Treebank (PDT) for Czech. $$$$$ Michael Collins was funded by NSF grants 0347631 and DMS-0434222.

The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). $$$$$ To overcome this, McDonald and Pereira (2006) use a two-stage approximate decoding process in which the output of their second-order parser is “deprojectivized” via greedy search.
The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). $$$$$ We show that our semi-supervised approach yields improvements for fixed datasets by performing parsing experiments on the Penn Treebank (Marcus et al., 1993) and Prague Dependency Treebank (Hajiˇc, 1998; Hajiˇc et al., 2001) (see Sections 4.1 and 4.3).
The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). $$$$$ For example, after training an initial parser, one could parse a large amount of unlabeled text and use those parses to improve the quality of the clusters.
The first extension is to combine our method with the cluster-based semi-supervised method of (Koo et al, 2008). $$$$$ For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.

Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model. $$$$$ We present a simple and effective semisupervised method for training dependency parsers.
Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model. $$$$$ Abbreviations: ht = head POS, hw = head word, hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head, hc* = full bit string of head; mt,mw,mc4,mc6,mc* = likewise for modifier; st,gt,sc4,gc4,... = likewise for sibling and grandchild.
Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model. $$$$$ Both of the comparisons between cluster-based and baseline features in Table 4 are statistically significant.16 Table 5 compares accuracy results on the PDT 1.0 test set for our parsers and several other recent papers.
Our experiments investigate the effectiveness of: 1) the basics S-SCM for dependency parsing; 2) a combination of the SS-SCM with Koo et al (2008)'s semi supervised approach (even in the case we used the same unlabeled data for both methods); 3) the two stage semi-supervised learning approach that in 551corporates a second-order parsing model. $$$$$ 6/21/1998.

We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. $$$$$ In natural language parsing, lexical information is seen as crucial to resolving ambiguous relationships, yet lexicalized statistics are sparse and difficult to estimate directly.
We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. $$$$$ 6/21/1998.
We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. $$$$$ For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
We simply use the cluster based feature-vector representation f (x, y) introduced by (Koo et al, 2008) as the basis of our approach. $$$$$ We present a simple and effective semisupervised method for training dependency parsers.

These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets. $$$$$ For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets. $$$$$ The authors thank the anonymous reviewers for their insightful comments.
These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets. $$$$$ For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
These data sets are identical to the unlabeled data used in (Koo et al, 2008), and are disjoint from the training, development and test sets. $$$$$ When N is between roughly 100 and 1,000, there is little effect on the performance of the cluster-based feature sets.4 In addition, the vocabulary restriction reduces the size of the feature sets to managable proportions.
