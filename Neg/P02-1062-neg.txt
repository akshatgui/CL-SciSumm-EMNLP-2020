We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). $$$$$ The first approach uses a boosting algorithm for ranking problems.
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). $$$$$ This resulted precision, recall, F-measure.
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). $$$$$ As an example, suppose that an entity seen in a candidate.
We used a feature set which included the current, next, and previous word; the previous two tags; various capitalization and other features of the word being tagged (the full feature set is described in (Collins 2002a)). $$$$$ Similar features could be created for all of the global features introduced in this paper.

For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). $$$$$ A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.
For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). $$$$$ This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.
For related work on the voted perceptron algorithm applied to NLP problems, see (Collins 2002a) and (Collins 2002b). $$$$$ The independence assumptions in maximum-entropy taggers of this form often lead points of local ambiguity (in this example the tag for the word for) to create globally implausible structures with unreasonably high scores.

(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. $$$$$ Both of the reranking algorithms show significant improvements over the baseline: a 15.6% relative reduction in error for boosting, and a 17.7% relative error reduction for the voted perceptron.
(Collins 2002a) describes experiments on the same named-entity dataset as in this paper, but using explicit features rather than kernels. $$$$$ Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the experiments.

 $$$$$ For example, Animal would be mapped to Aa, G.M. would again be mapped to A.A..
 $$$$$ Define to be if S, and =C for (i.e., if the sequence of words within the quotes is tagged as a single entity).
 $$$$$ Examples of such techniques are Markov Random Fields (Abney 1997; Della Pietra et al. 1997; Johnson et al.
 $$$$$ This section discusses why this is unlikely to be the case.

 $$$$$ As an example, suppose that an entity seen in a candidate.
 $$$$$ The maximum-entropy tagger gives 20 proposed segmentations for each input sentence.
 $$$$$ From this data we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).
 $$$$$ At this point, we have fully described the representation used as input to the reranking algorithms.

The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a). $$$$$ 2001).
The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a). $$$$$ Both algorithms give comparable, significant improvements over the maximum-entropy baseline.
The true segmentation can now be compared with the N-best list in order to train an averaged perceptron algorithm (Collins, 2002a). $$$$$ From this data we created a training set of 53,609 sentences (1,047,491 words), and a test set of 14,717 sentences (291,898 words).

However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead. $$$$$ Figure 2 shows an algorithm which implements this greedy procedure.
However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead. $$$$$ One appeal of these methods is their flexibility in incorporating features into a model: essentially any features which might be useful in discriminating good from bad structures can be included.
However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead. $$$$$ In our corpus, entities (typically with long names) are often seen surrounded by quotes.

To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). $$$$$ Without loss of generality we take to be the candidate for which has the most correct tags, i.e., is closest to being correct.3 is the probability that the base model assigns to .
To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). $$$$$ For example, Animal would be mapped to Aa in this feature, G.M. would again be mapped to A.A.. for is the same as , but has an additional flag appended.
To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). $$$$$ A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.
To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). $$$$$ Without loss of generality we take to be the candidate for which has the most correct tags, i.e., is closest to being correct.3 is the probability that the base model assigns to .

For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. $$$$$ The task we consider is to recover named-entity boundaries.
For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. $$$$$ The arrays and are reverse indices from training examples to features.

 $$$$$ This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.
 $$$$$ The algorithm then makes a pass over the training set, at each training example storing a parameter vector for .
 $$$$$ (Freund & Schapire 1999) describe a refinement of the perceptron, the voted perceptron.
 $$$$$ The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.

This approach has been used earlier by (Collins, 2002). $$$$$ Both algorithms give comparable, significant improvements over the maximum-entropy baseline.
This approach has been used earlier by (Collins, 2002). $$$$$ Similar features could be created for all of the global features introduced in this paper.
This approach has been used earlier by (Collins, 2002). $$$$$ For the voted perceptron, the representation was taken to be a vector where is a parameter that influences the relative contribution of the log-likelihood term versus the other features.
This approach has been used earlier by (Collins, 2002). $$$$$ Recent work in statistical approaches to parsing and tagging has begun to consider methods which incorporate global features of candidate structures.

This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). $$$$$ This section discusses why this is unlikely to be the case.
This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). $$$$$ Another attractive property of the voted perceptron is that it can be used with kernels, for example the kernels over parse trees described in (Collins and Duffy 2001; Collins and Duffy 2002).
This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). $$$$$ Both algorithms give comparable, significant improvements over the maximum-entropy baseline.
This result is used to explain the convergence of weighted or voted perceptron algorithms (Collins, 2002a). $$$$$ This example also illustrates why this approach is unlikely to improve the performance of the maximum-entropy tagger.

The detailed algorithm can be found in (Collins, 2002). $$$$$ The second approach uses the voted perceptron algorithm.
The detailed algorithm can be found in (Collins, 2002). $$$$$ Intuitively, it is useful to note that this loss function is an upper bound on the number of “ranking errors”, a ranking error being a case where an incorrect candidate gets a higher value for than a correct candidate.
The detailed algorithm can be found in (Collins, 2002). $$$$$ One straightforward way to incorporate global features into the maximum-entropy model would be to introduce new features which indicated whether the tagging decision in the history creates a particular global feature.

Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. $$$$$ This example also illustrates why this approach is unlikely to improve the performance of the maximum-entropy tagger.
Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. $$$$$ In the example, this means that the LWLC=1 feature can only lower the score for the segmentation by lowering the probability of tagging its as N. But its has almost probably of not appearing as part of an entity, so should be almost whether is or in this context!
Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. $$$$$ In this paper we take the features to be fixed, the learning problem being to choose a good setting for the parameters .

 $$$$$ Acknowledgements Many thanks to Jack Minisi for annotating the named-entity data used in the experiments.
 $$$$$ The voted perceptron algorithm can be considerably more efficient to train, at some cost in computation on test examples.
 $$$$$ Figure 5 shows the results for the three methods on the test set.
 $$$$$ The start of the feature string indicates the feature type (in this case WE), followed by =.

 $$$$$ Then two other templates are: QF= QF2= In the “The Day They Shot John Lennon” example we would have provided that the entire sequence within quotes was tagged as an entity.
 $$$$$ The idea behind the voted perceptron is to take each of the parameter settings to “vote” for a candidate, and the candidate which gets the most votes is returned as the most likely candidate.
 $$$$$ A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.
 $$$$$ This paper describes algorithms which rerank the top N hypotheses from a maximum-entropy tagger, the application being the recovery of named-entity boundaries in a corpus of web data.

Collins (2002) includes a number of interesting contextual predicates for NER. $$$$$ The second approach uses the voted perceptron algorithm.
Collins (2002) includes a number of interesting contextual predicates for NER. $$$$$ The first approach uses a boosting algorithm for ranking problems.
Collins (2002) includes a number of interesting contextual predicates for NER. $$$$$ It is also related to the Markov Random Field methods for parsing suggested in (Johnson et al. 1999), and the boosting methods for parsing in (Collins 2000).

Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. $$$$$ Thus the training phase can be thought of as a way of constructing different parameter settings.
Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. $$$$$ This discriminative property is shared by the methods of (Johnson et al. 1999; Collins 2000), and also the Conditional Random Field methods of (Lafferty et al.
Collins (2002) also describes a mapping from words to word types which groups words with similar orthographic forms into classes. $$$$$ A question regarding the approaches in this paper is whether the features we have described could be incorporated in a maximum-entropy tagger, giving similar improvements in accuracy.

Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002). $$$$$ The methods give significant improvements over the maximum-entropy tagger (a 17.7% relative reduction in error-rate for the voted perceptron, and a 15.6% relative improvement for the boosting method).
Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002). $$$$$ This follows because for all , , where we define to be for , and otherwise.
Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002). $$$$$ Thanks also to Nigel Duffy, Rob Schapire and Yoram Singer for several useful discussions.
Using a wider context window than 2 words may improve performance; a reranking phase using global features may also improve performance (Collins, 2002). $$$$$ A second set of feature templates is anchored around quotation marks.

Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b). $$$$$ We define We assume a set of additional features, for .
Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b). $$$$$ This section discusses why this is unlikely to be the case.
Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b). $$$$$ In our experiments we found the voted perceptron algorithm to be considerably more efficient in training, at some cost in computation on test examples.
Each shape replaces characters by their types (case sensitive letters, digits, and punctuation), and deletes repeated types - e.g., Confidence and 2,664,098 are respectively mapped to Aa and 0,0+,0+ (Collins, 2002b). $$$$$ The second approach uses the voted perceptron algorithm.
