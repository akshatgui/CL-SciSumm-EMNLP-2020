More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data.
More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ Templates are the easiest way to implement surface NLG.
More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data.

In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong).
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ Domains which require complex and lengthy phrase fragments to describe a single attribute will be more challenging to model with features that only look at word n-grams for n E {2, 3}.
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ The systems NLG2 and NLG3 learn to determine both the word choice and the word order of the phrase.
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ The results of the manual evaluation, as well as the values of the search and feature selection parameters for all systems, are shown in Tables 5, 6, 7, and 8.

The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications $$$$$ The annotation scheme used a total of 26 attributes to represent flights.
The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications $$$$$ We suspect that our statistical generation approach should perform accurately in domains of similar complexity to air travel.
The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications $$$$$ All systems attempt to produce a grammatical natural language phrase from a domain-specific semantic representation.

[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ Domains in which there is greater ambiguity in word choice will require a more thorough search, i.e., a larger value of N, at the expense of CPU time and memory.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ The systems take a &quot;corpus-based&quot; or &quot;machinelearning&quot; approach to surface NLG, and learn to generate phrases from semantic input by statistically analyzing examples of phrases and their corresponding semantic representations.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ We present experiments in which we generate phrases to describe flights in the air travel domain.

Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ The probability model for NLG3, shown in Figure 2, conditions on the parent, the two closest siblings, the direction of the child relative to the parent, and the attributes that remain to be generated.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ The determination of the content in the semantic representation, or &quot;deep&quot; generation, is not discussed here.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ Instead, the systems assume that the input semantic representation is fixed and only deal with how to express it in natural language.

Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ For example, if the test set contains the template &quot;flights to $city-to leaving at $time-dep&quot;, the surface generation systems will be told to generate a phrase for the attribute set { $city-to, $time-dep }.
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ (The values for N, M, and K were determined by manually evaluating the output of the 4 or 5 most common attribute sets in the training data).
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ The training and test sets used to evaluate NLG1, NLG2 and NLG3 were derived semi-automatically from a pre-existing annotated corpus of user queries in the air travel domain.
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ (Dependency tree structures are not shown.)

An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ When generating a word, it uses local information, captured by word n-grams, together with certain non-local information, namely, the subset of the original attributes that remain to be generated.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ The feature patterns for NLG3 are shown in Table 4.

Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ We believe that such a method does not accurately measure the correctness or grammaticality of the text. not ideal, but is arguably superior than an automatic evaluation that fails to correlate with human linguistic judgement.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ Information to solve the attribute ordering and lexical choice problems— which would normally be specified in a large handwritten grammar— is automatically collected from data with a few feature patterns, and is combined via the maximum entropy framework.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ Specifically, nlgi(A) returns the phrase that corresponds to the attribute set A: [empty string] TA = where TA are the phrases that have occurred with A in the training data, and where C(phrase, A) is the training data frequency of the natural language phrase phrase and the set of attributes A. NLG1 will fail to generate anything if A is a novel combination of attributes.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ This limitation can be overcome by using features on values, so that NLG2 and NLG3 might discover — to use a hypothetical example — that &quot;flights leaving $city-fr&quot; is preferred over &quot;flights from $city-fr&quot; when $city-fr is a particular value, such as &quot;Miami&quot;.

A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ (Dependency tree structures are not shown.)
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ 01 if w = from and wi-i = flight and $city — fr E attri otherwise Input to Step 1: 1 $city-fr, $city-to, $time-dep, $date-dep 1 Output of Step 1: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot; Input to Step 2: &quot;a flight to $city-to that departs from $city-fr at $time-dep on $date-dep&quot;, $city-fr = New York City, $city-to = Seattle , $time-dep = 6 a.m., $date-dep = Wednesday } Output of Step 2: &quot;a flight to Seattle that departs from New York City at 6 a.m. on Wednesday&quot; Low frequency features involving word n—grams tend to be unreliable; the NLG2 system therefore only uses features which occur K times or more in the training data.
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ This paper presents three trainable systems for surface natural language generation (NLG).

More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ Instead, NLG3 assumes that conditioning on syntactically related words in the history will result on more accurate surface generation.
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ We believe that such a method does not accurately measure the correctness or grammaticality of the text. not ideal, but is arguably superior than an automatic evaluation that fails to correlate with human linguistic judgement.
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ This paper discusses previous approaches to surface NLG, and introduces three trainable systems for surface NLG, called NLG1, NLG2, and NLG3.
