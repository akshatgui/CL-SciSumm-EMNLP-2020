More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ The systems take a &quot;corpus-based&quot; or &quot;machinelearning&quot; approach to surface NLG, and learn to generate phrases from semantic input by statistically analyzing examples of phrases and their corresponding semantic representations.
More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ (These errors could be corrected by post-processing with a morphological analyzer.)
More recently, context-based models of disambiguation have been shown to represent significant improvements over the baseline (Bangalore and Rambow, 2000), (Ratnaparkhi, 2000). $$$$$ We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data.

In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ We conjecture that NLG2 and NLG3 should work in other domains which have a complexity similar to air travel, as well as available annotated data.
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data.
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation.
In the case of Ratnaparkhi's generator for flight information in the air travel domain (Ratnaparkhi, 2000), the transformation algorithm is trivial as the generator uses the corpus itself (annotated with semantic information such as destination or flight number) as input to a surface realizer with an n-gram model of the domain, along with a maximum entropy probability model for selecting when to use which phrase. $$$$$ The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work.

The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. $$$$$ The goal, more specifically, is then to learn the optimal attribute ordering and lexical choice for the text to be generated from the attribute-value pairs.
The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. $$$$$ Most importantly, the semantic annotation scheme for air travel has the property that it is both rich enough to accurately represent meaning in the domain, but simple enough to yield useful corpus statistics.
The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. $$$$$ NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy.
The likelihood of realisations given concepts or semantic representations has been modeled directly, but is probably limited to small-scale and specialised applications: summarisation construed as term selection and ordering [Witbrock and Mittal, 1999], grammar-free stochastic surface realisation [Oh and Rudnicky, 2000], and surface realisation construed as attribute selection and lexical choice [Ratnaparkhi, 2000]. Some of the above papers compare the purely statistical methods to other machine learning methods such as memory-based learning and reinforcement learning. $$$$$ The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work.

[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ We present three systems for surface natural language generation that are trainable from annotated corpora.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ Surface NLG, for our purposes, consists of generating a grammatical natural language phrase that expresses the meaning of an input semantic representation.
[Ratnaparkhi, 2000] describes a sentence realizer that had been trained on a domain-specific corpus (in the air travel domain) augmented with semantic attribute value pairs. $$$$$ The actual features are created by matching the patterns over the training data, e.g., an actual feature derived from the word hi-gram template might be: f(w1,tvi-1,w2-2,attri).

Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ The first two systems, called NLG1 and NLG2, require a corpus marked only with domainspecific semantic attributes, while the last system, called NLG3, requires a corpus marked with both semantic attributes and syntactic dependency information.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ This work was supported in part by DARPA Contract # MDA972-97-C-0012.
Ratnaparkhi (2000) uses maximum entropy models to drive generation with word bigram or dependency representations taking into account (unrealised) semantic features. $$$$$ The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work.

Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ NLG1 has no chance of generating anything for 3% of the data — it fails completely on novel attribute sets.
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ Information to solve the attribute ordering and lexical choice problems— which would normally be specified in a large handwritten grammar— is automatically collected from data with a few feature patterns, and is combined via the maximum entropy framework.
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ We suspect that our statistical generation approach should perform accurately in domains of similar complexity to air travel.
Our work is more related to Ratnaparkhi (2000) in the sense that we also use a large collection of generation templates for surface realization, but still distinct in that we intend to generate text from minimal input. $$$$$ For example, in Figure 1, if w =&quot;flights&quot;, then chi (w) =&quot;evening&quot; when generating the left children, and chl(w) =&quot;from&quot; when generating the right children.

An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ Instead, the systems assume that the input semantic representation is fixed and only deal with how to express it in natural language.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ This work was supported in part by DARPA Contract # MDA972-97-C-0012.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ There are more sophisticated surface generation packages, such as FUF/SURGE (Elhadad and Robin, 1996), KPML (Bateman, 1996), MUMBLE (Meteer et al., 1987), and RealPro (Lavoie and Rambow, 1997), which produce natural language text from an abstract semantic representation.
An exception is Ratnaparkhi (2000), who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from a semantic representation of attribute-value pairs, restricted to an air travel domain. $$$$$ NLG2 shows that using just local n-gram information can outperform the baseline, and NLG3 shows that using syntactic information can further improve generation accuracy.

Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ This limitation can be overcome by using features on values, so that NLG2 and NLG3 might discover — to use a hypothetical example — that &quot;flights leaving $city-fr&quot; is preferred over &quot;flights from $city-fr&quot; when $city-fr is a particular value, such as &quot;Miami&quot;.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ Using the unweighted results, both judges found an improvement from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase in accuracy from NLG2 to NLG3.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ Quantitative evaluation of experiments in the air travel domain will also be discussed.
Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). $$$$$ In contrast, (Langkilde and Knight, 1998) uses corpus-derived statistical knowledge to rank plausible hypotheses from a grammarbased surface generation component.

A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ Just as in NLG2, p is a distribution over V U *stop*, and the Improved Iterative Scaling algorithm is used to find the feature weights a3.
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ The surface generation system NLG2 assumes that the best choice to express any given attribute-value set is the word sequence with the highest probability that mentions all of the input attributes exactly once.
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ The MT systems of (Berger et al., 1996) learn to generate text in the target language straight from the source language, without the aid of an explicit semantic representation.
A number of statistical surface realizers have been described, notably the FERGUS (BangaloreandRambow, 2000) and HALogen systems (LangkildeGeary, 2002), as well as experiments in (Ratnaparkhi, 2000). $$$$$ Templates are the easiest way to implement surface NLG.

More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ All systems used the same training set, and were tested on the attribute sets extracted from the phrases in the test set.
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ The author thanks Scott McCarley for serving as the second judge, and Scott Axelrod, Kishore Papineni, and Todd Ward for their helpful comments on this work.
More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen,1983), corpus-based statistical knowledge (Langkilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007), or probabilistic generation spaces (Belz, 2008) to name just a few. $$$$$ This work was supported in part by DARPA Contract # MDA972-97-C-0012.
