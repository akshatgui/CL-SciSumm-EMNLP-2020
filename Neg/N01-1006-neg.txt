For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ It is a flexible method which is easily extended to various tasks and domains, and it has been applied to a wide variety of NLP tasks, including part of speech tagging (Brill, 1995), noun phrase chunking (Ramshaw and Marcus, 1999), parsing (Brill, 1996), phrase chunking (Florian et al., 2000), spelling correction (Mangu and Brill, 1997), prepositional phrase attachment (Brill and Resnik, 1994), dialog act tagging (Samuel et al., 1998), segmentation and message understanding (Day et al., 1997).
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ The four algorithms were trained on training sets of different sizes; training times were recorded and averaged over 4 trials.
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.

The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ Syntactic parsing attempts to construct a parse tree from a sentence by identifying all phrasal constituents and their attachment points.
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ Given these two sets of lists, the system can then easily: These two processes are performed multiple times during the update process, and the modification results in a significant reduction in running time.
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ This prevents the generation of useless rules and saves computational time.

We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ Transformation-based learning has been successfully employed to solve many natural language processing problems.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ One of the most time-consuming steps in transformation-based learning is the updating step.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ Considering that the number of low-score rules is a considerably higher than the number of high-score rules, this leads to a dramatic reduction in the overall running time.

A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.
A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ Let s' 2 V (s) be a sample in the vicinity of s. There are 2 cases to be examined one in which b applies to s' and one in which b does not: Case I: c (s') = c (b (s')) (b does not modify the classification of sample s').

Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The number of examined rules is kept close to the minimum.
Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The additional memory space required to store the lists of pointers associated with these rules is about 450 MB, which is a rather large requirement to add to a system.l The ICA system (Hepple, 2000) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance.

The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ We will discuss how rules which should get their good or bad counts decremented (subcases (1) and (2)) can be generated, the other two being derived in a very similar fashion.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.

In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.
In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.
In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ We need to identify the rules r that are influenced by the change s ! b (s).

With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ Let V (s) denote the &quot;vicinity&quot; of a sample the set of samples on whose classification the sample s might depend on (for consistency, s 2 V (s)); if samples are independent, then V (s) = fsg.
With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ The last task is text chunking, where both independence and commitment assumptions do not seem to be valid.
With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ The rules are then applied sequentially to the evaluation set in the order they were learned.
With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ Let s' 2 V (s) be a sample in the vicinity of s. There are 2 cases to be examined one in which b applies to s' and one in which b does not: Case I: c (s') = c (b (s')) (b does not modify the classification of sample s').

The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ The key observation behind the proposed algorithm is: when investigating the effects of applying the rule b to sample s, only samples s' in the set V (s) need to be checked.
The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ Let r be such a rule. f (r) needs to be updated if and only if there exists at least one sample s' such that Each of the above conditions corresponds to a specific update of the good (r) or bad (r) counts.
The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ As was described in the introductory section, the long training time of TBL poses a serious problem.
The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ At every point in the algorithm, we assumed that all the rules that have at least some positive outcome (good (r) > 0) are stored, and their score computed.

This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ Let r be such a rule. f (r) needs to be updated if and only if there exists at least one sample s' such that Each of the above conditions corresponds to a specific update of the good (r) or bad (r) counts.
This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.

The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ Furthermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks.
The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ The two assumptions made by the ICA algorithm result in considerably less training time, but the performance is also degraded (the difference in performance is statistically significant, as determined by a signed test, at a significance level of 0.001).
The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ The stability, resistance to overtraining, the existence of probability estimates and, now, reasonable speed make TBL an excellent candidate for solving classification tasks in general.

We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ The rules are then applied sequentially to the evaluation set in the order they were learned.
We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ Obviously, if both sets are not modified when applying rule b, then the value of the objective function for rule r remains unchanged.
We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.

For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ This leads to the following approach: The system thus learns a list of rules in a greedy fashion, according to the objective function.
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ Any sample s' that is not in the set {sIb changes s} can be ignored since s' = b(s').
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ Transformation-based learning has been successfully employed to solve many natural language processing problems.

Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ We have presented in this paper a new and improved method of computing the objective function for transformation-based learning.
Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation-based learner.
Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.
Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ For example, the tuple corresponding to the two above sentences would be: Many approaches to solving this this problem have been proposed, most of them using standard machine learning techniques, including transformationbased learning, decision trees, maximum entropy and backoff estimation.

With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ Let s' 2 V (s) be a sample in the vicinity of s. There are 2 cases to be examined one in which b applies to s' and one in which b does not: Case I: c (s') = c (b (s')) (b does not modify the classification of sample s').
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ Any sample s' that is not in the set {sIb changes s} can be ignored since s' = b(s').
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ The experiment was performed with the part-ofspeech data set.
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ The approach presented here builds on the same foundation as the one in (Ramshaw and Marcus, 1994): instead of regenerating the rules each time, they are stored into memory, together with the two values good (r) and bad (r).

Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ Transformation-based learning (TBL) (Brill, 1995) is one of the most successful rule-based machine learning algorithms.
Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ Among them is the drastic slowdown in rule learning as the scores of the rules decrease.
Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.
Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ This is a valuable contribution to systems and algorithms which utilize transformation-based learning at any part of the execution.

The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 million word corpus.

Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ Their method requires each rule to store a list of pointers to samples that it applies to, and for each sample to keep a list of pointers to rules that apply to it.
Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ The following notations will be used throughout this section: t, and t, = T[s]g the samples on which the rule applies and changes them to the correct classification; therefore, good(r) = jG(r)j. t, and C[s] = T [s]g the samples on which the rule applies and changes the classification from correct to incorrect; similarly, bad(r) = jB(r)j.
Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.
Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ In this paper, we present a novel method which enables a transformation-based learner to reduce its training time dramatically while still retaining all of its learning power.

We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ This method allows a transformation-based algorithm to train an observed 13 to 139 times faster than the original one, while preserving the final performance of the algorithm.
We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ Considering that the number of low-score rules is a considerably higher than the number of high-score rules, this leads to a dramatic reduction in the overall running time.
We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.

Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.
Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ These formulae offer us a method of generating the rules r which are influenced by the modification s' —� b (s'): (a) If p (b (s')) = false then decrease good (r), where r is the rule created with predicate p s.t. target T [s'];
Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
