For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ We have presented in this paper a new and improved method of computing the objective function for transformation-based learning.
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ Obviously, if both sets are not modified when applying rule b, then the value of the objective function for rule r remains unchanged.
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
For English and Swedish, for which POS-tagged training data was available to us, the fnTBL algorithm (Ngai and Florian, 2001) based on Brill (1995) was used to annotate the data, while for Spanish a mildly-supervised POS-tagging system similar to the one presented in Cucerzan and Yarowsky (2000) was employed. $$$$$ We will discuss how rules which should get their good or bad counts decremented (subcases (1) and (2)) can be generated, the other two being derived in a very similar fashion.

The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ Given these two sets of lists, the system can then easily: These two processes are performed multiple times during the update process, and the modification results in a significant reduction in running time.
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ Let s' 2 V (s) be a sample in the vicinity of s. There are 2 cases to be examined one in which b applies to s' and one in which b does not: Case I: c (s') = c (b (s')) (b does not modify the classification of sample s').
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ The training set contained approximately 1M words and the test set approximately 200k words.
The experiments presented in this paper were performed using the fnTBL toolkit (Ngai and Florian, 2001), which implements several optimizations in rule learning to drastically speed up the time needed for training. $$$$$ The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000).

We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ Most previous work has concentrated on situations which are of the form VP NP1 P NP2.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data.
We replaced the English part-of-speech tags with those generated by a transformation-based learner (Ngai and Florian, 2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.

A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ This method allows a transformation-based algorithm to train an observed 13 to 139 times faster than the original one, while preserving the final performance of the algorithm.
A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ However, it does have a serious drawback: the training time is often intorelably long, especially on the large corpora which are often used in NLP.
A TB Lchunker trained on Wall Street Journal corpus (Ngai and Florian, 2001) maps each word to an associated chunk tag, encoding chunk type and relative word position (beginning of an NP, inside a VP, etc.). $$$$$ These formulae offer us a method of generating the rules r which are influenced by the modification s' —� b (s'): (a) If p (b (s')) = false then decrease good (r), where r is the rule created with predicate p s.t. target T [s'];

Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ These formulae offer us a method of generating the rules r which are influenced by the modification s' —� b (s'): (a) If p (b (s')) = false then decrease good (r), where r is the rule created with predicate p s.t. target T [s'];
Then non-recursive, or basic, noun phrases (NPB) are identified using the TBL method reported in (Ngai and Florian, 2001). $$$$$ The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems: a standard transformation-based learner, and the ICA system (Hepple, 2000).

The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ Let r be such a rule. f (r) needs to be updated if and only if there exists at least one sample s' such that Each of the above conditions corresponds to a specific update of the good (r) or bad (r) counts.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The set is split into a test set of 500 samples and a training set of 12,500 samples.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The following definitions and notations will be used throughout the paper: where Since we are not interested in rules that have a negative objective function value, only the rules that have a positive good (r) need be examined.
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ For instance, in POS tagging, a sample is dependent on the classification of the preceding and succeeding 2 samples (this assumes that there exists a natural ordering of the samples in S).

In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ The authors would like to thank David Yarowsky for his advice and guidance, Eric Brill and John C. Henderson for discussions on the initial ideas of the material presented in the paper, and the anonymous reviewers for useful suggestions, observations and connections with other published material.
In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ Transformation-based learning has been successfully employed to solve many natural language processing problems.
In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ The key observation behind the proposed algorithm is: when investigating the effects of applying the rule b to sample s, only samples s' in the set V (s) need to be checked.
In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FNTBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. $$$$$ For instance, in POS tagging, a sample is dependent on the classification of the preceding and succeeding 2 samples (this assumes that there exists a natural ordering of the samples in S).

With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ Despite its attractive features as a machine learning algorithm, TBL does have a serious drawback in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks.
With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ The results of this tagger are presented to provide a performance comparison with a widely used tagger.
With the POS extraction method, we first Penn tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001), training over the Brownand WSJ corpora with some spelling, number and hyphenation normalisation. $$$$$ In other words, at most one rule is allowed to apply to each sample.

The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ Furthermore, extra care was taken to run all comparable experiments on the same machine and under the same memory and processor load conditions.
The sentence was first part-of-speech tagged and chunked with the fnTBL transformation based learning tools (Ngai and Florian, 2001). $$$$$ In this paper, we present a novel and realistic method for speeding up the training time of a transformation-based learner without sacrificing performance.

This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ Therefore, at the beginning of the algorithm, all the rules that correct at least one wrong classification need to be generated.
This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ An initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.
This package, and the TBL framework itself, are described in detail by Ngai and Florian (2001). $$$$$ It achieves state-of-the-art performance on many natural language processing tasks and does not overtrain easily.

The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ We have presented in this paper a new and improved method of computing the objective function for transformation-based learning.
The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ Samuel (1998) proposed a Monte Carlo approach to transformation-based learning, in which only a fraction of the possible rules are randomly selected for estimation at each iteration.
The feature set used in the TBL algorithm is similar to those used in the NP Chunking task in (Ngai and Florian, 2001). $$$$$ For each task, the same training set was provided to each system, and the set of possible rule templates was kept the same.

We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ In addition, we will show that our method scales better with training data size.
We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ At every point in the algorithm, we assumed that all the rules that have at least some positive outcome (good (r) > 0) are stored, and their score computed.
We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
We use the LMR tagging output to train a Transformation Based learner, using fast TBL (Ngai and Florian, 2001). $$$$$ The stability, resistance to overtraining, the existence of probability estimates and, now, reasonable speed make TBL an excellent candidate for solving classification tasks in general.

For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ We have presented in this paper a new and improved method of computing the objective function for transformation-based learning.
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ Furthermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks.
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ Despite its attractive features as a machine learning algorithm, TBL does have a serious drawback in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks.
For our purposes, we use a Penn tree bank-style tagger custom-built using fnTBL 1.0 (Ngai and Florian, 2001). $$$$$ Much research in natural language processing has gone into the development of rule-based machine learning algorithms.

Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ Most previous work has concentrated on situations which are of the form VP NP1 P NP2.
Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re) tag the BNC. $$$$$ This leads to the following approach: The system thus learns a list of rules in a greedy fashion, according to the objective function.

With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ These algorithms are attractive because they often capture the linguistic features of a corpus in a small and concise set of rules.
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ As was described in the introductory section, the long training time of TBL poses a serious problem.
With the POS extraction method, we first tagged the BNC using an fnTBL-based tagger (Ngai and Florian, 2001) trained over the Brown and WSJ corpora and based on the Penn POS tag set. $$$$$ Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.

Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ 1: Find the rule b = argmaxrER f (r). have a straightforward access to all rules that have a given predicate; this amount is considerably smaller than the one used to represent the rules.
Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ The performance of the FastTBL algorithm is the same as of regular TBL's, and runs in an order of magnitude faster.
Chunking An in-house chunker implemented with fnTBL, a transformation based learner (Ngaiand Florian, 2001), and trained on the British National Corpus (BNC). $$$$$ We have presented in this paper a new and improved method of computing the objective function for transformation-based learning.

The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ We need to identify the rules r that are influenced by the change s ! b (s).
The PoS tagging was performed with the fnTBL toolkit (Ngai and Florian, 2001). $$$$$ The additional memory space required to store the lists of pointers associated with these rules is about 450 MB, which is a rather large requirement to add to a system.l The ICA system (Hepple, 2000) aims to reduce the training time by introducing independence assumptions on the training samples that dramatically reduce the training time with the possible downside of sacrificing performance.

Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ The work presented here was supported by NSF grants IRI-9502312, IRI-9618874 and IIS-9985033.
Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ We note that the condition and the formula s' 2 B (r) and b (s') 2� B (r) is equivalent to (for the full details of the derivation, inferred from the definition of G (r) and B (r), please refer to Florian and Ngai (2001)).
Other attempts to address efficiency include the fast Transformation Based Learning (TBL) Toolkit (Ngai and Florian,2001) which dramatically speeds up training TBL systems, and the translation of TBL rules into finite state ma chines for very fast tagging. $$$$$ Transformation-based learning has been successfully employed to solve many natural language processing problems.

We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ Given a newly learned rule b that is to be applied to S, the goal is to identify the rules r for which at least one of the sets G (r) , B (r) is modified by the application of rule b.
We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ The approach presented here builds on the same foundation as the one in (Ramshaw and Marcus, 1994): instead of regenerating the rules each time, they are stored into memory, together with the two values good (r) and bad (r).
We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ To provide a broad comparison between the systems, three NLP tasks with different properties were chosen as the experimental domains.
We used Florian and Ngai's Fast TBL system (fnTBL) (Ngai and Florian, 2001) to train rules using dis fluency annotated conversational speech data. $$$$$ Despite its attractive features as a machine learning algorithm, TBL does have a serious drawback in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks.

Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ The increased speed of the Fast TBL algorithm also enables its usage in higher level machine learning algorithms, such as adaptive boosting, model combination and active learning.
Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ At every point in the algorithm, we assumed that all the rules that have at least some positive outcome (good (r) > 0) are stored, and their score computed.
Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ Syntactic parsing attempts to construct a parse tree from a sentence by identifying all phrasal constituents and their attachment points.
Similarly, the space efficient algorithm using compound questions at the end of Section 2.2.1 can be thought of as a static probabilistic version of the efficient TBL of Ngai and Florian (2001). $$$$$ Their method requires each rule to store a list of pointers to samples that it applies to, and for each sample to keep a list of pointers to rules that apply to it.
