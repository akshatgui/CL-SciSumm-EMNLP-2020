Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ These results indicate that the relative performance of the resolution classes is consistent across corpora.
Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ Starting with this correct partial clustering, we run our classifier on all ordered pairs of CEs for which the second CE is of class C, essentially asking our coreference resolver to determine whether each member of class C is coreferent with each of its preceding CEs.

In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.
In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ The bulk of the relevant related work is described in earlier sections, as appropriate.
In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution.
In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ This finding explains previously published results that exhibit striking variability when run with annotated CEs vs. system-extracted CEs.

 $$$$$ “union” in “union members”).
 $$$$$ Nevertheless, we see that anaphoricity infomation is important.
 $$$$$ Results for each of the data sets are shown in box 4 of Table 3.
 $$$$$ As discussed in depth elsewhere (e.g. van Deemter and Kibble (2000)), the notions of coreference and anaphora are difficult to define precisely and to operationalize consistently.

A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set7 utilized in Culotta et al (2007), Bengston and Roth (2008) and Stoyanov et al (2009). $$$$$ When available, we use the standard test/train split.
A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set7 utilized in Culotta et al (2007), Bengston and Roth (2008) and Stoyanov et al (2009). $$$$$ B3 Complications.

A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009). $$$$$ In the next section, we take a closer look at the coreference resolution task, analyzing the impact of various subtasks irrespective of the data set differences.
A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009). $$$$$ We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.
A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009). $$$$$ In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).
A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al (2009). $$$$$ This has not traditionally been done in learning-based coreference resolution research — possibly because there is not much training data available to sacrifice as a validation set.

 $$$$$ We outline some of these differences below.
 $$$$$ As is common for many natural language processing problems, the state-of-the-art in noun phrase (NP) coreference resolution is typically quantified based on system performance on manually annotated text corpora.
 $$$$$ It is related to anaphora resolution: a NP is said to be anaphoric if it depends on another NP for interpretation.
 $$$$$ 2007.

We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ While other researchers have evaluated NP coreference resolvers with respect to pronouns vs. proper nouns vs. common nouns (Ng and Cardie, 2002b), our analysis focuses on measuring the complexity of data sets, predicting the performance of coreference systems on new data sets, and quantifying the effect of coreference system subcomponents on overall performance.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ Our work looks for the first time at predicting the performance of NP coreference resolvers.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ Unfortunately, it is unclear how the B3 score should be computed for twinless CEs.

The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Or do differences in the coreference task definitions account for the differences in performance?
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Results for this experiment (box 6 in Table 3) are similar to the previous experiment using perfect CEs: we observe big improvements across the board.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.

While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ What accounts for these differences?
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ The ACE definition, on the other hand, includes relative pronouns and gerunds, excludes all nested nouns that are not themselves NPs, and allows premodifier NE mentions of geo-political entities and locations, such as “Russian” in “Russian politicians”.

To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ To avoid ambiguity, we will use the term coreference element (CE) to refer to the set of linguistic expressions that participate in the coreference relation, as defined for each of the MUC and ACE tasks.1 At times, it will be important to distinguish between the CEs that are included in the gold standard — the annotated CEs — from those that are generated by the coreference resolution system — the extracted CEs.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ We also measure the performance of state-of-the-art resolvers on several classes of anaphora and use these results to develop a measure that can accurately estimate a resolver’s performance on new data sets.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ In particular, we distinguish between proper and common nouns that can be resolved via string matching, versus those that have no antecedent with a matching string.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task.

The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ Previous work has developed methods to predict system performance on NLP tasks given data set characteristics, e.g.
The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ Given a new text collection and domain, what level of performance should we expect?
The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ Coreference or Not: A Twin Model for Coreference Resolution.
The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ This research was supported in part by the Department of Homeland Security under ONR Grant N0014-07-1-0152 and Lawrence Livermore National Laboratory subcontract B573245.

A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.
A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task.
A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g.

We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ 17% of all resolutions in the MUC6 corpus were in the PN-e class).
We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ Our work looks for the first time at predicting the performance of NP coreference resolvers.
We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ CEAF assigns a zero score to each twinless extracted CE and weights all coreference chains equally, irrespective of their size.
We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ Results for this experiment (box 6 in Table 3) are similar to the previous experiment using perfect CEs: we observe big improvements across the board.

Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ This, of course, is not the case when the system automatically identifies the CEs, so the scoring algorithm requires a mapping between extracted and annotated CEs.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ Our experiments confirm this hypothesis and we use our empirical results to create a coreference performance prediction (CPP) measure that successfully estimates the expected level of performance on novel data sets.

We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.
We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ This scoring function directly measures the impact of each resolution class on the overall MUC score.

Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ ACE, on the other hand, permits “singleton” CEs, which are not coreferent with any other CE in the document.
Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ Except for some fairly general statements, for example that proper names are easier to resolve than pronouns, which are easier than common nouns, there has been little analysis of which aspects of the problem have achieved success and which remain elusive.
Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ The # columns show the frequency counts for each resolution class, and the % columns show the distributions of the classes in each corpus (i.e.

We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ For state-of-the-art performance on the MUC data sets see, e.g.
We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ Resolution classes partition the set of anaphoric CEs according to properties of the anaphor and (in some cases) the antecedent.
We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ This section examines the role of three such subtasks — named entity recognition, anaphoricity determination, and coreference element detection — in the performance of an end-to-end coreference resolution system.
We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.

 $$$$$ MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution.
 $$$$$ For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems.
 $$$$$ The MUC data sets include annotations only for CEs that are coreferent with at least one other CE.
 $$$$$ In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g.

B3 here is the B3All version of Stoyanov et al (2009). $$$$$ We use the RECONCILE coreference resolution platform (Stoyanov et al., 2009) to configure a coreference resolver that performs comparably to state-of-the-art systems (when evaluated on the MUC and ACE data sets under comparable assumptions).
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ These scores confirm our expectations about the relative difficulty of different types of resolutions.
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ To achieve roughly state-of-theart performance, RECONCILEACL09 employs a fairly comprehensive set of 61 features introduced in previous coreference resolution systems (see Bengtson and Roth (2008)).
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ Xiaoqiang Luo.

The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold= 0.45, SIG for ACE04 and AP for ACE05, ACE05 ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in their paper (p.c.). $$$$$ (2003)).
The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold= 0.45, SIG for ACE04 and AP for ACE05, ACE05 ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in their paper (p.c.). $$$$$ The right-hand side of Table 4 shows the average distribution and scores across all data sets.
The perceptron baseline in this work (Reconcile settings: 15 iterations, threshold= 0.45, SIG for ACE04 and AP for ACE05, ACE05 ALL) has different results from Stoyanov et al (2009) because their current publicly available code is different from that used in their paper (p.c.). $$$$$ In particular, we distinguish between proper and common nouns that can be resolved via string matching, versus those that have no antecedent with a matching string.
