Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.
Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ The bulk of the relevant related work is described in earlier sections, as appropriate.
Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ Mitkov (2002) and Byron (2001)).
Since we use automatically extracted markables, it is possible that some extracted markables and the gold standard markables are unmatched, or twinless as defined in Stoyanov et al (2009). $$$$$ “America” in “Bank of America”), (2) relative pronouns, and (3) gerunds, but allows (4) nested nouns (e.g.

In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ First, however, we describe the coreference resolver that we use for our study.
In this paper, we adopt the B3all variation proposed by Stoyanov et al (2009), which retains all twin less markables. $$$$$ We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.

 $$$$$ Similarly, we partition pronominal anaphora into several subcategories that we expect may behave differently.
 $$$$$ He starts on Monday.

A04CU $$$$$ Conclusion #1: Improving the performance of NE taggers is not likely to have a large impact on the performance of state-of-the-art coreference resolution systems.
A04CU $$$$$ For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems.
A04CU $$$$$ Here, he is anaphoric because it depends on its antecedent, John Hall, for interpretation.
A04CU $$$$$ Below, we resume our investigation of the role of three coreference resolution subtasks and measure the impact of each on overall performance.

A05ST $$$$$ We show the relative impact of perfect NE recognition, perfect anaphoricity information for coreference elements, and knowledge of all and only the annotated CEs.
A05ST $$$$$ He starts on Monday.
A05ST $$$$$ To avoid ambiguity, we will use the term coreference element (CE) to refer to the set of linguistic expressions that participate in the coreference relation, as defined for each of the MUC and ACE tasks.1 At times, it will be important to distinguish between the CEs that are included in the gold standard — the annotated CEs — from those that are generated by the coreference resolution system — the extracted CEs.

 $$$$$ For our investigations, we employ a state-of-the-art classification-based NP coreference resolver and focus on the widely used MUC and ACE coreference resolution data sets.
 $$$$$ The scr columns show the MUCRC score for each resolution class.
 $$$$$ Soon et al. (2001) and Yang et al.

We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ Our CPP measure can be used to produce a good estimate of the level of performance on a new corpus.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ For each data set, we compute the correlation between the vector of MUC-RC scores over the resolution classes and the average vector of MUC-RC scores for the remaining five data sets.
We utilized MUC (Vilain et al, 1995), B3All (Stoyanov et al, 2009), B3None (Stoyanov et al, 2009), and Pairwise F1. $$$$$ We attribute this to the fact that MUC6 includes annotations for nested nouns, which almost always fall in the CN-e and CN-p classes.

The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task.
The Stoyanov et al (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahmanand Ng (2009) numbers represent their highest performing cluster ranking model. $$$$$ Singletons.

While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ Second, further improvements in MUC score for the ACE data sets over the runs using perfect CEs (box 5) reveal that accurately determining anaphoricity can lead to substantial improvements in MUC score.
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ Intuitively, we expect that it is easier to resolve the cases that involve string matching.
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ These substantial differences in the task definitions (summarized in Table 1) make it extremely difficult to compare performance across the MUC and ACE data sets.
While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their co reference algorithm, Stoyanovet al (2009) argue that such evaluations are unrealistic, as NP extraction is an integral part of an end-to-end fully-automatic resolver. $$$$$ For each data set, we compute the correlation between the vector of MUC-RC scores over the resolution classes and the average vector of MUC-RC scores for the remaining five data sets.

To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ The latter occurs because the ACE data sets include singletons.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ The right-hand side of Table 4 shows the average distribution and scores across all data sets.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ In Section 4, we quantify the difficulty of a text corpus with respect to coreference resolution by analyzing performance on different resolution classes.
To apply these scorers to automatically extracted NPs ,different methods have been proposed (see Rahman and Ng (2009) and Stoyanov et al (2009)). $$$$$ This, of course, is not the case when the system automatically identifies the CEs, so the scoring algorithm requires a mapping between extracted and annotated CEs.

The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ At a high level, both the MUC and ACE evaluations define CEs as nouns, pronouns, and noun phrases.
The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ “America” in “Bank of America”), (2) relative pronouns, and (3) gerunds, but allows (4) nested nouns (e.g.
The majority of well studied co reference features (e.g. Stoyanov et al (2009)) are actually positive co reference indicators. $$$$$ In spite of the availability of several benchmark data sets (e.g.

A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ However, due to the significant differences in task definition, available datasets, and evaluation metrics, their conclusions are not directly applicable to the full coreference task.
A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ We gratefully acknowledge technical contributions from David Buttler and David Hysom in creating the Reconcile coreference resolution platform.
A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ Yang et al. (2003); for state-ofthe-art performance on the ACE data sets see, e.g.
A more detailed study of Reconcile-based co reference resolution systems in different evaluation scenarios can be found in Stoyanov et al (2009). $$$$$ Furthermore, the connections between them are extremely complex and go beyond the scope of this paper.

We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ Nevertheless, we see that anaphoricity infomation is important.
We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ For state-of-the-art performance on the MUC data sets see, e.g.
We contrast our work with (Stoyanov et al, 2009), who show that the co-reference resolution problem can be separated into different parts ac cording to the type of the mention. $$$$$ In Proceedings of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL 2007).

Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ We show the relative impact of perfect NE recognition, perfect anaphoricity information for coreference elements, and knowledge of all and only the annotated CEs.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ We show the relative impact of perfect NE recognition, perfect anaphoricity information for coreference elements, and knowledge of all and only the annotated CEs.
Our baseline differs most substantially from Stoyanov et al (2009) in using a decision tree classifier rather than an averaged linear perceptron. $$$$$ We aim to shed light on the state-of-the-art in NP coreference resolution by teasing apart the differences in the MUC and ACE task definitions, the assumptions made in evaluation methodologies, and inherent differences in text corpora.

We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ The ACE definition, on the other hand, includes relative pronouns and gerunds, excludes all nested nouns that are not themselves NPs, and allows premodifier NE mentions of geo-political entities and locations, such as “Russian” in “Russian politicians”.
We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ First, we examine three subproblems that play a role in coreference resolution: named entity recognition, anaphoricity determination, and coreference element detection.
We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.
We find the decision tree classifier to work better than the default averaged perceptron (used by Stoyanov et al (2009)), on multiple datasets using multiple metrics (see Section 4.3). $$$$$ Conclusion #2: Improving the ability of coreference resolvers to identify coreference elements would likely improve the state-of-the-art immensely — by 10-20 points in MUC F1 score and from 2-12 F1 points for B3.

Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ We expect CE detection to be an important subproblem for an end-to-end coreference system.
Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ Our results suggest that the availability of accurate detectors for anaphoricity or coreference elements could substantially improve the performance of state-ofthe-art resolvers, while improvements to named entity recognition likely offer little gains.
Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ 2007.
Note that B3 has two versions which handle twinless (spurious) mentions in different ways (see Stoyanov et al (2009) for details). $$$$$ We study the resolution complexity of a text corpus by defining resolution classes.

We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ Our goals are twofold: to measure the level of performance of state-of-the-art coreference resolvers on different types of anaphora, and to develop a quantitative measure for estimating coreference resolution performance on new data sets.
We start with the Reconcile baseline but employ the decision tree (DT) classifier, because it has significantly better performance than the default averaged perceptron classifier used in Stoyanov et al (2009). $$$$$ Consider the following: John Hall is the new CEO.

 $$$$$ In particular, we distinguish between proper and common nouns that can be resolved via string matching, versus those that have no antecedent with a matching string.
 $$$$$ We measure the impact of each subproblem on coreference resolution and confirm that certain assumptions regarding these subproblems in the evaluation methodology can dramatically simplify the overall task.
 $$$$$ Thus, B30 presumes that all twinless extracted CEs are spurious.
 $$$$$ In the related area of anaphora resolution, researchers have studied the influence of subsystems on the overall performance (Mitkov, 2002) as well as defined and evaluated performance on different classes of pronouns (e.g.

B3 here is the B3All version of Stoyanov et al (2009). $$$$$ Using perfect CEs solves a large part of the coreference resolution task: the annotated CEs divulge anaphoricity information, perfect NP boundaries, and perfect information regarding the coreference relation defined for the data set.
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ As a result, runs with extracted CEs exhibit very low CEAF precision, leading to unreliable scores. sion as above when ce has a twin, and computes the precision as 1/|RCe |if ce is twinless.
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ Semantic Types.
B3 here is the B3All version of Stoyanov et al (2009). $$$$$ In all remaining experiments, we learn the threshold from the training set as in the BASELINE system.

The perceptron baseline in this work (Reconcile settings $$$$$ 17% of all resolutions in the MUC6 corpus were in the PN-e class).
The perceptron baseline in this work (Reconcile settings $$$$$ Second, we measure the performance of a state-of-the-art coreference resolver on several classes of anaphora and use these results to develop a quantitative measure for estimating coreference resolution performance on new data sets.
