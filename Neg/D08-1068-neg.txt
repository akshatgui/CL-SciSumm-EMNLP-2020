For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ In this paper, we present the first unsupervised approach that is competitive with supervised ones.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ We thank the anonymous reviewers for their comments.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.

Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ Thus, extending Haghighi and Klein’s model to include richer linguistic features is a challenging problem.
Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ MLN-H The heads were determined using the head rules in the Stanford parser (Klein & Manning, 2003), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added.
Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ This can be very inefficient for highly correlated clauses.

In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ (For simplicity, from now on we drop X from the formula.)
In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ Like Haghighi and Klein’s, our model is cluster-based rather than pairwise, and implicitly imposes transitivity.
In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ The MUC-6 dataset consists of 30 documents for testing and 221 for training.

For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ This paper introduces the first unsupervised coreference resolution system that is as accurate as supervised systems.
For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses.
For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate.

To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. $$$$$ The expected count can be approximated as where yk are samples generated by MC-SAT.
To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. $$$$$ A mention should agree with its cluster in entity type.
To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators: appositives and predicate nominatives. $$$$$ Learning in MUC-6 took only one hour, and in ACE-2004 two and a half.

Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor & Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy.
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data.
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities.
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains.

As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ H is the Hessian matrix, with the (i, j)th entry being The Hessian can be approximated with the same samples used for the gradient.
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses.
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ It performs joint inference among mentions, using relations like apposition and predicate nominals.
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities.

A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ It performs joint inference among mentions, using relations like apposition and predicate nominals.
A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum & Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon & Domingos, 2008)).
A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ To evaluate the contribution of the major components in our model, we conducted five experiments, each differing from the previous one in a single aspect.

Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ Our optimization problem is not convex, so initialization is important.
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ Their approach is a major step forward in unsupervised coreference resolution, but extending it is challenging.
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data.

 $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.
 $$$$$ Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data.
 $$$$$ On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.
 $$$$$ This tests how much can be gained by pooling information.

We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). $$$$$ This is a key subtask in many NLP applications, including information extraction, question answering, machine translation, and others.
We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). $$$$$ This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals.
We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models: contrastive estimation (Smith and Eisner, 2005) and sampling (Poon and Domingos, 2008). $$$$$ Lowd & Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.

The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ In our approach, what it takes is just adding two formulas to the MLN.
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ It performs joint inference among mentions, using relations like apposition and predicate nominals.
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outperformed it trained on 381 documents.

It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ We thank the anonymous reviewers for their comments.
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ Our model deviates from Haghighi & Klein’s (2007) in several important ways.
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ We evaluated our systems using two commonly-used scoring programs: MUC (Vilain et al., 1995) and B3 (Amit & Baldwin, 1998).

Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ By convention, at each inference step we name each non-empty cluster after the earliest mention it contains.
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied.
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ We developed a general method for producing a “lazy” version of relational inference algorithms (Poon & Domingos, 2008), which carries exactly the same inference steps as the original algorithm, but only maintains a small subset of “active” predicates/clauses, grounding more as needed.
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1).

ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). $$$$$ This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.
ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). $$$$$ The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.
ACE2004-NWIRE: ACE 2004 Newswire set to compare against Poon and Domingos (2008). $$$$$ It performs joint inference among mentions, using relations like apposition and predicate nominals.

Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond). $$$$$ This is a key subtask in many NLP applications, including information extraction, question answering, machine translation, and others.
Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond). $$$$$ Since our learning uses sampling, all results are the average of five runs using different random seeds.
Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond). $$$$$ We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule.
Predicate Nominatives: Another syntactic constraint exploited in Poon and Domingos (2008) is the predicate nominative construction, where the object of a copular verb (forms of the verb be) is constrained to corefer with its subject (e.g. Microsoft is a company in Redmond). $$$$$ E.g., for the above m, MC-SAT now samples u uniformly from (0, 0), and requires that in the next state 0' be no less than u. Equivalently, the new cluster and head for m should satisfy wm,c, + wm,c,,t, > log(u).

 $$$$$ It uses Markov logic as a representation language, which allows it to be easily extended to incorporate additional linguistic and world knowledge.
 $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.
 $$$$$ ), where x can be either a cluster or mention, e E {Person,Organization,Location,Other}, n E {Singular,Plural} and g E {Male, Female, Neuter}.
 $$$$$ In this paper, we present the first unsupervised approach that is competitive with supervised ones.

On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute error in the number of clusters (Table 6).
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ Readers are referred to their paper for details.
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ Further, deterministic or strong non-deterministic dependencies cause Gibbs sampling to break down (Poon & Domingos, 2006), making it difficult to leverage many linguistic regularities.
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ It performs joint inference among mentions, using relations like apposition and predicate nominals.

Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ In the existing implementation in Alchemy (Kok et al., 2007), SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m, c!)).

 $$$$$ The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.
 $$$$$ Another difference is in identifying heads.
 $$$$$ MLN-HAN The predicate-nominal rule was added.
 $$$$$ It performs joint inference among mentions, using relations like apposition and predicate nominals.
