For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ Equivalently, for each k, with probability 1 — e−lk the next state must satisfy ck.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ We do not assume any other labeled information.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ We begin by reviewing the necessary background on Markov logic.
For example, Poon and Domingos (2008) has empirically reported that such global approaches achieve performance better than the ones based on incrementally processing a text. $$$$$ In this paper, we present the first unsupervised approach that is competitive with supervised ones.

Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ Most existing supervised learning approaches for coreference resolution are suboptimal since they resolve each mention pair independently, only imposing transitivity in postprocessing (Ng, 2005).
Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ Lowd & Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.
Ng (2008) used an Expectation-Maximization (EM) algorithm, and Poon and Domingos (2008) applied Markov Logic Network (MLN). $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.

In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ We generated 100 samples using MC-SAT for each expectation approximation.6 We conducted experiments on MUC-6, ACE-2004, and ACE Phrase-2 (ACE-2).
In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.
In fact, Markov logic has been previously used by Poon and Domingos (2008) for coreference resolution and achieved good results, but it was used for unsupervised coreference resolution and the method was based on a different model, the entity-mention model. $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.

For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ For example, apposition (as in “Bill Gates, the chairman of Microsoft”) suggests coreference, and thus the two mentions it relates should always be placed in the same cluster.
For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ We emphasize that our approach is unsupervised, and thus the data only contains raw text plus true mention boundaries.
For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ 2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates.
For coreference resolution, the most notable one is unsupervised coreference resolution by Poon and Domingos (2008). $$$$$ To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1).

To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators $$$$$ Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.
To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators $$$$$ The goal of coreference resolution is to identify mentions (typically noun phrases) that refer to the same entities.
To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators $$$$$ At iteration i — 1, the factor 0k for clause ck is either elk if ck is satisfied in x(z−1), or 1 otherwise.
To seek good performance in an unsupervised way, Poon and Domingos (2008) highly rely on two important strong indicators $$$$$ We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic (Richardson & Domingos, 2006).

Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ 2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates.
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ Our heuristics for identifying appositives and predicate nominals also make many errors, which often can be fixed with additional name entity recognition capabilities (e.g., given “Mike Sullivan, VOA News”, it helps to know that the former is a person and the latter an organization).
Poon and Domingos (Poon and Domingos, 2008) use an unsupervised technique based on joint inference across mentions and Markov logic as a representation language for their system on both MUC and ACE data. $$$$$ This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.

As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ The final clustering is found using the MaxWalkSAT weighted satisfiability solver (Kautz et al., 1997), with the appropriate extensions.
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together.
As we are not interested in unsupervised inference, the system of Poon and Domingos (2008) was unsuitable for our needs. $$$$$ This limits their applicability.

A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ However, unsupervised coreference resolution is much more difficult.
A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE.
A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities.
A constraint-based graph partitioning system has been experimented by (Sapena et al., 2010) and a coreference detection system based on Markov logic networks (MLNs) has been proposed by (Poon and Domingos, 2008). $$$$$ This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.

Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals.
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ Our full model, using apposition and other relations for joint inference, is often as accurate as the best supervised models, or more.
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ However, unsupervised coreference resolution is much more difficult.
Poon and Domingos (2008) introduced an unsupervised system in the framework of Markov logic. $$$$$ It uses Markov logic as a representation language, which allows it to be easily extended to incorporate additional linguistic and world knowledge.

 $$$$$ With Z, the optimization problem is no longer convex.
 $$$$$ The most challenging case involves phrases with different heads that are both proper nouns (e.g., “Mr.
 $$$$$ The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.
 $$$$$ The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE.

We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models $$$$$ This improves resolution accuracy and is always applicable.
We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models $$$$$ On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.
We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models $$$$$ To combat overfitting, a Gaussian prior is imposed on all weights.
We develop efficient learning and inference algorithms using a novel combination of two ideas from previous work on unsupervised learning with log-linear models $$$$$ We also adjusted A more conservatively than Lowd & Domingos (2007).

The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ This is ensured by the hard rule (which has infinite weight and must be satisfied) InClust(m, c) ==> (Type(m, e) <---> Type(c, e)) 3We used the following cues: Mr., Ms., Jr., Inc., Corp., corporation, company.
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ This paper introduces the first unsupervised coreference resolution system that is as accurate as supervised systems.
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ In this paper, we present the first unsupervised approach that is competitive with supervised ones.
The most popular formalism today is Markov Logic, which has already been used for natural language processing tasks such as semantic role labeling (Riedel and Meza-Ruiz, 2008) and coreference resolution (Poon and Domingos, 2008). $$$$$ We thank the anonymous reviewers for their comments.

It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor & Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy.
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ At the heart of their system is a mixture model with a few linguistically motivated features such as head words, entity properties and salience.
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text.
It has been successfully used in temporal relations recognition (Yoshikawa et al, 2009), co-reference resolution (Poon and Domingos, 2008), etc. $$$$$ Unsupervised learning in Markov logic maximizes the conditional log-likelihood where Z are unknown predicates.

Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ For example, apposition (as in “Bill Gates, the chairman of Microsoft”) suggests coreference, and thus the two mentions it relates should always be placed in the same cluster.
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones.
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ Lowd & Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.
Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks. $$$$$ MC-SAT is a “slice sampling” Markov chain Monte Carlo algorithm.

ACE2004-NWIRE $$$$$ Many of these are known for pronouns, and some can be inferred from simple linguistic cues (e.g., “Ms.
ACE2004-NWIRE $$$$$ ), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head mixture model, where the mixture component priors are represented by the unit clause InClust(+m, +c) and the head distribution is represented by the head prediction rule All free variables are implicitly universally quantified.
ACE2004-NWIRE $$$$$ We found that this significantly improved the quality of the results that MaxWalkSAT returned.
ACE2004-NWIRE $$$$$ We used 30 iterations of PSCG for learning.

Predicate Nominatives $$$$$ (The other systems did not report B3.)
Predicate Nominatives $$$$$ This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals.
Predicate Nominatives $$$$$ This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.

 $$$$$ We thank the anonymous reviewers for their comments.
 $$$$$ (Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.)

On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ MLN-HAN The predicate-nominal rule was added.
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate.
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ For simplicity, from now on we omit X, whose values are fixed and always conditioned on.
On the MUC6-TEST dataset, our system outperforms both Poon and Domingos (2008) (an unsupervised Markov Logic Network system which uses explicit constraints) and Finkel and Manning (2008) (a supervised system which uses ILP inference to reconcile the predictions of a pairwise classifier) on all comparable measures. $$$$$ While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date.

Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ Most importantly, our model leverages apposition and predicate nominals, which Haghighi & Klein did not use.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ We begin by reviewing the necessary background on Markov logic.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains.
Similarly, on the ACE2004-NWIRE dataset, we also outperform the state-of-the-art unsupervised system of Poon and Domingos (2008). $$$$$ In this paper, we present the first unsupervised approach that is competitive with supervised ones.

 $$$$$ ), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head mixture model, where the mixture component priors are represented by the unit clause InClust(+m, +c) and the head distribution is represented by the head prediction rule All free variables are implicitly universally quantified.
 $$$$$ It shares several features with Haghighi & Klein’s model, but removes or refines features where we believe it is appropriate to.
 $$$$$ It shares several features with Haghighi & Klein’s model, but removes or refines features where we believe it is appropriate to.
 $$$$$ E.g., for the above m, MC-SAT now samples u uniformly from (0, 0), and requires that in the next state 0' be no less than u. Equivalently, the new cluster and head for m should satisfy wm,c, + wm,c,,t, > log(u).
