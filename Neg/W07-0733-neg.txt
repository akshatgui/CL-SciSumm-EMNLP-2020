Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ The results also suggest that higher performance can be obtained by using two translation models through the Moses decoderâ€™s alternative decoding path framework.
Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ In a sense, we submitted a baseline performance of this system.
Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ In this approach, an input sentence is first split into text chunks (so-called phrases), which are then mapped one-to-one to target phrases using a large phrase translation table.
Factored translation models have also been used for the integration of CCG supertags (Birch et al, 2007), domain adaptation (Koehn and Schroeder, 2007) and for the improvement of English-Czech translation (Bojar, 2007). $$$$$ Since the training corpus and tokenization changed, our reused weights are not always optimal in this respect.

The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ In order to be able to re-use the old weights, we were limited to domain adaptation methods that did not change the number of components.
The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models (Bertoldi and Federico, 2009) and meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007). $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. $$$$$ Due to time constraints, we did not carry out this step for all but the Czech systems (a new language for us).
Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. $$$$$ Language modeling software such as the SRILM toolkit we used (Stolke, 2002) allows the interpolation of these language models.
Work on domain adaptation for statistical machine translation (Koehn and Schroeder, 2007) tries to bring solutions to this issue. $$$$$ For a more detailed description of this model, please refer to (Koehn et al., 2005).

Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. $$$$$ 2: Test set performance of our systems: and output/reference length ratio.
Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. $$$$$ Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks.
Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. $$$$$ The changed only by less than 0.05, and often worsened.
Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data. $$$$$ This and the next approach explore methods to bias the language model, while the final approach biases the translation model.

Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. $$$$$ The basic components used in our experiments are: (a) two phrase translation probabilities (both p(e|f) and p(f|e)), (b) two word translation probabilities (both p(e|f) and p(f|e)), (c) phrase count, (d) output word count, (e) language model, (f) distance-based reordering model, and (g) lexicalized reordering model.
Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. $$$$$ Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used.
Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. $$$$$ We decided to use the interpolated language model method described in Section 2.5.
Although the Moses decoder is able to work with two phrase tables at once (Koehn and Schroeder, 2007), it is difficult to use this method when there is more than one additional model. $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). $$$$$ But only in one case we felt compelled to manually adjust the weight for the word count feature, since the original setup led to a output/reference length ratio of 0.88 on the development test set.
Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). $$$$$ Collins et al. (2005) suggest a method to reorder the German input before translating using a set of manually crafted rules.
Genre adaptation is one of the major challenges in statistical machine translation since translation models suffer from data sparseness (Koehn and Schroeder, 2007). $$$$$ Hence, we do not expect the best performance from this simplistic approach.

Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ 4.3 Training and decoding parameters We tried to improve performance by increasing some of the limits imposed on the training and decoding setup.
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ We had access to a fairly large computer cluster to carry out our experiments over the course of a few weeks.
Combining multiple translation models has been investigated for domain adaptation by Foster and Kuhn (2007) and Koehn and Schroeder (2007) before. $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).

The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). $$$$$ However, speed issues with the decoder and load issues on the crowded cluster caused us to take a few shortcuts.
The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
The pooled model for pairing data from abstracts and claims is trained on data composed of 250,000 sentences from each text section. Another approach to exploit commonalities between tasks is to train separate language and translation models on the sentences from each task and combine the models in the global log-linear model of the SMT framework, following Foster and Kuhn (2007) and Koehn and Schroeder (2007). $$$$$ Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks.

See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ The different system components hi (phrase translation probabilities, language model, etc.) are combined in a log-linear model to obtain the score for the translation e for an input sentence f: The weights of the components Ai are set by a discriminative training method on held-out development data (Och, 2003).
See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ The results suggest that the language model is a useful tool for domain adaptation.
See, for example, Koehn and Schroeder (2007) or Bertoldi and Federico (2009). $$$$$ The performance with the interpolated language model (27.12) and two language models (27.30) are similar.

Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). $$$$$ In a sense, we submitted a baseline performance of this system.
Separate 5-gram language models were built from the target side of the two data set sand then they were interpolated using weights chosen to minimise the perplexity on the tuning set (Koehn and Schroeder, 2007). $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).

We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. $$$$$ All perform better than the three baseline approaches.
We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. $$$$$ The performance with the interpolated language model (27.12) and two language models (27.30) are similar.
We implemented a TM interpolation strategy following the ideas proposed in (Schwenk and Estve, 2008), where the authors present a promising technique of target LMs linear interpolation; in (Koehn and Schroeder, 2007) where a log-linear combination of TMs is performed; and specifically in (Foster and Kuhn, 2007) where the authors present various ways of TM combination and analyze in detail the TM domain adaptation. $$$$$ For a more detailed description of this model, please refer to (Koehn et al., 2005).

In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ All perform better than the three baseline approaches.
In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit).
In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ However, speed issues with the decoder and load issues on the crowded cluster caused us to take a few shortcuts.
In (Koehn and Schroeder, 2007), different ways to combine available data belonging to two different sources was explored; in (Bertoldi and Federico, 2009) similar experiments were performed, but considering only additional source data. $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).

(Koehn and Schroeder, 2007) used two language models and two translation models $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. $$$$$ Figure 2 displays all the weights we explored during this process and the corresponding perplexity of the resulting language model on the development set (nc-dev2007).
(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
(Koehn and Schroeder, 2007), instead, opted for combining the sub-models directly in the SMT log-linear framework. $$$$$ Compared to other submitted systems, these are very good scores, often the best or second highest scores for these tasks.

One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. $$$$$ We searched for the optimal weight setting by simply testing a set of weights and focusing on the most promising range of weights.
One is the log-linear combination of TMs trained on each subcorpus (Koehn and Schroeder, 2007), with weights of each model tuned under minimal error rate training using MIRA. $$$$$ One decoding path is the in-domain translation table, and the other decoding path is the out-of-domain translation table.

Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. $$$$$ Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used.
Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. $$$$$ In our next setup, we used only in-domain data for training the language model.
Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. $$$$$ HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
Researchers such as Foster and Kuhn (2007) and Koehn and Schroeder (2007) have investigated mixture model approaches to adaptation. $$$$$ 4.4 Germanâ€“English system The Germanâ€“English language pair is especially challenging due to the large differences in word order.

Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. $$$$$ But only in one case we felt compelled to manually adjust the weight for the word count feature, since the original setup led to a output/reference length ratio of 0.88 on the development test set.
Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. $$$$$ The %BLEU score changed only by less than 0.05, and often worsened.
Koehn and Schroeder (2007) learn mixture weights for language models trained with in-domain and out of-domain data respectively by minimizing the perplexity of a tuning (development) set and interpolating the models. $$$$$ We also tried to increase beam sizes and the limit on the number of translation options per coverage span (ttable-limit).

Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ BLEU and NIST scores for all our systems on the test sets are displayed in Table 2.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ The %BLEU score changed only by less than 0.05, and often worsened.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
Further approaches to domain adaptation for SMT include adaptation using in-domain language models (Bertoldi and Federico, 2009), meta-parameter tuning on in-domain development sets (Koehn and Schroeder, 2007), or translation model adaptation using self-translations of in-domain source language texts (Ueffing et al, 2007). $$$$$ Since we want to obtain a language model that gives us the best performance on the target domain, we set this weight so that the perplexity of the development set from that target domain is optimized.

There were three different adaptation measures $$$$$ The changed only by less than 0.05, and often worsened.
There were three different adaptation measures $$$$$ Phrases may be reordered, but typically a reordering limit (in our experiments a maximum movement over 6 words) is used.
There were three different adaptation measures $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
There were three different adaptation measures $$$$$ This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

 $$$$$ In a sense, we submitted a baseline performance of this system.
 $$$$$ Such a decomposition is called a decoding path.
 $$$$$ The %BLEU score changed only by less than 0.05, and often worsened.
 $$$$$ During training, long sentences are removed from the training data to speed up the GIZA++ word alignment process.
