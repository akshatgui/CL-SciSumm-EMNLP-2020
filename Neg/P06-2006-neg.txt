Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ However, it is impossible to meaningfully compare these results to those reported here.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ The distinction between arguments and adjuncts is expressed by adjunction of adjuncts to maximal projections (XP —* XP Adjunct) as opposed to government of arguments (i.e. arguments are sisters within X] projections; X] —* X0 Arg]... ArgN).

Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ They conclude from the results of these experiments that the cut-down XLE parser is two-thirds the speed of Collins’ Model 3 but 12% more accurate, while the complete XLE system is 20% more accurate but five times slower.
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ Therefore, in the results presented in Table 1 we have not counted cases where either DepBank or our system assign a premodifier adegree(positive) or num(sg).
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ Finally, differences in the linguistic intuitions of the annotators and errors of commission or omission on both sides can only be uncovered by manual comparison of output (e.g. xmod vs. xcomp for limit efforts above).

To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly developed without reliance on large in-domain manually-constructed treebanks.
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.

This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ We speculate that we achieve these results because our system is engineered to make minimal use of lexical information both in the grammar and in parse ranking, because the grammar has been developed to constrain ambiguity despite this lack of lexical information, and because we can compute the full packed parse forest for all the test sentences efficiently (without sacrificing speed of processing with respect to other statistical parsers).
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ We have also highlighted difficulties for relational evaluation schemes and argued that presenting individual scores for (classes of) relations and features is both more informative and facilitates system comparisons.
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ These advantages appear to effectively offset the disadvantage of relying on a coarser, purely structural model for probabilistic parse selection.

Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. $$$$$ §5 discusses these results and proposes further lines of research.
Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. $$$$$ So evaluation scores based on aggregated counts of correct decisions will be doubled for a system which structures this information as in DepBank.
Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. $$$$$ Unfortunately, Kaplan et al. do not report their results broken down by relation or feature so it is not possible, for example, on the basis of the arguments made above, to choose to compare the performance of our system on ccomp to theirs for comp, ignoring subord form.
Four sets are English text: jh5 described in Section 3; trec consisting of questions from TREC and included in the tree banks released with the ERG; a00 which is taken from the BNC and consists of fact sheets and newsletters; and depbank, the 700 sentences of the Briscoe and Carroll version of DepBank (Briscoe and Carroll, 2006) taken from the Wall Street Journal. $$$$$ There are also issues of incompatible tokenization and lemmatization between the systems and of differing syntactic annotation of similar information, which lead to problems mapping between our GR output and the current DepBank.

The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ Fourth, the lattice of lemma-affix-tags is parsed using a grammar over such tags.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ (See e.g.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ Table 1 shows accuracy results for each individual relation and feature, starting with the GR bilexical relations in the extended DepBank and followed by most DepBank features reported by Kaplan et al., and finally overall macro- and miThe DepBank num feature on nouns is evaluated by Kaplan et al. on the grounds that it is semantically-relevant for applications.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.

The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ Figure 1 illustrates some DepBank annotations used in the experiment reported by Kaplan et al. and our hand-corrected GR output for the example Ten of the nation’s governors meanwhile called on the justices to reject efforts to limit abortions.
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ The reannotated DepBank potentially supports evaluations which score according to the degree of agreement between this and the original annotation and/or development of future consensual versions through collaborative reannotation by the research community.
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.

We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ Bikel (2004) shows that, in Collins’ (1999) Model 2, bilexical parameters contribute less than 0.5% to accuracy on in-domain data while lexical subcategorization-like parameters contribute just over 1%.
We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ We have demonstrated that an unlexicalized parser with minimal manual modification for WSJ text – but no tuning of performance to optimize on this dataset alone, and no use of PTB – can achieve accuracy competitive with parsers employing lexicalized statistical models trained on PTB.
We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ 142 of the phrase structure schemata are manually identified as peripheral rather than core rules of English grammar.
We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ However, the performance of the system, as measured by microraveraged F1-score on GR extraction alone, has declined by 2.7% over the held-out Susanne data, so even the unlexicalized parser is by no means domain-independent. than this since some of the test sentences are elliptical or fragmentary, but in many cases are recognized as single complete constituents.

For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.
For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks.
For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ Thus, mod scores are microaveraged over the counts for the five fully specified modifier relations listed immediately after it in Table 1.

The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ The set of GRs associated with a given derivation define a connected, directed graph with individual nodes representing lemmaaffix-tags and arcs representing named grammatical relations.
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ Viewed as output appropriate to specific applications, either approach is justifiable.
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.

 $$$$$ Kaplan et al. report that the complete XLE system finds rooted analyses for 79% of section 23 of the WSJ but do not report coverage just for the test sentences.
 $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
 $$$$$ The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.

Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ The few features that cannot be computed from GRs and CLAWS tags directly, such as stmt type, could be computed from the derivation tree.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ We define a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and/or bilexical relations over tree configurations.

 $$$$$ There are also issues of incompatible tokenization and lemmatization between the systems and of differing syntactic annotation of similar information, which lead to problems mapping between our GR output and the current DepBank.
 $$$$$ We speculate that we achieve these results because our system is engineered to make minimal use of lexical information both in the grammar and in parse ranking, because the grammar has been developed to constrain ambiguity despite this lack of lexical information, and because we can compute the full packed parse forest for all the test sentences efficiently (without sacrificing speed of processing with respect to other statistical parsers).

C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ Klein and Manning (2003) argue that such results represent about 4% absolute improvement over a carefully constructed unlexicalized PCFGlike model trained and tested in the same manner.1 Gildea (2001) shows that WSJ-derived bilexical parameters in Collins’ (1999) Model 1 parser contribute less than 1% to parse selection accuracy when test data is in the same domain, and yield no improvement for test data selected from the Brown Corpus.
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks.
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.

In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ The parser we deploy, like the XLE one, is based on a manually-defined feature-based unification grammar.
In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ We, therefore, extend DepBank with a set of grammatical relations derived from our own system output and highlight how issues of representation and scoring can affect results and their interpretation.
In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ Most of these features can be computed from the full GR representation of bilexical relations between numbered lemma-affix-tags output by the parser.

Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ DepBank was constructed by parsing the selected section 23 WSJ sentences with the XLE system and outputting syntactic features and bilexical relations from the F-structure found by the parser.
Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ However, for evaluation, these DepBank relations add little or no information not already specified by the xcomp relations in which these verbs also appear as dependents.

For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ The encoding of this mapping within the grammar is similar to that of F-structure mapping in LFG.
For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ To build the parsing module, the unification grammar is automatically converted into an atomiccategoried context free ‘backbone’, and a nondeterministic LALR(1) table is constructed from this, which is used to drive the parser.
For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ The comparison of systems using DepBank is not straightforward, so we extend and validate DepBank and highlight a number of representation and scoring issues for relational evaluation schemes.

The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ Kiefer et al., 1999).
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ In these cases, the GR graph will not be fully connected.
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ Unfortunately, Kaplan et al. do not report their results broken down by relation or feature so it is not possible, for example, on the basis of the arguments made above, to choose to compare the performance of our system on ccomp to theirs for comp, ignoring subord form.

The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ We have demonstrated that an unlexicalized parser with minimal manual modification for WSJ text – but no tuning of performance to optimize on this dataset alone, and no use of PTB – can achieve accuracy competitive with parsers employing lexicalized statistical models trained on PTB.
The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ The output of the parser can be displayed as syntactic trees, and/or factored into a sequence of bilexical grammatical relations (GRs) between lexical heads and their dependents.

The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.
The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ Most of these features can be computed from the full GR representation of bilexical relations between numbered lemma-affix-tags output by the parser.
The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
