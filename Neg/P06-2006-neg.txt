Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ For instance, the subj and ncsubj relations overlap as both annotations contain such a relation between call(ed) and Ten), but the GR annotation also includes this relation between limit and effort(s) and reject and justice(s), while DepBank links these two verbs to a variable pro.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ We then manually modified and extended the grammar with a further 6 rules, mostly to handle cases of indirect and direct quotation that are very common in this dataset.
Briscoe and Carroll (2006) discuss issues raised by this re annotation. $$$$$ These features and relations were subsequently checked, corrected and extended interactively with the aid of software tools (King et al., 2003).

Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ More recent PTB-based parsers show small improvements over Collins’ Model 3 using PARSEVAL, while Clark and Curran (2004) and Miyao and Tsujii (2005) report 84% and 86.7% F1-scores respectively for their own relational evaluations on section 23 of WSJ.
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.
Briscoe and Carroll (2006) show that the system has equivalent accuracy to the PARC XLE parser when the morphosyntactic features in the original DepBank gold standard are taken into account. $$$$$ We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.

To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ Nevertheless, it is possible to see, for instance, that while both parsers perform badly on second objects ours is worse, presumably because of lack of lexical subcategorization information.
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.
To remove this variable, we carry out a second evaluation against the Briscoe and Carroll (2006) re annotation of DepBank (King et al, 2003), as described in Clark and Curran (2007a). $$$$$ The reannotated DepBank potentially supports evaluations which score according to the degree of agreement between this and the original annotation and/or development of future consensual versions through collaborative reannotation by the research community.

This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ (See e.g.
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ Considerable progress has been made in accurate statistical parsing of realistic texts, yielding rooted, hierarchical and/or relational representations of full sentences.
This evaluation is particularly relevant for NPs, as the Briscoe and Carroll (2006) corpus has been annotated for internal NP structure. $$$$$ We evaluate the accuracy of an unlexicalized statistical parser, trained on 4K treebanked sentences from balanced data and tested on the PARC DepBank.

Four sets are English text $$$$$ Collins (1999) for detailed exposition of one such very fruitful line of research.)
Four sets are English text $$$$$ But it does not explicitly include all the features of DepBank or even of the reduced set of semanticallyrelevant features used in the experiments and evaluation reported in Kaplan et al..
Four sets are English text $$$$$ Kaplan et al.’s microaveraged scores for Collins’ Model 3 and the cut-down and complete versions of the XLE parser are given in Table 2, along with the microaveraged scores for our parser from Table 1.
Four sets are English text $$$$$ In future work, we hope to improve the accuracy of the system by adding lexical information to the statistical parse selection component without exploiting in-domain treebanks.

The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ The parser is passed as input the sequence of most probable lemma-affix-tags found by the tagger.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
The test sets were POS-tagged and lemmatized using RASP (Briscoe and Carroll, 2006). $$$$$ §4 details the various experiments undertaken with the extended DepBank and gives detailed results.

The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ Thus we reannotated the DepBank sentences with GRs using our current system, and then corrected and extended this annotation utilizing a software tool to highlight differences between the extant annotations and our own.2 This exercise, though time-consuming, uncovered problems in both annotations, and yields a doubly-annotated and potentially more valuable resource in which annotation disagreements over complex attachment decisions, for instance, can be inspected.
The lemmatization was done by RASP (Briscoe and Carroll, 2006). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.

We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ First, text is tokenized using a deterministic finite-state transducer.
We use the Briscoe and Carroll (2006) version of DepBank, a 560 sentence subset used to evaluate the RASP parser. $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.

For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ We define a lexicalized statistical parser as one which utilizes probabilistic parameters concerning lexical subcategorization and/or bilexical relations over tree configurations.
For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers.
For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ The few features that cannot be computed from GRs and CLAWS tags directly, such as stmt type, could be computed from the derivation tree.
For parser evaluation, three hundred of these sentences were manually annotated with DepBank grammatical relations (King et al, 2003) in the style of Briscoe and Carroll (2006). $$$$$ We demonstrate that a parser which is competitive in accuracy (without sacrificing processing speed) can be quickly tuned without reliance on large in-domain manuallyconstructed treebanks.

The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ More recent PTB-based parsers show small improvements over Collins’ Model 3 using PARSEVAL, while Clark and Curran (2004) and Miyao and Tsujii (2005) report 84% and 86.7% F1-scores respectively for their own relational evaluations on section 23 of WSJ.
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ The reannotated DepBank potentially supports evaluations which score according to the degree of agreement between this and the original annotation and/or development of future consensual versions through collaborative reannotation by the research community.
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ Most of these features can be computed from the full GR representation of bilexical relations between numbered lemma-affix-tags output by the parser.
The first, Wiki 300, for testing accuracy, consists of 300 sentences manually annotated with grammatical relations (GRs) in the style of Briscoe and Carroll (2006). $$$$$ However, for evaluation, these DepBank relations add little or no information not already specified by the xcomp relations in which these verbs also appear as dependents.

 $$$$$ Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers.
 $$$$$ However, it is impossible to meaningfully compare these results to those reported here.
 $$$$$ We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.
 $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.

Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ There are similar issues with other DepBank features and relations.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ However, DepBank treats the majority of prenominal modifiers as adjectives rather than nouns and, therefore, associates them with an adegree rather than a num feature.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ So evaluation scores based on aggregated counts of correct decisions will be doubled for a system which structures this information as in DepBank.
Of course, it is always possible to look at accuracy figures by dependency type in order to understand what a parser is good at, as recommended by Briscoe and Carroll (2006), but it is also desirable to have a single score reflecting the overall accuracy of a parser, which means that the construction's overall contribution to the score is relevant. $$$$$ Therefore, although Kaplan et al. demonstrate an improvement in accuracy at some cost to speed, there remain questions concerning viability for applications, at some remove from the financial news domain, for which substantial treebanks are not available.

 $$$$$ However, reproducing the exact DepBank subord form relation from the GR ccomp one is non-trivial because DepBank treats modal auxiliaries as syntactic heads while the GRscheme treats the main verb as head in all ccomp relations.
 $$$$$ These advantages appear to effectively offset the disadvantage of relying on a coarser, purely structural model for probabilistic parse selection.
 $$$$$ DepBank was constructed by parsing the selected section 23 WSJ sentences with the XLE system and outputting syntactic features and bilexical relations from the F-structure found by the parser.

C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ As King et al. note, it is difficult to identify such informational redundancies to avoid doublecounting and to eradicate all system specific biases.
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ Third, deterministic morphological analysis is performed on each tokentag pair with a finite-state transducer.
C&C parser provides CCG predicate-argument dependencies and Briscoe and Carroll (2006) style grammatical relations. $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.

In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ We have demonstrated that an unlexicalized parser with minimal manual modification for WSJ text – but no tuning of performance to optimize on this dataset alone, and no use of PTB – can achieve accuracy competitive with parsers employing lexicalized statistical models trained on PTB.
In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ This makes it more practical to use statistical parsers in applications that need access to aspects of predicate-argument structure.
In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ The results we report below are derived by choosing the most probable tag for each word returned by the PoS tagger and by choosing the unweighted GR set returned for the most probable parse with no lexical information guiding parse ranking.
In this paper we evaluate a CCG parser (Clarkand Curran, 2004b) on the Briscoe and Carrollversion of DepBank (Briscoe and Carroll, 2006). $$$$$ This reflects a difference of philosophy about resolution of such ‘understood’ relations in different constructions.

Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ Both Collins’ Model 3 and the XLE Parser use lexicalized models for parse selection trained on the rest of the WSJ PTB.
Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ However, we do believe that they collectively damage scores for our system.
Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ However, reproducing the exact DepBank subord form relation from the GR ccomp one is non-trivial because DepBank treats modal auxiliaries as syntactic heads while the GRscheme treats the main verb as head in all ccomp relations.
Briscoe and Carroll (2006) re annotated this resource using their GRs scheme, and used it to evaluate the RASP parser. $$$$$ On the other hand, DepBank includes an adjunct relation between meanwhile and call(ed), while the GR annotation treats meanwhile as a text adjunct (ta) of governors, delimited by balanced commas, following Nunberg’s (1990) text grammar but conveying less information here.

For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ On the Susanne corpus, the geometric mean of the number of analyses for a sentence of length n is 1.31'.
For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ This reflects a difference of philosophy about resolution of such ‘understood’ relations in different constructions.
For the gold standard we chose the version of Dep Bank re annotated by Briscoe and Carroll (2006), consisting of 700 sentences from Section 23 of the Penn Treebank. $$$$$ These sentences have been annotated with syntactic features and with bilexical head-dependent relations derived from the F-structure representation of Lexical Functional Grammar (LFG).

The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ DepBank was constructed by parsing the selected section 23 WSJ sentences with the XLE system and outputting syntactic features and bilexical relations from the F-structure found by the parser.
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ To build the parsing module, the unification grammar is automatically converted into an atomiccategoried context free ‘backbone’, and a nondeterministic LALR(1) table is constructed from this, which is used to drive the parser.
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ To build the parsing module, the unification grammar is automatically converted into an atomiccategoried context free ‘backbone’, and a nondeterministic LALR(1) table is constructed from this, which is used to drive the parser.
The B&C scheme is similar to the original DepBank scheme (King et al, 2003), but overall contains less grammatical detail; Briscoe and Carroll (2006) describes the differences. $$$$$ We evaluate the comparative accuracy of an unlexicalized statistical parser trained on a smaller treebank and tested on a subset of section 23 of the WSJ using a relational evaluation scheme.

The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ Here we compare the accuracy of our parser with Kaplan et al.’s results, by repeating their experiment with our parser.
The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ We selected the same 560 sentences as test data as Kaplan et al., and all modifications that we made to our system (see §2.4) were made on the basis of (very limited) information from other sections of WSJ text.3 We have made no use of the further 140 held out sentences in DepBank.
The GRs are described in Briscoe and Carroll (2006) and Briscoe et al (2006). $$$$$ Even if these features and relations were drawn from the same experiment, however, they would still not be exactly comparable.

The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ Clearly, more work is needed to enable more accurate, informative, objective and wider comparison of extant parsers.
The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ Table 1 shows accuracy results for each individual relation and feature, starting with the GR bilexical relations in the extended DepBank and followed by most DepBank features reported by Kaplan et al., and finally overall macro- and miThe DepBank num feature on nouns is evaluated by Kaplan et al. on the grounds that it is semantically-relevant for applications.
The GRs are arranged in a hierarchy, with those in Table 1 at the leaves; a small number of more general GRs subsume these (Briscoe and Carroll, 2006). $$$$$ We have demonstrated that an unlexicalized parser with minimal manual modification for WSJ text – but no tuning of performance to optimize on this dataset alone, and no use of PTB – can achieve accuracy competitive with parsers employing lexicalized statistical models trained on PTB.
