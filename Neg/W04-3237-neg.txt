Chelba and Acero (2004) first traina classifier on the source data. $$$$$ We study the impact of using increasing amounts of training data as well as using a small amount of adaptation data on this simple problem that is well suited to data-driven approaches since vast amounts of “training” data are easily obtainable by simply wiping the case information in text.
Chelba and Acero (2004) first traina classifier on the source data. $$$$$ The most sensitive parameter is the prior variance σ2, as shown in Figure 1; its value is chosen to maximize classification accuracy on development data.
Chelba and Acero (2004) first traina classifier on the source data. $$$$$ The adaptation procedure proves to be quite effective in further reducing the capitalization error of the WSJ MEMM capitalizer on BN test data.

(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).
(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ Overall, the MEMM capitalizer adapted to BN data achieves 60% relative improvement in accuracy over the 1-gram baseline.

The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ The cost of moving away from the mean for each feature fi is specified by the magnitude of the variance σi: a small variance σi will keep the weight λi close to its mean; a large variance σi will make the regularized log-likelihood (see Eq.
The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets — one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text — from 1996.
The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ As expected, adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way.

Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ Automatic capitalization is a practically relevant problem: speech recognition output needs to be capitalized; also, modern word processors perform capitalization among other text proofing algorithms such as spelling correction and grammar checking.
Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ In a similar vein, the work of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.

Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario.
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004).
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ In the capitalization case we have studied, the relative performance improvement of the MEMM capitalizer over the 1-gram baseline drops from in-domain — WSJ — performance of 45% to 35-40% when used on the slightly mismatched BN data.

One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ Another interesting research direction is to explore the usefulness of the MAP adaptation of MaxEnt models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing.
One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ Overall, automatic capitalization error rate 1.4%is achieved on BN data.

In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach: More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach: More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task.
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The sufficient statistics that are extracted from the training data are tuples the tag assigned in context xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2} and # denotes the count with which this event has been observed in the training data.

This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994.
This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ In order to take advantage of the adaptation data in our scenario, a maximum a-posteriori (MAP) adaptation technique for maximum entropy (MaxEnt) models is developed.

Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ Another interesting research direction is to explore the usefulness of the MAP adaptation of MaxEnt models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing.
Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ Another interesting research direction is to explore the usefulness of the MAP adaptation of MaxEnt models for other problems among which we wish to include language modeling, part-of-speech tagging, parsing, machine translation, information extraction, text routing.
Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ The “There’s no data like more data” rule-of-thumb could be amended by “..., especially if it’s the right data!”.

Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 — following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models — then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5.
Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ The experimental results are presented in Section 5, followed by conclusions and suggestions for future work.
Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags: For training a given capitalizer one needs to convert running text into uniform case text accompanied by the above capitalization tags.
Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ The 1-gram capitalizer used a vocabulary of the most likely 100k wds derived from the training data.

Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ The prior mean is set at A0 = Abackground · 0; · denotes concatenation between the parameter vector for the background model and a 0-valued vector of length |Fadapt\ Fbackground |corresponding to the weights for the new features.
Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ It was shown to be very effective in adapting a background MEMM capitalization model, improving the accuracy by 20-25% relative.
Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ Both models benefit from using more training data.
Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994.

 $$$$$ The experimental results are presented in Section 5, followed by conclusions and suggestions for future work.
 $$$$$ The resulting model is thus equivalent with the background model.
 $$$$$ The relative performance improvement of the MEMM capitalizer over the 1gram baseline drops to 35-40% when using out-ofdomain Broadcast News data.
 $$$$$ We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti−1, ti−2) which allows for efficient algorithms that search for the most likely tag sequence T∗(W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W).

Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ The sufficient statistics that are extracted from the training data are tuples the tag assigned in context xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2} and # denotes the count with which this event has been observed in the training data.
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ There are a number of parameters to be tuned on development data.
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ Overall, automatic capitalization error rate 1.4%is achieved on BN data.
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ We used a simple count cut-off feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a pre-specified threshold.

The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ The prior mean is set at A0 = Abackground · 0; · denotes concatenation between the parameter vector for the background model and a 0-valued vector of length |Fadapt\ Fbackground |corresponding to the weights for the new features.
The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well.
The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ In order to take advantage of the adaptation data in our scenario, a maximum a-posteriori (MAP) adaptation technique for maximum entropy (MaxEnt) models is developed.
