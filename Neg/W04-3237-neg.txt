Chelba and Acero (2004) first traina classifier on the source data. $$$$$ The probability P(ti|xi(W,T i−1 1 )) is modeled using a maximum entropy model.
Chelba and Acero (2004) first traina classifier on the source data. $$$$$ A Gaussian prior for the model parameters A has been previously used in (Chen and Rosenfeld, 2000) for smoothing MaxEnt models.
Chelba and Acero (2004) first traina classifier on the source data. $$$$$ As in previous approaches, the problem is framed as an instance of the class of sequence labeling problems.
Chelba and Acero (2004) first traina classifier on the source data. $$$$$ The in-domain test data used was file WS94_000 (8.7kwds).

(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ A surprising result is that the adaptation performance gain is not due to adding more, domain-specific features but rather making better use of the background features for modeling the in-domain data.
(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ We have then adapted the best MEMM model built on 20Mwds on the two BN data sets (CNN/ABC) and compared performance against the 1-gram and the unadapted MEMM models.
(Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. $$$$$ The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data.

The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ Any sequence labeling algorithm can then be trained for tagging lowercase word sequences with capitalization tags.
The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996).
The first model, which we shall refer to as the PRIOR model, was first introduced by Chelba and Acero (2004). $$$$$ In a similar vein, the work of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs.

Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ By way of example, the event associated with the first word in the example in Section 2 is (*bdw* denotes a special boundary type): MXC 1 currentword=primetime previousword=*bdw* nextword=continues t1=*bdw* t1,2=*bdw*,*bdw* prefix1=p prefix2=pr prefix3=pri suffix1=e suffix2=me suffix3=ime The maximum entropy probability model P(y|x) uses features which are indicator functions of the type: Assuming a set of features F whose cardinality is F, the probability assignment is made according to: where A = {A1 ... AF} E RF is the set of realvalued model parameters.
Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ Overall, automatic capitalization error rate 1.4%is achieved on BN data.
Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea is more general. $$$$$ A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters.

Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form.
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ Another observation is that not only the features observed in the adaptation data get updated: even if E˜p(x,y)[fi] = 0, the weight λi for feature fi will still get updated if the feature fi triggers for a context x encountered in the adaptation data and some predicted value y — not necessarily present in the adaptation data in context x.
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ The MEMM tagger is very effective in reducing both in-domain and out-of-domain capitalization error by 35%-45% relative over a 1-gram capitalization model.
Chelba and Acero (2004) use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data. $$$$$ Count cut-off feature selection has been used for the MEMM capitalizer with the threshold set at 5, so the MEMM model size is a function of the training data.

One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data.
One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1: We have proceeded building both 1-gram and MEMM capitalizers using various amounts of background training data.
One recently proposed method (Chelba and Acero, 2004) for transfer learning in Maximum Entropy models involves modifying the mu's of this Gaussian prior. $$$$$ We have also presented a general technique for adapting MaxEnt probability models.

In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The common variance σ will thus balance optimally the log-likelihood of the adaptation data with the A0 mean values obtained from the background data.
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ Adaptation improves performance even further by another 20-25% relative.
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 — files WS87_{001-126}.
In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. $$$$$ The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets — one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text — from 1996.

This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ The sufficient statistics that are extracted from the training data are tuples the tag assigned in context xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2} and # denotes the count with which this event has been observed in the training data.
This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ The MEMM tagger is very effective in reducing both in-domain and out-of-domain capitalization error by 35%-45% relative over a 1-gram capitalization model.
This may also be the reason that the model of Chelba and Acero (2004) did not aid in adaptation. $$$$$ As future work we plan to investigate the best way to blend increasing amounts of less-specific background training data with specific, in-domain data for this and other problems.

Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ Table 4 presents the variation in model size with different count cut-off values for the feature selection procedure on the adaptation data.
Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ As expected, adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way.
Another piece of similar work is (Chelba and Acero, 2004), who also modify their prior. $$$$$ The results are presented in Table 3.

Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ As shown in Appendix A, the update equations are very similar to the 0-mean case: The effect of the prior is to keep the model parameters λi close to the background ones.
Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ The cost of moving away from the mean for each feature fi is specified by the magnitude of the variance σi: a small variance σi will keep the weight λi close to its mean; a large variance σi will make the regularized log-likelihood (see Eq.
Chelba and Acero (2004) use the parameters of the source domain maximum entropy classifier as the means of a Gaussian prior when training a new model on the target data. $$$$$ The cost of moving away from the mean for each feature fi is specified by the magnitude of the variance σi: a small variance σi will keep the weight λi close to its mean; a large variance σi will make the regularized log-likelihood (see Eq.

Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ In the capitalization case we have studied, the relative performance improvement of the MEMM capitalizer over the 1-gram baseline drops from in-domain — WSJ — performance of 45% to 35-40% when used on the slightly mismatched BN data.
Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ In fact, the difference in performance between this latter approach and ours could be quite large since the cardinality of Fbackground is typically several orders of magnitude larger than that of Fadapt and our approach also updates the weights corresponding to features in Fbackground \ Fadapt.
Chelba and Acero (2004) use amaximum entropy Markov model (MEMM) combining features involving words and their cases. $$$$$ The prior has 0 mean and diagonal covariance: A ∼ N(0, diag(σ2 i)).

 $$$$$ For example, one may wish to use a capitalization engine developed on newswire text for email or office documents.
 $$$$$ We did not experiment with various tying schemes although this is a promising research direction.
 $$$$$ In our experiments the variances were tied to σi = σ whose value was determined by line search on development data drawn from the adaptation data.

Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1: We have proceeded building both 1-gram and MEMM capitalizers using various amounts of background training data.
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ As can be seen, very few features are added to the background model.
Several approaches utilize source data for training on a limited number of target labels, including feature splitting (Daume, 2007) and adding the source classifier' s prediction as a feature (Chelba and Acero, 2004). $$$$$ The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data.

The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994.
The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ We did not experiment with various tying schemes although this is a promising research direction.
The approach proposed by Chelba and Acero (2004) is also related as they propose a MAP adaptation via Gaussian priors of a MaxEnt model for recovering the correct capitalization of text. $$$$$ Another observation is that not only the features observed in the adaptation data get updated: even if E˜p(x,y)[fi] = 0, the weight λi for feature fi will still get updated if the feature fi triggers for a context x encountered in the adaptation data and some predicted value y — not necessarily present in the adaptation data in context x.
