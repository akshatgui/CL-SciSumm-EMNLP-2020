The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (Gao and Vogel, 2008), a multi-thread version of GIZA++ (Och and Ney, 2003) which also allows previously trained IBM alignments models to be applied on the development and test corpora. $$$$$ The threads do alignment in their own stacks, and read required probabilities from global parameter tables, such as the TTable, which reside on the heap.
The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (Gao and Vogel, 2008), a multi-thread version of GIZA++ (Och and Ney, 2003) which also allows previously trained IBM alignments models to be applied on the development and test corpora. $$$$$ We trained the English-to-Spanish system, and tuned the system on two datasets, the WSMT 2006 Europal test set (TUNE1) and the WSMT news commentary devtest set 2007 (TUNE2).
The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (Gao and Vogel, 2008), a multi-thread version of GIZA++ (Och and Ney, 2003) which also allows previously trained IBM alignments models to be applied on the development and test corpora. $$$$$ Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved.
The experiments led on the alignment methods were evaluated on the development corpus using MGIZA++ (Gao and Vogel, 2008), a multi-thread version of GIZA++ (Och and Ney, 2003) which also allows previously trained IBM alignments models to be applied on the development and test corpora. $$$$$ We also have to put mutual locks on the accumulators used to calculate the alignment perplexity.

We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). $$$$$ After aligning the sentence pairs, the counts need to be collected.
We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). $$$$$ A master process takes these counts and combines them, and produces the normalized model parameters for the next iteration.
We word-aligned the corpus with MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of the standard word alignment tool GIZA++ (Och and Ney, 2003). $$$$$ Therefore, one should consider which version to choose when building systems.

The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). $$$$$ Due to the need for synchronization, there are always some CPU time wasted in waiting.
The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). $$$$$ In each iteration it first calculates the best word alignment for each sentence pairs in the corpus, accumulating various counts, and then normalizes the counts to generate the model parameters for the next iteration.
The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). $$$$$ Accelerating is limited by the number of CPUs the node has.
The parallel corpus was then word-aligned using MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). $$$$$ Secondly, the mechanism of assigning sentence pairs to the child processes can be improved in PGIZA++.

We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. $$$$$ This property makes it very simple to integrate MGIZA++ into machine translation packages, such as Moses(Koehn et al., 2007).
We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. $$$$$ However, as we will see in Section 5, the difference does not hurt translation performance significantly.
We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. $$$$$ While working on the required modification to GIZA++ to run the alignment step in parallel we identified a bug, which needed to be fixed.
We extend the multi-thread GIZA++ (Gao and Vogel, 2008) to load the alignments from a modified corpus file. $$$$$ One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology.

We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. $$$$$ Also, the normalization procedure needs to read all the count files from network storage.
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. $$$$$ To make more efficient use of available computing resources and thereby speed up the training of our SMT system, we decided to modify GIZA++ so that it can run in parallel on multiple CPUs.
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. $$$$$ For example, (Brown et al., 1993) suggested two different methods: using only the alignment with the maximum probability, the so-called Viterbi alignment, or generating a set of alignments by starting from the Viterbi alignment and making changes, which keep the alignment probability high.

We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. $$$$$ (Och and Ney, 2003) So in this paper we focus on Model 1, HMM, Model 3 and 4.
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. $$$$$ To compare the performance of system, we recorded the total training time and the BLEU score, which is a standard automatic measurement of the translation quality(Papineni et al., 2002).
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. $$$$$ Among these models, the lexicon probability table (TTable) is the largest.

We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ However, as we will see in Section 5, the difference does not hurt translation performance significantly.
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved.
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ The paper describes two parallel implementations of the well-known and widely used word alignment tool GIZA++.
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ The weight is normalized by pi = wi/ Ei wi, so that Ei pi = 1.

is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)). $$$$$ In order to express the probability in statistical way, several different parametric forms of P(fJ1 , aJ1 |eI1) = pθ(fJ1 , aJ1 |eI1) have been proposed, and the parameters θ can be estimated using maximum likelihood estimation(MLE) on a training corpus(Och and Ney, 2003).
is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)). $$$$$ Compared to PGIZA++ on the speed-up factor by each additional CPU, MGIZA++ also shows some deficiency.
is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)). $$$$$ The results are listed in Table 8.
is the translation probability score (as the one given for instance by GIZA++ (Gao and Vogel, 2008)). $$$$$ Although MGIZA++ can only utilize a single multi-processor computer, which limits the number of CPUs it can use, it avoids the overhead of slow network I/O.

Then, we applied a parallel version of GIZA++ (Gao and Vogel, 2008) that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at word form level. $$$$$ Making use of this property, the alignment procedure can be parallelized.
Then, we applied a parallel version of GIZA++ (Gao and Vogel, 2008) that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at word form level. $$$$$ Training word alignment models on large corpora is a very time-consuming processes.
Then, we applied a parallel version of GIZA++ (Gao and Vogel, 2008) that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at word form level. $$$$$ With compatible interface, MGIZA++ is suitable for a drop-in replacement for GIZA++, while PGIZA++ can utilize huge computation resources, which is suitable for building large scale systems that cannot be built using a single machine.
Then, we applied a parallel version of GIZA++ (Gao and Vogel, 2008) that gave us the translation dictionaries of content words only (nouns, verbs, adjective and adverbs) at word form level. $$$$$ The two versions of alignment tools are available online at http://www.cs.cmu.edu/˜qing/giza.

We will continue exploration on these directions. The extended GIZA++ is released to the research community as a branch of MGIZA++ (Gao and Vogel, 2008), which is available online. $$$$$ PGIZA++ does alignment on a number of independent processes, uses network file system to collect counts, and performs normalization by a master process.
We will continue exploration on these directions. The extended GIZA++ is released to the research community as a branch of MGIZA++ (Gao and Vogel, 2008), which is available online. $$$$$ It is also important to determine whether to use PGIZA++ rather than MGIZA++ according to the speed of network storage infrastructure.
We will continue exploration on these directions. The extended GIZA++ is released to the research community as a branch of MGIZA++ (Gao and Vogel, 2008), which is available online. $$$$$ (Och and Ney, 2003) So in this paper we focus on Model 1, HMM, Model 3 and 4.
We will continue exploration on these directions. The extended GIZA++ is released to the research community as a branch of MGIZA++ (Gao and Vogel, 2008), which is available online. $$$$$ While (Och and Ney, 2003) presents algorithm to implement counting over all the alignments for Model 1,2 and HMM, it is prohibitive to do that for Models 3 through 6.

In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the t table probabilities of the IBM models and the HMM model. $$$$$ Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved.
In this section, we will explain how to build a transliteration module on the extracted transliteration pairs and how to integrate it into MGIZA++ (Gao and Vogel, 2008) by interpolating it with the t table probabilities of the IBM models and the HMM model. $$$$$ Compared to PGIZA++ on the speed-up factor by each additional CPU, MGIZA++ also shows some deficiency.

Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005). $$$$$ For HMMTable and D4Table, which use maps as their data structure, we cannot allow concurrent read/write to the table, because the map structure may be changed when inserting a new entry.
Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005). $$$$$ Accelerating is limited by the number of CPUs the node has.
Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005). $$$$$ Therefore, I/O becomes a bottleneck, especially when the number of child processes is large.
Unidirectional word alignments were provided by MGIZA++ (Gao and Vogel, 2008), then symmetrized with the grow-diag-final-and heuristic (Koehn et al, 2005). $$$$$ A server can take responsibility to assign sentence pairs to available child processes dynamically.

For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel. $$$$$ In order to compare the acceleration rate of PGIZA++ and MGIZA++, we also ran PGIZA++ in the same dataset as described in the previous section with 4 children.
For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel. $$$$$ Child processes are then restarted for the new iteration.
For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel. $$$$$ PGIZA++ will not perform as good as MGIZA++ on small-size corpora.
For the word alignments, we chose MGIZA (Gaoand Vogel, 2008), using seven threads per MGIZA instance, with the parallel option ,i.e. one MGIZA in stance per pair direction running in parallel. $$$$$ To compare the difference in final Viterbi alignment output, we counted the number of sentences that have different alignments in these systems.

This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. $$$$$ One major disadvantage of MGIZA++ is also obvious: lack of scalability.
This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. $$$$$ In the E-step the counts for all the parameters are collected, and the counts are normalized in M-step.
This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. $$$$$ Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved.
This is largest publicly available parallel corpus, and it does strain computing resources, for instance forcing us to use multi-threaded GIZA++ (Gao and Vogel, 2008). Table 7 shows the gains obtained from using this corpus in both the translation model and the language model opposed to a baseline system trained with otherwise the same settings. $$$$$ We also have to put mutual locks on the accumulators used to calculate the alignment perplexity.

The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. $$$$$ In order to compare the acceleration rate of PGIZA++ and MGIZA++, we also ran PGIZA++ in the same dataset as described in the previous section with 4 children.
The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. $$$$$ The resulting word alignment is then used to extract phrase pairs and perhaps other information to be used in translation systems, such as block reordering models.
The parallel sentences are aligned using MGIZA++ (Gao and Vogel, 2008) and then the proposed rule extraction algorithm was used in extracting the SRL-aware SCFG rules. $$$$$ Compared to PGIZA++ on the speed-up factor by each additional CPU, MGIZA++ also shows some deficiency.

We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. $$$$$ The recent development of MapReduce framework shows its capability to parallelize a variety of machine learning algorithms, and we are attempting to port word alignment tools to this framework.
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. $$$$$ Therefore its performance also depends on the speed of the network infrastructure.
We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008). This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. $$$$$ This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process.

Word alignment scores $$$$$ That is mainly because of the required locking mechanism.
Word alignment scores $$$$$ This could reduce the I/O significantly when using the same number of CPUs.
Word alignment scores $$$$$ One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology.
Word alignment scores $$$$$ Results show a near-linear speedup according to the number of CPUs used, and alignment quality is preserved.

The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ However, as we will see in Section 5, the difference does not hurt translation performance significantly.
The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process.
The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ We also have to put mutual locks on the accumulators used to calculate the alignment perplexity.
The system translates from cased French to cased English; at no point do we lowercase data. The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ However, PGIZA++ also has significant drawbacks.

We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ Training state-of-the-art phrase-based statistical machine translation (SMT) systems requires several steps.
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ The residual result shows that the MGIZA++ has a very small (less than 0.2%) difference in alignment scores, while PGIZA++ has a larger residual.
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ The most widely used tool to perform this training step is the well-known GIZA++(Och and Ney, 2003).
We use an extended version of MGIZA++ (Gaoand Vogel, 2008) to perform the constrained semi supervised word alignment. $$$$$ This paper describes two parallel implementations of GIZA++ that accelerate this word alignment process.

For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized. We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the word aligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus. $$$$$ The results of experiments show the efficiency and also the fidelity of the alignment generated by the two versions of parallel GIZA++.
For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized. We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the word aligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus. $$$$$ The PGIZA++ implementation, which is based on (Lin et al, 2006), uses multiple aligning processes.
For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized. We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the word aligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus. $$$$$ We use GIZA++ with the bug fixed as the reference.
For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized. We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the word aligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus. $$$$$ One of the implementations runs on computer clusters, the other runs on multi-processor system using multi-threading technology.
