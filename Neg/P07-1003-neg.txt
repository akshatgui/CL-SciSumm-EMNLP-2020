Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ Figure 1B exposes the consequences: a wide array of desired rules are lost during extraction.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ These allocations are summed and normalized for each tree transition type to complete re-estimation.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ A novel, high performing alternative is the soft union, which we evaluate in the next section: Syntax-alignment compatibility can be further promoted with a simple posterior decoding heuristic we call competitive thresholding.

We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ This alignment pattern was observed in our test set and corrected by our model.
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ Daum´e III and Marcu (2005) employs a syntax-aware distortion model for aligning summaries to documents, but condition upon the roots of the constituents that are jumped over during a transition, instead of those that are visited during a walk through the tree.
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.

When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model.
When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.
When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ To allow syntax-sensitive distortion, we consider a new distortion model of the form pd(aj|aj_, j, t).

We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ In the case of syntactic machine translation, we want to condition on crossing constituent boundaries, even if no constituents are skipped in the process.
We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.
We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ Daum´e III and Marcu (2005) employs a syntax-aware distortion model for aligning summaries to documents, but condition upon the roots of the constituents that are jumped over during a transition, instead of those that are visited during a walk through the tree.

 $$$$$ Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences.
 $$$$$ Och and Ney (2003) gives a detailed exposition of the technique.
 $$$$$ We continue to PUSH until reaching a leaf.
 $$$$$ Stage 2 (MOVE(ft, d)): Again, conditioning on the type of the parent n� of the current node n, we choose a sibling n� based on the signed distance d = φf,(n) − φf,(n), where φf,(n) is the index of n in the child list of n. Zero distance moves are disallowed.

This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ The implementation that serves as our baseline uses a multinomial distribution with separate parameters for j = 1, j = J and shared parameters for all 1 < j < J. Null alignments have fixed probability at any position.
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ We present and evaluate an unsupervised word alignment model similar in character and computation to the HMM model (Ney and Vogel, 1996), but which incorporates a novel, syntax-aware distortion component which conditions on target language parse trees.
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ Our models substantially outperform GIZA++, confirming results in Liang et al. (2006).

DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ Figure 1B exposes the consequences: a wide array of desired rules are lost during extraction.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ The parameterization of this distortion model follows directly from its generative process.

This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.
This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model.

The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ Lopez and Resnik (2005) considers a simpler tree distance distortion model.
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ This evaluation computes precision, recall, and F1 of the rules extracted under a proposed alignment, relative to the rules extracted under the gold-standard sure alignments.
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ This model can be simplified by removing all conditioning on node types.
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ A novel, high performing alternative is the soft union, which we evaluate in the next section: Syntax-alignment compatibility can be further promoted with a simple posterior decoding heuristic we call competitive thresholding.

DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ Daum´e III and Marcu (2005) employs a syntax-aware distortion model for aligning summaries to documents, but condition upon the roots of the constituents that are jumped over during a transition, instead of those that are visited during a walk through the tree.
DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.
DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model.

 $$$$$ A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 and Dorr, 2006).
 $$$$$ These trees, while automatically generated and therefore imperfect, are nonetheless (1) a useful source of structural bias and (2) the same trees which constrain future stages of processing anyway.
 $$$$$ We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules.
 $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.

We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ By contrast, the tree transducer extraction method fails to extract any of these fragments: the alignment error causes all non-terminal nodes in the parse tree to be interior nodes, excluding preterminals and the root.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.

 $$$$$ Our model generates word alignments that better respect the parse trees upon which they are conditioned, without sacrificing alignment quality.
 $$$$$ Maximizing the number of frontier nodes supports this goal, while inducing many aligned interior nodes hinders it.
 $$$$$ The single alignment error prevents the extraction of all rules in (i) and many more.
 $$$$$ That is, alignments are locally monotonic more often than not.
