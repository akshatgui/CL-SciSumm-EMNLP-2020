Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ For this reason, MOVE and PUSH are actually parameterized by three values: a node type, a signed distance, and a range of options that dictates a normalization adjustment.
Approaches have been proposed recently towards getting better word alignment and thus better TTS templates, such as encoding syntactic structure information into the HMM-based word alignment model DeNero and Klein (2007), and building a syntax-based word alignment model Mayand Knight (2007) with TTS templates. $$$$$ In the case of syntactic machine translation, we want to condition on crossing constituent boundaries, even if no constituents are skipped in the process.

We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.
We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al, 2006), combined word-to-word posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-of the-art unsupervised baseline. $$$$$ Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model.

When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ In light of the need to reconcile word alignments with phrase structure trees for syntactic MT, we have proposed an HMM-like model whose distortion is sensitive to such trees.
When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ Our models substantially outperform GIZA++, confirming results in Liang et al. (2006).
When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.
When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. $$$$$ Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model.

We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ These rules are extracted from a bitext annotated with both English (target side) parses and word alignments.
We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.
We also trained an HMM aligner as described in DeNero and Klein (2007) and used the posteriors of this model as features. $$$$$ Our model substantially reduces the number of interior nodes in the aligned corpus and improves rule extraction while nearly retaining the speed and alignment accuracy of the HMM model.

 $$$$$ Given a source language sentence s, a target language parse tree t of its translation, and a word-level alignment, their algorithm identifies the constituents in t which map onto contiguous substrings of s via the alignment.
 $$$$$ Daum´e III and Marcu (2005) employs a syntax-aware distortion model for aligning summaries to documents, but condition upon the roots of the constituents that are jumped over during a transition, instead of those that are visited during a walk through the tree.
 $$$$$ Formally, the HMM model factors as: where j_ is the position of the last non-null-aligned source word before position j, pt is a lexical transfer model, and pd is a local distortion model.
 $$$$$ We also compared our French results to the public baseline GIZA++ using the script published for the NAACL 2006 Machine Translation Workshop Shared Task.5 Similarly, we compared our Chinese results to the GIZA++ results in Ayan and Dorr (2006).

This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ To understand the behavior of this model, we computed the standard alignment error rate (AER) performance metric.2 We also investigated extractionspecific metrics: the frequency of interior nodes – a measure of how often the alignments violate the constituent structure of English parses – and a variant of the CPER metric of Ayan and Dorr (2006).
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ Together, 54.
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.
This is a simplified version of and similar in spirit to the tree distance metric used in (DeNero and Klein, 2007). $$$$$ In light of the need to reconcile word alignments with phrase structure trees for syntactic MT, we have proposed an HMM-like model whose distortion is sensitive to such trees.

DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ While it remains to be seen whether these improvements impact final translation accuracy, it is reasonable to hope that, all else equal, alignments which better respect syntactic correspondences will be superior for syntactic MT.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ These rules are extracted from a bitext annotated with both English (target side) parses and word alignments.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ This evaluation computes precision, recall, and F1 of the rules extracted under a proposed alignment, relative to the rules extracted under the gold-standard sure alignments.
DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. $$$$$ For Chinese, we trained on the FBIS corpus and the LDC bilingual dictionary, then tested on 491 hand-aligned sentences from the 2002 Hansards data from the NAACL 2003 Shared Task.3 We trained on 100k sentences for each language.

This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ Figure 4 summarizes the results of the experiment for French: the Syntactic distortion with competitive thresholding reduces tree violations substantially.
This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ However, no technique has yet been shown to robustly extract smaller component rules from a large transducer rule.
This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ To investigate the differences, we measured the degree to which each set of alignments violated the supplied parse trees, by counting the frequency of interior nodes that are not null aligned.
This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). $$$$$ Both models were initialized using the same jointly trained Model 1 parameters (5 iterations), then trained independently for 5 iterations.

The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ The single alignment error prevents the extraction of all rules in (i) and many more.
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ We also compared our French results to the public baseline GIZA++ using the script published for the NAACL 2006 Machine Translation Workshop Shared Task.5 Similarly, we compared our Chinese results to the GIZA++ results in Ayan and Dorr (2006).
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ Our model generates word alignments that better respect the parse trees upon which they are conditioned, without sacrificing alignment quality.
The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and using agreement training for the HMM (Liang et al, 2006). $$$$$ Lopez and Resnik (2005) considers a simpler tree distance distortion model.

DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences.
DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ This proposed model is not the first variant of the HMM model that incorporates syntax-based distortion.
DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ These rules are extracted from a bitext annotated with both English (target side) parses and word alignments.
DeNero and Klein (2007) use a syntax based distance in an HMM word alignment model to favor syntax-friendly alignments. $$$$$ We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments.

 $$$$$ In the case of syntactic machine translation, we want to condition on crossing constituent boundaries, even if no constituents are skipped in the process.
 $$$$$ However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005).
 $$$$$ In this way, the alignment violates the constituent structure of the English parse.
 $$$$$ The most dramatic effect of competitive thresholding is to improve alignment quality for hard unions.

We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ We compared our model to the original HMM model, identical in implementation to our syntactic HMM model save the distortion component.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ Lopez and Resnik (2005) considers a simpler tree distance distortion model.
We used an out-of-the-box implementation of the Berkeley Aligner (DeNero and Klein, 2007), a competitive word alignment system, to construct an unsupervised alignment over the 75 test sentences, based on the larger training corpus. $$$$$ In this way, the alignment violates the constituent structure of the English parse.

 $$$$$ Galley et al. (2004) proposes a method for extracting tree transducer rules from a parallel corpus.
 $$$$$ Extracting tree transducer rules for syntactic MT systems can be hindered by word alignment errors that violate syntactic correspondences.
 $$$$$ Lopez and Resnik (2005) considers a simpler tree distance distortion model.
 $$$$$ With these derivation steps in place, we must address a handful of special cases to complete the generative model.
