Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ In this paper we explore the power of surface text patterns for open-domain question answering systems.
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ Pass each phrase in the suffix tree through a filter to retain only those phrases that contain both the question and the answer term.
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ Most of the recent open domain questionanswering systems use external knowledge and tools for answer pinpointing.

The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ For example, for “Where is London?” the system cannot locate the answer in “London, which has one of the most busiest airports in the world, lies on the banks of the river Thames” due to the explosive danger of unrestricted wildcard matching, as would be required in the pattern “<QUESTION>, (<any_word>)*, lies on <ANSWER>”.

Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ The abundance of data on the web makes it easier for the system to locate answers with high precision scores (the system finds many examples of correct answers among the top 20 when using the Web as the input source).
Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ Two sets of experiments were performed.
Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ For example, “What is micron?” is answered by “In Boise, Idaho, a spokesman for Micron, a maker of semiconductors, said Simms are ‘ a very high volume product for us ...’ ”.

Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. $$$$$ The apparent power of such patterns surprised many.
Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. $$$$$ The simplicity of this method makes it perfect for multilingual QA.
Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. $$$$$ (1756–1791)”, which the suffix tree would extract as one of the outputs along with the score of 3.

Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ In our example, for the pattern “<NAME> was born in <ANSWER>” we check the presence of the following strings in the answer sentence i) Mozart was born in <ANY_WORD> ii) Mozart was born in 1756 Calculate the precision of each pattern by the formula P = Ca / Co where Ca = total number of patterns with the answer term present Co = total number of patterns present with answer term replaced by any word 6.
Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ The system does not classify or make any distinction between upper and lower case letters.
Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ Algorithm 1 does not explicitly specify any particular question type.

Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ These questions were run through the testing phase of the algorithm.
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ These patterns are then applied to find answers to new questions.
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ These questions were run through the testing phase of the algorithm.
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ However, at the recent TREC-10 QA evaluation (Voorhees, 01), the winning system used just one resource: a fairly extensive list of surface patterns (Soubbotin and Soubbotin, 01).

To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers.
To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.
To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ For example, for BIRTHDATEs (with questions like “When was X born?”), typical answers are “Mozart was born in 1756.” “Gandhi (1869–1948)...” These examples suggest that phrases like “<NAME> was born in <BIRTHDATE>” “<NAME> (<BIRTHDATE>–” when formulated as regular expressions, can be used to locate the correct answer.

Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. $$$$$ We calculate the precision of each pattern, and the average precision for each question type.
Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. $$$$$ We use suffix trees for extracting substrings of optimal length.
Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. $$$$$ While easy to do for BIRTHDATE, this step can be problematic for question types such as DEFINITION, which may contain various acceptable answers.
Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. $$$$$ Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.

But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). $$$$$ The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.
But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). $$$$$ The simplicity of this method makes it perfect for multilingual QA.
But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). $$$$$ Thus “Mozart” could be written as “Wolfgang Amadeus Mozart”, “Mozart, Wolfgang Amadeus”, “Amadeus Mozart” or “Mozart”.

For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). $$$$$ “<QUESTION_TERM_1> situated in <ANSWER> <QUESTION_TERM_2>”, where <QUESTION_TERM_1> and <QUESTION_TERM_2> represent the terms “Long Beach” and “county” respectively.
For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). $$$$$ Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched.
For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). $$$$$ Most of the recent open domain questionanswering systems use external knowledge and tools for answer pinpointing.

 $$$$$ Patterns are then automatically extracted from the returned documents and standardized.
 $$$$$ It has been noted in several QA systems that certain types of answer are expressed using characteristic phrases (Lee et al., 01; Wang et al., 01).
 $$$$$ The presence of any of these names would cause it to be tagged as the original question term “Mozart”.

Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ This would work well for question types like BIRTHDATE or LOCATION but is not clear for question types like DEFINITION.
Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ This suggests that there is a need to integrate the outputs of the Web and the TREC corpus.
Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ The web results easily outperform the TREC results.

 $$$$$ These techniques are greatly aided by the fact that there is no need to hand-tag a corpus, while the abundance of data on the web makes it easier to determine reliable statistical estimates.
 $$$$$ To learn from such variations, in step 1 of Algorithm 1 we specify the various ways in which the question term could be specified in the text.
 $$$$$ 2, 1869”, “2nd October 1869”, “October 2 1869”, and so on).
 $$$$$ It fails to perform under certain conditions as exemplified by the question “When was Lyndon B. Johnson born?”.

Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ A similar result for QA was obtained by Brill et al. (2001).
Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.

Ravichandran and Hovy (2002) focus on scaling relation extraction to the Web. $$$$$ These questions were run through the testing phase of the algorithm.
Ravichandran and Hovy (2002) focus on scaling relation extraction to the Web. $$$$$ For “what is nepotism?” the pattern “<ANSWER>, <NAME>” matches “...in the form of widespread bureaucratic abuses: graft, nepotism...”; for “what is sonar?” the pattern “<NAME> and related <ANSWER>s” matches “...while its sonar and related underseas systems are built...”.
Ravichandran and Hovy (2002) focus on scaling relation extraction to the Web. $$$$$ (1756–1791)”, which the suffix tree would extract as one of the outputs along with the score of 3.

We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ Results of the experiments, measured by Mean Reciprocal Rank (MRR) score (Voorhees, 01), are:
We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ The web results easily outperform the TREC results.
We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers.

In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ Some of the patterns obtained along with their precision are as follows For each question type, we extracted the corresponding questions from the TREC-10 set.
In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ Some of the patterns obtained along with their precision are as follows For each question type, we extracted the corresponding questions from the TREC-10 set.
In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ It cannot work for types of question that require multiple words from the question to be in the answer sentence, possibly apart from each other.
In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ These questions were run through the testing phase of the algorithm.

RH02 $$$$$ Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers.
RH02 $$$$$ Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.
RH02 $$$$$ The pattern table for each of these question types was constructed using Algorithm 1.
RH02 $$$$$ The precision of the patterns obtained from one QA-pair example in algorithm 1 is calculated from the documents obtained in algorithm 2 for other examples of the same question type.

These patterns could be manually generated, such as the ones described here, or learned from text, as described in Ravichandran and Hovy (2002). $$$$$ Retain only the patterns matching a sufficient number of examples (we choose the number of examples > 5).

Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ The results indicate that the system performs better on the Web data than on the TREC corpus.
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ In the second case, the web was the input source and the IR was performed by the AltaVista search engine.
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers.
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ For example, the question: “Where are the Rocky Mountains located?” is answered by “Denver’s new airport, topped with white fiberglass cones in imitation of the Rocky Mountains in the background, continues to lie empty”, because the system picked the answer “the background” using the pattern “the <NAME> in <ANSWER>,”.
