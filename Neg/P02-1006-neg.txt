Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ Our method uses the machine learning technique of bootstrapping to build a large tagged corpus starting with only a few examples of QA pairs.
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ Canonicalization of words is also an issue.
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.
Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. $$$$$ We calculate the precision of each pattern, and the average precision for each question type.

The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ The simplicity of this method makes it perfect for multilingual QA.
The benefit of utilizing template-based inference rules between predicates was demonstrated in NLP tasks such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.

Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ In general the input example terms have to be carefully selected so that the questions they represent do not have a long list of possible answers, as this would affect the confidence of the precision scores for each pattern.
Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ It cannot work for types of question that require multiple words from the question to be in the answer sentence, possibly apart from each other.
Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ Algorithm 1 does not explicitly specify any particular question type.
Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). $$$$$ The answer returned by the system would have been perfect if the word “micron” had been capitalized in the question.

Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. $$$$$ It cannot work for types of question that require multiple words from the question to be in the answer sentence, possibly apart from each other.
Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. $$$$$ Many tools required by sophisticated QA systems (named entity taggers, parsers, ontologies, etc.) are language specific and require significant effort to adapt to a new language.
Ravichandran and Hovy (2002) presents a method that learns patterns from online data using some seed questions and answer anchors. $$$$$ The simplicity of this method makes it perfect for multilingual QA.

Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ The precision of the patterns obtained from one QA-pair example in algorithm 1 is calculated from the documents obtained in algorithm 2 for other examples of the same question type.
Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.
Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ It cannot work for types of question that require multiple words from the question to be in the answer sentence, possibly apart from each other.
Indeed, many researchers have recently tapped the Web as a data-source for improving performance on NLP tasks (e.g., Resnik (1999), Ravichandran and Hovy (2002), Keller and Lapata (2003)). $$$$$ Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched.

Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ The pattern table for each of these question types was constructed using Algorithm 1.
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ For example, the question: “Where are the Rocky Mountains located?” is answered by “Denver’s new airport, topped with white fiberglass cones in imitation of the Rocky Mountains in the background, continues to lie empty”, because the system picked the answer “the background” using the pattern “the <NAME> in <ANSWER>,”.
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ We calculate the precision of each pattern, and the average precision for each question type.
Other work such as Ravichandran and Hovy (2002) and Pantel and Pennacchiotti (2006) use the same formalism of learning regular expressions over words and part-of-speech tags to discover patterns indicating a variety of relations. $$$$$ These patterns are then applied to find answers to new questions.

To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ Some of the patterns obtained along with their precision are as follows For each question type, we extracted the corresponding questions from the TREC-10 set.
To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ The WHY-FAMOUS question type is an exception and may be due to the fact that the system was tested on a small number of questions.
To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ From our Webclopedia QA Typology (Hovy et al., 2002a) we selected 6 different question types: BIRTHDATE, LOCATION, INVENTOR, DISCOVERER, DEFINITION, WHY-FAMOUS.
To do so, Espresso uses a slight modification of the state of the art algorithm described in (Ravichandran and Hovy, 2002). $$$$$ The performance of the system depends significantly on there being only one anchor word, which allows a single word match between the question and the candidate answer sentence.

Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. $$$$$ We describe the pattern-learning algorithm with an example.
Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. $$$$$ Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched.

But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). $$$$$ These are some of the most common substrings of the extracted sentences that contain both <NAME> and <ANSWER>.
But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). $$$$$ We frequently observe the need for matching part of speech and/or semantic types, however.

For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). $$$$$ The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.
For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). $$$$$ Frequently the system’s patterns match a term that is too general, though correct technically.
For the special case of Rote extractors, a more attractive alternative has been proposed by Brin (1998), Agichtein and Gravano (2000), and Ravichandran and Hovy (2002). $$$$$ The abundance of data on the web makes it easier for the system to locate answers with high precision scores (the system finds many examples of correct answers among the top 20 when using the Web as the input source).

 $$$$$ Since the output from the Web contains many correct answers among the top ones, a simple word count could help in eliminating many unlikely answers.
 $$$$$ A similar result for QA was obtained by Brill et al. (2001).
 $$$$$ In this paper we present an approach for automatically learning such regular expressions (along with determining their precision) from the web, for given types of questions.

Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ These questions were run through the testing phase of the algorithm.
Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ These patterns are then applied to find answers to new questions.
Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ The results indicate that the system performs better on the Web data than on the TREC corpus.
Most approaches to automatic pattern generation have focused on precision, e.g., Ravichandran and Hovy (2002) report results in the Text Retrieval Conference (TREC) Question Answering track, where extracting one text of a relation instance can be sufficient, rather than detecting all texts. $$$$$ While giving examples in the bootstrapping procedure, say, for BIRTHDATE questions, the answer term could be written in many ways (for example, Gandhi’s birth date can be written as “1869”, “Oct.

 $$$$$ The WHY-FAMOUS question type is an exception and may be due to the fact that the system was tested on a small number of questions.

Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ In this paper we explore the power of surface text patterns for open-domain question answering systems.
Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ These questions were run through the testing phase of the algorithm.
Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006). $$$$$ The web results easily outperform the TREC results.

Ravichandran and Hovy (2002) focus on scaling relation extraction to the Web. $$$$$ Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.

We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically.
We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ A similar result for QA was obtained by Brill et al. (2001).
We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ Since the answer patterns used in this method are learned using only a small number of manual training terms, one can rapidly learn patterns for new languages, assuming the web search engine is appropriately switched.
We chose the state of the art algorithm described in (Ravichandran and Hovy 2002) with the following slight modification. $$$$$ The performance of the system depends significantly on there being only one anchor word, which allows a single word match between the question and the candidate answer sentence.

In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ It has been noted in several QA systems that certain types of answer are expressed using characteristic phrases (Lee et al., 01; Wang et al., 01).
In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ The pattern table for each of these question types was constructed using Algorithm 1.
In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ The TREC corpus does not have enough candidate answers with high precision score and has to settle for answers extracted from sentences matched by low precision patterns.
In (Ravichandran and Hovy 2002), a frequency threshold on the patterns in P is set to select the final patterns. $$$$$ From our Webclopedia QA Typology (Hovy et al., 2002a) we selected 6 different question types: BIRTHDATE, LOCATION, INVENTOR, DISCOVERER, DEFINITION, WHY-FAMOUS.

RH02: The algorithm by Ravichandran and Hovy (2002) described in Section 2. $$$$$ The system does not classify or make any distinction between upper and lower case letters.
RH02: The algorithm by Ravichandran and Hovy (2002) described in Section 2. $$$$$ This suggests that there is a need to integrate the outputs of the Web and the TREC corpus.
RH02: The algorithm by Ravichandran and Hovy (2002) described in Section 2. $$$$$ Patterns are then automatically extracted from the returned documents and standardized.


Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista.
Ravichandran and Hovy (2002) used seed instances of a relation to automatically obtain surface patterns by querying the web. $$$$$ 8.
