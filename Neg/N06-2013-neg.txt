Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ Arabic has a set of attachable clitics to be distinguished from inflectional features such as gender, number, person and voice.
Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ This scheme is intended to minimize differences between Arabic and English.

This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ We plan to study additional variants that these results suggest may be helpful.
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ We found that the effect of the choice of the preprocessing technique+scheme was amplified.

Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ We plan to study additional variants that these results suggest may be helpful.
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.

Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ We define the word No.

Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ Since BAMA produces multiple analyses, we always select one in a consistent arbitrary manner (first in a sorted list of analyses).
Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ Splitting off the pronominal clitic +hm without normalizing the t top in the nominal reading leads to the coexistence of two forms of the noun: ktbp and ktbt.
Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.

 $$$$$ In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
 $$$$$ We define the word No.
 $$$$$ Section 5 presents the tools and data sets used, along with the results of our experiments.
 $$$$$ They all studied the effects of various kinds of tokenization, lemmatization and POS tagging and show a positive effect on SMT quality.

Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ The first is very light and cheap.
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ MADA, The Morphological Analysis and Disambiguation for Arabic tool, is an off-the-shelf resource for Arabic disambiguation (Habash and Rambow, 2005).
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ MR: Morphemes.

The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ D1 splits off the class of conjunction clitics (w+ and f+).
The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ Section 3 presents some relevant background on Arabic linguistics to motivate the schemes discussed in Section 4.
The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ Her results show that morphological preprocessing helps, but only for the smaller corpora.
The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ All of the training data we use is available from the Linguistic Data Consortium (LDC).

We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ For example, a 19% improvement in BLEU score (for MT04 under MADA with 100% training) (from 37.1 in D2 to 44.3) was found from an oracle combination created by selecting for each input sentence the output with the highest sentence-level BLEU score.
We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ For example, a 19% improvement in BLEU score (for MT04 under MADA with 100% training) (from 37.1 in D2 to 44.3) was found from an oracle combination created by selecting for each input sentence the output with the highest sentence-level BLEU score.
We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ To determine whether a clitic or feature should be split off or abstracted off requires that we determine that said feature is indeed present in the word we are considering in context – not just that it is possible given an analyzer or, worse, because of regular expression matching.
We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ We assume all of the text we are using is undiacritized.

More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ For example, a 19% improvement in BLEU score (for MT04 under MADA with 100% training) (from 37.1 in D2 to 44.3) was found from an oracle combination created by selecting for each input sentence the output with the highest sentence-level BLEU score.
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ We plan to study additional variants that these results suggest may be helpful.
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ This scheme requires no disambiguation.

Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ They all studied the effects of various kinds of tokenization, lemmatization and POS tagging and show a positive effect on SMT quality.
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ Section 2 presents previous relevant research.
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ Another example is the optionality of diacritics in Arabic script.
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ Next comes the class of particle proclitics (PART+): l+ to/for, b+ by/with, k+ as/such and s+ will/future.

Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ We use the term “preprocessing” to describe various input modifications that can be applied to raw training and evaluation texts for SMT to make them suitable for model training and decoding, including different kinds of tokenization, stemming, part-of-speech (POS) tagging and lemmatization.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ Our results show that given large amounts of training data, splitting off only proclitics performs best.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ We plan to study additional variants that these results suggest may be helpful.

 $$$$$ MADA selects among BAMA analyses using a combination of classifiers for 10 orthogonal dimensions, including POS, number, gender, and pronominal clitics.
 $$$$$ Across different schemes, EN performs the best under scarce-resource condition; and D2 performs best under large-resource condition.
 $$$$$ For example, a 19% improvement in BLEU score (for MT04 under MADA with 100% training) (from 37.1 in D2 to 44.3) was found from an oracle combination created by selecting for each input sentence the output with the highest sentence-level BLEU score.
 $$$$$ D2 splits off the class of particles (l+, k+, b+ and s+) beyond D1.

For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ Splitting off the pronominal clitic +hm without normalizing the t top in the nominal reading leads to the coexistence of two forms of the noun: ktbp and ktbt.
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ We plan to study additional variants that these results suggest may be helpful.
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ For example, MADA+D2 (with 100% training) on non-news improved the system score 12% over the baseline ST (statistically significant) as compared to 2.4% for news only.
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ In this paper, we report on an extensive study of the effect on SMT quality of six preprocessing schemes2, applied to text disambiguated in three different techniques and across a learning curve.

Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ Specifically considering Arabic, Lee (2004) investigated the use of automatic alignment of POS tagged English and affix-stem segmented Arabic to determine appropriate tokenizations.
Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ This reduction can be achieved by increasing training data or via morphologically driven preprocessing (Goldwater and McClosky, 2005).
Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ Our results show that given large amounts of training data, splitting off only proclitics performs best.
Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ We examine six different schemes and three techniques.

A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ This reduction can be achieved by increasing training data or via morphologically driven preprocessing (Goldwater and McClosky, 2005).
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ Further analysis shows that combination of output from all six schemes has a large potential improvement over all of the different systems, suggesting a high degree of complementarity.
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA.

On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ Our results are as follows: (a) for large amounts of training data, splitting off only proclitics performs best; (b) for small amount of training data, following an English-like tokenization and using part-of-speech tags performs best; (c) suitable choice of preprocessing yields a significant increase in BLEU score if there is little training data and/or there is a change in genre between training and test data; (d) sophisticated morphological analysis and disambiguation help significantly in the absence of large amounts of data.
On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ D1 splits off the class of conjunction clitics (w+ and f+).

Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ Recent publications on the effect of morphology on SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005).
Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.
Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ Certain letters in Arabic script are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corresponding to multiple words).

The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ This reduction can be achieved by increasing training data or via morphologically driven preprocessing (Goldwater and McClosky, 2005).
The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ Further analysis shows that combination of output from all six schemes has a large potential improvement over all of the different systems, suggesting a high degree of complementarity.
The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ Our results are comparable to hers in terms of BLEU score and consistent in terms of conclusions.
The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.

The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ Since BAMA produces multiple analyses, we always select one in a consistent arbitrary manner (first in a sorted list of analyses).
The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ HR0011-06-C-0023.
