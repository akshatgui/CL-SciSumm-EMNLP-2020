Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ Further analysis shows that combination of output from all six schemes has a large potential improvement over all of the different systems, suggesting a high degree of complementarity.
Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ We plan to study additional variants that these results suggest may be helpful.
Similarly to Habash and Sadat (2006), the set of schemes we explore are all word-level. $$$$$ They all studied the effects of various kinds of tokenization, lemmatization and POS tagging and show a positive effect on SMT quality.

This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA.
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ Her results show that morphological preprocessing helps, but only for the smaller corpora.
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ Using BAMA prevents incorrect greedy REGEX matches.
This preprocessing technique we use here is the best performer amongst other explored techniques presented in Habash and Sadat (2006). $$$$$ However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.

Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ We refer to a specific kind of preprocessing as a “scheme” and differentiate it from the “technique” used to obtain it.
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002).
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ Our results show that given large amounts of training data, splitting off only proclitics performs best.
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al, 2008) translation, although the gains tend to decrease with increasing training data size. $$$$$ The baseline for MT05, which is fully in news genre like training data, is considerably higher than MT04 (mix of genres).

Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ The different techniques chosen illustrate three degrees of linguistic knowledge dependence.
Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ We use an Arabic-English parallel corpus of about 5 million words for translation model training data.4 We created the English language model from the English side of the parallel corpus together with 116 million words from the English Gigaword Corpus (LDC2005T12) and 128 million words from the English side of the UN Parallel corpus (LDC2004E13).
Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ It decliticizes similarly to D3; however, it uses lexeme and English-like POS tags instead of the regenerated word and it indicates the pro-dropped verb subject explicitly as a separate token.
Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size. $$$$$ Our results show that given large amounts of training data, splitting off only proclitics performs best.

Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ Next comes the class of particle proclitics (PART+): l+ to/for, b+ by/with, k+ as/such and s+ will/future.
Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ The different techniques chosen illustrate three degrees of linguistic knowledge dependence.
Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ This reduction can be achieved by increasing training data or via morphologically driven preprocessing (Goldwater and McClosky, 2005).
Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e.g., (Habash and Sadat, 2006), and (ii) it may result in even worse performance than a word-based system, e.g., (Durgar El-Kahlout and Oflazer, 2006). $$$$$ We plan to study additional variants that these results suggest may be helpful.

 $$$$$ Section 6 contains a discussion of the results.
 $$$$$ Decoding weights are optimized using Och’s algorithm (Och, 2003) to set weights for the four components of the log-linear model: language model, phrase translation model, distortion model, and word-length feature.
 $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.
 $$$$$ Under large-resource conditions, this difference between techniques is statistically insignificant, though it’s generally sustained across schemes.

Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ Further analysis shows that combination of output from all six schemes has a large potential improvement over all of the different systems, suggesting a high degree of complementarity.
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ Certain letters in Arabic script are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corresponding to multiple words).
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ In this paper, we report on an extensive study of the effect on SMT quality of six preprocessing schemes2, applied to text disambiguated in three different techniques and across a learning curve.
Habash and Sadat (2006) perform morphological decomposition of Arabic words, such as splitting of prefixes and suffixes. $$$$$ The regeneration guarantees the normalization of the word form.

The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ We performed similar experiments as reported above on this subset of MT04.
The MADA toolkit (Habash and Sadat, 2006) was used to perform Arabic morphological word decomposition and part-of-speech tagging. $$$$$ We use the term “preprocessing” to describe various input modifications that can be applied to raw training and evaluation texts for SMT to make them suitable for model training and decoding, including different kinds of tokenization, stemming, part-of-speech (POS) tagging and lemmatization.

We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ Section 3 presents some relevant background on Arabic linguistics to motivate the schemes discussed in Section 4.
We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ ST: Simple Tokenization is the baseline preprocessing scheme.
We use tokenisation scheme, which splits certain prefixes and has been reported to improve machine translation performance (Habash and Sadat, 2006). $$$$$ The baseline for MT05, which is fully in news genre like training data, is considerably higher than MT04 (mix of genres).

More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ Moreover, choosing the appropriate preprocessing produces a significant increase in BLEU score if there is a change in genre between training and test data.
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ Our results are comparable to hers in terms of BLEU score and consistent in terms of conclusions.
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ We found that the effect of the choice of the preprocessing technique+scheme was amplified.
More aggressive segmentation results in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). $$$$$ This reduction can be achieved by increasing training data or via morphologically driven preprocessing (Goldwater and McClosky, 2005).

Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ We use the evaluation metric BLEU-4 (Papineni et al., 2001).
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ D2 splits off the class of particles (l+, k+, b+ and s+) beyond D1.
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ Table 1 exemplifies the effect of the different schemes on the same sentence.
Other orthographic normalization schemes have been suggested for Arabic (Habash and Sadat, 2006), but we observe negligible parsing performance differences between these and the simple scheme used in this evaluation. $$$$$ We use the evaluation metric BLEU-4 (Papineni et al., 2001).

Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ In this paper, we study the effect of different word-level preprocessing decisions for Arabic on SMT quality.
Habash and Sadat (2006) use the Arabic morphological analyzer MADA (Habash and Rambow, 2005) to segment the Arabic source; they propose various segmentation schemes. $$$$$ 3Arabic transliterations are provided in the Buckwalter transliteration scheme (Buckwalter, 2002).

 $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.
 $$$$$ The first is very light and cheap.

For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ The anecdotal intuition in the field is that reduction of word sparsity often improves translation quality.
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ Recent publications on the effect of morphology on SMT quality focused on morphologically rich languages such as German (Nießen and Ney, 2004); Spanish, Catalan, and Serbian (Popovi´c and Ney, 2004); and Czech (Goldwater and McClosky, 2005).
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used. $$$$$ Finally D3 splits off what D2 does in addition to the definite article (Al+) and all pronominal clitics.

Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ We refer to a specific kind of preprocessing as a “scheme” and differentiate it from the “technique” used to obtain it.
Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ (and by extension its morphology) to be limited to written Modern Standard Arabic (MSA) strings separated by white space, punctuation and numbers.
Recent research on handling rich morphology has largely focused on translating from rich morphology languages, such as Arabic, into English (Habash and Sadat, 2006). $$$$$ These clitics are written attached to the word and thus increase its ambiguity.

A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ Next comes the class of particle proclitics (PART+): l+ to/for, b+ by/with, k+ as/such and s+ will/future.
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ Section 3 presents some relevant background on Arabic linguistics to motivate the schemes discussed in Section 4.
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ Section 3 presents some relevant background on Arabic linguistics to motivate the schemes discussed in Section 4.
A morphological analysis of the Arabic text is then done using the Arabic morphological analyzer and disambiguation tool MADA (Nizar Habash and Roth, 2009), with the MADA-D2 since it seems to be the most efficient scheme for large data (Habash and Sadat, 2006). $$$$$ In this paper, we report on an extensive study of the effect on SMT quality of six preprocessing schemes2, applied to text disambiguated in three different techniques and across a learning curve.

On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ We use an Arabic-English parallel corpus of about 5 million words for translation model training data.4 We created the English language model from the English side of the parallel corpus together with 116 million words from the English Gigaword Corpus (LDC2005T12) and 128 million words from the English side of the UN Parallel corpus (LDC2004E13).
On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ All of the training data we use is available from the Linguistic Data Consortium (LDC).
On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ Arabic has a set of attachable clitics to be distinguished from inflectional features such as gender, number, person and voice.
On different language pairs, (Koehn and Knight, 2003) and (Habash and Sadat, 2006) showed that data-driven methods for splitting and preprocessing can improve Arabic-English and German-English MT. $$$$$ However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.

Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ The anecdotal intuition in the field is that reduction of word sparsity often improves translation quality.
Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ For example, the word ktbthm has two possible readings (among others) as their writers or I wrote them.
Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.
Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. $$$$$ Arabic preprocessing was varied using the proposed schemes and techniques.

The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ In particular, we plan to include more syntactic knowledge and investigate combination techniques at the sentence and subsentence levels.
The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. $$$$$ We performed similar experiments as reported above on this subset of MT04.

The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ The weights are optimized over the BLEU metric (Papineni et al., 2001).
The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ Across techniques and under scarce-resource conditions, MADA is better than BAMA which is better than REGEX.
The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ To investigate the effect of different schemes and techniques on different genres, we isolated in MT04 those sentences that come from the editorial and speech genres.
The BLEU score improves uniformly, although the improvements are most dramatic for smaller datasets, which is consistent with previous work (Habash and Sadat, 2006). $$$$$ However, for small amounts of training data, it is best to apply English-like tokenization using part-of-speech tags, and sophisticated morphological analysis and disambiguation.
