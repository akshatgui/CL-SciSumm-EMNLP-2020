Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. $$$$$ The metaphor is as follows.
Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. $$$$$ The hierarchical Pitman-Yor language model produces discounts that grow gradually as a function of n-gram counts.
Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. $$$$$ We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothmethods for language models.
Smoothing techniques, such as Kneser-Ney and Witten-Bell back off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off to lower order distributions, thus providing an estimate for the probability of these unseen events. $$$$$ Given a context u, let Gu(w) be the probability of the current word taking on value w. We use a Pitman-Yor process as the prior for Gu[Gu(w)]wEW, in particular, where π(u) is the suffix of u consisting of all but the earliest word.

Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form. $$$$$ Plotted is the sum over test words which occurred c times of cross-entropies of IKN, MKN, HPYLM and HPYCV, where c is as given on the x-axis and MKN is used as a baseline.
Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form. $$$$$ Both the hierarchical Dirichlet process and the hierarchical Pitman-Yor process are examples of Bayesian nonparametric processes.
Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form. $$$$$ We approximate the integral with samples {S(i), Θ(i)}Ii=1 drawn from p(S, Θ|D): where the counts are obtained from the seating arrangement Su in the Chinese restaurant process corresponding to Gu.
Similar considerations apply to other sophisticated language modeling techniques like Pitman-Yor processes (Teh, 2006), recurrent neural networks (Mikolov et al, 2010) and FLMs in their general, more powerful form. $$$$$ Right: Break down of cross-entropy on test set as a function of the number of occurrences of test words.

 $$$$$ Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
 $$$$$ Notice that we have the following relationships among the cuw·’s and tuw·: Pseudo-code for drawing words using the hierarchical Chinese restaurant process is given as a recursive function DrawWord(u), while pseudocode for computing the probability that the next word drawn from Gu will be w is given in WordProb(u,w).
 $$$$$ G0(w) is the a priori probability of word w: before observing any data, we believe word w should occur with probability G0(w).
 $$$$$ This is repeated until we get to Go, the vector of probabilities over the current word given the empty context 0.

The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a). $$$$$ These have recently received much attention in the statistics and machine learning communities because they can relax previously strong assumptions on the parametric forms of Bayesian models yet retain computational efficiency, and because of the elegant way in which they handle the issues of model selection and structure learning in graphical models.
The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a). $$$$$ The hierarchical Chinese restaurant process is equivalent to the hierarchical Pitman-Yor language model insofar as the distribution induced on words drawn from them are exactly equal.
The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a). $$$$$ This is repeated until we get to Go, the vector of probabilities over the current word given the empty context 0.
The hyper parameters of our model are updated with the auxiliary variable technique (Teh, 2006a). $$$$$ I wish to thank the Lee Kuan Yew Endowment Fund for funding, Joshua Goodman for answering many questions regarding interpolated KneserNey and smoothing techniques, John Blitzer and Yoshua Bengio for help with datasets, Anoop Sarkar for interesting discussion, and Hal Daume III, Min Yen Kan and the anonymous reviewers for step function), and HPYLM (top curve).

Growing discounts of this sort were previously suggested by the model of Teh (2006). $$$$$ Returns a new word drawn from Gu.
Growing discounts of this sort were previously suggested by the model of Teh (2006). $$$$$ We propose a new hierarchical Bayesian model of natural languages.
Growing discounts of this sort were previously suggested by the model of Teh (2006). $$$$$ Though (MacKay and Peto, 1994) had the right intuition to look at smoothing techniques as the outcome of hierarchical Bayesian models, the use of the Dirichlet distribution as a prior was shown to lead to non-competitive cross-entropy results.

Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006). $$$$$ We see that the asymptotic behaviour depends on d but not on 0, with larger d’s producing more rare words.
Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006). $$$$$ In the next section we derive tractable inference schemes for the hierarchical Pitman-Yor language model based on these seating arrangements.
Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006). $$$$$ We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothmethods for language models.
Although the PYP has no known analytical form, we can marginalise out the GX's and reason about individual rules directly using the process described by Teh (2006). $$$$$ Second panel: same, with 0 = 10 and d = 0 (bottom), .5 (middle) and .9 (top).

 $$$$$ Since the discounts in a hierarchical Pitman-Yor language model are limited to between 0 and 1, we see that modified Kneser-Ney is not an approximation of the hierarchical PitmanYor language model.
 $$$$$ We propose a new hierarchical Bayesian model of natural languages.
 $$$$$ If we restrict tuw· to be at most 1, that is, we will get the same discount value so long as cuw· > 0, i.e. absolute discounting.
 $$$$$ For trigrams with n = 3, we varied the training set size between approximately 2 million and 14 million words by six equal increments, while we also experimented with n = 2 and 4 on the full 14 million word training set.

 $$$$$ Returns a new word drawn from Gu.
 $$$$$ We show experimental comparisons to interpolated and modified Kneser-Ney, and the hierarchical Dirichlet language model in Section 6 and conclude in Section 7.
 $$$$$ We believe this is because HPYLM is not a perfect model for languages and as a result posterior estimates of the parameters are not optimized for predictive performance.

 $$$$$ Let us introduce some notations.
 $$$$$ Further details can be obtained at (Teh, 2006).
 $$$$$ We use Gibbs sampling to obtain the posterior samples {S, Θ} (Neal, 1993).

Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. $$$$$ As expected, HDLM gives the worst performance, while HPYLM performs better than IKN.
Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. $$$$$ The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).
Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. $$$$$ This is given by where the first probability on the right is the predictive probability under a particular setting of seating arrangements S and parameters Θ, and the overall predictive probability is obtained by averaging this with respect to the posterior over S and Θ (second probability on right).
Conventional smoothing techniques, such as Kneser Ney and Witten-Bell back-off schemes (see (Chen and Goodman, 1996) for an empirical overview, and (Teh, 2006) for a Bayesian interpretation), perform back-off on lower order distributions to provide an estimate for the probability of these unseen events. $$$$$ We propose a new hierarchical Bayesian model of natural languages.

We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). $$$$$ Here we give a quick description of the Pitman-Yor process in the context of a unigram language model; good tutorials on such models are provided in (Ghahramani, 2005; Jordan, 2005).
We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). $$$$$ As a result direct maximum-likelihood parameter fitting severely overfits to the training data, and smoothing methods are indispensible for proper training of n-gram models.
We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). $$$$$ Both panels are for the full training set and n = 3. helpful comments.
We develop a Bayesian approach using a Pitman Yor process prior, which is capable of modellinga diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). $$$$$ Unfortunately cross-validation using a hierarchical Pitman-Yor language model inferred using Gibbs sampling is currently too costly to be practical.

Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. $$$$$ Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. $$$$$ We also show how interpolated Kneser-Ney can be interpreted as approximate inference in the model.
Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. $$$$$ Though some of these methods are intuitively appealing, the main justification has always been empirical—better perplexities or error rates on test data.
Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. $$$$$ We also show how interpolated Kneser-Ney can be interpreted as approximate inference in the model.

Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). $$$$$ In the future we plan to study in more detail the differences between our model and the variants of Kneser-Ney, to consider other approximate inference schemes, and to test the model on larger data sets and on speech recognition.
Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). $$$$$ The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).
Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). $$$$$ Gibbs sampling keeps track of the current state of each variable of interest in the model, and iteratively resamples the state of each variable given the current states of all other variables.
Nonparametric Bayesian modeling has recently become very popular in natural language processing (NLP), mostly because of its ability to provide priors that are especially suitable for tasks in NLP (Teh, 2006). $$$$$ Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.

While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006). $$$$$ However, the probability vectors Gu’s have been marginalized out in the procedure, replaced instead by the assignments of words xul to draws yuk from the parent distribution, i.e. the seating arrangement of customers around tables in the Chinese restaurant process corresponding to Gu.
While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006). $$$$$ Further supposing that the strength parameters are all θ|u |= 0, the predictive probabilities (12) now directly reduces to the predictive probabilities given by interpolated Kneser-Ney.
While the Dirichlet process is simply the Pitman Yor process with d= 0, it has been shown that the discount parameter allows for more effective modeling of the long-tailed distributions that are often found in natural language (Teh, 2006). $$$$$ This is given by where the first probability on the right is the predictive probability under a particular setting of seating arrangements S and parameters Θ, and the overall predictive probability is obtained by averaging this with respect to the posterior over S and Θ (second probability on right).

The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem. $$$$$ Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.
The Chinese Restaurant Process representation of Pt (Teh, 2006) lends itself to a natural and easily implementable solution to this problem. $$$$$ (Goldwater et al., 2006) has independently noted the correspondence between the hierarchical Pitman-Yor language model and interpolated Kneser-Ney, and conjectured improved performance in the hierarchical Pitman-Yor language model, which we verify here.

Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). $$$$$ This gives us an alternative representation of the hierarchical Pitman-Yor language model that is amenable to efficient inference using Markov chain Monte Carlo sampling and easy computation of the predictive probabilities for test words.
Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). $$$$$ Let S = {Sv : all contexts v}.
Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). $$$$$ Gibbs sampling keeps track of the current state of each variable of interest in the model, and iteratively resamples the state of each variable given the current states of all other variables.
Similarly, if the seating dynamics are constrained such that each dish is only served once (tw= 1 for any w), a single discount level is affected, establishing direct correspondence to original interpolated Kneser-Ney smoothing (Teh, 2006). $$$$$ The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).

The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. $$$$$ For each word w E W let G(w) be the (to be estimated) probability of w, and let where the three parameters are: a discount parameter 0 < d < 1, a strength parameter θ > —d and a mean vector G0 = [G0(w)]wEW.
The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. $$$$$ This choice of the prior structure expresses our belief that words appearing earlier in a context have (a priori) the least importance in modelling the probability of the current word, which is why they are dropped first at successively higher levels of the model.
The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. $$$$$ Notice that we have the following relationships among the cuw·’s and tuw·: Pseudo-code for drawing words using the hierarchical Chinese restaurant process is given as a recursive function DrawWord(u), while pseudocode for computing the probability that the next word drawn from Gu will be w is given in WordProb(u,w).
The hierarchical PYP (hPYP; Teh (2006)) is an extension of the PYP in which the base distribution G0 is itself a PYP distribution. $$$$$ We propose a new hierarchical Bayesian model of natural languages.

There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006). $$$$$ We find that HPYCV performs better than MKN (except marginally worse on small problems), and has best performance overall.
There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006). $$$$$ In fact we show a stronger result—that interpolated Kneser-Ney can be interpreted as a particular approximate inference scheme in the hierarchical Pitman-Yor language model.
There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006). $$$$$ We performed experiments on the hierarchical Pitman-Yor language model on a 16 million word corpus derived from APNews.
There is not space for a complete treatment of the hPYP and the particulars of inference; we refer the interested reader to Teh (2006). $$$$$ We propose a new hierarchical Bayesian model of natural languages.

 $$$$$ Modified Kneser-Ney uses the same values for the counts as in (15,16), but uses a different valued discount for each value of cuw· up to a maximum of c(max).
 $$$$$ Though (MacKay and Peto, 1994) had the right intuition to look at smoothing techniques as the outcome of hierarchical Bayesian models, the use of the Dirichlet distribution as a prior was shown to lead to non-competitive cross-entropy results.
 $$$$$ Since the posterior is well-behaved and the sampler converges quickly, we only used 125 iterations for burn-in, and 175 iterations to collect posterior samples.
 $$$$$ This is because w will be more likely under the context of the common suffix as well.

Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings. $$$$$ On the other hand parameters in the Kneser-Ney variants are optimized using cross-validation, so are given optimal values for prediction.
Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings. $$$$$ Plotted is the sum over test words which occurred c times of cross-entropies of IKN, MKN, HPYLM and HPYCV, where c is as given on the x-axis and MKN is used as a baseline.
Among these, smoothing techniques, such as Good-Turing, Witten-Bell and Kneser-Ney smoothing schemes (see (Chen and Goodman, 1996) for an empirical overview and (Teh, 2006) for a Bayesian interpretation) are used to compute estimates for the probability of unseen events, which are needed to achieve state-of-the-art performance in large-scale settings. $$$$$ We refer to this as the hierarchical Chinese restaurant process.
