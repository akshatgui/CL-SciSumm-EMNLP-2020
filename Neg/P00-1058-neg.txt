One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ Note that even in cases where the parser encounters a sentence for which the (fallible) extraction heuristics would have produced an unseen tree template, it is possible that the parser will use other trees to produce the correct bracketing.
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ For example, the attachment of an S depends on the presence or absence of the embedded subject (Collins, 1999); Treebank-style two-level NPs are mismodeled by PCFG (Collins, 1999; Johnson, 1998); the generation of a node depends on the label of its grandparent (Charniak, 2000; Johnson, 1998).
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ Such changes are not always obvious a priori and often must be devised anew for each language or each corpus.

The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ For example, one would imagine that the distribution of active verbs and their subjects would be similar to the distribution of passive verbs and their notional subjects, yet they are treated as independent in the current model.
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.

They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ Since nearly a quarter of nonempty subjects appear in such a configuration, this is not a small problem.
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ For each node T1, these rules classify exactly one child of T1 as a head and the rest as either arguments or adjuncts.

In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ They do not report parsing results, though their intention is to evaluate how the various grammars affect parsing accuracy and how k-best supertagging afffects parsing speed.
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ Sister-adjunction is not an operation found in standard definitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995).
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ (Xia, 1999) describes a grammar extraction process similar to ours, and describes some techniques for automatically filtering out invalid elementary trees.

We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ It produces, among other things, the analysis of auxiliary verbs described in the previous section.
We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ We ran the algorithm given in Section 4.1 on sections 02{21 of the Penn Treebank.
We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ Following (Collins, 1997), words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in (Ratnaparkhi, 1996).

In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ The majority of trees resulting from deficiencies in the heuristics involved complicated coordination structures, which is not surprising, since coordination has always been problematic for TAG.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ The approach of Chelba and Jelinek (1998) to language modeling is illustrative: even though the probability estimate of w appearing as the lith word can be conditioned on the entire history w1, ... , wk-1, the quantity of available training data limits the usable context to about two words but which two?

Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ Moreover, the greater flexibility of TAG suggests some potential improvements which would be cumbersome to implement using a lexicalized CFG.
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ Our variant adds another set of parameters: This is the probability of sister-adjoining y between the ith and i + 1th children of rl (as before, allowing for two imaginary children beyond the leftmost and rightmost children).
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ The majority of trees resulting from deficiencies in the heuristics involved complicated coordination structures, which is not surprising, since coordination has always been problematic for TAG.
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ Tree templates occurring only once in training were ignored entirely.

Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ The probabilities are decomposed as follows: where Ta is the tree template of a, to is the part-of-speech tag of the anchor, and wa is the anchor itself.
Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ In order to capture such dependencies in a PCFG-based model, they must be localized either by transforming the data or modifying the parser.
Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ Rather than coin a new acronym for this particular variant, we will simply refer to it as &quot;TAG&quot; and trust that no confusion will arise.
Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.

Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ The complicated restrictions on 0 are simply to ensure that a well-formed TIG derivation is produced.
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ Following (Collins, 1997), words occurring fewer than four times in training were replaced with the symbol *UNKNOWN* and tagged with the output of the part-of-speech tagger described in (Ratnaparkhi, 1996).
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ The majority of trees resulting from deficiencies in the heuristics involved complicated coordination structures, which is not surprising, since coordination has always been problematic for TAG.
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ One, also suggested by (Chen and Vijay-Shanker, 2000), is to group elementary trees into families and relate the trees of a family by transformations.

Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ For example, for the sentence &quot;John should have left,&quot; Magerman's rules make should and have the heads of their respective VPs, so that there is no dependency between left and its subject John (see Figure 2a).
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ Another possibility is the use of multiplyanchored trees.
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ Effectively, a dependency structure is made parasitic on the phrase structure so that they can be generated together by a context-free model.

While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ For each node T1, these rules classify exactly one child of T1 as a head and the rest as either arguments or adjuncts.
While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ Thus dependencies that have to be stipulated in a PCFG by tree transformations or parser modifications are captured for free in a PTAG model.
While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ The generation of the anchor has three backoff levels: the first two are as before, and the third just conditions the anchor on its POS tag.

While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ We ran the algorithm given in Section 4.1 on sections 02{21 of the Penn Treebank.
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Another approach, taken in (Magerman, 1995) and others for lexicalized PCFGs and (Neumann, 1998; Xia, 1999; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations.
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.

In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ Given that statistical natural language processing is concerned with the probable rather than the possible, it is not because TAG can describe constructions like arbitrarily large Dutch verb clusters.
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ In a lexicalized TAG, because each composition brings together two lexical items, every composition probability involves a bilexical dependency.
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (Magerman, 1995; Collins, 1997).
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ Our parser scored 84.4% compared with 82.4% for (Hwa, 1998), an error reduction of 11%.

Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ We also extended the parser to simulate sister-adjunction as regular adjunction and compute the flag f which distinguishes the first modifier from subsequent modifiers.
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ An arc corresponding to the sister-adjunction of a tree between the ith and i + 1th children of rl (allowing for two imaginary children beyond the leftmost and rightmost children) is labeled rl; i.
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ Our work has a great deal in common with independent work by Chen and VijayShanker (2000).
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ Removing all but these trees from the grammar increased the error rate by about 5% (testing on a subset of section 00).

The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ We emphasize that TAG is attractive not because it can do things that CFG cannot, but because it does everything that CFG can, only more cleanly.
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ The generation of the tree template has two backoff levels: at the first level, the anchor of il is ignored, and at the second level, the POS tag of the anchor as well as the flag f are ignored.
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ These results place our parser roughly in the middle of the lexicalized PCFG parsers.

A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem $$$$$ The approach of Chelba and Jelinek (1998) to language modeling is illustrative: even though the probability estimate of w appearing as the lith word can be conditioned on the entire history w1, ... , wk-1, the quantity of available training data limits the usable context to about two words but which two?
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem $$$$$ Srinivas's work on supertags (B. Srinivas, 1997) also uses TAG for statistical parsing, but with a rather different strategy: tree templates are thought of as extended parts-ofspeech, and these are assigned to words based on local (e.g., n-gram) context.

Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ S. D. G.
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ We discuss a few such cases in Section 3.
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ The smoothing method described above would have to be modified to account for multiple anchors.

Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ The ability of probabilistic LTAG to model bilexical dependencies was noted early on by (Resnik, 1992).
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ To see what the impact of this failure to converge is, we ran the grammar extractor on some held-out data (section 00).
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ Three possible explanations are: In a random sample of 100 once-seen elementary tree templates, we found (by casual inspection) that 34 resulted from annotation errors, 50 from deficiencies in the heuristics, and four apparently from performance errors.

Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ Out of 45082 tree tokens, 107 tree templates, or 0.2%, had not been seen in training.
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ A few of the most frequent tree-templates are shown in Figure 3.
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.

 $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
 $$$$$ One, also suggested by (Chen and Vijay-Shanker, 2000), is to group elementary trees into families and relate the trees of a family by transformations.
 $$$$$ The complicated restrictions on 0 are simply to ensure that a well-formed TIG derivation is produced.
 $$$$$ It turns out that there are other pieces of contextual information that need to be explicitly accounted for in a CFG by grammar transformations but come for free in a TAG.
