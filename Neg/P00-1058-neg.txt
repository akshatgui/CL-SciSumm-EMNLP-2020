One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ Our variant adds another set of parameters: This is the probability of sister-adjoining y between the ith and i + 1th children of rl (as before, allowing for two imaginary children beyond the leftmost and rightmost children).
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ But none of these cases really requires special treatment in a PTAG model, because each composition probability involves not only a bilexical dependency but a &quot;biarboreal&quot; (tree-tree) dependency.
One may object that this example is somewhat far-fetched, but Chiang (2000) notes that head-lexicalized stochastic grammars fall short in encoding even simple dependency relations such as between left and John in the sentence John should have left. $$$$$ We used a CKY-style parser similar to the one described in (Schabes and Waters, 1996), with a modification to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions.

The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ We also extended the parser to simulate sister-adjunction as regular adjunction and compute the flag f which distinguishes the first modifier from subsequent modifiers.
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ Moreover, the greater flexibility of TAG suggests some potential improvements which would be cumbersome to implement using a lexicalized CFG.
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ Rather, what makes TAG useful for statistical parsing are the structural descriptions it assigns to breadand-butter sentences.
The only other model that uses frontier lexicalization and that was tested on the standard WSJ split is Chiang (2000) who extracts a stochastic tree-insertion grammar or STIG (Schabes; Waters 1996) from the WSJ, obtaining 86.6% LP and 86.9% LR for sentences 40 words. $$$$$ Our parser scored 84.4% compared with 82.4% for (Hwa, 1998), an error reduction of 11%.

They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ (We could make VP the head of VP instead, but this would generate auxiliaries independently of each other, so that, for example, P(John leave) > 0.)
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ But beginning with (Magerman, 1995) statistical parsers have used bilexical dependencies with great success.
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ The generation of the tree template has two backoff levels: at the first level, the anchor of il is ignored, and at the second level, the POS tag of the anchor as well as the flag f are ignored.
They induce a probabilistic Tree Adjoining Grammar from a training set algning frames and sentences using the grammar induction technique of (Chiang, 2000) and use a beam search that uses weighted features learned from the training data to rank alternative expansions at each step. $$$$$ We introduce this operation simply so we can derive the flat structures found in the Penn Treebank.

In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ The complicated restrictions on 0 are simply to ensure that a well-formed TIG derivation is produced.
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ Sister-adjunction is not an operation found in standard definitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995).
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ This research is supported in part by ARO grant DAAG55971-0228 and NSF grant SBR89-20230-15.
In particular, our grammars differs from the traditional probabilistic Tree Adjoining Grammar extracted as described in e.g., (Chiang, 2000) in that they encode both syntax and semantics rather than just syntax. $$$$$ It turns out that there are other pieces of contextual information that need to be explicitly accounted for in a CFG by grammar transformations but come for free in a TAG.

We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ In a lexicalized TAG, because each composition brings together two lexical items, every composition probability involves a bilexical dependency.
We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Tree bank trees (Chiang, 2000). $$$$$ A few of the most frequent tree-templates are shown in Figure 3.

In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ Three possible explanations are: In a random sample of 100 once-seen elementary tree templates, we found (by casual inspection) that 34 resulted from annotation errors, 50 from deficiencies in the heuristics, and four apparently from performance errors.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
In LTAG-based statistical parsers, high accuracy is obtained by using the Magerman Collins head-percolation rules in order to provide the etrees (Chiang, 2000). $$$$$ Thus the (virtual) grammar serves to structure the history so that the two most useful words can be chosen, even though the structure of the problem itself is entirely linear.

Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ For each node T1, these rules classify exactly one child of T1 as a head and the rest as either arguments or adjuncts.
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ (We could make VP the head of VP instead, but this would generate auxiliaries independently of each other, so that, for example, P(John leave) > 0.)
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ The generation of the anchor has three backoff levels: the first two are as before, and the third just conditions the anchor on its POS tag.
Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution grammar with sister adjunction (TSG+SA) . $$$$$ Our work has a great deal in common with independent work by Chen and VijayShanker (2000).

Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ The backed-off models are combined by linear interpolation, with the weights chosen as in (Bikel et al., 1997).
Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.
Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ We use a beam search, computing the score of an item [TI, i, j] by multiplying it by the prior probability P(TI) (Goodman, 1997); any item with score less than 10-5 times that of the best item in a cell is pruned.
Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sister adjunction (TSG+SA). $$$$$ This research is supported in part by ARO grant DAAG55971-0228 and NSF grant SBR89-20230-15.

Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ Aside from cases where context-free derivations are incapable of encoding both constituency and dependency (which are somewhat isolated and not of great interest for statistical parsing) there are common cases where percolation of single heads is not sufficient to encode dependencies correctly for example, relative clause attachment or raising/auxiliary verbs (see Section 3).
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ Why use tree-adjoining grammar for statistical parsing?
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
Another kind of grammar is a TAG automatically extracted from a treebank using the techniques of (Chen, 2001) (cf. (Chiang, 2000), (Xia, 1999)). $$$$$ The smoothing method described above would have to be modified to account for multiple anchors.

Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by offering two such possibilities.
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ The complicated restrictions on 0 are simply to ensure that a well-formed TIG derivation is produced.
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ They do not report parsing results, though their intention is to evaluate how the various grammars affect parsing accuracy and how k-best supertagging afffects parsing speed.
Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.

While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ The results are shown in Figure 6.
While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ In a lexicalized TAG, because each composition brings together two lexical items, every composition probability involves a bilexical dependency.
While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ We find that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG.
While earlier approaches such as Hwa (1998) and Chiang (2000) relied on hueristic induction methods, they were nevertheless sucessful at parsing. $$$$$ They present a more detailed discussion of various grammar extraction processes and the performance of supertagging models (B. Srinivas, 1997) based on the extracted grammars.

While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Effectively, a dependency structure is made parasitic on the phrase structure so that they can be generated together by a context-free model.
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by offering two such possibilities.
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Since these dependencies are not encoded in plain phrase-structure trees, the standard approach has been to let the lexical heads percolate up the tree, so that when one lexical head is immediately dominated by another, it is understood to be dependent on it.
While different representations make direct comparison inappropriate, the OSTAG results lie in the same range as previous work with statistical TIG on this task, such as Chiang (2000) (86.00) and Shindo et al (2011) (85.03). $$$$$ Pi(a) is the probability of beginning a derivation with a; Ps(a j TI) is the probability of substituting a at TI; Pa(,3 j TI) is the probability of adjoining ,3 at TI; finally, Pa(NONE j TI) is the probability of nothing adjoining at rl.

In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ The smoothing method described above would have to be modified to account for multiple anchors.
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ The ability of probabilistic LTAG to model bilexical dependencies was noted early on by (Resnik, 1992).
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ For example, for the sentence &quot;John should have left,&quot; Magerman's rules make should and have the heads of their respective VPs, so that there is no dependency between left and its subject John (see Figure 2a).
In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). $$$$$ So the extracted grammar is fairly compact, but how complete is it?

Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ We used a CKY-style parser similar to the one described in (Schabes and Waters, 1996), with a modification to ensure completeness (because foot nodes are treated as empty, which CKY prohibits) and another to reduce useless substitutions.
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ Of course, the price that the PTAG model pays is sparser data; the backoff model must therefore be chosen carefully.
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ The probabilities are decomposed as follows: where Ta is the tree template of a, to is the part-of-speech tag of the anchor, and wa is the anchor itself.
Improvements along this line may be attained by use of a full TAG parser, such as Chiang (2000) for example. $$$$$ Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by offering two such possibilities.

The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ (Note that as it stands sister-adjunction is completely unconstrained; it will be constrained by the probability model.)
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ We discuss the advantages of lexicalized tree-adjoining grammar as an alternative to lexicalized PCFG for statistical parsing, describing the induction of a probabilistic LTAG model from the Penn Treebank and evaluating its parsing performance.
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ These results place our parser roughly in the middle of the lexicalized PCFG parsers.
The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. $$$$$ Moreover, our extraction heuristics evidently have room to improve.

A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP's children, which is not possible with TAGs. $$$$$ Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP's children, which is not possible with TAGs. $$$$$ In order to capture such dependencies in a PCFG-based model, they must be localized either by transforming the data or modifying the parser.
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP's children, which is not possible with TAGs. $$$$$ A more complex lexicalization scheme for CFG could as well (one which kept track of two heads at a time, for example), but the TAG account is simpler and cleaner.
A striking use of sister adjunction in (Chiang, 2000) is exactly the elegant way it solves this problem: the non-argument tree can be adjoined onto a node (say, VP), positioning itself in between the VP's children, which is not possible with TAGs. $$$$$ A more complex lexicalization scheme for CFG could as well (one which kept track of two heads at a time, for example), but the TAG account is simpler and cleaner.

Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ We discuss a few such cases in Section 3.
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ In sisteradjunction the root of a modifier tree is added as a new daughter to any other node.
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ Furthermore, there are some dependency analyses encodable by TAGs that are not encodable by a simple head-percolation scheme.
Our method is similar to (Chiang, 2000), but is even simpler in ignoring the distinction between arguments and adjuncts (and thus the sister adjunction operation). $$$$$ We first compared the parser with (Hwa, 1998): we trained the model on sentences of length 40 or less in sections 02{09 of the Penn Treebank, down to parts of speech only, and then tested on sentences of length 40 or less in section 23, parsing from part-of-speech tag sequences to fully bracketed parses.

Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ If the two configurations could be related, then the sparseness of verb-argument dependencies would be reduced.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ This research is supported in part by ARO grant DAAG55971-0228 and NSF grant SBR89-20230-15.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ Thanks to Mike Collins, Aravind Joshi, and the anonymous reviewers for their valuable help.

Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ It produces, among other things, the analysis of auxiliary verbs described in the previous section.
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ It is applied in a greedy fashion, with potential rjs considered top-down and potential Bs bottomup.

 $$$$$ Lexicalized TAG is such a formalism, because it assigns to each sentence not only a parse tree, which is built out of elementary trees and is interpreted as encoding constituency, but a derivation tree, which records how the various elementary trees were combined together and is commonly intepreted as encoding dependency.
 $$$$$ S. D. G.
 $$$$$ An arc corresponding to the sister-adjunction of a tree between the ith and i + 1th children of rl (allowing for two imaginary children beyond the leftmost and rightmost children) is labeled rl; i.
