This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ Specifically, for each alignment cell (i, j) which is not a possible alignment in a*, we incur a loss of 1 when azo =6 a*zo; note that if (i, j) is a possible alignment, our loss is indifferent to its presence in the proposal alignment.
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ In addition, we also created three new block-specific features types.
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ Finally, to scale up our system, we give a combination of pruning techniques that allows us to sum ITG alignments two orders of magnitude faster than naive inside-outside parsing.
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ In this work, we investigate large-scale, discriminative ITG word alignment.

In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models.
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ On the Chinese NIST data, however, where almost no alignment is in A1-1, the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results.
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ A crucial obstacle for using the likelihood objective is that a given a* may not be in the alignment family.
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ On this data set, our block ITG models make substantial performance improvements over the HMM, and moreover these results do translate into downstream improvements in BLEU score for the Chinese-English language pair.

Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ This work presented the first large-scale application of ITG to discriminative word alignment.
Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ In the 2002 NIST Chinese-English hand-aligned data (see Section 6.2), we constructed oracle alignment potentials as follows: sij is set to +1 if (i, j) is a sure or possible alignment in the hand-aligned data, 1 otherwise.
Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ First, rather than update towards the hand-labeled alignment a*, we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a*�.

Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ The likelihood objective is given by, Optimizing this objective with gradient methods requires summing over alignments.
Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ Table 1 illustrates results for the Hansards data set.

To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008).
To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ An important alignment pattern disallowed by A1-1 is the many-to-one alignment block.
To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ (2006), we assume the score a of a potential alignment a) decomposes as where sij are word-to-word potentials and siE and sEj represent English null and foreign null potentials, respectively.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ This slows learning and prevents us from finding a useful weight vector.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ As input, we have a training set D = (x1, a∗1), ... , (x, a∗�) of hand-aligned data, where x refers to a sentence pair.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ In this section, we discuss and compare alignment families used to train our discriminative models.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Some example features of this type are, For English blocks, for example, these features capture the behavior of phrases such as in spite of or in front of that are rendered as one word in Chinese.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003).

However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ An analogous set of grammar rules exists for the inverted case (see Figure 2(d) for an illustration).
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.

MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ Initially, as in Taskar et al. (2005) and Moore et al.
MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ In all, pruning reduces MIRA iteration time from 175 to 5 minutes on the NIST ChineseEnglish dataset with negligible performance loss.
MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here.
MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ For example, the HMM aligner achieves an AER of 20.7 when using the competitive thresholding heuristic of DeNero and Klein (2007).

We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ However, unlike with general matchings, we can also efficiently compute expectations over the set of ITG derivations, enabling the training of conditional likelihood models.
We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature.

 $$$$$ The reason is that initially the score for all alignments is low, so we are biased toward only using very high loss alignments in our constraint.
 $$$$$ Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008).
 $$$$$ We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG.
 $$$$$ We note that almost all of our performance gains (relative to both the HMM and 1-1 matchings) come from BITG and block features.

This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ However, in contrast to MIRA, the likelihood objective will implicitly penalize proposed alignments which have loss equal to a*�.
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockstructured alignments (i.e. phrases), which general matchings cannot efficiently do.
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG.
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ Finally, to scale up our system, we give a combination of pruning techniques that allows us to sum ITG alignments two orders of magnitude faster than naive inside-outside parsing.

This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ Even for non-ITG sentence pairs, we show that it is possible learn ITG alignment models by simple relaxations of structured discriminative learning objectives.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ Wu (1997)’s inversion transduction grammar (ITG) is a synchronous grammar formalism in which derivations of sentence pairs correspond to alignments.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ MIRA is an online procedure, where at each time step s.t. w · O(x, a*) ≥ w · O(x, a) + L(a, a*) where a� = arg max aEA In our data sets, many a* are not in A1-1 (and thus not in AITG), implying the minimum infamily loss must exceed 0.

The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008).
The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ We will use BITG to refer to this block ITG variant and ABITG to refer to the alignment family, which is neither contained in nor contains A1-1.
The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ In order to ensure unique derivations, we stipulate that a N cell can be constructed only from a sequence of smaller inverted cells I. Binarizing the rule N → I2+ introduces the intermediary symbol N (see Figure 2(a)).

When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines: the monolingual parser of Petrov and Klein (2007) and the discriminative word aligner of Haghighi et al (2009). $$$$$ These include features such as the number of occurrences of the phrasal (multi-word) side of a many-to-one block, as well as pointwise mutual information statistics for the multi-word parts of many-to-one blocks.

Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ For this alignment family, we expand the alignment potential decomposition in Equation (1) to incorporate block potentials sef and sef which represent English and foreign many-to-one alignment blocks, respectively.
Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ The maximum scoring alignment from AITG can be found in O(n6) time with synchronous CFG parsing; in practice, we can make ITG parsing efficient using a variety of pruning techniques.
Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ With ITG, it is relatively easy to allow contiguous many-to-one alignment blocks without added complexity.3 This is accomplished by adding additional unary terminal productions aligning a foreign phrase to a single English terminal or vice versa.

We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ Both discriminative methods require repeated model inference: MIRA depends upon lossaugmented Viterbi parsing, while conditional likelihood uses the inside-outside algorithm for computing cell posteriors.

Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ MIRA is an online procedure, where at each time step s.t. w · O(x, a*) ≥ w · O(x, a) + L(a, a*) where a� = arg max aEA In our data sets, many a* are not in A1-1 (and thus not in AITG), implying the minimum infamily loss must exceed 0.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ Summing and obtaining feature expectations over M(a*) can be done efficiently using a constrained variant of the inside-outside algorithm where sure alignments not present in a* are disallowed, and the number of missing sure alignments is appended to the state of the bitext cell.6 One advantage of the likelihood-based objective is that we can obtain posteriors over individual alignment cells, We obtain posterior ITG alignments by including all alignment cells (i, j) such that PIV((i, j)|x) exceeds a fixed threshold t. Posterior thresholding allows us to easily trade-off precision and recall in our alignments by raising or lowering t.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.

We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ As input, we have a training set D = (x1, a∗1), ... , (x, a∗�) of hand-aligned data, where x refers to a sentence pair.
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ Instead, we track bounds on the spans for which we have successfully built ITG cells, and we only iterate over larger spans that fall within those bounds.
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG.
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models.
