This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ The need for block alignments is especially acute in ChineseEnglish data, where oracle AERs drop from 10.2 without blocks to around 1.2 with them.
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008).
This directional model has been shown produce state-of-the art results with this setup (Haghighi et al, 2009). $$$$$ With ITG, it is relatively easy to allow contiguous many-to-one alignment blocks without added complexity.3 This is accomplished by adding additional unary terminal productions aligning a foreign phrase to a single English terminal or vice versa.

In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference (Liang et al., 2006).
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ This slows learning and prevents us from finding a useful weight vector.
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ For features on one-by-one cells, we consider Dice, the distance features from (Taskar et al., 2005), dictionary features, and features for the 50 most frequent lexical pairs.
In order to reduce spurious derivations, Wu (1997), Haghighi et al (2009), Liu et al (2010) propose different variations of the grammar. $$$$$ Likelihood training time is reduced by nearly two orders of magnitude.

Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ This yields the modified QP, where a� = arg max aEA wt · O(x, a) + AL(a, a*�) By setting A = 0, we recover the MIRA update from Equation (2).
Haghighi et al (2009) give some restrictions on null-aligned word attachment. $$$$$ Exhaustive computation of these quantities requires an O(n6) dynamic program that is prohibitively slow even on small supervised training sets.

Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003).
Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ A simple loss-augmented learning procedure is the margin infused relaxed algorithm (MIRA) (Crammer et al., 2006).
Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al, 2009), LiuG (Liu et al, 2010) and LGFN (Section 3.3). $$$$$ Our modified likelihood objective is given by, Note that this objective is no longer convex, as it involves a logarithm of a summation, however we still utilize gradient-based optimization.

To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ This model alone achieves an AER of 5.4.
To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ This work investigates supervised word alignment methods that exploit inversion transduction grammar (ITG) constraints.
To further study how spurious ambiguity affects the discriminative learning, we implemented a frame work following Haghighi et al (2009). $$$$$ Second, we perform lossaugmented inference to obtain a.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Pruning a one-by-one cell also indirectly prunes larger cells containing it.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ First, rather than update towards the hand-labeled alignment a*, we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a*�.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003).

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ The set of such ITG alignments, AITG, are a strict subset of A1-1 (Wu, 1997).
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ The maximum likelihood-trained normal form ITG model outperforms the HMM, even without including any features derived from the unlabeled data.

Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Instead, in all the experiments we report here, we begin with A = 0 and slowly increase it to A = 0.5.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model.
Haghighi et al (2009) also describe a pruning heuristic that results in average case runtime of O (n 3). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.

However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models.
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ As in Taskar et al. (2005), we utilize a loss that decomposes across alignments.
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG.
However, the space of block ITG alignments is expressive enough to include the vast majority of patterns observed in hand annotated parallel corpora (Haghighi et al, 2009). $$$$$ This slows learning and prevents us from finding a useful weight vector.

MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ As in our alteration to MIRA (Section 3), we could replace a* with a minimal loss in-class alignment a*�.
MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ Figure 1 contains an example.
MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al, 2009) and translation models (Chiang et al, 2008). $$$$$ For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008).

We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ By placing I00 in the lower right hand corner, we allow the larger N00 to unambiguously attach nulls.
We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ Pruning a one-by-one cell also indirectly prunes larger cells containing it.
We also include indicator features on lexical templates for the 50 most common words in each language, as in Haghighi et al (2009). $$$$$ We extracted such counts from 1.1 million French-English aligned sentence pairs of Hansards data (see Section 6.1).

 $$$$$ Specifically, for each alignment cell (i, j) which is not a possible alignment in a*, we incur a loss of 1 when azo =6 a*zo; note that if (i, j) is a possible alignment, our loss is indifferent to its presence in the proposal alignment.
 $$$$$ We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model.
 $$$$$ We opt instead to maximize the probability of the set of alignments M(a*) which achieve the same optimal in-class loss.
 $$$$$ For AITG and ABITG, we can efficiently sum over the set of ITG derivations in 0(n6) time using the inside-outside algorithm.

This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ We will assume the score of a alignment is given as a linear function of a feature vector φ(x, a).
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ This yielded 30.6 AER.
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ First, n-ary productions are not binarized to remove ambiguity; this results in an exponential number of derivations for diagonal alignments.
This supervised base line is a reimplementation of the MIRA-trained model of Haghighi et al (2009). $$$$$ Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006).

This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ This slows learning and prevents us from finding a useful weight vector.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ However, for the ITG grammar presented in Section 2.2, each alignment has multiple grammar derivations.
This training regimen on this data set has provided state-of-the-art unsupervised results that outperform IBM Model 4 (Haghighi et al., 2009). $$$$$ As they found, ITG approaches offer several advantages over general matchings.

The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ On the Chinese NIST data, however, where almost no alignment is in A1-1, the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results.
The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005).
The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ This yielded 30.6 AER.
The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al, 2009). $$$$$ Summing and obtaining feature expectations over M(a*) can be done efficiently using a constrained variant of the inside-outside algorithm where sure alignments not present in a* are disallowed, and the number of missing sure alignments is appended to the state of the bitext cell.6 One advantage of the likelihood-based objective is that we can obtain posteriors over individual alignment cells, We obtain posterior ITG alignments by including all alignment cells (i, j) such that PIV((i, j)|x) exceeds a fixed threshold t. Posterior thresholding allows us to easily trade-off precision and recall in our alignments by raising or lowering t.

When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines $$$$$ We will further assume the feature representation of an alignment, φ(x, a) decomposes as in Equation (1), In the framework of loss-augmented margin learning, we seek a w such that w · φ(x, a∗) is larger than w · φ(x, a) + L(a, a∗) for all a in an alignment family, where L(a, a∗) is the loss between a proposed alignment a and the gold alignment a∗.
When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines $$$$$ Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments.
When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines $$$$$ We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars.
When evaluated on parsing and word alignment, this model significantly improves over independently trained baselines $$$$$ We compute Viterbi alignments using the averaged weight vector from this procedure.

Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models.
Although this assumption does limit the space of possible word-level alignments, for the domain we consider (Chinese-English word alignment), the reduced space still contains almost all empirically observed alignments (Haghighi et al, 2009). $$$$$ Similarly for inverse cells, we insist an I cell only be built by an inverted combination of N cells; binarization of I --* N2+ requires the introduction of the intermediary symbol I (see Figure 2(b)).

We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ For AITG and ABITG, we can efficiently sum over the set of ITG derivations in 0(n6) time using the inside-outside algorithm.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ Likelihood training time is reduced by nearly two orders of magnitude.
We begin with the same set of alignment features as Haghighi et al (2009), which are defined only for terminal bi spans. $$$$$ For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008).

Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ We consider maximum margin and conditional likelihood objectives, including the presentation of a new normal form grammar for canonicalizing derivations.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ Given this normal form, we can efficiently compute model expectations over ITG alignments without double counting.5 To our knowledge, the alteration of the normal form to accommodate null emissions is novel to this work.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars.
Because of this, given a particular word alignment w, we maximize the marginal probability of the set of derivations A (w) that are consistent with w (Haghighi et al., 2009). $$$$$ The alignment that maximizes a set of potentials factored as in Equation (1) can be found in O(n3) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P-hard (Valiant, 1979).

We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ Pruning a one-by-one cell also indirectly prunes larger cells containing it.
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ For efficiency, we describe a set of pruning techniques that together allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing.
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature).
We prune our ITG forests using the same basic idea as Haghighi et al (2009), but we employ a technique that allows us to be more aggressive. $$$$$ For features on one-by-one cells, we consider Dice, the distance features from (Taskar et al., 2005), dictionary features, and features for the 50 most frequent lexical pairs.
