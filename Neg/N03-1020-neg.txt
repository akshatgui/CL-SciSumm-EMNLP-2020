In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ (2) The performance of weighted average n-gram scores is in the range between bi-gram and tri-gram co-occurrence scores.
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right).
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ With these formulas, we describe how to evaluate them in the next section.
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ The NIST (NIST 2002) scoring metric is based on BLEU.

Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ To evaluate the performance of automatic scoring metrics, we proposed two test criteria.
Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ The Ngram(1,4)n score is a weighted average of variable length n-gram matches.

it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ We plan to run similar experiments on DUC 2002 data to see if unigram does as well.

Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ Although these results are statistically significant (α _ 0.025) and are comparable to IBM BLEU's correlation figures shown in Section 3, they are not consistent across summary sizes and tasks.
Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.

Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ This enables us to use an automatic evaluation procedure in place of human assessments to compare system performance, as in the NIST MT evaluations (NIST 2002).
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ For example, given a model string &quot;United States, Japan, and Taiwan&quot;, a candidate string &quot;United States, Taiwan, and Japan&quot; has a unigram score of 1, bi-gram score of 0.5, and trigram and 4-gram scores of 0 when the stopword &quot;and' is ignored.
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ The straight line (AvgC) is the human ranking and n marks summaries of different sizes.
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries.

Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ The BLEU formula is then written as follows: N is set at 4 and wn, the weighting factor, is set at 1/N.
Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ As stated in Section 3, direct application of BLEU on the DUC 2001 data showed promising results.
Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years.
Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ Furthermore, instead of a brevity penalty that punishes overly short translations, a brevity bonus, BB, should be awarded to shorter summaries that contain equivalent content.

Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.
Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ In this paper, we gave a brief introduction of the manual summary evaluation protocol used in the Document Understanding Conference.
Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ Section 4 compares n-gram cooccurrence scoring procedures in terms of their correlation to human results and on the recall and precision of statistical significance prediction.

Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ To quantify the correlation, we compute the Spearman rank order correlation coefficient (p) for each Ngram(1,4)n run at different summary sizes (n).
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Ngram(1,1) has the best overall behavior.

Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ N-gram precision in BLEU is computed as follows: Where Countclip(n-gram) is the maximum number of ngrams co-occurring in a candidate translation and a reference translation, and Count(n-gram) is the number of n-grams in the candidate translation.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ These graphs confirm Ngram(1,1) (simple unigram) is a good automatic scoring metric with good statistical significance prediction power.

The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ Automated text summarization has drawn a lot of interest in the natural language processing and information retrieval communities in the recent years.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ We surmise that a reason for the difference between summarization and machine translation might be that extraction-based summaries do not really suffer from grammar problems, while translations do.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ In addition, baseline summaries were created automatically for each length as reference points.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ We use z-test in all the significance tests with ❑ level at 0.10, 0.05, 0.25, 0.01, and 0.005.

ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001).
ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ Therefore, it would be wise to use these valuable resources, i.e. manual summaries and evaluation results, not only in the formal evaluation every year but also in developing systems and designing automatic evaluation metrics.

Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ We used recall and precision of the agreement between the test statistics results to identify good automatic scoring metrics.
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ 2002).
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ Each year the human evaluation results can be used to evaluate the effectiveness of the various automatic evaluation metrics.
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ We are starting to explore various metrics suggested in Donaway et al. (2000).

One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows.
One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ Therefore, we need to change the weighting scheme to not penalize or even reward shorter equivalents.

We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ We surmise that a reason for the difference between summarization and machine translation might be that extraction-based summaries do not really suffer from grammar problems, while translations do.
We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ Due to the setup in DUC, the evaluations we discussed here are intrinsic evaluations (Sparck Jones and Galliers 1996).

Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ Both of them exclude stopwords.
Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.
Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ This would be very useful during the system development cycle to gauge if an improvement is really significant or not.

 $$$$$ The Ngram(1,4)n score is a weighted average of variable length n-gram matches.
 $$$$$ It is encouraging to know that the simple unigram cooccurrence metric works in the DUC 2001 setup.
 $$$$$ To quantify the correlation, we compute the Spearman rank order correlation coefficient (p) for each Ngram(1,4)n run at different summary sizes (n).
 $$$$$ The document sets were of four types: a single natural disaster event; a single event; multiple instances of a type of event; and information about an individual.

Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. $$$$$ Analyzing all runs according to Tables 2 and 3, we make the following observations: outperform (0.99 ≥ Spearman p ≥ 0.75) the weighted average of n-gram of variable length Ngram(1, 4) (0.88 ≥ Spearman p ≥ 0.55) in single and multiple document tasks when stopwords are ignored.
Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. $$$$$ The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. $$$$$ Only one baseline (baseline1) was created for the single document summarization task.
Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. $$$$$ In contrast, the weighted average of variable length n-gram matches derived from IBM BLEU did not always give good correlation and high recall and precision.

We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ Longer n-grams tend to score for grammaticality rather than content.
We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ From Figures 3 and 4, we can see Ngram(1,1) and Ngram(2,2) reside on the upper right corner of the recall and precision graphs.
We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ Both of them exclude stopwords.

Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ To achieve this goal, they used a weighted average of variable length n-gram matches between system translations and a set of human reference translations and showed that a weighted average metric, i.e.
Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ We used recall and precision of the agreement between the test statistics results to identify good automatic scoring metrics.
Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ The x-axis is the human ranking and the y-axis gives the corresponding Ngram(1,4) rankings for summaries of difference sizes.
Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ Two other human summaries were also created at each length.

We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998).
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization.
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ The training set comprised 30 sets of approximately 10 documents, each provided with their 50, 100, 200, and 400-word human written summaries.
