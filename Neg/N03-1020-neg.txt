In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ Section 3 discusses the IBM BLEU (Papineni et al. 2001) and NIST (2002) n-gram co-occurrence scoring procedures and the application of a similar idea in evaluating summaries.
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ The straight line (AvgC) is the human ranking and n marks summaries of different sizes.
In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram co occurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counter part. $$$$$ In order to meet the requirement of the first criterion stated in Section 3, we need better results.

Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ Importantly, unigram performs especially well with Spearman p ranging from 0.88 to 0.99 that is better than the best case in which weighted average of variable length n-gram matches is used and is consistent across different data sets.
Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
Summarization evaluation is done using ROUGE-2 (R-2) (Lin and Hovy, 2003). $$$$$ The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three.

it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ We plan to run similar experiments on DUC 2002 data to see if unigram does as well.
it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ Recall and precision curves of Ngram co-occurrence statistics versus human assessment for DUC 2001 single document task.
it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ Ngram(1, 4) is a weighted variable length n-gram match score similar to the IBM BLEU score; while Ngram(k, k), i.e. i = j = k, is simply the average k-gram coverage score Ck.
it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). $$$$$ For example, the correlations of the single document task are at the 60% level; while they range from 50% to 80% for the multidocument task.

Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ Two other human summaries were also created at each length.
Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.
Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ The straight line (AvgC) is the human ranking and n marks summaries of different sizes.
Lin and Hovy (2003) have shown that a unigram co-occurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highly correlates with the scores assigned by human evaluators at DUC. $$$$$ According to our experiments, we found that unigram co-occurrence statistics is a good automatic scoring metric.

Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ The results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations, based on various statistical metrics; while direct application of the BLEU evaluation procedure does not always give good results.
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ We surmise that a reason for the difference between summarization and machine translation might be that extraction-based summaries do not really suffer from grammar problems, while translations do.
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ Notice that the average n-gram coverage score, Cn, as shown in equation 5 is a recall metric 8 The number of instances is 14 (11 systems, 2 humans, and 1 baseline) for the single document task and is 16 (12 systems, 2 humans, and 2 baselines) for the multi-document task. ings versus human ranking for the multidocument task data from DUC 2001.
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. $$$$$ Section 2 gives an overview of the evaluation procedure used in DUC.

Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ We first ran IBM's BLEU evaluation script unmodified over the DUC 2001 model and peer summary set.
Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human summary, the better it is.
Another recent study (Lin and Hovy, 2003) investigated the extent to which extractive methods may be sufficient for summarization in the single-document case. $$$$$ A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al.

Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ According to our experiments, we found that unigram co-occurrence statistics is a good automatic scoring metric.
Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ To evaluate the performance of automatic scoring metrics, we proposed two test criteria.
Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
Automatic evaluation was performed with ROUGE (Lin and Hovy, 2003) using TAC-2008parameter settings. $$$$$ If it does, we will make available our code available via a website to the summarization community.

Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ The discrepancy led us to examine the effectiveness of individual n-gram cooccurrence statistics as a substitute for expensive and error-prone manual evaluation of summaries.
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ For each set of DUC 2001 data, single document 100word summarization task, multi-document 50, 100, 200, and 400 -word summarization tasks, we compute 4 different correlation statistics: Spearman rank order correlation coefficient (Spearman p), linear regression t-test (LRt, 11 degree of freedom for single document task and 13 degree of freedom for multi-document task), Pearson product moment coefficient of correlation (Pearson p), and coefficient of determination (CD) for each Ngram(i,�) evaluation metric.
Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.

Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ Two other human summaries were also created at each length.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics.
Rouge (Lin and Hovy, 2003) represents another such effort. $$$$$ A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al.

The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ Ideally, we would like there to be a positive correlation between them.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length). $$$$$ N-gram precision in BLEU is computed as follows: Where Countclip(n-gram) is the maximum number of ngrams co-occurring in a candidate translation and a reference translation, and Count(n-gram) is the number of n-grams in the candidate translation.

ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ According to our experiments, we found that unigram co-occurrence statistics is a good automatic scoring metric.
ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ In this way the evaluation technologies can advance at the same pace as the summarization technologies improve.
ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ Another was to compare the statistical significance test results between automatic scoring metrics and human assessments.
ROUGE (Lin and Hovy, 2003) has been adopted as a standard evaluation metric in various summarization tasks. $$$$$ Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998).

Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ Only one baseline (baseline1) was created for the single document summarization task.
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ Therefore, instead of pure sentence recall score, we use coverage score C. We define it as follows7: Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none.
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al.
Although ROUGE (Lin and Hovy, 2003) is one of the most popular methods for evaluation of summaries, it may not be appropriate for the evaluation of the mediatory summary because the scoring based on N-gram in this method can not be used to consider the fairness described in Section 2. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.

One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ Longer n-grams tend to score for grammaticality rather than content.
One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ Following the recent adoption by the machine translation community of automatic evaluation using the BLEU/NIST scoring process, we conduct an in-depth study of a similar idea for evaluating summaries.
One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ In this way the evaluation technologies can advance at the same pace as the summarization technologies improve.
One semi automatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on n gram co-occurrence between automatic and human summaries. $$$$$ In fact, a length adjusted average coverage score was used as an alternative performance metric in DUC 2002.

We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ For the multi-document summarization task, one baseline, lead baseline, took the first 50, 100, 200, and 400 words in the last document in the collection.
We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ For example, a system at (5,8) means that human ranks its performance at the 5th rank while Ngram(1,4)400 ranks it at the 8th.
We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ The NIST (NIST 2002) scoring metric is based on BLEU.
We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram co occurrence between machine summaries and ideal human summaries. $$$$$ For example, if an automatic evaluation shows there is a significant difference between run A and run B at α = 0.05 using the z-test (t-test or bootstrap resampling), how does this translate to &quot;real&quot; significance, i.e. the statistical significance in a human assessment of run A and run B?

Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ According to Papineni et al. (2001), BLEU is essentially a precision metric.
Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ For example, if &quot;United States of America&quot; occurs in a reference summary, while one peer summary, A, uses &quot;United States&quot; and another summary, B, uses the full phrase &quot;United States of America&quot;, summary B gets more contribution to its overall score simply due to the longer version of the name.
Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ We then discussed the IBM BLEU MT evaluation metric, its application to summary evaluation, and the difference between precisionbased BLEU translation evaluation and recall-based DUC summary evaluation.
Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. $$$$$ These numbers indicate that they positively correlate at α = 0.018.

 $$$$$ The straight line (AvgC) is the human ranking and n marks summaries of different sizes.
 $$$$$ The reason for this might be that most of the systems participating in DUC generate summaries by sentence extraction.
 $$$$$ The straight line marked by AvgC is the ranking given by human assessment.
 $$$$$ In that study, an automatic scoring script developed by Chris Buckley showed high correlation with human evaluations, although the experiment was only tested on a small set of 3 topics.

Our experiments employ the aforementioned AMI meeting corpus $$$$$ We first ran IBM's BLEU evaluation script unmodified over the DUC 2001 model and peer summary set.
Our experiments employ the aforementioned AMI meeting corpus $$$$$ This would be very useful during the system development cycle to gauge if an improvement is really significant or not.
Our experiments employ the aforementioned AMI meeting corpus $$$$$ We also test the effect of inclusion or exclusion of stopwords.
Our experiments employ the aforementioned AMI meeting corpus $$$$$ Although this study shows that unigram co-occurrence statistics exhibit some good properties in summary evaluation, it still does not correlate to human assessment 100% of the time.

We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization.
We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ The second criterion is critical in interpreting the significance of automatic evaluation results.
We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ In contrast, the weighted average of variable length n-gram matches derived from IBM BLEU did not always give good correlation and high recall and precision.
We use the ROUGE (Lin and Hovy, 2003) evaluation measure. $$$$$ A series of workshops on automatic text summarization (WAS 2000, 2001, 2002), special topic sessions in ACL, COLING, and SIGIR, and government sponsored evaluation efforts in the United States (DUC 2002) and Japan (Fukusima and Okumura 2001) have advanced the technology and produced a couple of experimental online systems (Radev et al. 2001, McKeown et al.

Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001).
Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ There is more to be desired in the recall and precision of significance test agreement with manual evaluation.
Our initial experimental results show that our approach is feasible, since it produces summaries, which when evaluated against the TAC 2009 data yield ROUGE scores (Lin and Hovy, 2003) comparable to the participating systems in the Summarization task at TAC 2009. $$$$$ We would like to propose an annual automatic evaluation track in DUC that encourages participants to invent new automated evaluation metrics.

We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ Figure 4.
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ This might suggest some summaries are over-penalized by the weighted average metric due to the lack of longer n-gram matches.
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.
We used the standard ROUGE evaluation (Lin and Hovy, 2003) which has been also used for TAC. $$$$$ Longer n-grams tend to score for grammaticality rather than content.
