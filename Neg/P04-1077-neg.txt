 $$$$$ Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence.
 $$$$$ Since ROUGE computed similarity on surface word level, stemmed version allowed ROUGE to perform more lenient matches.
 $$$$$ The skip-bigram-based ROUGE-S* (without skip distance restriction) had the best Pearson's p correlation of 0.95 in adequacy when all words were lower case and stemmed.
 $$$$$ Correlation analysis based on two different correlastatistics, Pearson’s Spearman’s with respect to adequacy and fluency are shown in Table 1.

The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ Therefore it is a useful correlation indicator even when good linear correlation, for example, according to Pearson’s correlation coefficient between two variables could a quick overview of the Spearman’s coefficient, see: http://davidmlane.com/hyperstat/A62436.html. not be found.
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ Akiba et al. (2001) extended the idea to accommodate multiple references.
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: Y1 and Y2 have the same ROUGE-L score.

Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ Section 5 presents the evaluation results of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency.
Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased.
Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ Adjusting Equations 13, 14, and 15 to use maximum skip distance limit is straightforward: we only count the skip-bigram matches, SKIP2(X,Y), within the maximum skip distance and replace denominators of Equations 13, C(m,2), and 14, C(n,2), with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively.

In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ Section 5 presents the evaluation results of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency.
In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ Correlation analysis based on two different correlation statistics, Pearson’s p and Spearman’s p, with respect to adequacy and fluency are shown in Table 1.
In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ In two recent large-scale machine translation evaluations sponsored by NIST, a closely related automatic evaluation method, simply called NIST score, was used.
In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: into lower case, i.e. no case information was used.

 $$$$$ For example, f(k)=αk – β when k >= 0, and α, β > 0.
 $$$$$ In two recent large-scale machine translation evaluations sponsored by NIST, a closely related automatic evaluation method, simply called NIST score, was used.
 $$$$$ BLEU4 and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pearson’s p. Among them WER (0.48) that tends to penalize small word movement is the worst performer.
 $$$$$ Using objective functions to automatically evaluate machine translation quality is not new.

Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ We conclude this paper and discuss extensions of the current work in Section 6.
Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories.
Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ NP-LCS can be shown as a special case of Equation (6) with β = 1.

Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations.
Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ In other words, consecutive matches are awarded more scores than non-consecutive matches.
Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995).
Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ Equation 15, ROUGE-S.

 $$$$$ Since unigram matches do not distinguish words in consecutive positions from words in the wrong order, measures based on position-independent unigram matches are not sensitive to word order and sentence level structure.
 $$$$$ Since ROUGE computed similarity on surface word level, stemmed version allowed ROUGE to perform more lenient matches.
 $$$$$ They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments.

 $$$$$ We plan to explore their correlation with human judgments on sentence-level in the future.
 $$$$$ Of course we have to make sure the resulting composite function of precision and recall is still correlates highly with human judgments.
 $$$$$ In the next step, we plan to extend them to accommodate synonyms and paraphrases.
 $$$$$ The LCS-based F-measure can be computed as follows: where ,B = Plcs-multi/Rlcs-multi when aFlcs-multi/aRlcsmulti=aFlcs-multi/aPlcs-multi.

 $$$$$ N=2, for the purpose of explanation and call this BLEU-2.
 $$$$$ For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms.
 $$$$$ In the next step, we plan to extend them to accommodate synonyms and paraphrases.
 $$$$$ Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches.

 $$$$$ Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations.
 $$$$$ However, these measures in their current forms are still only applying string-to-string matching.
 $$$$$ Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches.
 $$$$$ In this scenario, statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased.

 $$$$$ It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics.
 $$$$$ Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003).
 $$$$$ They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments.

Stemming is enabled (Lin and Och, 2004a). $$$$$ The Pearson's p correlation values in the Stem set of the Fluency Table, indicates that BLEU12 has the highest correlation (0.93) with fluency.
Stemming is enabled (Lin and Och, 2004a). $$$$$ However, they have the advantage that we can apply them on sentence level while longer BLEU such as BLEU12 would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches).
Stemming is enabled (Lin and Och, 2004a). $$$$$ We can limit the maximum skip distance, dskip, between two in-order words that is allowed to form a skip-bigram.

The optimal set is $$$$$ We first computed averages of the adequacy and fluency scores of each system assigned by human evaluators.
The optimal set is $$$$$ One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations.
The optimal set is $$$$$ Equation 6, ROUGE-L. Notice that ROUGE-L is 1 when X = Y since LCS(X,Y) = m or n; while ROUGE-L is zero when LCS(X,Y) = 0, i.e. there is nothing in common between X and Y. Fmeasure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor (Van Rijsbergen 1979).
The optimal set is $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.

Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ N-gram precision in BLEU is computed as follows: Where Countclip(n-gram) is the maximum number of n-grams co-occurring in a candidate translation and a reference translation, and Count(ngram) is the number of n-grams in the candidate translation.
Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ In the next section, we present the evaluations of ROUGE-L, ROUGE-S, and compare their performance with other automatic evaluation measures.
Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ In Section 4, we will introduce skip-bigram co-occurrence statistics that do not have this problem while still keeping the advantage of in-sequence (not necessary consecutive) matching that reflects sentence level word order.

ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ In the next step, we plan to extend them to accommodate synonyms and paraphrases.
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ Recently, Turian et al. (2003) indicated that standard accuracy measures such as recall, precision, and the F-measure can also be used in evaluation of machine translation.

 $$$$$ A variant of BLEU developed by NIST (2002) has been used in two recent large-scale machine translation evaluations.
 $$$$$ Using objective functions to automatically evaluate machine translation quality is not new.
 $$$$$ Third, BLEU is a geometric mean of unigram to N-gram precisions.
 $$$$$ We conclude this paper and discuss extensions of the current work in Section 6.

 $$$$$ Since we would like to use automatic evaluation metric not only in comparing systems a quick overview of the Pearson’s coefficient, see: http://davidmlane.com/hyperstat/A34739.html. but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores.
 $$$$$ In the next section, we introduce the skip-bigram cooccurrence statistics.
 $$$$$ However, LCS only counts the main in-sequence words; therefore, other longest common subsequences and shorter sequences are not reflected in the final score.
 $$$$$ The lower and upper values of 95% confidence interval are also shown in the table.

Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ BLEU4 and 12, ROUGE-W, GTM20 and 30, ROUGE-S0 (bigram), and WER have lower Pearson’s p. Among them WER (0.48) that tends to penalize small word movement is the worst performer.
Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output. lower case and stemmed using the Porter stemmer (Porter 1980).
Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ In the next section, we present the evaluations of ROUGE-L, ROUGE-S, and compare their performance with other automatic evaluation measures.

Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 1.
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ We can limit the maximum skip distance, dskip, between two in-order words that is allowed to form a skip-bigram.
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ The composite factors are LCS-based recall and precision in this case.
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
