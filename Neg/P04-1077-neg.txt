 $$$$$ We have shown that better correlation with adequacy can be reached by applying stemmer.
 $$$$$ He used as an approximate string matching algorithm.

The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ Since BLEU has been used to report the performance of many machine translation systems and it has been shown to correlate well with human judgments, we will explain BLEU in more detail and point out its limitations in the next section.
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ Therefore S2 is better than S3 according to ROUGE-L.
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ Dark (green) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories.
The other metric is ROUGE (Lin and Och, 2004), here named R. $$$$$ Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1; while the reverse was true in the terms of fluency.

Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ Third, BLEU is a geometric mean of unigram to N-gram precisions.
Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ To estimate the significance of these correlation statistics, we applied bootstrap resampling, generating random samples of the 919 different sentence segments.
Lin and Och (2004) experimented, unlike previous works, with a wide set of metrics, including NIST, WER (Nießen et al, 2000), PER (Tillmann et al, 1997), and variants of ROUGE, BLEU and GTM. $$$$$ To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the formula: Where |c |is the length of the candidate translation and |r |is the length of the reference translation.

In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ ROUGE-L as defined in Equation 6 has the property that its value is less than or equal to the minimum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X (reference translation) that are also present in Y (candidate translation); while unigram precision is the proportion of words in Y that are also in X. Unigram recall and precision count all co-occurring words regardless their orders; while ROUGE-L counts only in-sequence co-occurrences.
In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ Notice that by providing different weighting function f, we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches.
In order to improve sentence-level evaluation performance, several metrics have been proposed, including ROUGE-W, ROUGE-S (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). $$$$$ They used a weighted average of variable length ngram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments.

 $$$$$ One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations.
 $$$$$ The Pearson's p correlation values in the Stem set of the Fluency Table, indicates that BLEU12 has the highest correlation (0.93) with fluency.
 $$$$$ GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy.
 $$$$$ Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence ngrams automatically while this is a free parameter in BLEU.

Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ The Pearson’s correlation coefficient5 measures the strength and direction of a linear relationship between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.
Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations.
Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ Therefore, computation time can be reduced and hopefully performance can be as good as the version without such constraint.
Lin and Och (2004) proposed an LCS-based automatic evaluation measure called ROUGE-L. $$$$$ Third, BLEU is a geometric mean of unigram to N-gram precisions.

Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ We have shown that better correlation with adequacy can be reached by applying stemmer.
Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were equal performers to BLEU in measuring fluency.
Therefore, Lin and Och (2004) introduced skip-bigram statistics for the evaluation of machine translation. $$$$$ We conclude this paper and discuss extensions of the current work in Section 6.

 $$$$$ Therefore S2 is better than S3 according to ROUGE-L.
 $$$$$ GTM10 has good correlation with human judgments in adequacy but not fluency; while GTM20 and GTM30, i.e.
 $$$$$ Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995).
 $$$$$ This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output. lower case and stemmed using the Porter stemmer (Porter 1980).

 $$$$$ To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the formula: Where |c |is the length of the candidate translation and |r |is the length of the reference translation.
 $$$$$ Therefore, Pearson’s correlation coefficient is a good measure to look at. correlation coefficient 6is also a measure of correlation between two variables.
 $$$$$ Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for BLEU7-12 (only BLEU12 is shown).

 $$$$$ We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations.
 $$$$$ Using Equation 15 with ,B = 1 and S1 as the reference, S2’s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333.
 $$$$$ Of course we have to make sure the resulting composite function of precision and recall is still correlates highly with human judgments.
 $$$$$ However, they have the advantage that we can apthem on sentence level while longer would not differentiate any sentences with length shorter than 12 words (i.e. no 12-gram matches).

 $$$$$ In two recent large-scale machine translation evaluations sponsored by NIST, a closely related automatic evaluation method, simply called NIST score, was used.
 $$$$$ Analyzing all runs according to the adequacy and fluency table, we make the following observations: Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for (only is shown).
 $$$$$ However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n > 1) matches achieved similar performance as Bleu.

 $$$$$ Since we would like to use automatic evaluation metric not only in comparing systems but also in in-house system development, a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores.
 $$$$$ If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams.

Stemming is enabled (Lin and Och, 2004a). $$$$$ Section 5 presents the evaluation results of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency.
Stemming is enabled (Lin and Och, 2004a). $$$$$ To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence.
Stemming is enabled (Lin and Och, 2004a). $$$$$ To prevent very short translations that try to maximize their precision scores, BLEU adds a brevity penalty, BP, to the formula: Where |c |is the length of the candidate translation and |r |is the length of the reference translation.

The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). $$$$$ Using Equation 15 with ,B = 1 and S1 as the reference, S2’s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333.
The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). $$$$$ To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches, we proposed counting skip bigram co-occurrence.
The optimal set is: { METEOR wnsyn, ROUGE w 1.2} which includes variants of METEOR, and ROUGE (Lin and Och, 2004). $$$$$ However, S2 and S3 have very different meanings.

Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ However, high quality large-scale human judgments are hard to come by.
Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ Equation 15, ROUGE-S.
Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS.
Furthermore, we attempt to achieve additional generalization by using skip n-grams (Lin and Och, 2004). $$$$$ We have shown that better correlation with adequacy can be reached by applying stemmer.

ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ According to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed BLEU and NIST.
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ The main idea of BLEU is to measure the similarity between a candidate translation and a set of reference translations with a numerical metric.
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ Spearman’s correlation coefficient does not assume the correlation between the variables is linear.
ROUGE utilizes skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ Given two sentences X and Y, the WLCS score of X and Y can be computed using the following dynamic programming procedure: (2) For (i = 1; i <= m; i++) For (j = 1; j <= n; j++) If xi = yj Then // the length of consecutive matches at // position i-1 and j-1 Where c is the dynamic programming table, c(i,j) stores the WLCS score ending at word xi of X and yj of Y, w is the table storing the length of consecutive matches ended at c table position i and j, and f is a function of consecutive matches at the table position, c(i,j).

 $$$$$ We call the skip-bigram-based Fmeasure, i.e.
 $$$$$ For the input of automatic evaluation methods, we created three evaluation sets from the MT outputs: into lower case, i.e. no case information was used.
 $$$$$ For example, we can use an existing thesaurus such as WordNet (Miller 1990) or creating a customized one by applying automated synonym set discovery methods (Pantel and Lin 2002) to identify potential synonyms.
 $$$$$ ROUGE-L, ROUGE-W, ROUGE-S*, ROUGE-S4, and ROUGE-S9 were equal performers to BLEU in measuring fluency.

 $$$$$ Skipbigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations.
 $$$$$ The LCS-based F-measure can be computed as follows: where ,B = Plcs-multi/Rlcs-multi when aFlcs-multi/aRlcsmulti=aFlcs-multi/aPlcs-multi.
 $$$$$ GTM with exponent larger than 1.0, has good correlation with human judgment in fluency but not adequacy.
 $$$$$ However, results based on their method, General Text Matcher (GTM), showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram (n > 1) matches achieved similar performance as Bleu.

Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ The Pearson’s correlation coefficient5 measures the strength and direction of a linear relationship between any two variables, i.e. automatic metric score and human assigned mean coverage score in our case.
Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ We plan to explore their correlation with human judgments on sentence-level in the future.
Skip bigrams, generally speaking, are pairs of words in a sentence order with arbitrary gap (Lin and Och, 2004a). $$$$$ In the next step, we plan to extend them to accommodate synonyms and paraphrases.

Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ Once we have acquired synonym and paraphrase data, we then need to design a soft matching function that assigns partial credits to these approximate matches.
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ Although brevity penalty will penalize candidate translations with low recall by a factor of e(1|r|/|c|), it would be nice if we can use the traditional recall measure that has been a well known measure in NLP as suggested by Melamed (2003).
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ We can limit the maximum skip distance, dskip, between two in-order words that is allowed to form a skip-bigram.
Different from the previous skip bigram statistics which compare sentence similarities through overlapping skip bigrams (Lin and Och, 2004a), the skip bigrams we used are weighted by a decaying factor of the skipping gap in a sentence, giving higher scores to closer occurrences of skip bigrams. $$$$$ For example, f(k)=αk – β when k >= 0, and α, β > 0.
