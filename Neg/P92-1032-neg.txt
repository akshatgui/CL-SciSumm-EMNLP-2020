Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good.
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ Bar-Hillel's real objection was an empirical one.
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ In addition to the sampling questions, one feels uncomfortable about comparing results across experiments, since there are many potentially important differences including different corpora, different words, different judges, differences in treatment of precision and recall, and differences in the use of tools such as parsers and part of speech taggers, etc.

Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Word-sense disambiguation is a long-standing problem in computational linguistics (e.g., Kaplan (1950), Yngve (1955), Bar-Hillel (1960), Masterson (1967)), with important implications for a number of practical applications including text-to-speech (TTS), machine translation (MT), information retrieval (IR), and many others.
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Unfortunately, since it isn't clear just how this estimation should be accomplished, we decided to &quot;cheat&quot; and let the baseline system peek at the test set and &quot;estimate&quot; the most likely sense for each word as the more frequent sense in the test set.
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Under quite different conditions, we have found 96.8% agreement over judges.
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.

 $$$$$ The estimate depends very much, not surprisingly, on the particular experimental design.
 $$$$$ In such a case, a lexicographer might be quite happy receiving a long list of potential candidates, only a small fraction of which are actually the case of interest.
 $$$$$ The number of facts we human beings know is, in a certain very pregnant sense, infinite.&quot; (Bar-Hillel, 1960) Ironically, much of the research cited above is taking exactly the approach that Bar-Hillel ridiculed as utterly chimerical and hardly deserving of any further discussion.
 $$$$$ Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.

The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ In particular, (Gale et al., to appear) achieved considerable progress by using well-understood statistical methods and very large datasets of tens of millions of words of parallel English and French text (e.g., the Canadian Hansards).
The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ More than thirty years ago, BarHillel (1960) predicted that it would be &quot;futile&quot; to write expert-system-like rules by-hand (as they had been doing at Georgetown at the time) because there would be no way to scale up such rules to cope with unrestricted input.
The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ Jorgensen, who was interested in highlighting differences among informants, found a very low estimate (68%), well below the baseline (75%), and also well below the level that BarHillel asserted as not-good-enough.
The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ But much of the recent work cited above goes much further; not only does it supply a machine with a dictionary and an encyclopedia, but many other extensive references works as well, including Roget's Thesaurus and numerous large corpora.

(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ It has been our experience that it is very hard to design an experiment of any kind which will produce the desired agreement among judges.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ In our own experiments, we have set out to find word-sense disambiguation tasks where the judges can agree often that we could show that they were outperforming the baseline system.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ Understandable as this reaction is, it is very easy to show its futility.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ We will attempt to estimate an upper bound on performance by estimating the ability for human judges to agree with one another (or themselves).

This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ An estimate of the upper bound is obtained by assuming that our ability to measure performance is largely limited by our ability obtain reliable judgments from human informants.
This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ This constraint will probably prove useful for improving the performance of future word-sense disambiguation algorithms.
This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ After using both the monolingual and bilingual classifiers for a few months, we have convinced ourselves that the performance is remarkably good.
This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ Moreover, if we accept Bar-Hillel's argument that 75% is not-goodenough, then it would be hard to show that a system was doing well-enough.

To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ We began this discussion with a review of our recent work on word-sense disambiguation, which extends the approach of using massive lexicographic resources (e.g., parallel corpora, dictionaries, thesauruses and encyclopedia) in order to attack the knowledgeacquisition bottleneck that Bar-Hillel identified over thirty years ago.
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ Bar-Hillel's real objection was an empirical one.
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ Since, however, the idea of a machine with encyclopedic knowledge has popped up also on other occasions, let me add a few words on this topic.

The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ We have found it convenient to transform the A-D ratio into a quantity which we call the percent agreement, the number of observed agreements over the total number of possible agreements.
The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ The resulting discriminator is then tested on documents whose authorship is disputed.
The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ In particular, (Gale et al., to appear) achieved considerable progress by using well-understood statistical methods and very large datasets of tens of millions of words of parallel English and French text (e.g., the Canadian Hansards).
The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ The grand mean percent agreement over all subjects and words is only 68%.
