Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases.
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ Bar-Hillel's real objection was an empirical one.
Large In (Gale et al, 1992), it was argued that any wide coverage WSD program must be able to perform significantly better than the most-frequent-sense classifier to be worthy of serious consideration. $$$$$ Nevertheless, we would really like to be able to make a stronger statement, and therefore, we decided to try to develop some more objective evaluation measures.

Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ See Gale et al. (to appear) for further details.
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ On the other hand, if we are correct and interest is a relatively difficult word, then it is possible that we have made some progress over the past thirty years...
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Five subjects (the three authors and two of their colleagues) were given a questionnaire starting with a set of definitions selected from OALD (Crowie et al., 1989) and followed by a number of pairs of concordance lines, randomly selected from Grolier's Encyclopedia (1991).
Hearst (1997, pp. 53-54) attempted to adapt π∗ to award partial credit for near misses by using the percentage agreement metric of Gale et al (1992, p. 254) to compute actual agreement which conflates multiple manual segmentations together according to whether a majority of coders agree upon a boundary or not. $$$$$ Similar attempts to establish upper and lower bounds on performance have been made in other areas of computational linguistics, specifically part of speech tagging.

 $$$$$ Unfortunately, we also need a complementary measure that would penalize a system like the baseline system that simply assigned all instances of a polysemous word to the same sense.
 $$$$$ Under quite different conditions, we have found 96.8% agreement over judges.
 $$$$$ Moreover, there doesn't seem to be a very clear sense of what is possible.

The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ It is natural to try to use these numbers to predict performance on new words, but the study was not designed for that purpose.
The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ This method has been used very effectively by Kelly and Stone (1975), Black (1988), Hearst (1991), and many others.
The lower bound as Gale et al (1992c) suggested should be very low and it is more difficult to disambiguate if there are more senses. $$$$$ In another experiment, we selected a random sample of 97 words; 67 of them were unambiguous and therefore had a baseline performance of 100%.10 The remaining thirty words are listed along with the number of senses and baseline performance: virus (2, 98%), device (3, 97%), direction (2, 96%), reader (2, 96%), core (3, 94%), hull (2, 94%), right (5, 94%), proposition (2, 89%), deposit (2, 88%), hour (4, 87%), path (2, 86%), view (3, 86%), pyramid (3, 82%), antenna (2, 81%), trough (3, 77%), tyranny (2, 75%), figure (6, 73%), institution (4, 71%), crown (4, 64%), drum (2, 63%), pipe (4, 60%), processing (2, 59%), coverage (2, 58%), execution (2, 57%), min (2, 57%), interior (4, 56%), campaign (2, 51%), output (2, 51%), gin (3, 50%), drive (3, 49%).

(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ Fortunately, we have found in (Gale et al., 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ One might have preferred not to do so, but we simply don't know how one could establish enough dynamic range in that case to show any interesting differences.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ Although there has been a fair amount of literature on sense-disambiguation, the literature does not offer much guidance in how we might establish the success or failure of a proposed solution such as the two systems mentioned in the previous paragraph.
(Gale et al, 1992) reports that word sense disambiguation would be at least 75% correct if a system assigns the most frequently occurring sense. $$$$$ In so doing, we were able to obtain a much more usable estimate of (96.8%) by redefining the task from a classification task to a discrimination task.

This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ What such a suggestion amounts to, if taken seriously, is the requirement that a translation machine should not only be supplied with a dictionary but also with a universal encyclopedia.
This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ An estimate of the lower bound of 75% (averaged over ambiguous types) is obtained by measuring the performance produced by a baseline system that ignores context and simply assigns the most likely sense in all cases.
This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. $$$$$ We will estimate the lower bound by evaluating the performance of a straw man system, which ignores context and simply assigns the most likely sense in all cases.

To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ The test words were selected from the literature in order to make comparisons over systems.
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ With the exception of judge 2, all of the judges agreed with the majority opinion in all but one or two of the 82 cases.
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ If one desires extremely high recall and is willing to sacrifice precision in order to obtain this level of recall, then it might be sensible to tune a system to produce behavior which might appear to fall below the baseline.
To address the last problem, (Gale et al 1992) argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges. $$$$$ Not surprisingly, the upper bound is very dependent on the instructions given to the judges.

The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ Lesk (1986), an extremely innovative and commonly cited reference on the subject, provides a short discussion of evaluation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms.3 Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge.
The need to ascertain the agreement and reliability between coders for segmentation was recognized by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al (1992, p. 254) for usage in segmentation. $$$$$ In other words, one assumes that there are two (or more) sources of word probabilities, rel and irrel, in the IR application, and author1 and author 2 in the author identification application.
