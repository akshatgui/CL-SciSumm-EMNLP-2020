It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows: a reordering violates the ITG constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as a subsequence. $$$$$ Therefore, we introduce a threshold pruning parameter q, with 0 < q < 1.
It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows: a reordering violates the ITG constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as a subsequence. $$$$$ In our experiments, we use the following error criteria: The WER is computed as the minimum number of substitution, insertion and deletion operations that have to be performed to convert the generated sentence into the target sentence.
It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows: a reordering violates the ITG constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as a subsequence. $$$$$ These methods cannot be applied to the CYK-style search for the ITG constraints.
It has been pointed out in (Zens and Ney, 2003) that the ITG constraints can be characterized as follows: a reordering violates the ITG constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as a subsequence. $$$$$ In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.

A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). $$$$$ In this section, we will present the translation results for both the IBM constraints and the baseline ITG constraints.
A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). $$$$$ It can be further decomposed into alignment and lexicon model.
A comparison of the ITG constraints and the IBM constraints for single-word based models can be found in (Zens and Ney, 2003). $$$$$ We observe that a given permutation may be constructed in several ways by the above method.

(Zens and Ney, 2003) did comparative study over different reordering constraints. $$$$$ We also allow 2-transitions.
(Zens and Ney, 2003) did comparative study over different reordering constraints. $$$$$ Let W(jl, jr) denote the word graph for the source sequence fjr jl .
(Zens and Ney, 2003) did comparative study over different reordering constraints. $$$$$ For longer sentences, the ITG constraints allow for much more flexibility than the IBM constraints.

6 compares our results to related work, in particular Zens and Ney (2003). $$$$$ Especially for the English-French translation direction, the ITG coverage of 73.6% is very low.
6 compares our results to related work, in particular Zens and Ney (2003). $$$$$ We observe that a given permutation may be constructed in several ways by the above method.
6 compares our results to related work, in particular Zens and Ney (2003). $$$$$ In the next section, we will describe these constraints from a theoretical point of view.
6 compares our results to related work, in particular Zens and Ney (2003). $$$$$ For all source positions k with jl < k < jr, we combine the word graphs W (jl, k) and W (k + 1, jr) as described above.

Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). $$$$$ We have shown the translation results for the Verbmobil task.
Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). $$$$$ On the Canadian Hansards task the baseline ITG constraints were not sufficient.
Related work includes Wu (1997), Zens and Ney (2003) and Wellington et al (2006). $$$$$ Especially for the English-French translation direction, the ITG coverage of 73.6% is very low.

xITGs (Zens and Ney, 2003) in part solves this problem. $$$$$ We have presented a polynomial-time search algorithm for statistical machine translation based on the ITG constraints and its extension for the generation of word graphs.
xITGs (Zens and Ney, 2003) in part solves this problem. $$$$$ If arbitrary word-reorderings are allowed, the search problem is NP-hard (Knight, 1999).
xITGs (Zens and Ney, 2003) in part solves this problem. $$$$$ It allows an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1).
xITGs (Zens and Ney, 2003) in part solves this problem. $$$$$ Here, we use a subset of the data containing only sentences with a maximum length of 30 words.

 $$$$$ The evaluation consists of two parts: First, we check how many of the Viterbi alignments of the training corpus satisfy each of these constraints.
 $$$$$ Especially for the English-French translation direction, the ITG coverage of 73.6% is very low.
 $$$$$ We have to maximize over all possible target language sentences.

Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. $$$$$ With the extended ITG constraints the coverage improves significantly on both tasks.
Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. $$$$$ For each translation hypothesis, the WER to the most similar sentence is calculated (Nießen et al., 2000).
Zens and Ney (2003) used GIZA++ to word-align the Verbmobil task (English and German) and the Canadian Hansards task (English and French) and tested the coverage of ITGs and xITGs, i.e. the ratio of the number of alignment configurations that could be induced by the theories and the sentences in the two tasks. $$$$$ A significantly higher coverage of about 96% is obtained with the extended ITG constraints.

ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bi text data (Zens and Ney, 2003). $$$$$ Then, we will compare the translation results when restricting the search to either of these constraints.
ITG imposes constraints on which alignments are possible, and these constraints have been shown to be a good match for real bi text data (Zens and Ney, 2003). $$$$$ The (small) Schr¨oder numbers have been first described in (Schr¨oder, 1870) as the number of bracketings of a given sequence (Schr¨oder’s second problem).

For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). $$$$$ We have to keep in mind, that the Verbmobil task consists of transcriptions of spontaneous speech.
For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). $$$$$ The first task we will present results on is the Verbmobil task (Wahlster, 2000).
For a comparison and a more detailed discussion of the two approaches see (Zens and Ney, 2003). $$$$$ Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible.

Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). $$$$$ The ITG constraints were introduced in (Wu, 1995).
Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). $$$$$ A permutation derived by the above method can be represented as a binary tree where the inner nodes are colored either black or white.
Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). $$$$$ Therefore, we expect the search with the extended ITG constraints to outperform the search with the IBM constraints.
Such reordering was realized either by an additional constraint for decoding, such as window constraints, IBM constraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature functions (Tillman, 2004). $$$$$ We see that the results on this task are similar.

Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). $$$$$ With the extended ITG constraints the coverage improves significantly on both tasks.
Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). $$$$$ Table 2 shows the corpus statistics of this corpus.
Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). $$$$$ In this paper, we compare two different reordering constraints, namely the ITG constraints and the IBM constraints.
Zens and Ney (2003) found that the constraints of ITG were a better match to the decoding task than the heuristics used in the IBM decoder of Berger et al (1996). $$$$$ The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task.

The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). $$$$$ If arbitrary wordreorderings are permitted, the search problem is NP-hard.
The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). $$$$$ On the Canadian Hansards task the coverage increases from about 87% to about 96%.
The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). $$$$$ On the Canadian Hansards task the baseline ITG constraints were not sufficient.
The ITG we apply in our experiments has more structural labels than the primitive bracketing grammar: it has a start symbol S, a single preterminal C, and two intermediate nonterminals A and B used to ensure that only one parse can generate any given word-level alignment, as discussed by Wu (1997) and Zens and Ney (2003). $$$$$ The first task we will present results on is the Verbmobil task (Wahlster, 2000).

Some of the properties of these alignments are studied in (Zens and Ney, 2003). $$$$$ With this constraint, each of these binary trees is unique and equivalent to a parse tree of the ’canonical-form’ grammar in (Wu, 1997).
Some of the properties of these alignments are studied in (Zens and Ney, 2003). $$$$$ We draw the following conclusions: especially for long sentences the ITG constraints allow for higher flexibility in word-reordering than the IBM constraints.
Some of the properties of these alignments are studied in (Zens and Ney, 2003). $$$$$ Here, the coverage increases from about 87% for the IBM constraints to about 96% for the extended ITG constraints.
Some of the properties of these alignments are studied in (Zens and Ney, 2003). $$$$$ Finally, we will extend the basic algorithm for the generation of word graphs.

A comparison of both methods can be found in Zens and Ney (2003). $$$$$ Here, the coverage increases from about 87% for the IBM constraints to about 96% for the extended ITG constraints.
A comparison of both methods can be found in Zens and Ney (2003). $$$$$ About 3 million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC).
A comparison of both methods can be found in Zens and Ney (2003). $$$$$ Here, the coverage increases from about 87% for the IBM constraints to about 96% for the extended ITG constraints.
A comparison of both methods can be found in Zens and Ney (2003). $$$$$ In statistical machine translation, we are given a source language (‘French’) sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language (‘English’) sentence eI1 = e1 ... ei ... eI.

A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). $$$$$ Reordering constraints are more or less useless, if they do not allow the maximization of Eq.
A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). $$$$$ In (Vilar, 1998) a model similar to Wu’s method was considered.
A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). $$$$$ It can be further decomposed into alignment and lexicon model.
A typical value of m is 4 (Zens and Ney, 2003), and we write IBM constraints with m= 4 as IBM (4). $$$$$ Here, we use a subset of the data containing only sentences with a maximum length of 30 words.

Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). $$$$$ The applications were, for instance, the segmentation of Chinese character sequences into Chinese “words” and the bracketing of the source sentence into sub-sentential chunks.
Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). $$$$$ Now, let us get back to more practical aspects.
Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). $$$$$ The search with the ITG constraints is able to produce a correct translation.
Zens and Ney (2003) [3] show that ITG constraints yield significantly better alignment coverage than the constraints used in IBM statistical machine translation models on both German-English (Verbmobil corpus) and French-English (Canadian Hansards corpus). $$$$$ On the Canadian Hansards task the coverage increases from about 87% to about 96%.

Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. $$$$$ We see that the results on this task are similar.
Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. $$$$$ The experiments will show that the baseline ITG constraints are not sufficient on the Canadian Hansards task.
Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. $$$$$ Here, we use a subset of the data containing only sentences with a maximum length of 30 words.
Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. $$$$$ In (Vilar, 1998) a model similar to Wu’s method was considered.

This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. $$$$$ In this paper, we will focus on the alignment problem, i.e. the mapping between source sentence positions and target sentence positions.
This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. $$$$$ Therefore, we have to restrict the possible reorderings in some way to make the search problem feasible.
This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. $$$$$ Therefore, we present an extension to the ITG constraints.
This source of overcounting is considered and fixed by Wu (1997) and Zens and Ney (2003), which we briefly review here. $$$$$ For each test sentence, not only a single reference translation is used, as for the WER, but a whole set of reference translations.

 $$$$$ Especially for the English-French translation direction, the ITG coverage of 73.6% is very low.
 $$$$$ The search with the ITG constraints is able to produce a correct translation.
 $$$$$ For a more detailed analysis, subjective judgments by test persons are necessary.
 $$$$$ It consists of transcriptions of spontaneous speech.
