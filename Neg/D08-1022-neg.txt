Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ When combined with our previous work on forest-based decoding, the final result is even better than the hierarchical system Hiero.
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ 1) (Galley et al., 2006).
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest.

 $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.
 $$$$$ These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig.

This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.
This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ BLEU score Our experiments will use both default 1-best decoding and forest-based decoding.

 $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.
 $$$$$ When combined with our previous work on forest-based decoding, the final result is even better than the hierarchical system Hiero.
 $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.
 $$$$$ Experiments on a state-ofthe-art tree-to-string system show that this method improves BLEU score significantly, with reasonable extraction speed.

Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline: using packed forests instead of 1-best trees (Mi and Huang,2008), word lattices instead of 1-best segmentations (Dyer et al, 2008), and weighted alignment matrices instead of 1-best alignments (Liu et al, 2009). $$$$$ The last row shows that 16.3% of the rules used in 1-best derivations are indeed only extracted from non 1-best parses in the forests.

The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in tree-to-string string-to-tree handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors.
The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.

We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ In addition, many subtrees are repeated across different parses, so it is also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k2 similar tree-pairs in tree-to-tree models).
We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.
We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses.
We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ Typically, these models extract rules using parse trees from both or either side(s) of the bitext.

While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ So this fragment, frag1 = {e2}, is now complete and we can extract a rule, IP (x1:NPB x2:VP) → x1 x2.
While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ These extracted rules are called minimal rules, which can be glued together to form composed rules with larger tree fragments (e.g. r1 in Fig.
While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ Figure 6 plots the extraction speed and translation quality of forest-based extraction with various pruning thresholds, compared to 1-best and 30-best baselines.

Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ 2006AA010108 (H. M.), and by NSF ITR EIA0205456 (L. H.).
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ In addition, many subtrees are repeated across different parses, so it is also inefficient to extract rules separately from each of these very similar trees (or from the cross-product of k2 similar tree-pairs in tree-to-tree models).
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ For example, in Figure 4, BS(IP0, 6) = {e1, e2}.

Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.
Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ These results confirm that our novel forest-based rule extraction approach is a promising direction for syntaxbased machine translation.
Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.
Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ Current tree-based systems perform translation in two separate steps: parsing and decoding.

To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ : For example, sibling nodes VV and AS in the tree have non-faithful spans (crossed out in the Figure), because they both map to “held”, thus neither of them can be translated to “held” alone.
To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.
To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.
To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ Consider the example in Figure 3.

Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ However, following the other hyperedge e1 IP0, 6 → NP0, 3 VPB3, 6 will leave the new fragment frag2 = {e1} incomplete with one non-admissible node NP0, 3.
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ The basic idea is to decompose the source (Chinese) parse into a series of tree fragments, each of which will form a rule with its corresponding English translation.
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest.

Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ The final result with both forest-based extraction and forest-based decoding reaches a BLEU score of 0.2816, outperforming that of Hiero (Chiang, 2005), one of the best performing systems to date.
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in tree-to-string string-to-tree handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors.
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ This situation worsens with longer sentences as the number of possible parses grows exponentially with the sentence length and a k-best list will only capture a tiny fraction of the whole space.
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.

Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.
Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ When combined with our previous orthogonal work on forest-based decoding (Mi et al., 2008), the forest-forest approach achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero, one of the best-performing systems to date.

Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules.
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT.
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts.

As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.
As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ When combined with our previous work on forest-based decoding, the final result is even better than the hierarchical system Hiero.
As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.

Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ We now briefly explain the algorithm of Galley et al. (2004) that can extract these translation rules from a word-aligned bitext with source-side parses.
Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.
Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).
Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.

Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses.
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ The forest concept is also used in machine translation decoding, for example to characterize the search space of decoding with integrated language models (Huang and Chiang, 2007).

Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ We instead propose a novel approach that extracts rules from packed forests (Section 3), which compactly encodes many more alternatives than kbest lists.
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest.

Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.
Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ The final BLEU score results are shown in Table 4.
Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ The former case, with trees on both sides, is often called tree-to-tree models; while the latter case, with trees on either source or target side, include both treeto-string and string-to-tree models (see Table 1).
Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ Since the data scale is larger than the small data, we are forced to use harsher pruning thresholds, with pe = 5 for extraction and pd = 10 for decoding.
