Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989).
Forest-based rule extractor (Mi and Huang 2008) is used with a pruning thresh old p=3. $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.

 $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
 $$$$$ These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6, as shown in Figure 4.
 $$$$$ Informally, a packed parse forest, or forest in short, is a compact representation of all the derivations (i.e., parse trees) for a given sentence under a context-free grammar (Earley, 1970; Billot and Lang, 1989).

This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ In other words, it becomes nondeterministic how to “cut” a forest into tree fragments, which is analogous to the non-deterministic pattern-match in forest-based decoding (Mi et al., 2008).
This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ Each fragment is associated with a frontier (variable front in the Pseudocode), being the subset of nonadmissible leaf nodes (recall that expansion stops at admissible nodes).
This classification is inspired by and extends the Table 1 in (Mi and Huang, 2008). $$$$$ This work, on the other hand, is in the orthogonal direction, where we utilize forests in rule extraction instead of decoding.

 $$$$$ So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.
 $$$$$ So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.
 $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.
 $$$$$ These parameters Al ... As are tuned by minimum error rate training (Och, 2003) on the dev sets.

Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ The conditional probability P(d  |T) decomposes into the product of rule probabilities: where the first three are conditional probabilities based on fractional counts of rules defined in Section 3.3, and the last two are lexical probabilities.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.

The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ These two parse trees can be represented as a single forest by sharing common subtrees such as NPB0, 1 and VPB3, 6, as shown in Figure 4.
The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext.
The GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.

We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ We now briefly explain the algorithm of Galley et al. (2004) that can extract these translation rules from a word-aligned bitext with source-side parses.
We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ There is also a parallel work on extracting rules from k-best parses and k-best alignments (Venugopal et al., 2008), but both their experiments and our own below confirm that extraction on k-best parses is neither efficient nor effective.
We follow Mi and Huang (2008) to assign a fractional count to each well-formed structure. $$$$$ For future work we would like to apply this approach to other types of syntax-based translation systems, namely the string-to-tree systems (Galley et al., 2006) and tree-to-tree systems.

While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ : For example, sibling nodes VV and AS in the tree have non-faithful spans (crossed out in the Figure), because they both map to “held”, thus neither of them can be translated to “held” alone.
While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ Leveraging from structural and linguistic information from parse trees, these models are believed to be better than their phrase-based counterparts in tree-to-string string-to-tree handling non-local reorderings, and have achieved promising translation results.1 However, these systems suffer from a major limitation, that the rule extractor only uses 1-best parse tree(s), which adversely affects the rule set quality due to parsing errors.
While Mi and Huang (2008) and we both use forests for rule extraction, there remain two major differences. $$$$$ examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext.

Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ The concept of packed forest has been previously used in translation rule extraction, for example in rule composition (Galley et al., 2006) and tree binarization (Wang et al., 2007).
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ When combined with our previous work on forest-based decoding, it achieves a 2.5 BLEU points improvement over the baseline, and even outperforms the hierarchical system of Hiero by 0.7 points.
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ Following Huang (2008), we also modify this parser to output a packed forest for each sentence, which can be pruned by the marginal probability-based insideoutside algorithm (Charniak and Johnson, 2005; Huang, 2008).
Firstly, Mi and Huang (2008) use a packed forest, while we use a dependency forest. $$$$$ Nevertheless we suspect that their extraction algorithm is in principle similar to ours, although they do not provide details of forest-based fragmentation (Algorithm 1) which we think is non-trivial.

Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ We also conduct experiments on a larger training dataset, FBIS, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively.
Secondly, the GHKM algorithm (Galley et al, 2004), which is originally developed for extracting tree-to-string rules from 1-best trees, has been successfully extended to packed forests recently (Mi and Huang, 2008). $$$$$ Each fragment is associated with a frontier (variable front in the Pseudocode), being the subset of nonadmissible leaf nodes (recall that expansion stops at admissible nodes).

To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ For example, the tree in Figure 3 is cut into 6 pieces, each of which corresponds to a rule on the right.
To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ Experiments show that this method improves translation quality by over 1 BLEU point on a state-of-the-art tree-to-string system, and is 0.5 points better than (and twice as fast as) extracting on 30best parses.
To overcome parse error for SMT, Mi and Huang (2008) propose forest-based translation by using a packed forest instead of a single syntax tree as the translation input. $$$$$ These parameters Al ... As are tuned by minimum error rate training (Och, 2003) on the dev sets.

Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ By contrast, formally syntax-based models (Chiang, 2005) do not rely on parse trees, yet usually perform better than these linguistically sophisticated counterparts.
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ examples (partial) Abstract source target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for linguistically syntax-based systems that need parse trees from either or both sides of the bitext.
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ Experiments on a state-ofthe-art tree-to-string system show that this method improves BLEU score significantly, with reasonable extraction speed.
Instead, it does top-down recursive matching from each node one-by-one with each translation rule in the rule set (Mi and Huang 2008). $$$$$ So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.

Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ Each variable xi E X occurs exactly once in lhs(r) and exactly once in rhs(r).
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext.
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ examples (partial) target tree-to-tree Ding and Palmer (2005) Translation rule extraction is a fundamental problem in machine translation, especially for syntax-based that need parse trees from either or both sides of the bitext.
Following (Mi and Huang 2008), we use viterbi algorithm to prune the forest. $$$$$ 2006AA010108 (H. M.), and by NSF ITR EIA0205456 (L. H.).

Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ Like in the M-step in EM algorithm, we now extend the maximum likelihood estimation to fractional counts for three conditional probabilities regarding a rule, which will be used in the experiments:
Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ However, both of these efforts only use 1-best parses, with the second one packing different binarizations of the same tree in a forest.
Instead of using a static pruning threshold (Mi and Huang 2008), we set the threshold as the distance of the probabilities of the nth best tree and the 1st best tree. $$$$$ We would also like to thank Qun Liu for supporting this work, and the three anonymous reviewers for improving the earlier version.

Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ In this case, a larger tree fragment rooted at VPB has to be extracted.
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ So we propose a novel approach which extracts rules from a packed forest that compactly encodes exponentially many parses.
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008).
Mi and Huang (2008) propose a forest-based rule extraction algorithm, which learn tree to string rules from source forest and target string. $$$$$ So we propose a novel approach which extracts rules a forest compactly encodes exponentially many parses.

As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ These BLEU score results are summarized in Table 2, which also shows that decoding with forest-extracted rules is less than twice as slow as with 1-best rules, and only fractionally slower than with 30-best rules.
As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ The first direct application of parse forest in translation is our previous work (Mi et al., 2008) which translates a packed forest from a parser; it is also the base system in our experiments (see below).
As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
As we know, the traditional tree-to-string rules can be easily extracted from ? using the algorithm of Mi and Huang (2008). $$$$$ Typically, these models extract rules using parse trees from both or either side(s) of the bitext.

Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ So the Galley et al. (2004) algorithm for tree-based rule extraction (Sec.
Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ The current dominant practice only uses 1-best trees, which adversely affects the rule set quality due to parsing errors.
Mi and Huang (2008) extend the tree-based rule extraction. $$$$$ But the real question is, are these extra rules really useful in generating the final (1-best) translation?

Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses.
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ Typically, these models extract rules using parse trees from both or either side(s) of the bitext.
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ For example, the tree in Figure 3 is cut into 6 pieces, each of which corresponds to a rule on the right.
Employ the forest-based tree rule extraction algorithm (Mi and Huang, 2008) to extract our rules from the non-complete forest. $$$$$ In this paper, we have presented a novel approach that extracts translation rules from a packed forest encoding exponentially many trees, rather than from 1-best or k-best parses.

Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ When combined with our previous work on forest-based decoding, the final result is even better than the hierarchical system Hiero.
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ To make things worse, modern statistical parsers are often trained on domains quite different from those used in MT.
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
Then we can easily extract our rules from the CF using the tree rule extraction algorithm (Mi and Huang, 2008). $$$$$ However, a k-best list, with its limited scope, has too few variations and too many redundancies (Huang, 2008).

Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ This work was funded by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.
Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ Besides tree-to-string systems, our method is also applicable to other paradigms such as the string-totree models (Galley et al., 2006) where the rules are in the reverse order, and easily generalizable to pairs of forests in tree-to-tree models.
Finally, to calculate rule feature probabilities for our model, we need to calculate the fractional counts (it is a kind of probability defined in Mi and Huang, 2008) of each translation rule in a parse forest. $$$$$ The input string is first parsed by a parser into a 1-best tree, which will then be converted to a target language string by applying a set of tree-to-string transformation rules.
