In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. $$$$$ In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2).
In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. $$$$$ Our task will be to propose a summary S consisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1).
In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. $$$$$ Acknowledgements The authors would like to thank Bob Moore, Chris Brockett, Chris Quirk, and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments.
In TOPICSUM (Haghighi and Vanderwende, 2009), each word is generated by a single topic which can be a corpus-wide background distribution over common words, a distribution of document-specific words or a distribution of the core content of a given cluster. $$$$$ These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended.

Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). $$$$$ These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended.
Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). $$$$$ This model outperforms PYTHY with and without sentence simplification, but not with statistical significance.
Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). $$$$$ This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution.
Models that use more structure in the representation of documents have also been proposed for generating more coherent and less redundant summaries, such as HIERSUM (Haghighi and Vanderwende, 2009) and TTM (Celikyilmaz and Hakkani-Tur, 2011). $$$$$ In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams)6.

In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. $$$$$ We present an exploration of generative probabilistic models for multi-document summarization.
In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. $$$$$ In SUMBASIC, each sentence where PD(·) initially reflects the observed unigram probabilities obtained from the document collection D. A summary S is progressively built by adding the highest scoring sentence according to (1).7 In order to discourage redundancy, the words in the selected sentence are updated PDnew (w) a til the summary word limit has been reached.
In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. $$$$$ At the task of producing generic DUC-style summaries, state-of-the-art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al. (2007)’s state-of-the-art discriminative system.
In our experiments, we follow the same approach as in (Haghighi and Vanderwende, 2009) by greedily adding sentences to a summary so long as they decrease KL divergence. $$$$$ 12In contrast to previous models, stop words are not removed in pre-processing.

Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by Haghighiand Vanderwende (2009). $$$$$ Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.
Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by Haghighiand Vanderwende (2009). $$$$$ In order to provide a reference for ROUGE and manual evaluation results, we compare against PYTHY, a state-of-the-art supervised sentence extraction summarization system.
Once this is done, one of the learned collections can be used to generate the summary that best approximates this collection, using the greedy algorithm described by Haghighiand Vanderwende (2009). $$$$$ 24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams.

The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighiand Vanderwende, 2009) were defined over single words (unigrams). $$$$$ Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system-generated summary against a set of humangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries.
The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighiand Vanderwende, 2009) were defined over single words (unigrams). $$$$$ This system, labeled HIERSUM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperforming HIERSUM unigram.
The original implementations of SUMBASIC (Nenkova and Vanderwende, 2005) and TOPICSUM (Haghighiand Vanderwende, 2009) were defined over single words (unigrams). $$$$$ All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004).

This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. $$$$$ The order of automatic summaries was determined randomly. focused in its content, not conveying irrelevant details?
This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. $$$$$ In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics.
This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. $$$$$ We present an exploration of generative probabilistic models for multi-document summarization.
This model is very similar to the one used by Haghighi and Vanderwende (2009) in the context of text summarization. $$$$$ While it is difficult to qualitatively compare one summarization system over another, we can broadly characterize HIERSUM summaries compared to some of the other systems discussed.

Effective ways of representing content and ensuring coverage are the subject of ongoing research in the field (e.g., Gillick et al 2009, Haghighi and Vanderwende 2009). $$$$$ Since globally optimizing the KLSUM criterion in equation (equation (2)) is exponential in the total number of sentences in a document collection, we 21We choose σ = 0.75 in our experiments. opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence.
Effective ways of representing content and ensuring coverage are the subject of ongoing research in the field (e.g., Gillick et al 2009, Haghighi and Vanderwende 2009). $$$$$ In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics.
Effective ways of representing content and ensuring coverage are the subject of ongoing research in the field (e.g., Gillick et al 2009, Haghighi and Vanderwende 2009). $$$$$ Previous sections have treated the content of a document set as a single (perhaps learned) unigram distribution.
Effective ways of representing content and ensuring coverage are the subject of ongoing research in the field (e.g., Gillick et al 2009, Haghighi and Vanderwende 2009). $$$$$ The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.

The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. $$$$$ Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.
The most relevant work is by (Haghighi and Vanderwende, 2009) on exploring content models for multi-document summarization. $$$$$ In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics.

Entity-aspect model is similar with 'HIERSUM' content model proposed by Haghighi and Vanderwende (2009). $$$$$ In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific ‘subtopics’ within a document set.
Entity-aspect model is similar with 'HIERSUM' content model proposed by Haghighi and Vanderwende (2009). $$$$$ For each sentence S of each document D, draw a distribution ψT over topics (CONTENT, DOCSPECIFIC, BACKGROUND) from a Dirichlet prior with pseudo-counts (1.0, 5.0,10.0).14 For each word position in the sentence, we draw a topic Z from ψT, and a word W from the topic distribution Z indicates.
Entity-aspect model is similar with 'HIERSUM' content model proposed by Haghighi and Vanderwende (2009). $$$$$ In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2).

In this baseline, we directly compare our method with "HIERSUM" proposed by (Haghighi and Vanderwende, 2009). $$$$$ The reason for this might be that PYTHY is discriminatively trained to maximize ROUGE which does not directly penalize redundancy.
In this baseline, we directly compare our method with "HIERSUM" proposed by (Haghighi and Vanderwende, 2009). $$$$$ Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.
In this baseline, we directly compare our method with "HIERSUM" proposed by (Haghighi and Vanderwende, 2009). $$$$$ In this task we assume a document collection D consisting of documents Di, ... , D,,, describing the same (or closely related) narrative (Lapata, 2003). set of events.
In this baseline, we directly compare our method with "HIERSUM" proposed by (Haghighi and Vanderwende, 2009). $$$$$ Reflecting intuition that adjacent sentences are likely to share specific content vocabulary, we utilize a ‘sticky’ HMM as in Barzilay and Lee (2004) over the each sentences’ ZS.

One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. $$$$$ We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.
One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. $$$$$ We present R-2 without stop words in the running text, but full development results are presented in table 1.
One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. $$$$$ We may use HIERSUM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by 0C1, ... , 0CK (for an example see figure 3).
One could also think of this as a version of the KLSum summarization system (Haghighi and Vanderwende, 2009) that stops after one sentence. $$$$$ We also contend that they provide convenient building blocks for adding more structure to a summarization model.

In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. $$$$$ Document collections presented to users were randomly selected from those evaluated fewest.
In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. $$$$$ We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.
In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. $$$$$ However when the CONTENT topic is drawn, we must decide whether to emit a general content word (from 0C0) or from one of the specific content distributions (from one of 0Ci for i = 1, ... , K).
In more recent work, Haghighi and Vanderwende (2009) built a summarization system based on topic models, where both topics at general document level as well as those at specific subtopic levels were learnt. $$$$$ We explore capacity to produce multiple ‘topical summaries’ in order to facilitate content discovery and navigation.

Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization.
Such methods have been successfully applied to a myriad of tasks including word sense discrimination (Brody and Lapata, 2009), document summarisation (Haghighi and Vanderwende, 2009), areal linguistic analysis (Daume III, 2009) and text segmentation (Sun et al, 2008). $$$$$ For example output from HIERSUM and PYTHY see table 2.

The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. $$$$$ Acknowledgements The authors would like to thank Bob Moore, Chris Brockett, Chris Quirk, and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments.
The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. $$$$$ 24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams.
The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. $$$$$ In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007).
The following models are used as benchmark: (i) PYTHY (Toutanova et al, 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. $$$$$ The resulting model, HIERSUM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics.

Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. $$$$$ Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004).
Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. $$$$$ Acknowledgements The authors would like to thank Bob Moore, Chris Brockett, Chris Quirk, and Kristina Toutanova for their useful discussions as well as the reviewers for their helpful comments.
Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. $$$$$ Our model, utilizes a hierarchical LDA-style model (Blei et al., 2004) to represent content specificity as a hierarchy of topic vocabulary distributions.
Haghighi and Vanderwende (2009) demonstrated that these models can improve the quality of generic multi-document summaries over simpler surface models. $$$$$ All topic models utilize Gibbs sampling for inference (Griffiths, 2002; Blei et al., 2004).

We re-implement the HIERSUM system from Haghighi and Vanderwende (2009), and show that using our objective dramatically improves the content of extracted summaries. $$$$$ It is safe to conclude that users in this study strongly preferred the HIERSUM summaries over the PYTHY summaries.
We re-implement the HIERSUM system from Haghighi and Vanderwende (2009), and show that using our objective dramatically improves the content of extracted summaries. $$$$$ In the common Document Understanding Conference (DUC) formulation of the task, a system takes as input a document set as well as a short description of desired summary focus and outputs a word length limited summary.1 To avoid the problem of generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007).
We re-implement the HIERSUM system from Haghighi and Vanderwende (2009), and show that using our objective dramatically improves the content of extracted summaries. $$$$$ Over the past several years, there has been much interest in the task of multi-document summarization.
We re-implement the HIERSUM system from Haghighi and Vanderwende (2009), and show that using our objective dramatically improves the content of extracted summaries. $$$$$ Lin et al. (2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems.

This idea was first presented by Daume and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by Haghighi and Vanderwende (2009). $$$$$ In addition to presenting automated results, we also present a user evaluation in section 5.2.
This idea was first presented by Daume and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by Haghighi and Vanderwende (2009). $$$$$ These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended.
This idea was first presented by Daume and Marcu (2006) for their BAYESUM system for query-focused summarization, and later adapted for non-query summarization in the TOPICSUM system by Haghighi and Vanderwende (2009). $$$$$ In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of structured topic models can benefit summarization quality as measured by automatic and manual metrics.

Haghighi and Vanderwende (2009) presented a version of HIERSUM that models documents as a bag of bigrams, and provides results comparable to PYTHY. $$$$$ Beginning with a simple word frequency based model (Nenkova and Vanderwende, 2005), we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way.
Haghighi and Vanderwende (2009) presented a version of HIERSUM that models documents as a bag of bigrams, and provides results comparable to PYTHY. $$$$$ 24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams.
Haghighi and Vanderwende (2009) presented a version of HIERSUM that models documents as a bag of bigrams, and provides results comparable to PYTHY. $$$$$ The study had 16 users and each was asked to compare five summary pairs, although some did fewer.
Haghighi and Vanderwende (2009) presented a version of HIERSUM that models documents as a bag of bigrams, and provides results comparable to PYTHY. $$$$$ However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization.

These are based on the manual evaluation questions from DUC 2007, and are the same questions asked in Haghighi and Vanderwende (2009). $$$$$ In order to ensure tight lexical cohesion amongst the specific topics, we assume that each sentence draws a single specific topic ZS used for every specific content word in that sentence.
These are based on the manual evaluation questions from DUC 2007, and are the same questions asked in Haghighi and Vanderwende (2009). $$$$$ Since globally optimizing the KLSUM criterion in equation (equation (2)) is exponential in the total number of sentences in a document collection, we 21We choose σ = 0.75 in our experiments. opted instead for a simple approximation where sentences are greedily added to a summary so long as they decrease KL-divergence.
These are based on the manual evaluation questions from DUC 2007, and are the same questions asked in Haghighi and Vanderwende (2009). $$$$$ Document collections presented to users were randomly selected from those evaluated fewest.
These are based on the manual evaluation questions from DUC 2007, and are the same questions asked in Haghighi and Vanderwende (2009). $$$$$ In order to obtain a more accurate measure of summary quality, we performed a simple user study.

Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). $$$$$ We attempted more complex inference procedures such as McDonald (2007), but these attempts only yielded negligible performance gains.
Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). $$$$$ Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004).
Such models also provide a framework for adding additional structure to a summarization model (Haghighi and Vanderwende, 2009). $$$$$ The most prevalent example of this data setting is document clusters found on news aggregator sites.
