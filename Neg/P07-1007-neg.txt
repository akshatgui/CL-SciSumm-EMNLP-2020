Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. $$$$$ Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship (ref no.
Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. $$$$$ The denominator in Equation (4) is simply a normalizing factor.
Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. $$$$$ This highlights the importance of domain adaptation for word sense disambiguation.
Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus. $$$$$ Then, by using the predominant sense predicted by expectation-maximization (EM) and adopting a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learning approach.

 $$$$$ Hence, we just use a β value of 3 in our experiments involving count-merging.
 $$$$$ This highlights the importance of domain adaptation for word sense disambiguation.
 $$$$$ The new a posteriori probabilities bp(s)(ωi|xk) at step s in Equation (4) are simply the a posteriori probabilities in the conditions of the labeled data, bpL(ωi|xk), weighted by the ratio of the new priors bp(s)(ωi) to the old priors bpL(ωi).

In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). $$$$$ We then investigate the utility of adding additional in-domain training data from WSJ.
In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). $$$$$ The approach represented by atruePred is a compromise between ensuring that the sense priors in the training data follow as closely as possible the sense priors in the evaluation data, while retaining enough training examples.
In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). $$$$$ In both papers, out-of-domain and indomain data were simply mixed together without MAP estimation such as count-merging.
In building training dataset by active learning, we use uncertainty sampling like (Chan and Ng, 2007) (Figure 1 line 30-31). $$$$$ We let c denote the counts from out-of-domain training data, c� denote the counts from in-domain adaptation data, and p� denote the probability estimate by count-merging.

Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation. $$$$$ Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship (ref no.
Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation. $$$$$ SMF-20041076).
Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation. $$$$$ Performance of domain adaptation using active learning and count-merging is then presented.
Instance weighting (Jiang and Zhai, 2007) and active learning (Chan and Ng, 2007) are also employed in domain adaptation. $$$$$ To determine the a priori probability estimates bp(ωi) of the new data set that will maximize the likelihood of (3) with respect to p(ωi), we can apply the iterative procedure of the EM algorithm.

AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where Inter Annotator Agreement (IAA) is low and thus the consistency of the human annotations decreases. $$$$$ With this information on the predominant sense of the new domain and incorporating count-merging, we have shown that we are able to improve the effectiveness of the original adaptation process achieved by the basic active learning approach.
AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where Inter Annotator Agreement (IAA) is low and thus the consistency of the human annotations decreases. $$$$$ In effect, through maximizing the likelihood of (3), we obtain the a priori probability estimates as a by-product.
AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where Inter Annotator Agreement (IAA) is low and thus the consistency of the human annotations decreases. $$$$$ Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship (ref no.
AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where Inter Annotator Agreement (IAA) is low and thus the consistency of the human annotations decreases. $$$$$ Assuming we have access to an “oracle” which determines the predominant sense, or most frequent sense (MFS), of each noun in our WSJ test data perfectly, and we assign this most frequent sense to each noun in the test data, we will have achieved an accuracy of 61.1% as shown in the column MFS accuracy of Table 1.

Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al, 2007), parsing (Tang et al, 2002) and word sense disambiguation (Chan and Ng, 2007). $$$$$ Most of this section is based on (Saerens et al., 2002).
Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al, 2007), parsing (Tang et al, 2002) and word sense disambiguation (Chan and Ng, 2007). $$$$$ In this paper, we first show that an active learning approach can be successfully used to perform domain adaptation of WSD systems.
Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al, 2007), parsing (Tang et al, 2002) and word sense disambiguation (Chan and Ng, 2007). $$$$$ SMF-20041076).

Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data. $$$$$ We also employ a technique known as countmerging in our domain adaptation study.
Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data. $$$$$ The adaptation process continues until all the adaptation examples are added.
Here, we take inspiration from the target-word specific results reported by Chan and Ng (2007) where by using just 30% of the target data they obtained the same performance as that obtained by using the entire target data. $$$$$ SMF-20041076).

 $$$$$ Countmerging assigns different weights to different examples to better reflect their relative importance.
 $$$$$ (2004) performed language model adaptation of automatic speech recognition systems.
 $$$$$ Since BC is a balanced corpus, and since priors across different domains.
 $$$$$ In applying active learning for domain adaptation, Zhang et al. (2003) presented work on sentence boundary detection using generalized Winnow, while Tur et al.

Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al (2005) and Agirre et al (2009b). $$$$$ (2004) performed language model adaptation of automatic speech recognition systems.
Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al (2005) and Agirre et al (2009b). $$$$$ This is higher than an average of 83 BC examples for these 9 nouns if BC examples are selected to follow the sense priors of WSJ evaluation data as described in the last subsection 6.2.
Domain specific WSD for selected target words has been attempted by Ng and Lee (1996), Agirre and de Lacalle (2009), Chan and Ng (2007), Koeling et al (2005) and Agirre et al (2009b). $$$$$ When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed.

Our main inspiration comes from the target word specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). $$$$$ With this new set of training examples, we perform adaptation using active learning and obtain the a-truePrior curve in Figure 2.
Our main inspiration comes from the target word specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). $$$$$ SMF-20041076).
Our main inspiration comes from the target word specific results reported by Chan and Ng (2007) and Agirre and de Lacalle (2009). $$$$$ 2 Experimental Setting In this section, we discuss the motivations for choosing the particular corpus and the set of nouns to conduct our domain adaptation experiments.

With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. $$$$$ Countmerging assigns different weights to different examples to better reflect their relative importance.
With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. $$$$$ When a word sense disambiguation (WSD) system is trained on one domain but applied to a different domain, a drop in accuracy is frequently observed.
With the exception of (Chan and Ng, 2007) which tried to adapt a WSD system trained on the BC part of the DSO corpus to the WSJ part of the DSO corpus, the other researchers simply applied active learning to reduce the annotation effort required and did not deal with the issue of adapting a WSD system to a new domain. $$$$$ Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship (ref no.

It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). $$$$$ With this information on the predominant sense of the new domain and incorporating count-merging, we have shown that we are able to improve the effectiveness of the original adaptation process achieved by the basic active learning approach.
It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). $$$$$ The a posteriori bp(s)(ωi|xk) and a priori probabilities bp(s)(ωi) are re-estimated sequentially during each iterations for each new instance xk and each class ωi, until the convergence of the estimated probabilities bp(s)(ωi), which will be our estimated sense priors.
It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). $$$$$ SMF-20041076).
It has also been applied to the problem of domain adaptation for word sense disambiguation in (Chan and Ng, 2007). $$$$$ Yee Seng Chan is supported by a Singapore Millennium Foundation Scholarship (ref no.

Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. $$$$$ In both papers, out-of-domain and indomain data were simply mixed together without MAP estimation such as count-merging.
Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. $$$$$ Countmerging assigns different weights to different examples to better reflect their relative importance.
Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. $$$$$ Next, we describe an EMbased algorithm to estimate the sense priors in the new domain.
Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. $$$$$ In this paper, we have shown that active learning is effective in reducing the annotation effort required in porting a WSD system to a new domain.
