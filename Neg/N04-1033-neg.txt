We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ For the German-English Verbmobil task it is significantly lower.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ The described search is monotone at the phrase level.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ Some examples are the alignment template system in (Och et al., 1999; Och and Ney, 2002) that we used for comparison.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ Verbmobil Task.

We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ We present the formulae for a bigram language model.
We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ The same considerations hold for the monotone search.
We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ All experiments were carried out on an AMD Athlon with 2.2GHz.

We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ Q(J + 1, $) is the probability of the optimum translation.
We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ Table 1 shows the corpus statistics of this task.

The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ An alternative to the classical source-channel approach is the direct modeling of the posterior probability Pr(eI1|fJ1 ).
The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ It can be further decomposed into alignment and lexicon model.

There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ Here, we use a subset of the data containing only sentences with a maximum length of 30 words.
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ We do not perform the optimization on N-best lists but we retranslate the whole development corpus for each iteration of the optimization algorithm.
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ We have to maximize over all possible target language sentences.

Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. $$$$$ The phrase-based approach clearly outperformed the singleword based systems.
Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. $$$$$ (Marcu and Wong, 2002) presents a joint probability model for phrase-based translation.

Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ For the German-English Verbmobil task it is significantly lower.
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ The abbreviation for this variant is GE.
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ For the Canadian Hansards task, the translation process is much slower, but the average time per sentence is still less than 1 second.

 $$$$$ This is only for notational convenience.
 $$$$$ (Tomas and Casacuberta, 2003) describes a linear interpolation of a phrase-based and an alignment template-based approach.
 $$$$$ We obtain the following dynamic programming recursion.

In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards.
In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ For the Xerox task, the degree of monotonicity is 87.0%.
In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ Table 1 shows the corpus statistics of this task.
In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ In this section, we analyze the translation speed of the phrase-based translation system.

In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ Using efficient data structures and taking into account that not all possible target language phrases can occur in translating a specific source language sentence, we can perform a very efficient search.
In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ This leads to an impressive translation speed of more than 1000 words per second for the Verbmobil task and for the Xerox task.
In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ In the following sections, we will present results on three tasks: Verbmobil, Xerox and Canadian Hansards.
In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length.

(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ This is only for notational convenience.
(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ It consists of transcriptions of spontaneous speech.
(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ The system loads only phrase pairs into memory if the source phrase occurs in the test corpus.

The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ SpanishEnglish or French-English.
The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ We will see in the next section that the monotone search will perform very well on both the Xerox task and the Canadian Hansards task.
The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ The translation results for the Xerox and Canadian Hansards task are very promising.

In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ It penalizes reorderings with costs that are linear in the jump width.
In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ We see a clear linear dependency.
In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards.

 $$$$$ The abbreviation for this variant is GE.
 $$$$$ We think that this slowdown can be attributed to the large training corpus.
 $$$$$ The translation results for the Xerox and Canadian Hansards task are very promising.
 $$$$$ For the Xerox task, the nonmonotone search can produce 75.1% of the sentence pairs whereas the monotone can produce 65.3%.

For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ In statistical machine translation, the currently best performing systems are based in some way on phrases or word groups.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ Additionally, we carried out experiments on the Spanishâ€“English Xerox task.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ We obtain the following dynamic programming recursion.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ Within the phrases arbitrary reorderings are allowed.

combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ As refinements of the baseline model, we described two simple heuristics: the word penalty feature and the phrase penalty feature.
combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ The translation model links the source language sentence to the target language sentence.
combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ For the German-English Verbmobil task it is significantly lower.

This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ We compare this to the number of sentence pairs that can be produced with a nonmonotone phrase-based search.
This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ It takes about 1.6 seconds to translate the whole test set.

The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ We describe a highly efficient monotone search algorithm with a complexity linear in the input sentence length.
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ In statistical machine translation, we are given a source language (â€˜Frenchâ€™) sentence fJ1 = f1 ... fj ... fJ, which is to be translated into a target language (â€˜Englishâ€™) sentence eI1 = e1 ... ei ... eI.
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ We used the best performing systems to measure the translation times.
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ We have to maximize over all possible target language sentences.

The following methods were investigated $$$$$ In Fig.
The following methods were investigated $$$$$ This is also reflected by the large vocabulary size.
The following methods were investigated $$$$$ In (Koehn et al., 2003), various aspects of phrase-based systems are compared, e.g. the phrase extraction method, the underlying word alignment model, or the maximum phrase length.

We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ We will see in the next section that the monotone search will perform very well on both the Xerox task and the Canadian Hansards task.
