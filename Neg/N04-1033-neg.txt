We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ Therefore, the large test corpus size for this task also affects the translation speed.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ Q(J + 1, $) is the probability of the optimum translation.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ Note that the systems were not optimized for speed.
We use a phrase-based translation approach as described in (Zens and Ney, 2004). $$$$$ Table 4 shows the percentage of the training corpus that can be generated with monotone and nonmonotone phrase-based search.

We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ Table 2 shows the training, development and test corpus statistics.
We extended the monotone search algorithm from (Zens and Ney, 2004) such that reorderings are possible. $$$$$ We described a phrase-based translation approach.

We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ Table 1 shows the corpus statistics of this task.
We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ To make the results as comparable as possible, the alignment template system and the phrase-based system start from the same word alignment.
We exchange the baseline lexical scoring with a noisy-or (Zens and Ney, 2004) lexical scoring variant. $$$$$ The phrase-based approach clearly outperformed the singleword based systems.

The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ This is not only a hard search problem, but also a complicated modeling problem.
The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ It does not use the word alignment for extracting the phrases, but directly generates a phrase alignment.
The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). $$$$$ The experiments on the German-English Verbmobil task outlined the limitations of the monotone search.

There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ It has the advantage that additional models or feature functions can be easily integrated into the overall system.
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ For the Xerox task, the translation process takes less than 7 seconds for the whole 10K words test set.
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards.
There is, however, a large body of work using morphological analysis to define cluster-based translation models similar to ours but in a supervised manner (Zens and Ney, 2004), (Niessen and Ney, 2004). $$$$$ We describe the baseline phrase-based translation system and various refinements.

Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. $$$$$ We have to maximize over all possible target language sentences.
Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. $$$$$ The target language model describes the well-formedness of the target language sentence.
Our approach to phrase-table smoothing contrasts to previous work (Zens and Ney, 2004) in which smoothed phrase probabilities are constructed from word-pair probabilities and combined in a log-linear model with an unsmoothed phrase-table. $$$$$ For the maximization problem in (11), we define the quantity Q(j,e) as the maximum probability of a phrase sequence that ends with the language word e and covers positions 1 to j of the source sentence.

Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ The model scaling factors λM1 are trained according to the maximum entropy principle, e.g. using the GIS algorithm.
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ Nevertheless, when ignoring the word order and looking at the mPER only, the monotone search is competitive with the best performing system.
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ Then, we will describe refinements of the baseline model.
Each includes relative frequency estimates and lexical estimates (based on Zens and Ney, 2004) of forward and backward conditional probabilities. $$$$$ The resulting system is then evaluated on the test corpus (Test).

 $$$$$ This task contains the proceedings of the Canadian parliament.
 $$$$$ Verbmobil Task.
 $$$$$ Note that all these monotonicity consideration are done at the phrase level.

In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ In combination with the scaling factor this results in a constant cost per produced target language word.
In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ For the Xerox task, the translation process takes less than 7 seconds for the whole 10K words test set.
In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ Recently, phrase-based translation approaches became more and more popular.
In order to complete the conversion from a pipeline approach to a joint approach, we fold our input segmentation step into the exact search framework by replacing a separate segmentation module (#2) with a monotone phrasal decoder (Zens and Ney, 2004). $$$$$ In (Zens et al., 2002), a simple phrase-based approach is described that served as starting point for the system in this work.

In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ SpanishEnglish or French-English.
In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ (Tomas and Casacuberta, 2003) describes a linear interpolation of a phrase-based and an alignment template-based approach.
In the joint approach (Figure 1c), we perform segmentation and L2P prediction simultaneously by applying the monotone search algorithm developed for statistical machine translation (Zens and Ney, 2004). $$$$$ We used the best performing systems to measure the translation times.

(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ The first task we will present results on is the German–English Verbmobil task (Wahlster, 2000).
(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ Therefore, we use the following approach: first, we train statistical alignment models using GIZA++ and compute the Viterbi word alignment of the training corpus.
(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ Compared to the AT system, the BLEU score improves by 4.1% absolute.
(Logic MONOTONE) This is the algorithm of Zens and Ney (2004). $$$$$ Therefore on this task, we compute all the preceding criteria with respect to multiple references.

The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ Here, Ve denotes the vocabulary size of the target language and E denotes the maximum number of phrase translation candidates for a source language phrase.
The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ The translation times were measured for the translation of the 5432 test sentences of the Canadian Hansards task.
The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ The resulting complexity is linear in the sentence length.
The First d Uncovered Words strategy (FdUW) is described by Tillman and Ney (2003) and Zens and Ney (2004), who call it the IBM Constraint. $$$$$ After performing the search, we can generate the optimum translation.

In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ The lexicon probabilities are based only on single words.
In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ For the Verbmobil task, the system is even slightly faster.
In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ It takes about 1.6 seconds to translate the whole test set.
In (Zens and Ney, 2004) the downhill simplex method is used to estimate the weights; around 200 iterations are required for convergence to occur. $$$$$ Table 1 shows the corpus statistics of this task.

 $$$$$ (Marcu and Wong, 2002) presents a joint probability model for phrase-based translation.
 $$$$$ This means the monotone search can produce 87.0% of the sentence pairs that can be produced with the nonmonotone search.
 $$$$$ Among all possible target language sentences, we will choose the sentence with the highest probability: The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al., 1990).
 $$$$$ It takes about 1.6 seconds to translate the whole test set.

For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ For the maximization problem in (11), we define the quantity Q(j,e) as the maximum probability of a phrase sequence that ends with the language word e and covers positions 1 to j of the source sentence.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ The translation results of the different systems are shown in Table 6.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ Nevertheless, when ignoring the word order and looking at the mPER only, the monotone search is competitive with the best performing system.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004). $$$$$ The next section contains the statistics of the corpora that were used.

combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.
combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ Among all possible target language sentences, we will choose the sentence with the highest probability: The decomposition into two knowledge sources in Equation 2 is known as the source-channel approach to statistical machine translation (Brown et al., 1990).
combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ In the following sections, we will present results on three tasks: Verbmobil, Xerox and Canadian Hansards.
combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. $$$$$ Using a log-linear model (Och and Ney, 2002), we obtain: Here, Z(fJ1 ) denotes the appropriate normalization constant.

This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ Even for the Canadian Hansards task the translation of sentences of length 30 takes only about 1.5 seconds.
This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ Therefore, we use the following approach: first, we train statistical alignment models using GIZA++ and compute the Viterbi word alignment of the training corpus.
This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ We are using relative frequencies to estimate the phrase translation probabilities.
This finding fails to echo the promising results in the previous study (Zens and Ney,2004). $$$$$ Recently, phrase-based translation approaches became more and more popular.

The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ It allows an independent modeling of target language model Pr(eI1) and translation model Pr(fJ1 |eI1)1.
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ For the Canadian Hansards task, the translation process is much slower, but the average time per sentence is still less than 1 second.
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ (Tomas and Casacuberta, 2003) describes a linear interpolation of a phrase-based and an alignment template-based approach.
The source text, annotated with name translations, is then passed to a statistical, phrase-based MT system (Zens and Ney, 2004). $$$$$ We see that for the SpanishEnglish Xerox task and for the French-English Canadian Hansards task, the degree of monotonicity is rather high.

The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. $$$$$ (Tomas and Casacuberta, 2003) describes a linear interpolation of a phrase-based and an alignment template-based approach.
The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. $$$$$ Here, M denotes the maximum phrase length in the source language.
The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. $$$$$ During the search, we store backpointers to the maximizing arguments.
The following methods were investigated: (Monotone) Phrase-based MT on character level: A state-of-the-art phrase-based SMT system (Zens and Ney, 2004) was used for name transliteration, i.e. translation of characters instead of words. $$$$$ This happens if a sentence pair contains a word that occurs only once in the training corpus.

We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ The experiments on the German-English Verbmobil task outlined the limitations of the monotone search.
We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ Therefore, there is no constraint on the reordering within the phrases.
We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ The first one is a uniform distribution and the second one is the unigram distribution: Here, Vf denotes the vocabulary size of the source language and N(f) denotes the unigram count of a source word f.
We use the RWTH Aachen Chinese-to-English statistical phrase-based machine translation system (Zens and Ney, 2004) for these purposes. $$$$$ We present translation results for three tasks: Verbmobil, Xerox and the Canadian Hansards.
