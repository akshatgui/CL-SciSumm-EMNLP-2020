As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are statistically very significant.
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ Remarkably, this is the same rank order assigned to these “systems” by human judges, as we discuss later.
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ To account for the intrinsic difference between judges and the sentences, we compared each judge’s rating for a sentence across systems.

For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ The high correlation coefficient of 0.99 indicates that BLEU tracks human judgment well.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Human evaluations of machine translation are extensive but expensive.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe BLEU could be adapted to evaluating summarization or similar NLG tasks.

One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ Human evaluations can take months to finish and involve human labor that can not be reused.
One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ The systems maintain the same rank order as with multiple references.
One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ These translations may vary in word choice or in word order even when they use the same words.

We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ The first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate.
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ We prepared a web page with these translation pairs randomly ordered to disperse the five translations of each source sentence.

The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ The high correlation coefficient of 0.99 indicates that BLEU tracks human judgment well.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ For comparison, we acquired human translations of the same documents by a native English speaker.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.

BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ Again, the difference between the human translators is significant beyond the 95% level.
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ The MT systems S2 and S3 are very close in this metric.
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of ChineseEnglish MT systems.
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ The longer n-gram matches account for fluency.

The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent
The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas.
The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ We had two groups of human judges.
The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ 5 BLEU vs The Human Evaluation Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems.

We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas.
We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ BLEU’s strength is that it correlates highly with human judg$Crossing this chasm for Chinese-English translation appears to be a significant challenge for the current state-of-the-art systems. ments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.
We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.
We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ N66001-99-2-8916.

We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ For example, t = 6 for the pair S1 and S2.
We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ The systems maintain the same rank order as with multiple references.
We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent

We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas.
We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ We thus have 20 samples of the BLEU metric for each system.
We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ BLEU’s strength is that it correlates highly with human judg$Crossing this chasm for Chinese-English translation appears to be a significant challenge for the current state-of-the-art systems. ments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.

 $$$$$ The first candidate recalls more words from the references, but is obviously a poorer translation than the second candidate.
 $$$$$ Human evaluations of machine translation are extensive but expensive.
 $$$$$ We propose such an evaluation method in this paper.

BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ The humans judged our 5 standard systems on a Chinese sentence subset extracted at random from our 500 sentence test corpus.
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ For this reason, even a human translator will not necessarily score 1.
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ Moreover, they can take weeks or months to finish.
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ In Section 5, we compare our baseline metric performance with human evaluations.

Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ As must be expected, some judges were more liberal than others.
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ We had two groups of human judges.
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ Thus, our MT evaluation system requires two ingredients: We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order.
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ The t-statistic compares each system with its left neighbor in the table.

A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe BLEU could be adapted to evaluating summarization or similar NLG tasks.
A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ This is not surprising given that H1 is not a native speaker of either Chinese or English, whereas H2 is a native English speaker.
A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas.

The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ Furthermore, it must distinguish between two human translations of differing quality.
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ Thus, naive recall computed over the set of all reference words is not a good measure.
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ Table 1 shows the BLEU scores of the 5 systems against two references on this test corpus.
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ This is the central idea behind our proposal.

Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics $$$$$ How many reference translations do we need?
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics $$$$$ Human evaluations of machine translation are extensive but expensive.
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics $$$$$ The MT systems S2 and S3 are very close in this metric.
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics $$$$$ How does one measure translation performance?

To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ Both differences are significant at the 95% level.7 The human H1 is much better than the best system, though a bit worse than human H2.
To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ In Section 5, we compare our baseline metric performance with human evaluations.

To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed $$$$$ We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas.
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed $$$$$ The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus.
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed $$$$$ A translation using the same words (1-grams) as in the references tends to satisfy adequacy.
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed $$$$$ To this end, we obtained a human translation by someone lacking native proficiency in both the source (Chinese) and the target language (English).

For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ Furthermore, it must distinguish between two human translations of differing quality.
For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of ChineseEnglish MT systems.
For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ Figure 3 shows the mean difference between the scores of two consecutive systems and the 95% confidence interval about the mean.
For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ Human evaluations of machine translation are extensive but expensive.

For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ Human evaluations can take months to finish and involve human labor that can not be reused.
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ We especially wish to thank our colleagues who served in the monolingual and bilingual judge pools for their perseverance in judging the output of ChineseEnglish MT systems.
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
