As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ Thus, our MT evaluation system requires two ingredients: We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order.
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ We believe that BLEU will accelerate the MT R&D cycle by allowing researchers to rapidly home in on effective modeling ideas.
As an overall decoding performance measure, we used the BLEU metric (Papineni et al, 2002). $$$$$ We wish to make the brevity penalty 1.0 when the candidate’s length is the same as any reference translation’s length.

For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ 5 BLEU vs The Human Evaluation Figure 5 shows a linear regression of the monolingual group scores as a function of the BLEU score over two reference translations for the 5 systems.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ We performed four pairwise t-test comparisons between adjacent systems as ordered by their aggregate average score.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ Human evaluations of machine translation are extensive but expensive.
For the bilingual tasks, the publicly available system of Moses (Koehn et al, 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al, 2002) was used to evaluate the quality. $$$$$ In Section 3, we evaluate the performance of BLEU.

One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ The figure also highlights the relatively large gap between MT systems and human translators.8 In addition, we surmise that the bilingual group was very forgiving in judging H1 relative to H2 because the monolingual group found a rather large difference in the fluency of their translations.
One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe BLEU could be adapted to evaluating summarization or similar NLG tasks.
One of the standards for such tuning is minimum error rate training (MERT) (Och, 2003), which directly minimize the loss of translation evaluation measures, i.e. BLEU (Papineni et al, 2002). $$$$$ This view gives rise to a family of metrics using various weighting schemes.

We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ Table 1 shows the BLEU scores of the 5 systems against two references on this test corpus.
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ Human evaluations can take months to finish and involve human labor that can not be reused.
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ We first compute the test corpus’ effective reference length, r, by summing the best match lengths for each candidate sentence in the corpus.
We evaluate the translation quality using case-insensitive BLEU metric (Papineni et al., 2002) without dropping OOV words, and the feature weights are tuned by minimum error rate training (Och, 2003). $$$$$ The figure also highlights the relatively large gap between MT systems and human translators.8 In addition, we surmise that the bilingual group was very forgiving in judging H1 relative to H2 because the monolingual group found a rather large difference in the fluency of their translations.

The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ Human evaluations of machine translation are extensive but expensive.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ We also obtained machine translations by three commercial systems.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ BLEU’s strength is that it correlates highly with human judg$Crossing this chasm for Chinese-English translation appears to be a significant challenge for the current state-of-the-art systems. ments by averaging out individual sentence judgment errors over a test corpus rather than attempting to divine the exact human judgment for every sentence: quantity leads to quality.
The experiments were evaluated using BLEU (Papineni et al, 2002) and METEOR (Lavie and Agarwal, 2007). $$$$$ We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories.

BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ It appears that any single n-gram precision score can distinguish between a good translation and a bad translation.
BLEU (Papineni et al 2002) is a system for automatic evaluation of machine translation. $$$$$ In Section 4, we describe a human evaluation experiment.

The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ 4BLEU only needs to match human judgment when averaged over a test corpus; scores on individual sentences will often vary from human judgments.
The baseline score using all phrase pairs was 59.11 (BLEU, Papineni et al, 2002) with a 95% confidence interval of [57.13, 61.09]. $$$$$ They rated each translation from 1 (very bad) to 5 (very good).

We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ We propose such an evaluation method in this paper.
We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ And yet humans can clearly distinguish a good translation from a bad one.
We utilize BLEU (Papineni et al, 2002) for the automatic evaluation of MT quality in this paper. $$$$$ We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent

We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ Finally, since MT and summarization can both be viewed as natural language generation from a textual context, we believe BLEU could be adapted to evaluating summarization or similar NLG tasks.
We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
We performed 4 runs of 10-fold cross validation, and measured the performance of the learned generators using the BLEU score (Papineni et al, 2002) and the NIST score (Doddington, 2002). $$$$$ None of the human judges was a professional translator.

We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ Of particular interest is the accuracy of BLEU’s estimate of the small difference between S2 and S3 and the larger difference between S3 and H1.
We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ A comprehensive catalog of MT evaluation techniques and their rich literature is given by Reeder (2001).
We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ Few translations will attain a score of 1 unless they are identical to a reference translation.
We employ the phrase-based SMT framework (Koehn et al, 2003), and use the Moses toolkit (Koehn et al, 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al, 2002), using a single reference translation. $$$$$ We computed the means, variances, and paired t-statistics which are displayed in Table 2.

 $$$$$ The closer a machine translation is to a professional human translation, the better it is.
 $$$$$ The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred.

BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ To compute precision, one simply counts up the number of candidate translation words (unigrams) which occur in any reference translation and then divides by the total number of words in the candidate translation.
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ The first group, called the monolingual group, consisted of 10 native speakers of English.
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ This figure illustrates the high correlation between the BLEU score and the monolingual group.
BLEU (Papineni et al, 2002b; Papineni et al, 2002a) showed high correlation with human judgments and is still used as the de facto standard automatic evaluation metric. $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.

Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ In Example 1, Candidate 1 achieves a modified unigram precision of 17/18; whereas Candidate 2 achieves a modified unigram precision of 8/14.
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ We simulated a single-reference test corpus by randomly selecting one of the 4 reference translations as the single reference for each of the 40 stories.
Many widely used metrics like Bleu (Papineni et al, 2002) and Ter (Snover et al, 2006) are based on measuring string level similarity between the reference translation and translation hypothesis, just like Meteor. $$$$$ This is the central idea behind our proposal.

A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ The systems maintain the same rank order as with multiple references.
A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ Human evaluations can take months to finish and involve human labor that can not be reused.
A common criterion to optimize the coefficients of the log-linear combination of feature functions is to maximize the BLEU score (Papineni et al, 2002) on a development set (Och and Ney, 2002). $$$$$ Since a paired t-statistic of 1.7 or above is 95% significant, the differences between the systems’ scores are statistically very significant.

The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ This figure illustrates the high correlation between the BLEU score and the monolingual group.
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ This is a big problem because developers of machine translation systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas.
The results show a statistically-significant (p < 0.1) improvement in terms of both BLEU (Papineni et al., 2002) and Meteor (Lavie et al, 2004a) scores. $$$$$ Thus, our MT evaluation system requires two ingredients: We fashion our closeness metric after the highly successful word error rate metric used by the speech recognition community, appropriately modified for multiple reference translations and allowing for legitimate differences in word choice and word order.

Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). $$$$$ S3 appears better than S2 which in turn appears better than S1.
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
Thus, extrinsic evaluation was carried out on the MT quality using the well known automatic MT evaluation metrics: BLEU (Papineni et al, 2002) and NIST (Doddington, 2002). $$$$$ Next, let c be the length of the candidate translation and r be the effective reference corpus length.

To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ This latter requirement ensures the continued validity of the metric as MT approaches human translation quality.
To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ A translation using the same words (1-grams) as in the references tends to satisfy adequacy.
To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ Human evaluations of machine translation are extensive but expensive.
To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al, 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. $$$$$ Particularly interesting is how well BLEU distinguishes between S2 and S3 which are quite close.

To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. $$$$$ We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. $$$$$ We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. $$$$$ In Section 4, we describe a human evaluation experiment.
To evaluate sentence automatically generated with taking consideration word concatenation into by using references varied among humans, various metrics using n-gram precision and word accuracy have been proposed: word string precision (Hori and Furui, 2000b) for summarization through word extraction, ROUGE (Lin and Hovy, 2003) for abstracts, and BLEU (Papineni et al, 2002) for machine translation. $$$$$ N66001-99-2-8916.

For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ This figure illustrates the high correlation between the BLEU score and the monolingual group.
For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ The reported variance on 25-sentence blocks serves as an upper bound to the variance of sizeable test sets like the 500 sentence corpus.
For both systems, we report BLEU scores (Papineni et al, 2002) on untokenized, recapitalized output. $$$$$ BLEU uses the average logarithm with uniform weights, which is equivalent to using the geometric mean of the modified n-gram precisions.5,6 Experimentally, we obtain the best correlation with monolingual human judgments using a maximum n-gram order of 4, although 3-grams and 5-grams give comparable results.

For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ The strong signal differentiating human (high precision) from machine (low precision) is striking.
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ Remarkably, this is the same rank order assigned to these “systems” by human judges, as we discuss later.
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ A candidate translation should be neither too long nor too short, and an evaluation metric should enforce this.
For a certain bilingual test dataset d, we consider a set of observations Od={ (x1 ,y1), (x2 ,y2) ... (xn ,yn)}, where yi is the performance on d (measured using BLEU (Papineni et al, 2002)) of a translation model trained on a parallel corpus of size xi. $$$$$ For example, a system which produces the fluent phrase “East Asian economy” is penalized heavily on the longer n-gram precisions if all the references happen to read “economy of East Asia.” The key to BLEU’s success is that all systems are treated similarly and multiple human translators with different styles are used, so this effect cancels out in comparisons between systems.
