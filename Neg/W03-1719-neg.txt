For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). $$$$$ We feel that this First International Chinese Word Segmentation Bakeoff has been useful in that it has provided us with a good sense of the range of performance of various systems, both from academic and industrial institutions.
For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). $$$$$ Using this criterion all systems are significantly different from each other except that on PK closed S10 is not significantly different from S09, and S07 is not significantly different from S04.
For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). $$$$$ This is problematic in systems that utilize Unicode internally, since transcoding back to the original encoding may lose information.
For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). $$$$$ Language Information Sciences Research Centre, City University of Hong Kong.

We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. $$$$$ In the closed test, participants could only use training material from the training data for the particular corpus being testing on.
We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. $$$$$ It seems reasonable to treat two systems as significantly different (at the 95% confidence level), if at least one of their recall-based or precision-based confidences are different.
We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. $$$$$ We would like to expand the testing data to include texts of various lengths, particularly short strings, in order to emulate query strings seen in commercial search engines.

We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. $$$$$ 2.
We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. $$$$$ See (Yao, 2001; Yao, 2002) for the first and second of these; the third evaluation will be held in August 2003.
We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. $$$$$ Performance on the Penn Chinese Treebank (CTB) corpus was generally lower than all the other corpora; omitting S02, which only ran on CTBo,c the scores for the other systems were uniformly higher on other corpora than they were on CTB, the single exception being S11 which did better on CTBo than on HKo.
We conduct experiments on the SIGHAN 2003 (Sproat and Emerson, 2003) and 2005 (Emerson, 2005) bake-off datasets to evaluate the effectiveness of the proposed dual decomposition algorithm. $$$$$ It has also been observed that different segmentation standards are appropriate for different purposes; that the segmentation standard that one might prefer for information retrieval applications is likely to be different from the one that one would prefer for text-to-speech synthesis; see (Wu, 2003) for useful discussion.

After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. $$$$$ This has been studied indirectly through the cross-language information retrieval work performed for the TREC 5 and TREC 6 competitions (Smeaton and Wilkinson, 1997; Wilkinson, 1998).
After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. $$$$$ For the closed-track entries that did well on OOV, one must conclude that they have effective unknown-word detection methods.
After analyzing the results presented in the first and second Bakeoffs, (Sproat and Emerson,2003) and (Emerson, 2005), we created a new Chinese word segmentation system named as? Achilles? that consists of four modules mainly: Regular expression extractor, dictionary-based Ngramsegmentation, CRF-based sub word tagging (Zhang et al, 2006), and confidence-based segmentation. $$$$$ Generally a file is said to be “Big Five” or “GB”, when in actuality the file is encoded in a variation of these.

In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). $$$$$ Finally note that the top performance of any system on any track was S09 on ASc (F=0.961).
In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). $$$$$ Thus, an approach that seems very promising based on its published report is nonetheless hard to compare fairly with other systems, since the systems are often tested on their own selected test corpora.
In the last SIGHAN bakeoff, there is no single system consistently outperforms the others on different test standards of Chinese WS and NER standards (Sproat and Emerson, 2003). $$$$$ This is demonstrated in the Encoding column of Table 1: This variation of encoding is exacerbated by the usual lack of specific declaration in the files.

The official scorer program is publicly available and described in (Sproat and Emerson, 2003). $$$$$ In most cases people performed above the baseline, though well below the ideal topline; note though that the two participants in the Academia Sinica open track underperformed the baseline.
The official scorer program is publicly available and described in (Sproat and Emerson, 2003). $$$$$ Since this is better than one could hope for in practice, we would expect systems to generally underperform this topline.
The official scorer program is publicly available and described in (Sproat and Emerson, 2003). $$$$$ We believe that there should be future competitions of this kind, possibly not every year, but certainly every couple of years and we have some specific recommendations on how things might be improved in such future competitions: to the restriction that participants may not be evaluated on data from their own institution.
The official scorer program is publicly available and described in (Sproat and Emerson, 2003). $$$$$ A number of segmentation contests have been held in recent years within Mainland China, in the context of more general evaluations for ChineseEnglish machine translation.

Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. $$$$$ Note that the scoring software does not correct for cases where a participant converted from one coding scheme into another, and any such cases were counted as errors.
Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. $$$$$ Results were returned to participants within a couple of days of submission of the segmented test data.
Measuring homogeneity by counting word/ lexeme frequencies introduces additional difficulties as it assumes that the word is an obvious, well-defined unit, which is not the case in the Chinese (Sproat and Emerson 2003) or Japanese language (Matsumoto et al, 2002), for instance, where word segmentation is not trivial. $$$$$ Without the generous contribution of these resources, this competition would not have been possible.

The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). $$$$$ The first author wishes to thank Bill DuMouchel of AT&T Labs for advice on the statistics.
The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). $$$$$ This is demonstrated in the Encoding column of Table 1: This variation of encoding is exacerbated by the usual lack of specific declaration in the files.
The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). $$$$$ The contest followed a strict set of guidelines and a rigid timetable.
The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). $$$$$ While the test sets turned out to be large enough to measure significant differences between systems in most cases, a larger test set would allow even better statistics.

Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. $$$$$ Most of the training data comprise texts about Mainland China, whereas most of the testing data is about Taiwan.
Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. $$$$$ This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.
Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. $$$$$ Finally we thank Fei Xia and Qing Ma for their work on the Second meeting of SIGHAN of which this bakeoff is a part.
Following (Sproat and Emerson, 2003), we also measured the recall onOOV (ROOV) tokens and in-vocabulary (RIV) tokens. $$$$$ Thus, an approach that seems very promising based on its published report is nonetheless hard to compare fairly with other systems, since the systems are often tested on their own selected test corpora.

In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). $$$$$ The values for are given in Tables 5–12, under the heading “c ”.
In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). $$$$$ Performance on the Penn Chinese Treebank (CTB) corpus was generally lower than all the other corpora; omitting S02, which only ran on CTBo,c the scores for the other systems were uniformly higher on other corpora than they were on CTB, the single exception being S11 which did better on CTBo than on HKo.
In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). $$$$$ Finally, one question that we did not ask that should have been asked was whether the tested system is used as part of a commercial product or not.
In 2003 SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics (ACL) conducted the first International ChineseWord Segmentation Bakeoff (Sproat and Emerson, 2003). $$$$$ This report summarizes the results of this First International Chinese Word Segmentation Bakeoff, provides some analysis of the results, and makes specific recommendations for future bakeoffs.

A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier. $$$$$ Let us assume that the recall rates for the various system represent the probability that a word will be successfully identified, and let us further assume that a binomial distribution is appropriate for this experiment.
A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier. $$$$$ Generally a file is said to be “Big Five” or “GB”, when in actuality the file is encoded in a variation of these.
A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier. $$$$$ These are a subset of the sites who had registered for the bakeoff, as some sites withdrew due to technical difficulties.
A recent Chinese word segmentation competition (Sproat and Emerson, 2003) has made comparisons easier. $$$$$ The results of this experiment are presented in Table 3.

 $$$$$ It is often believed of natural language and speech applications that deployed commercial systems are about a generation behind the systems being developed in research laboratories.
 $$$$$ This is demonstrated in the Encoding column of Table 1: This variation of encoding is exacerbated by the usual lack of specific declaration in the files.
 $$$$$ The final set of participants in the bakeoff include two from Mainland China, three from Hong Kong, one from Japan, one from Singapore, one from Taiwan and four from the United States.

This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). $$$$$ Column headings are as above, except for “c ”, and “c ” for which see Section 4.3.
This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). $$$$$ It is with OOV recall that we see the widest variation among systems, which in turn is consistent with the observation that dealing with unknown words is the major outstanding problem of Chinese word segmentation.
This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). $$$$$ We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future.
This is due to significant inconsistent segmentation in training and testing (Sproat and Emerson, 2003). $$$$$ Language Information Sciences Research Centre, City University of Hong Kong.

No other material was allowed (Sproat and Emerson, 2003). $$$$$ Finally we thank Fei Xia and Qing Ma for their work on the Second meeting of SIGHAN of which this bakeoff is a part.
No other material was allowed (Sproat and Emerson, 2003). $$$$$ We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future.
No other material was allowed (Sproat and Emerson, 2003). $$$$$ While the test sets turned out to be large enough to measure significant differences between systems in most cases, a larger test set would allow even better statistics.

In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). $$$$$ Generally a file is said to be “Big Five” or “GB”, when in actuality the file is encoded in a variation of these.
In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). $$$$$ Given the Central Limit Theorem for Bernouilli trials — e.g.
In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). $$$$$ Using this criterion all systems are significantly different from each other except that on PK closed S10 is not significantly different from S09, and S07 is not significantly different from S04.
In order to show the impact to the evaluation result caused by EIs existing in test data of Bakeoff, we conduct the baseline close test with PK and AS corpora, i.e. we compile lexicons only containing words in their training data and then use the lexicons with a forward maximum matching algorithm to segment their test data respectively (Sproat and Emerson, 2003). $$$$$ The script used for scoring can be downloaded from http://www.sighan.org/ bakeoff2003/score; it is a simple Perl script that depends upon a version of diff (e.g.

For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). $$$$$ Inconsistencies were also noted by another participant for the AS corpus.
For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). $$$$$ The final set of participants in the bakeoff include two from Mainland China, three from Hong Kong, one from Japan, one from Singapore, one from Taiwan and four from the United States.
For instance, 'vice president' is considered to be one word in the Penn Chinese Treebank (Xue et al, 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). $$$$$ The Chinese Treebank Project, University of Pennsylvania, and the Linguistic Data Consortium.

We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. $$$$$ Andi Wu and Aitao Chen provided useful feedback on errors in some of the corpora.
We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. $$$$$ First of all, by making the contest international, we are encouraging participation from people and institutions who work on Chinese word segmentation anywhere in the world.
We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. $$$$$ Upon initial registration sites were required to declare which corpora they would be training and testing on, and whether they would be participating in the open or closed tracks (or both) on each corpus, where these were defined as follows: For the open test sites were allowed to train on the training set for a particular corpus, and in addition they could use any other material including material from other training corpora, proprietary dictionaries, material from the WWW and so forth.
We use three Chinese word-segmented corpora, the Academia Sinica corpus (AS), the Hong Kong City University corpus (HK) and the Beijing University corpus (PK), all of which were used in the First International Chinese Word Segmentation Bake off (Sproat and Emerson, 2003) at ACL-SIGHAN 2003. $$$$$ (Grinstead and Snell, 1997, page 330), then the 95% confidence interval is given as , where is the number of trials (words).

The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). $$$$$ The problem with this literature has always been that it is very hard to compare systems, due to the lack of any common standard test set.
The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). $$$$$ 2.
The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). $$$$$ The results of this experiment are presented in Table 3.
The top three systems participated in the SIGHAN Bakeoff (Sproat and Emerson, 2003). $$$$$ This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan.

In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). $$$$$ Let us assume that the recall rates for the various system represent the probability that a word will be successfully identified, and let us further assume that a binomial distribution is appropriate for this experiment.
In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). $$$$$ Finally note that the top performance of any system on any track was S09 on ASc (F=0.961).
In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). $$$$$ Generally a file is said to be “Big Five” or “GB”, when in actuality the file is encoded in a variation of these.
In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003). $$$$$ We then used this dictionary with a simple maximum matching algorithm to segment the test corpus.

 $$$$$ A number of segmentation contests have been held in recent years within Mainland China, in the context of more general evaluations for ChineseEnglish machine translation.
 $$$$$ We also wish to thank Professor Tianshun Yao of Northeast (Dongbei) University for sending us the reports of the Chinese national competitions.
 $$$$$ The results of this experiment are presented in Table 3.
 $$$$$ Per normal usage, OOV is defined as the set of words in the test corpus not occurring in the training corpus.2 We expect systems to do at least as well as this baseline.
