We call this setting 'SemEval' because the SemEval-2007 competition (Pradhan et al, 2007) was performed using this configuration. $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.
We call this setting 'SemEval' because the SemEval-2007 competition (Pradhan et al, 2007) was performed using this configuration. $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.
We call this setting 'SemEval' because the SemEval-2007 competition (Pradhan et al, 2007) was performed using this configuration. $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
We call this setting 'SemEval' because the SemEval-2007 competition (Pradhan et al, 2007) was performed using this configuration. $$$$$ Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation.

Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). $$$$$ Fortunately, the distribution ofwords was amenable to an acceptable number of in stances for each lemma in the test set.
Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). $$$$$ Itis also encouraging that the more informative Verb Net roles which have better/direct applicability indownstream systems, can also be predicted with al most the same degree of accuracy as the PropBank arguments from which they are mapped.
Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). $$$$$ Since the coarse and fine-grained disambiguation tasks have been part ofthe two previous Senseval competitions, and we happen to have access to that data, we can take this op portunity to look at the disambiguation performancetrend.

Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.
Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
Various aspects of the model discussed in Section 3 are evaluated in the English lexical sample tasks from Senseval2 (Edmonds and Cotton, 2001) and SemEval2007 (Pradhan et al, 2007). $$$$$ UBC-UPC Open 78.57 74.70 76.60?0.8 UBC-UPC Closed 78.67 73.94 76.23?0.8 RTV Closed 74.15 57.85 65.00?0.9Table 5: System performance on PropBank argu ments.

Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.
Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.
Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.

We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ The results in the previous discussion seem to confirm the hypothesis that there is a predictable correlation between human annotator agreement and sys tem performance.
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation.
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ Given high enough ITA rates we can can hope to build sense disambiguation systemsthat perform at a level that might be of use to a con suming natural language processing application.
We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al, 2007) English All Words data sets. $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.

English Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
English Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.

It outperforms most of the systems participating in the task (Pradhan et al., 2007). $$$$$ SemEval-2007Task-17 (English Lexical Sample, SRL and All Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.It includes three subtasks: i) the traditional AllWords task comprising fine-grained word sense dis ambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grainedword sense disambiguation on a selected set of lex emes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes.
It outperforms most of the systems participating in the task (Pradhan et al., 2007). $$$$$ It should be noted that since there was no additional VerbNet role data to be used by the Open system, the performance of that on PropBank arguments as well as VerbNet roles is exactly identical.
It outperforms most of the systems participating in the task (Pradhan et al., 2007). $$$$$ Train Test Total Verb 8988 2292 11280 Noun 13293 2559 15852 Total 22281 4851 Table 2: The number of instances for Verbs andNouns in the Train and Test sets for the Lexical Sam ple WSD task.
It outperforms most of the systems participating in the task (Pradhan et al., 2007). $$$$$ 2 Hwee Tou Ng <nght@comp.nus.edu.sg> NUS-PT SVM 58.7?4.5.

Experimental results are provided for two datasets $$$$$ We proposed two levels of participation in thistask: i) Closed ? the systems could use only the an notated data provided and nothing else.
Experimental results are provided for two datasets $$$$$ It should be noted that since there was no additional VerbNet role data to be used by the Open system, the performance of that on PropBank arguments as well as VerbNet roles is exactly identical.
Experimental results are provided for two datasets $$$$$ SRL systems are an important building block for many larger semantic systems.

 $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.
 $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
 $$$$$ Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation.
 $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.

In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ Itis also encouraging that the more informative Verb Net roles which have better/direct applicability indownstream systems, can also be predicted with al most the same degree of accuracy as the PropBank arguments from which they are mapped.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ Systems marked with an * were post-competition bug-fix submissions.grained senses provides consistently higher per formance than previous more fine-grained LexicalSample Tasks.
In this paper the relevance feedback approach described by Stevenson et al (2008a) is evaluated using three data sets $$$$$ We tab ulate and analyze the results of participating systems.

We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ SemEval-2007Task-17 (English Lexical Sample, SRL and All Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.It includes three subtasks: i) the traditional AllWords task comprising fine-grained word sense dis ambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grainedword sense disambiguation on a selected set of lex emes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes.
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ The training and test set composition is described in Table 2.
We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al, 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. $$$$$ 2 Oier Lopez de Lacalle <jibloleo@si.ehu.es> UBC-ALM SVD+kNN 86.9?1.2.

Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). $$$$$ Overall, however, this data indicates that theapproach suggested by (Palmer, 2000) and that is be ing adopted in the ongoing OntoNotes project (Hovyet al, 2006) does result in higher system perfor mance.
Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.
Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.
Lexical Sample The Semeval workshop holds WSD tasks such as the English Lexical Sample (ELS) (Pradhan et al, 2007). $$$$$ Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation.

The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al, 2007). $$$$$ SemEval-2007Task-17 (English Lexical Sample, SRL and All Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.It includes three subtasks: i) the traditional AllWords task comprising fine-grained word sense dis ambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grainedword sense disambiguation on a selected set of lex emes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes.
The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al, 2007). $$$$$ We tab ulate and analyze the results of participating systems.
The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al, 2007). $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
The improvements seen using our system are substantial, beating most of the systems originally proposed for the task (Pradhan et al, 2007). $$$$$ we alsocomputed the performances after excluding exam ples of ?say?

The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al, 2007). $$$$$ Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation.
The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al, 2007). $$$$$ The results in the previous discussion seem to confirm the hypothesis that there is a predictable correlation between human annotator agreement and sys tem performance.
The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al, 2007). $$$$$ Given high enough ITA rates we can can hope to build sense disambiguation systemsthat perform at a level that might be of use to a con suming natural language processing application.
The use of coarse-grained sense groups (Palmer et al, 2007) has led to considerable advances in WSD performance, with accuracies of around 90% (Pradhan et al, 2007). $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.

Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.
Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ SemEval-2007Task-17 (English Lexical Sample, SRL and All Words) focuses on both of these challenges, WSD and SRL, using annotated English text taken from the Wall Street Journal and the Brown Corpus.It includes three subtasks: i) the traditional AllWords task comprising fine-grained word sense dis ambiguation using a 3,500 word section of the Wall Street Journal, annotated with WordNet 2.1 sense tags, ii) a Lexical Sample task for coarse-grainedword sense disambiguation on a selected set of lex emes, and iii) Semantic Role Labeling, using two different types of arguments, on the same subset of lexemes.
Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
Given that the WSD literature has shown that all features, including local and syntactic features, are necessary for optimal performance (Pradhan et al, 2007), we propose the following alternative to construct the matrix. $$$$$ 10 Davide Buscaldi <dbuscaldi@dsic.upv.es> UPV-WSD Unsupervised 46.9?4.5.

The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al, 2007). Specifically, they addressed these issues $$$$$ 3 Rada Mihalcea <rada@cs.unt.edu> UNT-Yahoo Memory-based 58.3?4.5.
The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al, 2007). Specifically, they addressed these issues $$$$$ This paper describes our experience in preparing the data and evaluating the results for three subtasks of SemEval-2007 Task-17 ? Lexical Sample, Semantic Role Labeling(SRL) and All-Words respectively.
The algorithm proved effective at Senseval-3 (Mihalcea and Edmonds, 2004) and, nowadays, it still represents the state-of-the-art in WSD (Pradhan et al, 2007). Specifically, they addressed these issues $$$$$ Correctly disambiguating words (WSD), and correctly identifying the semantic relationships be tween those words (SRL), is an important step forbuilding successful natural language processing applications, such as text summarization, question an swering, and machine translation.

For unsupervised WSD (applied to text only), we use WordNet $$$$$ 89 Lemma S s T t 1 2 3 4 5 6 7 8 Average Min Max turn.v 13 8 340 62 58 61 40 55 52 53 27 44 49 27 61 go.v 12 6 244 61 64 69 38 66 43 46 31 39 49 31 69 come.v 10 9 186 43 49 46 56 60 37 23 23 49 43 23 60 set.v 9 5 174 42 62 50 52 57 50 57 36 50 52 36 62 hold.v 8 7 129 24 58 46 50 54 54 38 50 67 52 38 67 raise.v 7 6 147 34 50 44 29 26 44 26 24 12 32 12 50 work.v 7 5 230 43 74 65 65 65 72 67 46 65 65 46 74 keep.v 7 6 260 80 56 54 52 64 56 52 48 51 54 48 64 start.v 6 4 214 38 53 50 47 55 45 42 37 45 47 37 55 lead.v 6 6 165 39 69 69 85 69 51 69 36 46 62 36 85 see.v 6 5 158 54 56 54 46 54 57 52 48 48 52 46 57 ask.v 6 3 348 58 84 72 72 78 76 52 67 66 71 52 84 find.v 5 3 174 28 93 93 86 89 82 82 75 86 86 75 93 fix.v 5 3 32 2 50 50 50 50 50 0 0 50 38 0 50 buy.v 5 3 164 46 83 80 80 83 78 76 70 76 78 70 83 begin.v 4 2 114 48 83 65 75 69 79 56 50 56 67 50 83 kill.v 4 1 111 16 88 88 88 88 88 88 88 81 87 81 88 join.v 4 4 68 18 44 50 50 39 56 57 39 44 47 39 57 end.v 4 3 135 21 90 86 86 90 62 87 86 67 82 62 90 do.v 4 2 207 61 92 90 90 93 93 90 85 84 90 84 93 examine.v 3 2 26 3 100 100 67 100 100 67 100 33 83 33 100 report.v 3 2 128 35 89 91 91 91 91 91 91 86 90 86 91 regard.v 3 3 40 14 93 93 86 86 64 86 57 93 82 57 93 recall.v 3 1 49 15 100 100 87 87 93 87 87 87 91 87 100 prove.v 3 2 49 22 90 88 82 80 90 86 70 74 82 70 90 claim.v 3 2 54 15 67 73 80 80 80 80 80 87 78 67 87 build.v 3 3 119 46 74 67 74 61 54 74 61 72 67 54 74 feel.v 3 3 347 51 71 69 69 74 76 69 61 71 70 61 76 care.v 3 3 69 7 43 43 43 43 100 29 57 57 52 29 100 contribute.v 2 2 35 18 67 72 72 67 50 61 50 67 63 50 72 maintain.v 2 2 61 10 80 80 70 100 80 90 90 80 84 70 100 complain.v 2 1 32 14 93 86 86 86 86 86 86 79 86 79 93 propose.v 2 2 34 14 100 86 100 86 100 93 79 79 90 79 100 promise.v 2 2 50 8 88 88 75 88 75 75 62 88 80 62 88 produce.v 2 2 115 44 82 82 77 73 75 75 77 80 78 73 82 prepare.v 2 2 54 18 94 83 89 89 83 86 83 83 86 83 94 explain.v 2 2 85 18 94 89 94 89 94 89 89 94 92 89 94 believe.v 2 2 202 55 87 78 78 86 84 78 74 80 81 74 87 occur.v 2 2 47 22 86 73 91 96 86 96 86 82 87 73 96 grant.v 2 2 19 5 100 80 80 80 40 80 60 80 75 40 100 enjoy.v 2 2 56 14 50 57 57 50 64 57 50 57 55 50 64 need.v 2 2 195 56 89 82 86 89 86 78 70 70 81 70 89 disclose.v 1 1 55 14 93 93 93 93 93 93 93 93 93 93 93 point.n 9 6 469 150 91 91 89 91 92 87 84 79 88 79 92 position.n 7 6 268 45 78 78 78 53 56 65 58 64 66 53 78 defense.n 7 7 120 21 57 48 52 43 48 29 48 48 46 29 57 carrier.n 7 3 111 21 71 71 71 71 67 71 71 62 70 62 71 order.n 7 4 346 57 93 95 93 91 93 92 90 91 92 90 95 exchange.n 5 3 363 61 92 90 92 85 90 88 82 79 87 79 92 system.n 5 3 450 70 79 73 66 67 59 63 63 61 66 59 79 source.n 5 5 152 35 86 80 80 63 83 68 60 29 69 29 86 space.n 5 2 67 14 93 100 93 93 93 86 86 71 89 71 100 base.n 5 4 92 20 75 80 75 50 65 40 50 75 64 40 80 authority.n 4 3 90 21 86 86 81 62 71 33 71 81 71 33 86 people.n 4 4 754 115 96 96 95 96 95 90 91 91 94 90 96 chance.n 4 3 91 15 60 67 60 60 67 73 20 73 60 20 73 part.n 4 3 481 71 90 90 92 97 90 74 66 66 83 66 97 hour.n 4 2 187 48 83 85 92 83 77 90 58 92 83 58 92 development.n 3 3 180 29 100 79 86 79 76 62 79 62 78 62 100 president.n 3 3 879 177 98 97 98 97 93 96 97 85 95 85 98 network.n 3 3 152 55 91 87 98 89 84 88 87 82 88 82 98 future.n 3 3 350 146 97 96 94 97 83 98 89 85 92 83 98 effect.n 3 2 178 30 97 93 80 93 80 90 77 83 87 77 97 state.n 3 3 617 72 85 86 86 83 82 79 83 82 83 79 86 power.n 3 3 251 47 92 87 87 81 77 77 77 74 81 74 92 bill.n 3 3 404 102 98 99 98 96 90 96 96 22 87 22 99 area.n 3 3 326 37 89 73 65 68 84 70 68 65 73 65 89 job.n 3 3 188 39 85 80 77 90 80 82 69 82 80 69 90 management.n 2 2 284 45 89 78 87 73 98 76 67 64 79 64 98 condition.n 2 2 132 34 91 82 82 56 76 78 74 76 77 56 91 policy.n 2 2 331 39 95 97 97 87 95 97 90 64 90 64 97 rate.n 2 2 1009 145 90 88 92 81 92 89 88 91 89 81 92 drug.n 2 2 205 46 94 94 96 78 94 94 87 78 89 78 96 Average Overall 86 83 83 82 82 79 76 77 Verbs 78 75 73 76 73 70 65 70 Nouns 89 87 86 81 83 80 77 76 Table 4: All Supervised system performance per predicate.
For unsupervised WSD (applied to text only), we use WordNet $$$$$ HR0011-06-C-0022; National Science Foundation Grant NSF-0415923, Word Sense Disambiguation; the DTO-AQUAINT NBCHC040036 grant under the University of Illinois subcontract to University of Pennsylvania 2003-07911-01; and NSF-ITR-0325646: Domain-Independent Semantic Interpretation.
For unsupervised WSD (applied to text only), we use WordNet $$$$$ 14 Rafael Berlanga <berlanga@uji.es> tkb-uo Unsupervised 32.5?4.5.
For unsupervised WSD (applied to text only), we use WordNet $$$$$ We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA/IPTO) under the GALE program, DARPA/CMO Contract No.
