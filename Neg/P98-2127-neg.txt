 $$$$$ Let w1, , wr, be a list of words in descending order of their similarity to a given word w. The similarity tree for w is created as follows: among lw, w1, wz-11.
 $$$$$ For example, kook, obj, *11 is the total occurrences of cookâ€”object relationships in the parsed corpus, and 11*,*,*11 is the total number of dependency triples extracted from the parsed corpus.
 $$$$$ For example, one can go a step further by constructing a tree structure among the most similar words so that different senses of a given word can be identified with different subtrees.

Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ The similarity measure allows us to construct a thesaurus using a parsed corpus.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ For example, one of the 8 senses of &quot;company&quot; in WordNet 1.5 is a &quot;visitor/visitant&quot;, which is a hyponym of &quot;person&quot;.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ The first number behind a word is the similarity of the word to its parent.

This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. $$$$$ However, formal evaluation of its accuracy remains to be future work.
This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. $$$$$ While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri.
This scheme utilizes the symmetric similarity measure of (Lin, 1998) to induce improved feature weights via bootstrapping. $$$$$ Few pairs of RNNs in Appendix A have clearly better alternatives.

We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998). $$$$$ There have been many approaches to automatic detection of similar words from text corpora.
We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998). $$$$$ Table 1 shows the average similarity between corresponding entries in different thesauri and the standard deviation of the average, which is the standard deviation of the data items divided by the square root of the number of data items.
We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998). $$$$$ The first number behind a word is the similarity of the word to its parent.
We will take advantage of the flexibility provided by our framework and use syntax based measure of similarity in the computation of the verb vectors, following (Lin, 1998). $$$$$ A word w possesses the feature f if f and w belong to a same Roget category.

Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. $$$$$ Inspection of sample outputs shows that this algorithm works well.
Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. $$$$$ In (Hindle, 1990), a small set of sample results are presented.
Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. $$$$$ Bootstrapping semantics from text is one of the greatest challenges in natural language learning.
Chantree et al (2005) applied the distributional similarity proposed by Lin (1998) to coordination disambiguation. $$$$$ Manually constructed general-purpose dictionaries and thesauri include many usages that are very infrequent in a particular corpus or genre of documents.

Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002). $$$$$ In (Hindle, 1990), a small set of sample results are presented.
Accurate measurement of semantic similarity between lexical units such as words or phrases is important for numerous tasks in natural language processing such as word sense disambiguation (Resnik, 1995), synonym extraction (Lin, 1998a), and automatic thesauri generation (Curran, 2002). $$$$$ To determine whether or not the differences are statistically significant, we computed their differences in similarities to WordNet and Roget thesaurus for each individual entry.

 $$$$$ In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri.
 $$$$$ Ours is similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed.
 $$$$$ We computed the pairwise similarity between all the nouns, all the verbs and all the adjectives/adverbs, using the above similarity measure.
 $$$$$ The second number is the similarity of the word to the root node of the tree.

 $$$$$ Bootstrapping semantics from text is one of the greatest challenges in natural language learning.

Lin (1998) created a thesaurus using syntactic relationships with other words. $$$$$ Firstly, the terms can be corpus- or genre-specific.
Lin (1998) created a thesaurus using syntactic relationships with other words. $$$$$ While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri.
Lin (1998) created a thesaurus using syntactic relationships with other words. $$$$$ Our experiments also surpasses previous experiments on automatic thesaurus construction in scale and (possibly) accuracy.

Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998). $$$$$ The probability of A, B and C cooccurring is estimated by where PALE is the maximum likelihood estimation of a probability distribution and II cell, obj -of, bludgeon11=1 lIcell, obj-of, cal111.11 cell, obj-of, come from11=3 When the value of II w, r, w' II is known, we can obtain PALE (A, B, C) directly: Let /(w, r, w') denote the amount information contained in 11w, r, w1 II=c.
Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998). $$$$$ Since the differences among sim SiMJacard cosine, SiMdzce and are very small, we only included the results for sitncosi, in Table 1 for the sake of brevity.
Like McCarthy et al (2004) we use k= 50 and obtain our thesaurus using the distributional similarity metric described by Lin (1998). $$$$$ The similarity measure simwN is based on the proposal in (Lin, 1997).

The thesaurus was acquired using the method described by Lin (1998). $$$$$ Our program found 543 pairs of RNN nouns, 212 pairs of RNN verbs and 382 pairs of RNN adjectives/adverbs in the automatically created thesaurus.
The thesaurus was acquired using the method described by Lin (1998). $$$$$ The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs.
The thesaurus was acquired using the method described by Lin (1998). $$$$$ While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri.

For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998). $$$$$ There have been many approaches to automatic detection of similar words from text corpora.
For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998). $$$$$ The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.
For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998). $$$$$ It can be seen that sim, Hindle, and cosine are significantly more similar to WordNet than Roget is, but are significantly less similar to Roget than WordNet is.
For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998). $$$$$ In addition to the long-term goal of bootstrapping semantics from text, automatic identification of similar words has many immediate applications.

 $$$$$ The results show that our automatically created thesaurus is significantly closer to WordNet than Roget Thesaurus is.
 $$$$$ We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus.
 $$$$$ With simwN and siMRoget, we transform WordNet and Roget into the same format as the automatically constructed thesauri in the previous section.

As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. $$$$$ We first define a word similarity measure based on the distributional pattern of words.
As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. $$$$$ Our experiments also surpasses previous experiments on automatic thesaurus construction in scale and (possibly) accuracy.
As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. $$$$$ Our experiments also surpasses previous experiments on automatic thesaurus construction in scale and (possibly) accuracy.
As in (Lin, 1998) or (Cur ran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. $$$$$ The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs.

This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). $$$$$ For example, given a corpus that includes the sentences in (1), our goal is to be able to infer that tezgiiino is similar to &quot;beer&quot;, &quot;wine&quot;, &quot;vodka&quot;, etc.
This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). $$$$$ The similarity measure allows us to construct a thesaurus using a parsed corpus.
This seems to be a reasonable compromise between the approach of (Freitag et al, 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). $$$$$ A word w possesses the feature f if f and w belong to a same Roget category.

Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences. $$$$$ To measure the information contained in the statement Ilw, r, Il=c, we first measure the amount of information in the statement that a randomly selected dependency triple is (w, r, w') when we do not know the value of 11w, r, will.
Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences. $$$$$ In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time.
Finally, the results of Table 2 are compatible with those of (Lin, 1998) for instance (R-prec. = 11.6 and MAP = 8.1 with WM as reference for all entries of the thesaurus at http://webdocs.cs.ualberta.ca/lindek/Downloads/sim.tgz) if we take into account the fact that the thesaurus of Lin was built from a much larger corpus and with syntactic co-occurrences. $$$$$ In (Hindle, 1990), a small set of sample results are presented.

For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). $$$$$ From the parsed corpus, we extracted 56.5 million dependency triples (8.7 million unique).
For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). $$$$$ For example, Figure 3 shows the similarity tree for the top-40 most similar words to duty.
For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person (Lin, 1998). $$$$$ The contexts in which the word tezgiiino is used suggest that tezgiiino may be a kind of alcoholic beverage made from corn mash.

For instance, Lin (1998) used dependency relation as word features to compute word similarities from large corpora, and compared the thesaurus created in such a way with WordNet and Roget classes. $$$$$ In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer.

One of the most important approaches is Lin (1998). $$$$$ We then measure the amount of information in the same statement when we do know the value of11w, r, w' II.
One of the most important approaches is Lin (1998). $$$$$ Few pairs of RNNs in Appendix A have clearly better alternatives.
One of the most important approaches is Lin (1998). $$$$$ Let w1, , wr, be a list of words in descending order of their similarity to a given word w. The similarity tree for w is created as follows: among lw, w1, wz-11.

 $$$$$ If one needs to search hostage-related articles, &quot;westerner&quot; may well be a good search term.
 $$$$$ While previous methods rely on indirect tasks or subjective judgments, our method allows direct and objective comparison between automatically and manually constructed thesauri.
