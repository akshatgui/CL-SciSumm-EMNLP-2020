The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. $$$$$ Training and evaluation corpora were annotated manually for hedge/weasel cues and their scope by two independent linguist annotators.
The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. $$$$$ In natural language processing (NLP) – and in particular, in information extraction (IE) – many applications seek to extract factual information from text.
The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. $$$$$ The evaluation dataset was based on 15 biomedical articles downloaded from the publicly available PubMedCentral database, including 5 random articles taken from the BMC Bioinformatics journal in October 2009, 5 random articles to which the drosophila MeSH term was assigned and 5 random articles having the MeSH terms human, blood cells and transcription factor (the same terms which were used to create the Genia corpus).
The recent CoNLL-2010 shared task (Farkas et al, 2010), aimed at detecting uncertainty cues in texts, focused on these phrases in trying to determine whether sentences contain uncertain information. $$$$$ Applications should handle detected speculative parts in a different manner.

Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel. $$$$$ The manually annotated datasets and software tools developed for the shared task may act as benchmarks for these future experiments (they are freely available at http://www.inf. u-szeged.hu/rgai/conll2010st).
Whereas the CoNLL-2010 shared task (Farkas et al, 2010) annotated all occurrences of weasels as uncertainty markers, we acknowledge the possibility of sources (e.g. citations) that actually nullify the weasel. $$$$$ During the manual annotation process, the following cue marking principles were employed.

We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. $$$$$ The evaluation for Task1 was carried out at the sentence level, i.e. the cue annotations in the sentence were not taken into account.
We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
We are also interested in understanding whether, and which, linguistic features of the discussion are important for dispute detection. Drawing inspiration from studies of human mediation of on line conflicts (e.g. Billings and Watts (2010), Kittur et al (2007), Kraut and Resnick (2012)), we hypothesize that effective methods for dispute detection should take into account the sentiment and opinions expressed by participants in the collaborative endeavor. $$$$$ The manually annotated datasets and software tools developed for the shared task may act as benchmarks for these future experiments (they are freely available at http://www.inf. u-szeged.hu/rgai/conll2010st).

We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2 $$$$$ This work was supported in part by the National Office for Research and Technology (NKTH, http://www.nkth.gov.hu/) of the Hungarian government within the framework of the projects TEXTREND, BELAMI and MASZEKER.
We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2 $$$$$ This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results.
We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2 $$$$$ Hedging is typically expressed by using specific linguistic devices (which we refer to as cues in this article) that modify the meaning or reflect the author’s attitude towards the content of the text.
We extract the initial unigram, bigram, and trigram of each utterance as dis Lexical Features Syntactic/Semantic Featuresunigram/bigramunigram with POS tag number of words all uppercased dependency relation number of words Conversation Features Discourse Features quote overlap with target initial uni-/bi-/tri-gram TFIDF similarity with target repeated punctuations (remove quote first) hedging phrases collected from Sentiment Features Farkas et al (2010) connective+ sentiment words number of negators sentiment dependency relation sentiment words Table 2 $$$$$ Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns.

 $$$$$ The CoNLL-2010 Shared Task introduced the novel task of uncertainty detection.
 $$$$$ Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) provides a competitive shared task for the Computational Linguistics community.
 $$$$$ The manually annotated datasets and software tools developed for the shared task may act as benchmarks for these future experiments (they are freely available at http://www.inf. u-szeged.hu/rgai/conll2010st).
 $$$$$ The relatively high number of participants indicates that the problem is rather interesting for the Natural Language Processing community.

We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations $$$$$ The most intense participation was on Task1B.
We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations $$$$$ The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.
We also compare with two state-of-the-art methods that are used in sentiment prediction for conversations $$$$$ The XML format enabled us to provide more detailed information about the documents such as segment boundaries and types (e.g. section titles, figure captions) and it is the straightforward format to represent nested scopes.

Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. $$$$$ Each Task2 system was built upon a Task1 system, i.e.
Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. $$$$$ However, hedge detection has received considerable interest just recently in the NLP community.
Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. $$$$$ The authors would like to thank Joakim Nivre and Llu´ıs M´arquez for their useful suggestions, comments and help during the organisation of the shared task.
Recently, the Negation and Speculation in NLP Workshop (Morante and Sporleder, 2010) and the CoNLL-2010 Shared Task (Farkas et al, 2010) targeted negation mostly on those subfields. $$$$$ The relatively high number of participants indicates that the problem is rather interesting for the Natural Language Processing community.

Councill et al (2010) present a supervised scope detector using their own annotation. $$$$$ Medlock and Briscoe (2007) used single words as input features in order to classify sentences from biological articles (FlyBase) as speculative or non-speculative based on semi-automatically collected training examples.
Councill et al (2010) present a supervised scope detector using their own annotation. $$$$$ The term hedging was originally introduced by Lakoff (1972).
Councill et al (2010) present a supervised scope detector using their own annotation. $$$$$ For example, in the following sentence the subordinated clause starting with although contains factual information while uncertain information is included in the main clause and the embedded question.

As annotation tool, we use Jubilee (Choi et al,2010). $$$$$ The feature sets used here are the same as for Task1, extended by several features describing the relationship between the cue phrase and the token in question mostly by describing the dependency path between them.
As annotation tool, we use Jubilee (Choi et al,2010). $$$$$ Here, while an uncertain relation might be of some interest for an end-user as well, such information must not be confused with factual textual evidence (reliable information).
As annotation tool, we use Jubilee (Choi et al,2010). $$$$$ Participants uploaded their results through the shared task website, and the official evaluation was performed centrally.
As annotation tool, we use Jubilee (Choi et al,2010). $$$$$ Light et al. (2004) used a handcrafted list of hedge cues to identify speculative sentences in MEDLINE abstracts and several biomedical NLP applications incorporate rules for identifying the certainty of extracted information (Friedman et al., 1994; Chapman et al., 2007; Aramaki et al., 2009; Conway et al., 2009).

This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. $$$$$ Nested scopes have overlapping text spans which may contain cues for multiple scopes (there were 1058 occurrences in the training and evaluation datasets together).
This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. $$$$$ Thus, we considered the strict boundary matching to be a straightforward and unambiguous evaluation criterion.
This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. $$$$$ For example, in the following sentence the subordinated clause starting with although contains factual information while uncertain information is included in the main clause and the embedded question.
This approach was first used by Morante et al (2008) and subsequently in many of the studies presented in the CoNLL-2010 Conference Shared Task (Farkas et al, 2010a), and is the one used in this paper. $$$$$ The most recent approaches to uncertainty detection exploit machine learning models that utilize manually labeled corpora.

Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. $$$$$ A typical example is protein-protein interaction extraction from biological texts, where the aim is to mine text evidence for biological entities that are in a particular relation with each other.
Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. $$$$$ The aim of this article selection procedure was to have a theme that was close to the training corpus.
Task 2 of the CoNLL-2010 Conference Shared Task (Farkas et al, 2010b) proposed solving the problem of in-sentence hedge cue phrase identification and scope detection in two different domains (biological publications and Wikipedia articles), based on manually annotated corpora. $$$$$ Any differences between the two annotations were later resolved by the chief annotator, who was also responsible for creating the annotation guidelines and training the two annotators.

The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. $$$$$ These texts were manually annotated for hedge cues and their scope.
The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. $$$$$ The annotation of weasel/hedge cues was carried out on the phrase level, and sentences containing at least one cue were considered as uncertain, while sentences with no cues were considered as factual.
The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. $$$$$ We think that this is due to the practical importance of the task for (principally biomedical) applications and because it addresses several open research questions.
The best result on hedge cue identification (Tanget al, 2010) obtained an F-score of 81.3 using a supervised sequential learning algorithm to learn BIOclasses from lexical and shallow parsing information, also including certain linguistic rules. $$$$$ In order to distinguish facts from unreliable or uncertain information, linguistic devices such as hedges (indicating that authors do not or cannot back up their opinions/statements with facts) have to be identified.

For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). $$$$$ The annotation of weasel/hedge cues was carried out on the phrase level, and sentences containing at least one cue were considered as uncertain, while sentences with no cues were considered as factual.
For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). $$$$$ The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.
For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). $$$$$ The machine learning methods applied can be again categorized into sequence labeling (SL) sification (TC), hand-crafted rules (HC); Machine learners: Entropy Guided Transformation Learning (ETL), Averaged Perceptron (AP), k-nearest neighbour (KNN); The way of identifying scopes: predicting first/last tokens (FL), first/inside/last tokens (FIL), just inside tokens (I); Multiple Hedges: the system applied a mechanism for handling multiple hedges inside a sentence and token classification (TC) approaches (see Table 7).
For scope detection, Morante et al (2010) obtained an F-score of 57.3, using also a sequence classification approach for detecting boundaries (tagged in FOL format, where the first token of the span is marked with an F, while the last one is marked with an L). $$$$$ However, the annotation of cues in datasets is essential for training scope detectors since locating the cues usually precedes the identification of their scope.

Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). $$$$$ The chief editors of Wikipedia have drawn the attention of the public to uncertainty issues they call weasel1.
Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). $$$$$ This work was supported in part by the National Office for Research and Technology (NKTH, http://www.nkth.gov.hu/) of the Hungarian government within the framework of the projects TEXTREND, BELAMI and MASZEKER.
Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). $$$$$ The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction.
Similarly, Kilicoglu and Bergler (2010) used a pure rule-based approach based on constituent parse trees in addition to syntactic dependency relations, and achieved the fourth best F score for scope detection, and the highest precision of the whole task (62.5). $$$$$ The manually annotated datasets and software tools developed for the shared task may act as benchmarks for these future experiments (they are freely available at http://www.inf. u-szeged.hu/rgai/conll2010st).

The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note. $$$$$ The biological training dataset consisted of the biological part of the BioScope corpus (Vincze et al., 2008), hence it included abstracts from the GENIA corpus, 5 full articles from the functional genomics literature (related to the fruit fly) and 4 articles from the open access BMC Bioinformatics website.
The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note. $$$$$ Task2 systems differ in the number of class labels used as target and in the machine learning approaches applied.
The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note. $$$$$ The identification of the scope for a certain cue was typically carried out by classifying each token in the sentence.
The best results so far for this task used a token classification approach or sequential labelling techniques, as Farkas et al (2010b) note. $$$$$ There were three systems which incorporated information about other hedge cues (e.g.

 $$$$$ The system of Kilicoglu and Bergler (2010) is the only open submission.
 $$$$$ Here, participants applied various precision/recall trade-off strategies.
 $$$$$ This work was supported in part by the National Office for Research and Technology (NKTH, http://www.nkth.gov.hu/) of the Hungarian government within the framework of the projects TEXTREND, BELAMI and MASZEKER.
 $$$$$ The biological training dataset consisted of the biological part of the BioScope corpus (Vincze et al., 2008), hence it included abstracts from the GENIA corpus, 5 full articles from the functional genomics literature (related to the fruit fly) and 4 articles from the open access BMC Bioinformatics website.

 $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
 $$$$$ Using BioScope for training and evaluation, Morante and Daelemans (2009) developed a scope detector following a supervised sequence labeling approach while ¨Ozg¨ur and Radev (2009) developed a rule-based system that exploits syntactic patterns.
 $$$$$ they attempted to recognize the scopes for the predicted cue phrases (however, Zhang et al. (2010) have argued that the objective functions of Task1 and Task2 cue detection problems are different because of sentences containing multiple hedge spans).
 $$$$$ The latter task falls within the scope of semantic analysis of sentences exploiting syntactic patterns, as hedge spans can usually be determined on the basis of syntactic patterns dependent on the keyword.

The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. $$$$$ For each task, a separate XML file was made available containing the whole document set for the given task.
The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. $$$$$ Some linguistic phenomena (e.g. passive voice or raising) can change scope boundaries in the sentence, thus they were given special attention during the annotation phase.
The goal of the CoNLL 2010 Shared Task (Farkas et al, 2010) was to develop linguistic scope detectors as well. $$$$$ The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction.

The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. $$$$$ The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts.
The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. $$$$$ The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.
The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. $$$$$ For example, including or excluding punctuations, citations or some bracketed expressions, like (see Figure 1) from a scope is not crucial for an otherwise accurate scope detector.
The shared task at the 2010 Conference on Natural Language Learning (CoNLL) focused on speculation detection for the domain of biomedical research literature (Farkas et al, 2010), with data sets based on the BioScope corpus (Vincze et al, 2008) which annotates so called speculation cues along with their scopes. $$$$$ Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) provides a competitive shared task for the Computational Linguistics community.

Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. $$$$$ Every year since 1999, the Conference on Computational Natural Language Learning (CoNLL) provides a competitive shared task for the Computational Linguistics community.
Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. $$$$$ Another observation is that the top systems in both Task1B and Task1W are the ones which did not derive features from syntactic parsing.
Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. $$$$$ The relatively high number of participants indicates that the problem is rather interesting for the Natural Language Processing community.
Prabhakaran et al (2010) report experiments with belief tagging, which in many ways is similar to factuality detection. $$$$$ Ganter and Strube (2009) proposed an approach for the automatic detection of sentences containing uncertainty based on Wikipedia weasel tags and syntactic patterns.
