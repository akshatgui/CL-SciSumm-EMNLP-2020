Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.

We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ Such dictionaries are large and embody a great deal of lexical knowledge.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ Second, we did not want to overly tune this list, though experiments below suggest that tuning could greatly reduce the error rate.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ Example sentences are shown in figure 1(a) and (b).

For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ Therefore distributional prototype features will encourage labels to persist, naturally giving the “sticky” effect of the domain.
For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ Adding prototypes in this way gave an accuracy of 68.8% on all tokens, but only 47.7% on non-prototype occurrences, which is only a marginal improvement over BASE.
For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ Since we utilize a trigram tagger, we are able to naturally capture the effect that the BOUNDARY tokens typically indicate transitions between the fields before and after the boundary token.
For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ As can be seen, many of our confusions involve the FEATURE field, which serves as a general purpose background state, which often differs subtly from other fields such as SIZE.

Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ If our model family were HMMs, we could use the EM algorithm to perform a local search.
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ Given enough refinement set of the CLASSIFIEDS data.
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ Similar to supervised maximum entropy problems, the partial derivative of L(θ; D) with respect to each parameter θj (associated with feature fj) is given by a difference in feature expectations: The first expectation is the expected count of the feature under the model’s p(y|x, θ) and is again easily computed with the forward-backward algorithm, just as for CRFs or HMMs.

Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ Incorporating the prototype list in the simplest possible way, we fixed prototype occurrences in the data to their respective annotation labels.
Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ A natural consequence of this definition of distributional similarity is that many neighboring words will share the same prototypes.
Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ In particular, we use L-BFGS (Liu and Nocedal, 1989), a standard numerical optimization technique, which requires the ability to evaluate L(θ; D) and its gradient at a given θ.
Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ Distributional prototype features provide substantial error rate reductions on all three tasks.

We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.
We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ Furthermore, our PROTO+SIM+BOUND model comes close to the supervised HMM accuracy of 74.4% reported in Grenager et al. (2005).
We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ We then performed an SVD on the matrix to obtain a reduced rank approximation.

The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ See section 5.3 for further comparison of this setting to semi-supervised learning.
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.

Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ For each document x = [xi], we would like to predict a sequence of labels y = [yi], where xi E X and yi E Y.
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ We believe the performance for Chinese POS tagging is not as high as English for two reasons: the general difficulty of Chinese POS tagging (Tseng et al., 2005) and the lack of a larger segmented corpus from which to build distributional models.
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ We also compare to semi-supervised learning and discuss the system’s error trends.
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ Adding the prototype list (see figure 2) without distributional features yielded a slightly improved accuracy of 53.7%.

(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ So far we have ignored the issue of how we learn model parameters θ which maximize L(θ; D).
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ For instance, the parenthical comment: ( master has walk - in closet with vanity ) is labeled as a SIZE field in the data, but our model proposed it as a FEATURE field.
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ For each word w, we find the set of prototype words with similarity exceeding a fixed threshold of 0.35.
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ For this domain, we utilized a slightly different notion of distributional similarity: we are not interested in the syntactic behavior of a word type, but its topical content.

Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ token it is given the same label as the previous non-BOUNDARY token, which reflects the annotational convention that boundary tokens are given the same label as the field they terminate.
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ For instance, the parenthical comment: ( master has walk - in closet with vanity ) is labeled as a SIZE field in the data, but our model proposed it as a FEATURE field.

For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ We also compared our method to the most basic semi-supervised setting, where fully labeled documents are provided along with unlabeled ones.
For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ The overall approach of Grenager et al. (2005) typifies the process involved in fully unsupervised learning on new domain: they first alter the structure of their HMM so that diagonal transitions are preferred, then modify the transition structure to explicitly model boundary tokens, and so on.

In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ In supervised learning, model behavior is primarily determined by labeled examples, whose production requires a certain kind of expertise and, typically, a substantial commitment of resources.
In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.

The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ We then encode these prototype links as features in a log-linear generative model, which is trained to fit unlabeled data (see section 4.1).
The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ Adding the BOUNDARY label yields significant improvements, as indicated by the PROTO+SIM+BOUND setting in Table 5.3, surpassing the best unsupervised result of Grenager et al. (2005) which is 72.4%.
The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ As a post-processing step, when a token is tagged as a BOUNDARY
The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.

Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ Example sentences are shown in figure 1(a) and (b).
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ In providing a prototype, however, we generally mean something stronger than a constraint on that word.
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)’s unsupervised HMM baseline.
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ This notion of distributional similarity is more similar to latent semantic indexing (Deerwester et al., 1990).

Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ With just these features (our baseline BASE) the problem is symmetric in the 45 model labels.
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.

For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.
For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ Labels in this domain tend to be “sticky” in that the correct annotation tends to consist of multi-element fields of the same label.
For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.

In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ Grenager et al. (2005) experiment with manually adding boundary states and biasing transitions from these states to not self-loop.
In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.
In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ investigate for primarily unsupervised sequence modeling.

Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ This manner of specifying prior knowledge about the task has several advantages.
Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ We assume prior knowledge about the target structure via a prototype list, which specifies the set of target labels Y and, for each label y E Y, a set of prototypes words, py E Py.

Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ Given enough refinement set of the CLASSIFIEDS data.
Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ For each word w, we find the set of prototype words with similarity exceeding a fixed threshold of 0.35.
Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.
Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ In particular, we may intend that words which are in some sense similar to a prototype generally be given the same label(s) as that prototype.

We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ We also compared our method to the most basic semi-supervised setting, where fully labeled documents are provided along with unlabeled ones.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ This manner of specifying prior knowledge about the task has several advantages.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ For example, Merialdo (1991) presents experiments learning HMMs using EM.
