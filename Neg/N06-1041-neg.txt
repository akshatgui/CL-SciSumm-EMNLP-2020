Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ Labels in this domain tend to be “sticky” in that the correct annotation tends to consist of multi-element fields of the same label.
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ Adding distributional similarity features to our model (PROTO+SIM) improves accuracy substantially, yielding 71.5%, a 38.4% error reduction over BASE.6 Another feature of this domain that Grenager et al. (2005) take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one.
Haghighi and Klein (2006) use a small list of labeled prototypes and no dictionary. $$$$$ For our part-of-speech tagging experiments, we used data from the English and Chinese Penn treebanks (Marcus et al., 1994; Ircs, 2002).

We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ Since we have a log-linear formulation, we instead use a gradientbased search.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ In section 5.3, we discuss an approach to this task which does not require customization of model structure, but rather centers on feature engineering.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ In order to break initial symmetry we initialized our potentials to be near one, with some random noise.
We report greedy one-to-one mapping accuracy (1-1) (Haghighi and Klein, 2006) and the information-theoretic score V measure (V-m), which also varies from 0 to 100% (Rosenberg and Hirschberg, 2007). $$$$$ We also compare to semi-supervised learning and discuss the system’s error trends.

For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.
For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
For example, both Haghighi and Klein (2006) and Mann and McCallum (2008) have demonstrated results better than 66.1% on the apartments task described above using only a list of 33 highly discriminative features and the labels they indicate. $$$$$ This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Intel.

Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ In table 5.3, we provide the top confusions made by our PROTO+SIM+BOUND model.
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ We present more details in section 5, but for now assume that a similarity function over word types is given.
Consequently, we abstract away from specifying a distribution by allowing the user to assign labels to features (c.f. Haghighi and Klein (2006), Druck et al (2008)). $$$$$ The BOUNDARY field is not present in the original annotation, but added to model boundaries (see Section 5.3).

Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ We assume prior knowledge about the target structure via a prototype list, which specifies the set of target labels Y and, for each label y E Y, a set of prototypes words, py E Py.
Similarly, prototype-driven learning (PDL) (Haghighi and Klein, 2006) optimizes the joint marginal likelihood of data labeled with prototype input features for each label. $$$$$ We also compare to semi-supervised learning and discuss the system’s error trends.

We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ Adding the BOUNDARY label yields significant improvements, as indicated by the PROTO+SIM+BOUND setting in Table 5.3, surpassing the best unsupervised result of Grenager et al. (2005) which is 72.4%.
We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ token it is given the same label as the previous non-BOUNDARY token, which reflects the annotational convention that boundary tokens are given the same label as the field they terminate.
We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)’s unsupervised HMM baseline.
We use the same feature processing as Haghighi and Klein (2006), with the addition of context features in a window of 3. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.

The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ For our part-of-speech tagging experiments, we used data from the English and Chinese Penn treebanks (Marcus et al., 1994; Ircs, 2002).
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ This manner of specifying prior knowledge about the task has several advantages.
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ Grenager et al. (2005) experiment with manually adding boundary states and biasing transitions from these states to not self-loop.
The 74.6% final accuracy on apartments is higher than any result obtained by Haghighi and Klein (2006) (the highest is 74.1%), higher than the supervised HMM results reported by Grenager et al (2005) (74.4%), and matches the results of Mann and McCallum (2008) with GE with more accurate sampled label distributions and 10 labeled examples. $$$$$ We followed the common approach in the literature, greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset.

Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness (Radford, 1988).
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ One approach to unsupervised learning of partof-speech models is to induce HMMs from unlabeled data in a maximum-likelihood framework.
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ Finally, natural language does exhibit proform and prototype effects (Radford, 1988), which suggests that learning by analogy to prototypes may be effective for language tasks.
Another interesting idea is to select some exemplars (Haghighi and Klein, 2006). $$$$$ We then encode these prototype links as features in a log-linear generative model, which is trained to fit unlabeled data (see section 4.1).

(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ In this case, the model is no longer symmetric, and we no longer require random initialization or post-hoc mapping of labels.
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ For this domain, we utilized a slightly different notion of distributional similarity: we are not interested in the syntactic behavior of a word type, but its topical content.
(Haghighi and Klein, 2006) exploits prototype words (e.g., close, near, shoping for the NEIGHBORHOOD attribute) in an unsupervised setting. $$$$$ A straightforward way to implement this is to constrain each prototype word to take only its given label(s) at training time.

Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work.
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ The overall approach of Grenager et al. (2005) typifies the process involved in fully unsupervised learning on new domain: they first alter the structure of their HMM so that diagonal transitions are preferred, then modify the transition structure to explicitly model boundary tokens, and so on.
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ For instance, the parenthical comment: ( master has walk - in closet with vanity ) is labeled as a SIZE field in the data, but our model proposed it as a FEATURE field.
Haghighi and Klein (2006) ask the user to suggest a few prototypes (examples) for each class and use those as features. $$$$$ Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label.

For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ Adding distributional similarity features to our model (PROTO+SIM) improves accuracy substantially, yielding 71.5%, a 38.4% error reduction over BASE.6 Another feature of this domain that Grenager et al. (2005) take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one.
For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
For comparison, Haghighi and Klein (2006) report an unsupervised baseline of 41.3%, and a best result of 80.5% from using hand-labeled prototypes and distributional similarity. $$$$$ This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model.

In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ One approach to unsupervised learning of partof-speech models is to induce HMMs from unlabeled data in a maximum-likelihood framework.
In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ We tested our framework on the CLASSIFIEDS data described in Grenager et al. (2005) under conditions similar to POS tagging.
In the same spirit, Christodoulopoulos et al (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). $$$$$ In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words (Sch¨utze, 1995; Clark, 2001).

The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.
The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ In our prototype-driven approach, we never provide a single fully labeled example sequence.
The approach of Christodoulopoulos et al. (2010) falls between pre-processing and data exploration, as the clusters of tokens produced are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006). $$$$$ In this paper, we consider three sequence modeling tasks: part-of-speech tagging in English and Chinese and a classified ads information extraction task.

Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words (Sch¨utze, 1995; Clark, 2001).
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ Table 5 lists the most common confusions for PROTO+SIM.
Still, however, such techniques often require "seeds", or "prototypes" (c.f., (Haghighi and Klein, 2006)) which are used to prune search spaces or direct learners. $$$$$ Unsupervised learning, while minimizing the usage of labeled data, does not necessarily minimize total effort.

Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Intel.
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ Example sentences are shown in figure 1(a) and (b).
Haghighi and Klein (2006) showed that adding a small set of prototypes to the unlabeled data can improve tagging accuracy significantly. $$$$$ For example, Merialdo (1991) presents experiments learning HMMs using EM.

For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ This notion of distributional similarity is more similar to latent semantic indexing (Deerwester et al., 1990).
For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.
For instance, the frequency collected from the data can be used to bias initial transition and emission probabilities in an HMM model; the tagged words in IGT can be used to label the resulting clusters produced by the word clustering approach; the frequent and unambiguous words in the target lines can serve as prototype examples in the prototype-driven approach (Haghighi and Klein, 2006). $$$$$ We construct a generative model, p(x, y|0), where 0 are the model’s parameters, and choose parameters to maximize the log-likelihood of our observed data D: We take our model family to be chain-structured Markov random fields (MRFs), the undirected equivalent of HMMs.

In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ Since we utilize a trigram tagger, we are able to naturally capture the effect that the BOUNDARY tokens typically indicate transitions between the fields before and after the boundary token.
In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints.
In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ Both of these works require specification of the legal tags for each word.
In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. $$$$$ This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Intel.

Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ NEIGHBORHOOD and ADDRESS is another natural confusion resulting from the fact that the two fields share much of the same vocabulary (e.g [ADDRESS 2525 Telegraph Ave.] vs. [NBRHD near Telegraph]).
Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ In unsupervised learning, model behavior is largely determined by the structure of the model.
Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ Therefore distributional prototype features will encourage labels to persist, naturally giving the “sticky” effect of the domain.
Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples which are compactly represented in WFSTs. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.

Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ Grenager et al. (2005) presents an unsupervised approach to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see figure 1(c)).
Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ The BOUNDARY field is not present in the original annotation, but added to model boundaries (see Section 5.3).
Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ The overall approach of Grenager et al. (2005) typifies the process involved in fully unsupervised learning on new domain: they first alter the structure of their HMM so that diagonal transitions are preferred, then modify the transition structure to explicitly model boundary tokens, and so on.
Specifically we use the features described in Haghighiand Klein (2006), namely initial-capital, contains hyphen, contains-digit and we add an extra feature contains-punctuation. $$$$$ As a post-processing step, when a token is tagged as a BOUNDARY

We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ In table 5.3, we provide the top confusions made by our PROTO+SIM+BOUND model.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ token it is given the same label as the previous non-BOUNDARY token, which reflects the annotational convention that boundary tokens are given the same label as the field they terminate.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ As can be seen, many of our confusions involve the FEATURE field, which serves as a general purpose background state, which often differs subtly from other fields such as SIZE.
We can clearly see that morphological features are helpful in both languages; however the extended features of Haghighi and Klein (2006) seem to help only on the English data. $$$$$ These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.
