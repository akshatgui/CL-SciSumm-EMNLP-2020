In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. $$$$$ Assuming the length of the n-best list is N and the size of the corpus is S (in number of sentences), we compute Orange as follows: ORANGE = )1( )( 1 + ???
In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. $$$$$ We conclude this paper and discuss future directions in Section 5.
In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. $$$$$ For example, candidate translations shorter than 12 words would have zero BLEU12 score but BLEU12 has the best correlation with human judgment in fluency as shown in Table 1.
In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement. $$$$$ ORANGE scores for BLEUS1 to 9.

This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). $$$$$ We call the smoothed BLEU: BLEUS.
This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). $$$$$ Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores.
This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). $$$$$ The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y. In other words, consecutive matches are awarded more scores than non-consecutive matches.
This locality assumption eases efficient implementation of our algorithm, and can be realized using a sentence-level evaluation measure such as BLEU+1 (Lin and Och, 2004). $$$$$ to 2.0 with increment of 0.1

156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. $$$$$ ROUGE-W with weight ranging from 1.1.
156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. $$$$$ NIST is very good in adequacy correlation but not as good as GTM30 in fluency correlation.
156 at the sentence levels moothed-bleu (Lin and Och, 2004) was used in this case. $$$$$ To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS.

ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ Based on these observations, we are not able to conclude which metric is the best because it depends on the manual evaluation criteria.
ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ GTM with smaller exponent has better adequacy correlation and GTM with larger exponent has better fluency correlation.
ROUGE utilizes, skip n-grams, which allow for matches of sequences of words that are not necessarily adjacent (Lin and Och, 2004a). $$$$$ A natural fit to the existing statistical machine translation framework ? A metric that ranks a good translation high in an n best list could be easily integrated in a minimal error rate statistical machine translation training framework (Och 2003).

BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. $$$$$ Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence.
BLEU is smoothed (Lin and Och, 2004b), and it considers only matching up to bi grams because this has higher correlations with human judgments than when higher-ordered n-grams are included. $$$$$ However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.

Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. $$$$$ ROUGE-W 1.1 does have better ORANGE score but it is equivalent to other ROUGE-W variants and ROUGE-L.
Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. $$$$$ GTM10, 20, and 30 are GTM with exponents of 1.0, 2.0, and 3.0 respectively.
Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. $$$$$ For exploration study, we generate 1024-best list using AlTemp for 872 source sentences.
Smoothed per-sentence BLEU (Lin and Och, 2004) was used as a similarity metric. $$$$$ BLEUS1 to 9.

Stemming is enabled (Lin and Och, 2004a). $$$$$ We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
Stemming is enabled (Lin and Och, 2004a). $$$$$ Therefore, for candidate translations with less than n words, they can still get a positive smoothed BLEU score from shorter n-gram matches; however if nothing matches then they will get zero scores.
Stemming is enabled (Lin and Och, 2004a). $$$$$ Unfortunately, these judgments are often inconsistent and very expensive to acquire.
Stemming is enabled (Lin and Och, 2004a). $$$$$ This example illustrated that ROUGE-L can work reliably at sentence level.

Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ To automatically evaluate machine translations, the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al 2001).
Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ where -?
Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ BLEUS1 to 9.
Metrics in the Rouge family allow for skip n-grams (Lin and Och,2004a); Kauchak and Barzilay (2006) take para phrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamedetal., 2003) calculate both recall and precision; ME TEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ Comparisons of automatic evaluation metrics for machine translation are usually conducted on corpus level using correlation statistics such as Pearson?s product moment correlation coefficient or Spearman?s rank order correlation coefficient between human scores and automatic scores.

BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. $$$$$ From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation.
BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. $$$$$ sentences.
BLEU is smoothed to be more appropriate for sentence level evaluation (Lin and Och, 2004b), and the bi gram versions of BLEU and HWCM are reported because they have higher correlations than when longer n-grams are included. $$$$$ Only one objective function to optimize ? Minimize a single ORANGE score instead of maximize Pearson?s correlation coefficients between automatic scores and human judgments in adequacy, fluency, or other quality metrics.

We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). $$$$$ We then rank these translations, calculate the average rank of the references in the n-best list, and compute the ratio of the average reference rank to the length of the n-best list.
We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). $$$$$ 2 Combinations: C(4,2) = 4!/(2!*2!) = 6..
We then used these coefficients to estimate the confidence interval, after excluding the top 25 and bottom 25 coefficients, following (Lin and Och, 2004). $$$$$ Using the example given in Section 3.1: S1.

Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). $$$$$ We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). $$$$$ However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.
Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). $$$$$ Therefore S2 is better than S3 according to ROUGE-L.
Unless otherwise stated, we will assume the use of sentence BLEU with add1 smoothing (Lin and Och, 2004). $$$$$ This is a feature that many machine translation researchers look for.

As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). $$$$$ Subsequence LCS has many nice properties as we have described in the previous sections.
As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). $$$$$ Many existing data points ? Every sentence is a data point instead of every system (corpus-level).
As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). $$$$$ However, such comparisons rely on human judgments of translation qualities such as adequacy and fluency.
As F1 score is not decomposable, we optimize sentence-level F1 score which serves as an approximation of the corpus-level F1 score. Similarly, Hopkins and May optimize a sentence level BLEU approximation (Lin and Och, 2004) in stead of the corpus-level BLEU score (Papinenietal., 2002). $$$$$ Therefore, for candidate translations with less than n words, they can still get a positive smoothed BLEU score from shorter n-gram matches; however if nothing matches then they will get zero scores.

The optimization objective is sentence level BLEU (Lin and Och, 2004). $$$$$ the gunman kill police S4.
The optimization objective is sentence level BLEU (Lin and Och, 2004). $$$$$ For example, a statistical machine translation system such as ISI?s AlTemp SMT system (Och 2003) can generate a list of n-best alternative translations given a source sentence.
The optimization objective is sentence level BLEU (Lin and Och, 2004). $$$$$ From the BLEU group, we found that shorter BLEU has better adequacy correlation while longer BLEU has better fluency correlation.
The optimization objective is sentence level BLEU (Lin and Och, 2004). $$$$$ BLEU1, 4, and 12 are BLEU with maximum n-gram lengths of 1, 4, and 12 respectively.

As we would like to avoid this problem, we use the smoothed sentence-level Bleuscoreas suggested in (Lin and Och, 2004). $$$$$ We also show the results of comparing several existing automatic metrics and three new automatic metrics using ORANGE.
As we would like to avoid this problem, we use the smoothed sentence-level Bleuscoreas suggested in (Lin and Och, 2004). $$$$$ NIST is very good in adequacy correlation but not as good as GTM30 in fluency correlation.

They obtained similar results in both cases (Lin and Och, 2004a). $$$$$ We call this ratio ?ORANGE?
They obtained similar results in both cases (Lin and Och, 2004a). $$$$$ Unfortunately, these judgments are often inconsistent and very expensive to acquire.
They obtained similar results in both cases (Lin and Och, 2004a). $$$$$ BLEU1, 4, and 12 are BLEU with maximum n-gram lengths of 1, 4, and 12 respectively.

While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them $$$$$ Another possible function family is the polynomial family of the form k?
While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them $$$$$ This example illustrated that ROUGE-L can work reliably at sentence level.
While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them $$$$$ This indicates that the ORANGE evaluation method provides a natural automatic evaluation metric development cycle.
While the F measure over Precision and Recall satisfies these constraints, precision and recall in isolation do not satisfy all of them $$$$$ Only 474 source sentences that have more than 16384 alternative translations are used in this experiment.

The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. $$$$$ One reason for this might be due to data sparseness since only 8 systems are available.
The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. $$$$$ ranging from 0 to 9 (ROUGE-S0 to S9) and without any skip distance limit (ROUGE-S*) We compute the average score of the references and then rank the candidate translations and the references according to these automatic scores.
The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. $$$$$ According to Table 3, we find that shorter BLEUS has better correlation with adequacy.
The introduction of smoothing (Lin and Och, 2004) solves this problem only partially. $$$$$ We call this ratio ?ORANGE?

As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. $$$$$ We compute the automatic scores for the n-best translations and their reference translations.
As the BLEU score is unsuitable for sentence level evaluation in its original definition, BLEU-S smoothing as described by (Lin and Och, 2004) is performed. $$$$$ Applicable on sentence-level ? Diagnostic error analysis on sentence-level is naturally provided.

Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. $$$$$ One reason for this might be due to data sparseness since only 8 systems are available.
Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. $$$$$ Using Equation 9 with ? = 1 and S1 as the reference, S2?s ROUGE-S score is 0.5, S3 is 0.167, and S4 is 0.333.
Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. $$$$$ For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: X: [A B C D E F G] Y1: [A B C D H I K] Y2: [A H B K C I D] Y1 and Y2 have the same ROUGE-L score.

Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y). $$$$$ We call the smoothed BLEU: BLEUS.
Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y). $$$$$ The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.
Using the notation by (Lin and Och, 2004), we denote the skip bigram overlap between two sentences X and Y as Skip2 (X, Y). $$$$$ We call the smoothed BLEU: BLEUS.
