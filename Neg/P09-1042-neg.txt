Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ Also, we define our regularization with respect to inequality constraints (the model is not penalized for exceeding the required model expectations), while they require moments to be close to an estimated value.
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ We also used a generative model based on dependency model with valence (Klein and Manning, 2004).
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ They call their method generalized expectation constraints or alternatively expectation regularization.

Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ They adjusted for this by introducing on the order of one or two dozen language-specific transformation rules to complete target parses for unaligned words and to account for diverging annotation rules.
Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ Recall that standard EM iterates two steps.
Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ In this paper, we present a flexible learning framework for transferring dependency grammars via bitext using the posterior regularization framework (Graça et al., 2008).

In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ We explored two parsing models: a generative model used by several authors for unsupervised induction and a discriminative model used for fully supervised training.
In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ Viterbi decoding is done using Eisner’s algorithm (Eisner, 1996).
In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ Transferring from English to Spanish in this way, they achieve 72.1% and transferring to Chinese they achieve 53.9%.
In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ In this paper, we present a flexible learning framework for transferring dependency grammars via bitext using the posterior regularization framework (Graça et al., 2008).

Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ In the fully supervised experiments we run for comparison, parameter estimation is performed by stochastic gradient ascent on the conditional likelihood function, similar to maximum entropy models or conditional random fields.
Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ Also, we define our regularization with respect to inequality constraints (the model is not penalized for exceeding the required model expectations), while they require moments to be close to an estimated value.
Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ In our experiments we evaluate the learned models on dependency treebanks (Nivre et al., 2007).

While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ The maximum unsupervised accuracy it achieved on the Bulgarian data is 47.6% with initialization from Klein and Manning (2004) and this result is not stable.
While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ Dependency grammars are arguably more robust to transfer since syntactic relations between aligned words of parallel sentences are better conserved in translation than phrase structure (Fox, 2002; Hwa et al., 2005).

However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ We typically also add standard regularization term on θ, resulting from a parameter prior − log p(θ) = R(θ), where p(θ) is Gaussian for the MST-Parser models and Dirichlet for the valence model.

We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis.
We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ We show that discriminative training generally outperforms generative approaches even in this very weakly supervised setting.
We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ We see that performance is still always below the accuracy achieved by supervised training on 20 annotated sentences.

While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ Alshawi et al. (2000) and Hwa et al.
While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis.

P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ A version of the insideoutside algorithm (Lee and Choi, 1997) performs this computation.
P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ The optimization problem in Equation 3 can be efficiently solved in its dual formulation: Given λ, the primal solution is given by: q(z) = pθ(z  |x) exp{−λTf(x, z)}/Z, where Z is a normalization constant.
P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ Also, we define our regularization with respect to inequality constraints (the model is not penalized for exceeding the required model expectations), while they require moments to be close to an estimated value.

For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ While the generative model can get confused and perform poorly when the training data contains very long sentences, the discriminative parser does not appear to have this drawback.
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ Specifically, whenever an auxiliary precedes a main verb the main verb becomes its parent and adopts its children; if there is only one main verb it becomes the root of the sentence; main verbs also become parents of pronouns, adverbs, and common nouns that directly preceed auxiliary verbs.
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ While this captures some of the semantic shared information in the two languages, we have no expectation that the noun “vélo” will have a similar syntactic behavior to the verb “bike”.
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ In this volume (Druck et al., 2009) use this framework to train a dependency parser based on constraints stated as corpus-wide expected values of linguistic rules.

Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ The valencies vr/vl are marked as true if x has any children on the left/right in z, false otherwise.
Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ The probability of any particular parse is where z is a directed edge contained in the parse tree z and φ is a feature function.
Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ For example dealing with auxiliary verbs and reflexive constructions.
Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ The standard objective is to minimize the negative marginal log-likelihood of the data : E[− log pθ(x)] = �E[− log Ez pθ(z, x)] over the parameters θ (we � use E to denote expectation over the sample sentences x).

 $$$$$ The parsing model defines a conditional distribution pg(z I x) over each projective parse tree z for a particular sentence x, parameterized by a vector 0.
 $$$$$ A more serious problem for transferring parse information across languages are structural differences and grammar annotation choices between the two languages.

Table 4 $$$$$ Hwa et al. (2005) found that transferring dependencies directly was not sufficient to get a parser with reasonable performance, even when both the source language parses and the word alignments are performed by hand.
Table 4 $$$$$ Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.
Table 4 $$$$$ As we shall see below, the discriminative parser performs even better than the generative model. u We trained our discriminative parser for 100 iterations of online EM with a Gaussian prior variance of 100.

Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ A more serious problem for transferring parse information across languages are structural differences and grammar annotation choices between the two languages.
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ By adding easily specified languagespecific constraints, our models begin to rival strong supervised baselines for small amounts of data.

 $$$$$ We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
 $$$$$ The standard objective is to minimize the negative marginal log-likelihood of the data : E[− log pθ(x)] = �E[− log Ez pθ(z, x)] over the parameters θ (we � use E to denote expectation over the sample sentences x).
 $$$$$ We typically also add standard regularization term on θ, resulting from a parameter prior − log p(θ) = R(θ), where p(θ) is Gaussian for the MST-Parser models and Dirichlet for the valence model.
 $$$$$ The probability of any particular parse is where z is a directed edge contained in the parse tree z and φ is a feature function.

Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ We conducted experiments on two languages: Bulgarian and Spanish, using each of the parsing models.
Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ In the fully supervised experiments we run for comparison, parameter estimation is performed by stochastic gradient ascent on the conditional likelihood function, similar to maximum entropy models or conditional random fields.
Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ The parsing model defines a conditional distribution pg(z I x) over each projective parse tree z for a particular sentence x, parameterized by a vector 0.

An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ In grammar transfer, our basic constraint is of the form: the expected proportion of conserved edges in a sentence pair is at least η (the exact proportion we used was 0.9, which was determined using unlabeled data as described in Section 5).
An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.

PR $$$$$ In the fully supervised experiments we run for comparison, parameter estimation is performed by stochastic gradient ascent on the conditional likelihood function, similar to maximum entropy models or conditional random fields.
PR $$$$$ We explored two parsing models: a generative model used by several authors for unsupervised induction and a discriminative model used for fully supervised training.

The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.
The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ They adjusted for this by introducing on the order of one or two dozen language-specific transformation rules to complete target parses for unaligned words and to account for diverging annotation rules.

Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ In the fully supervised experiments we run for comparison, parameter estimation is performed by stochastic gradient ascent on the conditional likelihood function, similar to maximum entropy models or conditional random fields.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ The main difference between this work and theirs is the source of the information (a linguistic informant vs. cross-lingual projection).
