Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ Recently, dependency parsing has gained popularity as a simpler, computationally more efficient alternative to constituency parsing and has spurred several supervised learning approaches (Eisner, 1996; Yamada and Matsumoto, 2003a; Nivre and Nilsson, 2005; McDonald et al., 2005) as well as unsupervised induction (Klein and Manning, 2004; Smith and Eisner, 2006).
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ First, parser and word alignment errors cause much of the transferred information to be wrong.
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ A more serious problem for transferring parse information across languages are structural differences and grammar annotation choices between the two languages.
Hwa et al (2005) and Ganchev et al (2009) induce dependency grammar via projection from aligned bilingual corpora, and use some thresholds to filter out noise and some hand-written rules to handle heterogeneity. $$$$$ As these numbers illustrate, directly transferring information one dependency edge at a time is unfortunately error prone for two reasons.

Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ The discriminative parser is based on the edge-factored model and features of the MSTParser (McDonald et al., 2005).
Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.
Ganchev et al (2009) presented a parser projection approach via parallel text using the posterior regularization framework (Graca et al, 2007). $$$$$ One needs to be able to compute expectations of the features φ(z, x) under the distribution pθ(z  |x).

In 1597 Spanish and Bulgarian projected data extracted by Ganchev et al (2009), the figures are 3.2% and 12.9% respectively. $$$$$ Finally, we address challenge (3) by introducing a very small number of language-specific constraints that disambiguate arbitrary annotation choices.

Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ We show that discriminative training generally outperforms generative approaches even in this very weakly supervised setting.
Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ The main difference between this work and theirs is the source of the information (a linguistic informant vs. cross-lingual projection).
Ganchev et al (2009) handle partial projected parses by avoiding committing to entire projected tree during training. $$$$$ By adding easily specified languagespecific constraints, our models begin to rival strong supervised baselines for small amounts of data.

While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ We also used a generative model based on dependency model with valence (Klein and Manning, 2004).
While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ Link-left baselines for these corpora are much lower: 33.8% and 27.9% for Bulgarian and Spanish respectively.
While Hwa et al (2005) requires full projected parses to train their parser, Ganchev et al (2009) and Jiang and Liu (2010) can learn from partially projected trees. $$$$$ By adding easily specified languagespecific constraints, our models begin to rival strong supervised baselines for small amounts of data.

However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ In our experiments we evaluate the learned models on dependency treebanks (Nivre et al., 2007).
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ A version of the insideoutside algorithm (Lee and Choi, 1997) performs this computation.
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ We consider several types of constraints that range from generic dependency conservation to language-specific annotation rules for auxiliary verb analysis.
However, the discriminative training in (Ganchev et al, 2009) doesn't allow for richer syntactic context and it doesn't learn from all the relations in the partial dependency parse. $$$$$ We see that our transfer approach consistently outperforms unsupervised methods and, given just a few (2 to 7) languagespecific constraints, performs comparably to a supervised parser trained on a very limited corpus (30 - 140 training sentences).

We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.
We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ The reason for this is probably that these words are relatively rare, but by encouraging the model to add an edge, it also rules out incorrect edges that would cross it.
We evaluated our system (section 5) on Bulgarian and Spanish projected dependency data used in (Ganchev et al, 2009) for comparison. $$$$$ In this paper, we proposed a novel and effective learning scheme for transferring dependency parses across bitext.

While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ We also used a generative model based on dependency model with valence (Klein and Manning, 2004).
While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
While the Hindi projected tree bank was obtained using the method described in section 4, Bulgarian and Spanish projected datasets were obtained using the approach in (Ganchev et al, 2009). $$$$$ The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.

P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ The posterior-regularized EM algorithm leaves the M-step unchanged, but involves projecting the posteriors onto a constraint set after they are computed for each sentence x: arg min KL(q(z) II pθ(z|x)) where pθ(z|x) are the posteriors.
P (GNPPA) is the percentage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous partial tree constraint and P (E-GNPPA) is the per Exactly 10K sentences were selected in order to compare our results with those of (Ganchev et al, 2009). $$$$$ Transferring from English to Spanish in this way, they achieve 72.1% and transferring to Chinese they achieve 53.9%.

For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ The rules select a class of edges (e.g. auxiliary verb to main verb) and require that the expectation of these be close to some value.
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ The wide availability of parallel text and accurate parsers in English has opened up the possibility of grammar induction through partial transfer across bitext.
For Bulgarian and Spanish, we used the same test data that was used in the work of Ganchev et al (2009). $$$$$ For Spanish, we saw conservation of 64.4% and an average of 5.9 projected edges per sentence (out of 11.5 tokens on average).

Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ Both corpora also contain sentence fragments, either because of question responses or fragmented speech in movie subtitles or because of voting announcements and similar formulaic sentences in the parliamentary proceedings.
Table 4 compares our accuracies with those reported in (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ Finally, we address challenge (3) by introducing a very small number of language-specific constraints that disambiguate arbitrary annotation choices.

 $$$$$ We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees.
 $$$$$ One needs to be able to compute expectations of the features φ(z, x) under the distribution pθ(z  |x).
 $$$$$ Our learning method is very closely related to the work of (Mann and McCallum, 2007; Mann and McCallum, 2008) who concurrently developed the idea of using penalties based on posterior expectations of features not necessarily in the model in order to guide learning.

Table 4: Comparison of baseline, GNPPA and E GNPPA with baseline and discriminative model from (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ The main difference between this work and theirs is the source of the information (a linguistic informant vs. cross-lingual projection).
Table 4: Comparison of baseline, GNPPA and E GNPPA with baseline and discriminative model from (Ganchev et al, 2009) for Bulgarian and Spanish. $$$$$ We deal with this problem by constraining groups of edges rather than a single edge.

Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ Note that we are not restricting ourselves to one-to-one alignments here; p, c, p', and c' can all also align to other words.
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ In particular, we address challenges (1) and (2) by avoiding commitment to an entire projected parse tree in the target language during training.
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ The valencies vr/vl are marked as true if x has any children on the left/right in z, false otherwise.
Ganchev et al (2009)'s baseline is similar to the first iteration of their discriminative model and hence performs better than ours. $$$$$ Unlike previous approaches, our framework does not require full projected parses, allowing partial, approximate transfer through linear expectation constraints on the space of distributions over trees.

 $$$$$ We evaluate our approach on Bulgarian and Spanish CoNLL shared task data and show that we consistently outperform unsupervised methods and can outperform supervised learning for limited training data.
 $$$$$ Unfortunately the sentence in Figure 1(b) is highly unusual in its amount of dependency conservation.
 $$$$$ Table 2 shows attachment accuracy of our method and the baseline for both language pairs under several conditions.

Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.
Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.
Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ Broad-coverage annotated treebanks necessary to train parsers do not exist for many resource-poor languages.
Since posterior regularization is closely related to constraint driven learning, this makes our algorithm also similar to the parser projection approach of Ganchev et al (2009). $$$$$ Note that we are not restricting ourselves to one-to-one alignments here; p, c, p', and c' can all also align to other words.

An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ The posterior-regularized EM algorithm leaves the M-step unchanged, but involves projecting the posteriors onto a constraint set after they are computed for each sentence x: arg min KL(q(z) II pθ(z|x)) where pθ(z|x) are the posteriors.
An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ Dependency representation has been used for language modeling, textual entailment and machine translation (Haghighi et al., 2005; Chelba et al., 1997; Quirk et al., 2005; Shen et al., 2008), to name a few tasks.
An empirical comparison to Ganchev et al (2009) is given in Section 5. $$$$$ Hwa et al. (2005) also note these problems and solve them by introducing dozens of rules to transform the transferred parse trees.

PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. $$$$$ We see that our transfer approach consistently outperforms unsupervised methods and, given just a few (2 to 7) languagespecific constraints, performs comparably to a supervised parser trained on a very limited corpus (30 - 140 training sentences).
PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. $$$$$ In this paper, we present a flexible learning framework for transferring dependency grammars via bitext using the posterior regularization framework (Graça et al., 2008).
PR: The posterior regularization (PR) approach of Ganchev et al (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. $$$$$ To optimize these objectives, we follow an Expectation Maximization-like scheme.

The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ We consider generative and discriminative models for dependency grammar induction that use word-level alignments and a source language parser (English) to constrain the space of possible target trees.
The PR system of Ganchev et al (2009) is similar to ours as it also projects syntax across parallel corpora. $$$$$ They call their method generalized expectation constraints or alternatively expectation regularization.

Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ The valencies vr/vl are marked as true if x has any children on the left/right in z, false otherwise.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ The discriminative parser is based on the edge-factored model and features of the MSTParser (McDonald et al., 2005).
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ Note that we are not restricting ourselves to one-to-one alignments here; p, c, p', and c' can all also align to other words.
