Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). $$$$$ Future work in this direction includes further exploration of the appropriate inventory of semantic classes used as SP’s.
Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). $$$$$ Since many of the criteria for developing such a set are not even known, we decided to experiment with two very different sets of semantic classes, in the hope that in addition to learning semantic preferences, we might also uncover some clues for the eventual decisions about what makes good semantic classes in general.
Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). $$$$$ Similarly to JRM, we extract each instance p, of each semantic relation p and retrieve the set of semantic classes C(x) and C(y) that x and y belong to, accumulating the frequencies of the triples p, and p, where tic class given the relation p, according to Equations 3.3. where p, denotes the frequency of observing cy d c(y) in our equations.
Though, at least, roles of participants in the event have to be preserved by some means, such as the way presented in (Pantel et al, 2007). $$$$$ In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC.

This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. $$$$$ For example, inference rule (1) should only be applied if X is a Person and Y is a Law Enforcement Agent or a Law Enforcement Agency.
This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. $$$$$ We present empirical evidence of its effectiveness.
This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. $$$$$ The aim of this paper is to learn inferential selectional preferences for filtering inference rules.
This work was refined by Pantel et al (2007) by assigning the x and y terms semantic types (inferential selectional preferences - ISP) based on lexical abstraction from empirically observed argument types. $$$$$ Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences.

Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). $$$$$ A confusion matrix captures the filtering performance on both correct and incorrect inferences: where A represents the number of correct instances correctly identified by the system, D represents the number of incorrect instances correctly identified by the system, B represents the number of false positives and C represents the number of false negatives.
Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). $$$$$ For each pattern pi, we then extracted its instances from the Aquaint 1999 AP newswire collection (approximately 22 million words), and randomly selected 10 distinct instances, resulting in a total of 1000 instances.
Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). $$$$$ Finally, we propose inference filtering algorithms in Section 3.3. cx Resnik (1996) defined the selectional preferences of a predicate as the semantic classes of the words that appear as its arguments.
Most distributional methods so far extract representations from large texts, and only as a follow-on step they either 1) alter these in order to reflect a disambiguated word (such as (Erk and Pado, 2008)) or 2) directly asses the appropriateness of a similarity judgment, given a specific context (such as (Pantel et al, 2007)). $$$$$ CBC generated 1628 noun concepts and these were used as our semantic classes for SPs.

Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. $$$$$ While these systems differ in their approaches, neither provides for the extracted inference rules to hold or fail based on SPs.
Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. $$$$$ However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering.
Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. $$$$$ Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences.
Context-sensitive extensions of DIRT (Pantelet al, 2007) and (Basili et al, 2007) focus on making DIRT rules context-sensitive by attaching appropriate semantic classes to the X and Y slots of an inference rule. $$$$$ Also, we experimented with various values for the i parameter described in Section 3.3.

For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. $$$$$ Similarly, we define the relational selectional preferences of a binary semantic relation pi as the semantic classes C(x) of the words that can be instantiated for x and as the semantic classes C(y) of the words that can be instantiated for y.
For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. $$$$$ Intuitively, we have more confidence in a particular candidate if its semantic classes are closely associated given the relation p. Pointwise mutual information (Cover and Thomas 1991) is a commonly used metric for measuring this association strength between two events e1 and e2: 2 In this paper, the semantic classes C(x) and C(y) are extracted from WordNet and CBC (described in Section 4.2).
For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. $$$$$ We investigated this technique but discarded it due to subtle yet critical issues with pattern canonicalization that resulted in rejecting nearly all inferences.
For this (Pantel et al, 2007) build a set of semantic classes using WordNet in one case and CBC clustering algorithm in the other; for each rule, they use the overlap of the fillers found in the input corpus as an indicator of the correct semantic classes. $$$$$ The goal of the filtering task is to minimize false positives (incorrectly accepted inferences) and false negatives (incorrectly rejected inferences).

On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. $$$$$ Future work in this direction includes further exploration of the appropriate inventory of semantic classes used as SP’s.
On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. $$$$$ The intersection of the two sets of SPs forms the candidate inferential SPs for the inference pi => pj: (Law Enforcement Agent, *) (*, Person) We use the same minimum, maximum, and average ranking strategies as in JIM.
On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. $$$$$ The absence of an attested high-quality set of semantic classes for this task makes discovering preferences difficult.
On a common data set (Pantel et al, 2007) and (Basili et al, 2007) achieve significant improvements over DIRT at 95% confidence level when employing the clustering methods. $$$$$ Since the noun hierarchy in WordNet has an average depth of 12, our truncation created a set of concepts considerably coarser-grained than WordNet itself.

A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. $$$$$ Our evaluation tests how well our models can filter these so that only correct inferences are made.
A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. $$$$$ For the 70 disagreements between the judges, a third judge acted as an adjudicator.
A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. $$$$$ Let (x, p, y) be an instance of relation p. Formal task definition: Given an inference rule pi => pj and the instance (x, pi, y), our task is to determine if (x, pj, y) is valid.
A couple of earlier works utilized a cluster-based model (Pantel et al, 2007) and an LSA-based model (Szpektor et al, 2008), in a selectional-preferences style approach. $$$$$ We presented algorithms for learning what we call inferential selectional preferences, and presented evidence that learning selectional preferences can be useful in filtering out incorrect inferences.

While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). $$$$$ For example, for (Law Enforcement Agent, Person), the respective scores would be 1.45, 2.01, and 1.73.
While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). $$$$$ Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g.
While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pantel, 2001), are based on word-level similarity measures, almost all prior models addressing context sensitive predicate inference rules are based on topic models (except for (Pantel et al, 2007), which was outperformed by later models). $$$$$ Semantic inference is a key component for advanced natural language understanding.

In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ This knowledge does not guarantee that the inference rule will hold, but, as we show in this paper, goes a long way toward filtering out erroneous applications of rules.
In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ Overviews of NLP research on this theme are (Wilks and Fass 1992), which includes the influential theory of Preference Semantics by Wilks, and more recently (Light and Greiff 2002).
In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ ISP derives inferential selectional preferences by aggregating statistics of inference rule instantiations over a large corpus of text.
In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al (2007). $$$$$ We did not include in Figure 2 an analysis of the minimum, maximum, and average ranking strategies presented in Section 3.2 since they generally produced nearly identical results.

Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. $$$$$ In the next sections, we describe our collection of inference rules, the semantic classes used for forming selectional preferences, and evaluation criteria for measuring the filtering quality.
Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. $$$$$ We presented algorithms for learning what we call inferential selectional preferences, and presented evidence that learning selectional preferences can be useful in filtering out incorrect inferences.
Terminal nodes of the resultant structure were used as the basis for inferring semantic type restrictions, reminiscent of the use of CBC clusters (Pantel and Lin, 2002) by Pantel et al (2007), for typing the arguments of paraphrase rules. $$$$$ Future work in this direction includes further exploration of the appropriate inventory of semantic classes used as SP’s.

Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. $$$$$ Below we propose joint and independent models, based on a corpus analysis, for automatically determining relational selectional preferences.
Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. $$$$$ Semantic inference is a key component for advanced natural language understanding.
Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. $$$$$ In this section, we provide empirical evidence to support the main claim of this paper.
Pantel et al (2007) and Szpektor et al (2008) represented the context of such rules as the intersection of preferences of the rule's LHS and RHS, namely the observed argument instantiations or their semantic classes. $$$$$ We present empirical evidence of its effectiveness.

To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering.
To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ We present empirical evidence of its effectiveness.
To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ The remainder of this section describes the ISP approach.
To add the necessary context, ISP (Pantel et al, 2007) learned selectional preferences (Resnik, 1997) for DIRT's rules. $$$$$ This papresents a collection of methods for automatically learning admissible argument values to which an inference rule be applied, which we call and methods for filtering out incorrect inferences.

We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. $$$$$ This work constitutes a step towards better understanding of the interaction of selectional preferences and inferences, bridging these two aspects of semantics.
We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. $$$$$ What is missing is knowledge about the admissible argument values for which an inference rule holds, which we call Inferential Selectional Preferences.
We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. $$$$$ In response, several researchers have created resources for enabling semantic inference.
We follow Pantel et al (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. $$$$$ This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001).

MI was also recently used for inference-rule SPs by Pantel et al (2007). $$$$$ For example, here are DIRT’s top 3 inference rules for “X solves Y”: “Y is solved by X”, “X resolves Y”, “X finds a solution to Y” The choice of semantic classes is of great importance for selectional preference.
MI was also recently used for inference-rule SPs by Pantel et al (2007). $$$$$ CBC generated 1628 noun concepts and these were used as our semantic classes for SPs.
MI was also recently used for inference-rule SPs by Pantel et al (2007). $$$$$ Whereas in Section 3.1 we learned selectional preferences for the arguments of a relation p, in this section we learn selectional preferences for the arguments of an inference rule pi => pj.

As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. $$$$$ Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al.
As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. $$$$$ The differences often were in the instances for which one of the judges fails to see the right context under which the inference could hold.
As a way of enriching such a template-like knowledge, Pantel et al (2007) proposed the notion of inferential selectional preference and collected expressions that would fill those slots. $$$$$ However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering.

 $$$$$ Semantic classes can be specified manually or derived automatically.
 $$$$$ Semantic inference is a key component for advanced natural language understanding.
 $$$$$ To minimize disagreements, the judges went through an extensive round of training.

Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. $$$$$ Using these resources in applications has been hindered by the large amount of incorrect inferences they generate, either because of altogether incorrect rules or because of blind application of plausible rules without considering the context of the relations or the senses of the words.
Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. $$$$$ This work constitutes a step towards better understanding of the interaction of selectional preferences and inferences, bridging these two aspects of semantics.
Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. $$$$$ However, existing collections of automatically acquired inference rules have shown disappointing results when used in applications such as textual entailment and question answering.
Pantel et al (2007) apply a collection of rules to filter out incorrect inferences for SP. $$$$$ This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led researchers to focus on automatic methods for building inference resources such as inference rule collections (Lin and Pantel 2001; Szpektor et al. 2004) and paraphrase collections (Barzilay and McKeown 2001).

The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). $$$$$ We present empirical evidence of its effectiveness.
The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). $$$$$ Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995).
The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). $$$$$ Within ISP, we explore different probabilistic models of selectional preference to accept or reject specific inferences.
The notion of Inferential Selectional Preference (ISP) has been introduced by Pantel et al (2007). $$$$$ Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences.

In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. $$$$$ To properly test WordNet as a source of semantic classes for our selectional preferences, we would need to experiment with different extraction algorithms.
In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. $$$$$ Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach.
In (Pantel et al, 2007), they augment each relation with its selectional preferences, i.e. fine-grained entity types of two arguments, to handle polysemy. $$$$$ These different ranking strategies produced nearly identical results in our experiments, as discussed in Section 5.

This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path. $$$$$ To alleviate this problem, we propose a second model that is less strict by considering the arguments of the binary semantic relations independently, as in example (3).
This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path. $$$$$ Several trends can be observed from this figure.
This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path. $$$$$ Model 1: Joint Inferential Model (JIM) Given an inference rule pi => pj, our joint model defines the set of inferential SPs as the intersection of the relational SPs for pi and pj, as defined in the Joint Relational Model (JRM).
This approach can be seen as a proxy to ISP (Pantel et al, 2007), since selectional preferences are one way of distinguishing multiple senses of a path. $$$$$ In this paper, we propose ISP, a collection of methods for learning inferential selectional preferences and filtering out incorrect inferences.
