We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate sev eral tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ For example, animal is an (indirect1) hypernym of dog and conversely dog is an (indi rect) hyponym of animal.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ Another is the ?-skew diver gence measure, which uses the p distribution tosmooth the q distribution.

Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ In Section 5, we relate relative fre quency to a concept of distributional generalityand the semantic relation of hyponymy.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ The cor relation using the recall measure is significant at the 5% level; thus we can conclude that if the simplex verb has high recall retrieval of the phrasal verb?s co-occurrences, then the phrasal is likely to be compositional.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ We have seen that there is a large amount of variation in the neighbours selected by different measures andtherefore the choice of measure in a given appli cation is likely to be important.

Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ Further, the second approach is clearly advanta geous when one wishes to apply distributional similarity methods in a particular application area.
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ By excludingmid-frequency nouns, we obtain a clear sepa ration between high and low frequency nouns.The complete data-set consists of 1,596,798 cooccurrence tokens distributed over 331,079 co occurrence types.
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word.

Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ They ex tract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents(e.g. rip) from the BNC using RASP and cal culate the value of simlin between each phrasal verb and its simplex constituent.
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ We now consider the impact that this has on a potential application of distributional similarity methods.
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ However, it is not at all obvious that oneuniversally best measure exists for all applica tions (Weeds and Weir, 2003).

Approaches that rely on distributional data have two major drawbacks $$$$$ ?).p)) where p = P (c|w1) and q = P (c|w2) conf.
Approaches that rely on distributional data have two major drawbacks $$$$$ Fur ther, the more biased the measure is, the more highly ranked these high frequency words will tend to be.
Approaches that rely on distributional data have two major drawbacks $$$$$ We found that the direc tion of the hyponymy relation is correlated in the predicted direction with the precision-recall 1There may be other concepts in the hypernym chain between dog and animal e.g. carnivore and mammal.values in 71% of cases and correlated in the pre dicted direction with relative frequency in 70% of cases.

 $$$$$ We found that the direc tion of the hyponymy relation is correlated in the predicted direction with the precision-recall 1There may be other concepts in the hypernym chain between dog and animal e.g. carnivore and mammal.values in 71% of cases and correlated in the pre dicted direction with relative frequency in 70% of cases.
 $$$$$ prob.
 $$$$$ We have seen that there is a large variation inneighbours selected by different similarity mea sures.
 $$$$$ Thus, applying adistributional similarity technique to a new ap plication necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm.

Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ The harmonic mean of precision and re call prefers words that have both high precision and high recall.
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ Thus, a large numberof high frequency words in the positions clos est to the target word is considered more biased than a large number of high frequency words distributed throughout the neighbour set.
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ Another is the ?-skew diver gence measure, which uses the p distribution tosmooth the q distribution.

Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ In this section, we introduce some basic con cepts and then discuss the ten distributional similarity measures used in this study.
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ If two sets share roughly half their items and these shared items are dispersed throughout the sets in a roughlysimilar order, we would expect the overlap be tween sets to be around 0.5.
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ target nouns target nouns cm 0.90 0.87 js 0.94 0.70 ? 0.98 0.90 cp 1.00 0.99 ja 0.99 0.21 ja+mi 0.95 0.14 lin 0.85 0.38 P 0.12 0.04 R 0.99 0.98 hm 0.92 0.28 Table 4: Mean value of biashigh according to measure and frequency of target noun.
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy.

Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ The test simplexscore is used to rank the phrasal verbs according to their similarity with their simplexconstituent.
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word.
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ is a way of ?ripping?.

Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ Further, the second approach is clearly advanta geous when one wishes to apply distributional similarity methods in a particular application area.
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ The first approach is not ideal since it assumes that the goal of distributional similarity methods is topredict semantic similarity and that the semantic resource used is a valid gold standard.
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ We then calculatedthe proportion for which the direction of the hy ponymy relation could be accurately predicted by the relative values of precision and recall andthe proportion for which the direction of the hy ponymy relation could be accurately predictedby relative frequency.
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).

For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner $$$$$ Although overlap between most pairs of measures is greater than expected if sets of 200 neighbours were generated randomly from WScomp (in this case, average overlap would be 0.08 and only the overlap between the pairs (?,P) and (cp,P) is not significantly greaterthan this at the 1% level), there are substantial differences between the neighbour sets gen erated by different measures.
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner $$$$$ To do this, we measure the bias in neighbour sets towards high frequency nouns and consider how this varies depending on whether the target noun is itself a high frequency noun or low frequency noun.
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner $$$$$ We also identified one of the major axes ofvariation in neighbour sets as being the fre quency of the neighbours selected relative to the frequency of the target word.
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).

Work on measuring distributional semantic distance $$$$$ We have seen that there is a large amount of variation in the neighbours selected by different measures andtherefore the choice of measure in a given appli cation is likely to be important.
Work on measuring distributional semantic distance $$$$$ The probability of this being the case increasesas the frequency of the potential neighbour increases and so, recall tends to select high fre quency words.
Work on measuring distributional semantic distance $$$$$ The standard deviations (not shown) all lie in the range [0,0.2].
Work on measuring distributional semantic distance $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).

As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ There are a variety of ways in which this workmight be extended.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ This work investigates the variation in a word?s dis tributionally nearest neighbours with respect to the similarity measure used.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ To do this, we measure the bias in neighbour sets towards high frequency nouns and consider how this varies depending on whether the target noun is itself a high frequency noun or low frequency noun.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ The value of the pa rameter ? controls the extent to which the KL divergence is approximated.

Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ An overlap score of 0 is obtained if the sets do not contain any common items.
Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ We found that the direc tion of the hyponymy relation is correlated in the predicted direction with the precision-recall 1There may be other concepts in the hypernym chain between dog and animal e.g. carnivore and mammal.values in 71% of cases and correlated in the pre dicted direction with relative frequency in 70% of cases.
Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ For ex ample, ?strong tea?

of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ In this section, we introduce some basic con cepts and then discuss the ten distributional similarity measures used in this study.
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ An overlap score of 0 is obtained if the sets do not contain any common items.
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy.
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ If Chigh = C(NScomp,NShigh) and Clow =C(NScomp,NSlow), then we compute the bias towards high frequency neighbours for word w us ing measure m as: biashighm(w) = Chigh Chigh+Clow The value of this normalised score lies in the range [0,1] where 1 indicates a neighbour set completely made up of high frequency words, 0 indicates a neighbour set completely made up oflow frequency words and 0.5 indicates a neighbour set with no biases towards high or low fre quency words.

Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ Table 5 shows the results of using different similarity measures with the simplexscore test and data of McCarthy et al (2003).
Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better.

Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ Finally, we obtained a very similar result (0.217) by ranking phrasals according to their inverse relative frequency with their simplex constituent (i.e., freq(simplex)freq(phrasal) ).
Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ Thus, applying adistributional similarity technique to a new ap plication necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm.
Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ Thus, we might ex pect that distributional generality is correlated with semantic generality ? a word has high recall/low precision retrieval of its hyponyms?co-occurrences and high precision/low recall re trieval of its hypernyms?

Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy.
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ However, it is not at all obvious that oneuniversally best measure exists for all applica tions (Weeds and Weir, 2003).
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ We now see significant correlation between compositionality judgements and distributional similarity of thephrasal verb and its head constituent.
