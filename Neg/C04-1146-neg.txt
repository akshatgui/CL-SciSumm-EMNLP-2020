We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ 4.1 Measuring bias.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ ?).p)) where p = P (c|w1) and q = P (c|w2) conf.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ The k nearest neighbours of a target word w are the k words for which similarity with w is greatest.
We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al, 2004). $$$$$ However, it is not at all obvious that oneuniversally best measure exists for all applica tions (Weeds and Weir, 2003).

Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ Finally, we consider theimpact that this has on one application of distributional similarity methods (judging the composition ality of collocations).
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ In Section 4, we take one fundamental statisticalproperty (word frequency) and analyse correla tion between this and the nearest neighbour setsgenerated.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ From this data, we computedthe similarity between every pair of nouns according to each distributional similarity mea sure.
Amongst the many proposals for distributional similarity measures, (Lin, 1998) is maybe the most widely used one, while (Weeds et al, 2004) provides a typical example for recent research. $$$$$ The final three measures are settings in the additive MI-based Co-occurrence Retrieval Model (AMCRM) (Weeds and Weir, 2003; Weeds, 2003).

Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word.
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ If a measure is biased towards selecting high frequency words as neighbours, then we would ex pect that neighbour sets for this measure wouldbe made up mainly of words from WShigh.
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ Thus, a large numberof high frequency words in the positions clos est to the target word is considered more biased than a large number of high frequency words distributed throughout the neighbour set.
Abstracting from results for concrete test sets, Weeds et al (2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends. $$$$$ In our work, theco-occurrence types are always grammatical de pendency relations.

Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ Thus, applying adistributional similarity technique to a new ap plication necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm.
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ We do this by calculating the overlap between neighbour setsfor 2000 nouns generated using different mea sures from direct-object data extracted from the British National Corpus (BNC).
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ We then generated ranked sets of nearest neighbours (of size k = 200 and where a word is excluded from being a neighbour of itself) for each word and each measure.For a given word, we compute the overlap between neighbour sets using a comparison tech nique adapted from Lin (1998).
Therefore to compensate this deficiency (i.e. to eliminate the bias discussed in (Weeds et al, 2004)) an edge length from a property to a ranked term e (pk ,vj) is weighted by the square root of its absolute frequency freq (vj). $$$$$ AcknowledgementsThis work was funded by a UK EPSRC stu dentship to the first author, UK EPSRC project GR/S26408/01 (NatHab) and UK EPSRC project GR/N36494/01 (RASP).

Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004). $$$$$ Thus,this measure cannot be used directly on maxi mum likelihood estimate (MLE) probabilities.One possible solution is to use the JS diver gence measure, which measures the cost of usingthe average distribution in place of each individual distribution.
Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004). $$$$$ Thus, if n1 and n2 are related and P(n2, n1) >R(n2, n1), we might expect that n2 is a hy ponym of n1 and vice versa.
Approaches that rely on distributional data have two major drawbacks: they need a lot of data, generally syntactically parsed sentences, that is not always available for a given language (English is an exception), and they do not discriminate well among lexical relations (mainly hyponyms, antonyms, hypernyms) (Weeds et al, 2004). $$$$$ Measure rs P (rs) under H0 simlin 0.0525 0.2946 precision -0.160 0.0475 recall 0.219 0.0110 harmonic mean 0.011 0.4562 Table 5: Correlation with compositionality for different similarity measures rip up?

 $$$$$ We also identified one of the major axes ofvariation in neighbour sets as being the fre quency of the neighbours selected relative to the frequency of the target word.
 $$$$$ Second, we could consider the impact of frequency characteristics in other applications.Third, for the general application of distribu tional similarity measures, it would be usefulto find other characteristics by which distribu tional similarity measures might be classified.
 $$$$$ In order to test these hypotheses, we ex tracted all of the possible hyponym-hypernym pairs (20, 415 pairs in total) from our list of 2000 nouns (using WordNet 1.6).

Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., ?Xs and other Ys?
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ Thus, if n1 and n2 are related and P(n2, n1) >R(n2, n1), we might expect that n2 is a hy ponym of n1 and vice versa.
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ There are threemajor classes of distributional similarity mea sures which can be characterised as 1) higher frequency selecting or high recall measures; 2)lower frequency selecting or high precision mea sures; and 3) similar frequency selecting or high precision and recall measures.
Over recent years, many applications (Lin, 1998), (Lee, 1999), (Lee, 2001), (Weeds et al,2004), and (Weeds and Weir, 2006) have been investigating the distributional similarity of words. $$$$$ We now con sider whether there is substantial variation ina word?s distributionally nearest neighbours ac cording to the chosen measure.

Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ Further, in the majority ofcases, it tends to be more frequent than its hy ponyms and less frequent than its hypernyms.Thus, there would seem to a three way corre lation between word frequency, distributional generality and semantic generality.We have considered the impact of these observations on a technique which uses a distributional similarity measure to determine compositionality of collocations.
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ We would liketo thank Adam Kilgarriff and Bill Keller for use ful discussions.
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).
Our notion of entailment is 113 based on the concept of distributional generality (Weeds et al, 2004), a generalisation of the distributional hypothesis of Harris (1985), in which it is assumed that terms with a more general meaning will occur in a wider array of contexts, an idea later developed by Geffet and Dagan (2005). $$$$$ Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methodsdo not seem to offer any obvious way to distinguish between the semantic relations of syn onymy, antonymy and hyponymy.

Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ distjs(w2, w1) = 12 ( D ( p||p+q2 ) +D ( q||p+q2 )) where p = P (c|w1) and q = P (c|w2) ?-skew dist?(w2, w1) = D (p||(?.q + (1?
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ Further, the second approach is clearly advanta geous when one wishes to apply distributional similarity methods in a particular application area.
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).
Weeds et al (2004) also found that frequency played a large role in determining the direction of entailment, with the more general term often occurring more frequently. $$$$$ 3.1 Experimental set-up.

Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ The cor relation using the recall measure is significant at the 5% level; thus we can conclude that if the simplex verb has high recall retrieval of the phrasal verb?s co-occurrences, then the phrasal is likely to be compositional.
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ We now consider the impact that this has on a potential application of distributional similarity methods.
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ First, we could use the ob servations about distributional generality andrelative frequency to aid the process of organising distributionally similar words into hierar chies.
Typically the function is empirically chosen based on a performance benchmark and different functions have been shown to provide application specific benefits (Weeds et al, 2004). $$$$$ We then demonstrate a three-way connec tion between relative frequency of similar words, aconcept of distributional gnerality and the seman tic relation of hyponymy.

For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. $$$$$ Fur ther, the more biased the measure is, the more highly ranked these high frequency words will tend to be.
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. $$$$$ The Jensen-Shannon (JS) divergence measure (Rao, 1983) and the ?-skew divergence measure (Lee, 1999) are based on the Kullback-Leibler (KL) divergence measure.
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. $$$$$ This supports the idea of a three-waylinking between distributional generality, rela tive frequency and semantic generality.
For details on DPs and distributional measures, see Weeds et al (2004) and Turney and Pantel (2010). The search of the corpus for paraphrase candidates is performed in the following manner:. $$$$$ Thus, it would seem that the three-way connection betweendistributional generality, hyponymy and rela tive frequency exists for verbs as well as nouns.

Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). $$$$$ However, it is not at all obvious that oneuniversally best measure exists for all applica tions (Weeds and Weir, 2003).
Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). $$$$$ and ?to2Other tests for compositionality investigated by Mc Carthy et al (2003) do much better.
Work on measuring distributional semantic distance: For one survey of this rich topic, see Weeds et al (2004) and Turney and Pantel (2010). $$$$$ We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word.

As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ We have presented an analysis of a set of dis tributional similarity measures.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ rm2(w ?, w) ?k i=1 i2 The overlap score indicates the extent to which sets share members and the extent to whichthey are in the same order.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ The KL divergence,or relative entropy, D(p||q), between two prob ability distribution functions p and q is defined (Cover and Thomas, 1991) as the ?inefficiency of assuming that the distribution is q when the true distribution is p?: D(p||q) = ? c p log p q .However, D(p||q) = ? if there are any con texts c for which p(c) > 0 and q(c) = 0.
As a result, researchers have proposed different approaches to produce transformed vectors using more sophisticated association statistics (see Dumais, 1991, Weeds et al, 2004, Turney and Pantel, 2010, inter alia). $$$$$ However, having discussed a connection between frequency and distributional generality, we might also expect to find that the frequency of the hypernymic term is greater than that of the hyponymicterm.

Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ The prob ability of this being the case increases as the frequency of the potential neighbour decreases and so, precision tends to select low frequencywords.
Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ We also identified one of the major axes ofvariation in neighbour sets as being the fre quency of the neighbours selected relative to the frequency of the target word.
Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ is used as evidence that X is a hyponym of Y. Our work explores the connection between relativefrequency, distributional generality and seman tic generality with promising results.
Using the framework of Weeds et al (2004), we found that the bias of lower frequency words for preferring high-frequency neighbours was higher for RFF (0.58 against 0.35 for Lin? s measure). $$$$$ We identify one type ofvariation as being the relative frequency of the neighbour words with respect to the frequency of the target word.

of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ In both tables, standard devia tions are given in brackets and boldface denotes the highest levels of overlap for each measure.
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ However, it is not at all obvious that oneuniversally best measure exists for all applica tions (Weeds and Weir, 2003).
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ Second, we could consider the impact of frequency characteristics in other applications.Third, for the general application of distribu tional similarity measures, it would be usefulto find other characteristics by which distribu tional similarity measures might be classified.
of Weeds et al (2004), who analyzed the variation in a word's distribution ally nearest neighbours with respect to a variety of similarity measures. $$$$$ Thus, it would seem that the three-way connection betweendistributional generality, hyponymy and rela tive frequency exists for verbs as well as nouns.

Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words.McCarthy et al (2003) also investigate sev eral tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar inmeaning to their constituent parts.
Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ Similarity-based smoothing (Brown et al, 1992; Dagan et al, 1999) is an intuitivelyappealing approach to this problem where prob abilities of unseen co-occurrences are estimatedfrom probabilities of seen co-occurrences of dis tributionally similar events.Other potential applications apply the hy pothesised relationship (Harris, 1968) betweendistributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.One advantage of automatically generated the sauruses (Grefenstette, 1994; Lin, 1998; Curranand Moens, 2002) over large-scale manually cre ated thesauruses such as WordNet (Fellbaum,1998) is that they might be tailored to a partic ular genre or domain.However, due to the lack of a tight defini tion for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see Section 2).
Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ Further, in the majority ofcases, it tends to be more frequent than its hy ponyms and less frequent than its hypernyms.Thus, there would seem to a three way corre lation between word frequency, distributional generality and semantic generality.We have considered the impact of these observations on a technique which uses a distributional similarity measure to determine compositionality of collocations.
Their analysis showed that there are three classes of measures ,i.e. those selecting distribution ally more general neighbours (e.g. cosine), those selecting distribution ally less general neighbours (e.g. AMCRM Precision (Weeds et al, 2004)) and those without abias towards the distributional generality of a neigh bour (e.g. Jaccard). $$$$$ If NS(w,m) is the vector of such scores for word w and measure m, then theoverlap, C(NS(w,m1),NS(w,m2)), of two neigh bour sets is the cosine between the two vectors: C(NS(w,m1),NS(w,m2)) = ? w? rm1(w ?, w)?

Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ We saw that in this ap plication we achieve significantly better resultsusing a measure that tends to select higher frequency words as neighbours rather than a mea sure that tends to select neighbours of a similar frequency to the target word.
Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ We then calculatedthe proportion for which the direction of the hy ponymy relation could be accurately predicted by the relative values of precision and recall andthe proportion for which the direction of the hy ponymy relation could be accurately predictedby relative frequency.
Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ The value obtained (0.0525) is dis appointing since it is not statistically significant(the probability of this value under the null hy pothesis of ?no correlation?
Weeds et al (2004) attempted to refine the distributional similarity goal to predict whether one term is a generalization/specification of the other. $$$$$ In Section 5, we relate relative fre quency to a concept of distributional generalityand the semantic relation of hyponymy.

Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ The correlation score using the precision measure is negative since we would not expect the simplex verb to be a hyponym of the phrasal verb and thus, ifthe simplex verb does have high precision re trieval of the phrasal verb?s co-occurrences, it is less likely to be compositional.
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., ?Xs and other Ys?
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ We use ? = 0.99 since this provides a close approximation to the KL divergence and has been shown to provide good results in previous research (Lee, 2001).
Weeds et al (2004), Lenciand Benotto (2012) and Santus et al (2014) identified hypernyms in distributional spaces. $$$$$ We now see significant correlation between compositionality judgements and distributional similarity of thephrasal verb and its head constituent.
