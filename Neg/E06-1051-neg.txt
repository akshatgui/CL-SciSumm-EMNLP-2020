We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). $$$$$ Furthermore, under-sampling allows us to halve the data set size and reduce the data skewness.
We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). $$$$$ We would like to thank Razvan Bunescu for providing detailed information about the AImed data set and the settings of the experiments.
We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). $$$$$ Recent evaluation campaigns on bio-entity recognition, such as BioCreAtIvE and JNLPBA 2004 shared task, have shown that several systems are able to achieve good performance (even if it is a bit worse than that reported on news articles).
We propose a hybrid kernel by combining the proposed feature based kernel (outlined above) with the Shallow Linguistic (SL) kernel (Giuliano et al., 2006) and the Path-enclosed Tree (PET) kernel (Moschitti, 2004). $$$$$ Other relevant approaches include those that adopt kernel methods to perform relation extraction.

An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. $$$$$ We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.
An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. $$$$$ High precision is particularly advantageous when extracting knowledge from large corpora, because it avoids overloading end users with too many false positives.
An interesting finding is that the Shallow Linguistic (SL) kernel (Giuliano et al 2006) (to be discussed in Section 4.2), despite its simplicity, is on par with the best kernels in most of the evaluation settings. $$$$$ In each example we assign the attribute CANDIDATE to each of the candidate interacting entities, while the other entities in the example are assigned the attribute OTHER, meaning that they do not participate in the relation.

The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006). $$$$$ For this reason, kernels have been recently used to develop innovative approaches to relation extraction based on syntactic information, in which the examples preserve their original representations (i.e. parse trees) and are compared by the kernel function (Zelenko et al., 2003; Culotta and Sorensen, 2004; Zhao and Grishman, 2005).
The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006). $$$$$ The problem considered here is that of identifying interactions between genes and proteins from biomedical literature.
The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006). $$$$$ Zelenko et al. (2003) describe a relation extraction algorithm that uses a tree kernel defined over a shallow parse tree representation of sentences.
The Shallow Linguistic (SL) kernel was proposed by Giuliano et al (2006). $$$$$ We performed experiments on extracting gene and protein interactions from two different data sets.

 $$$$$ Finally, we obtained worse results performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema).
 $$$$$ Second, we plan to test the portability of our model on ACE and MUC data sets.
 $$$$$ The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions).
 $$$$$ In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified.

 $$$$$ Our long term goal is to populate databases and ontologies by extracting information from large text collections such as Medline.
 $$$$$ It is divided in a training set (80 sentences describing 271 interactions) and a test set (87 sentences describing 106 interactions).
 $$$$$ We performed experiments on extracting gene and protein interactions from two different data sets.

A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006). $$$$$ The type of the candidate interacting entities can provide useful clues for detecting the agent and target of the relation, as well as the presence of the relation itself.
A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006). $$$$$ Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities.
A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006). $$$$$ Bunescu and Mooney (2005a) present an alternative approach which uses information concentrated in the shortest path in the dependency tree between the two entities.
A similar finding can be seen, for example, in the relatively flat learning curve of Giuliano et al (2006). $$$$$ We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.

 $$$$$ We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities.
 $$$$$ Therefore it is essential to better investigate the potential of approaches based exclusively on simple linguistic features.
 $$$$$ For example, there are three occurrences of the interaction between IGF-IR and p52Shc (i.e. number 1, 3 and 7).

The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). $$$$$ In each example we assign the attribute CANDIDATE to each of the candidate interacting entities, while the other entities in the example are assigned the attribute OTHER, meaning that they do not participate in the relation.
The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). $$$$$ We performed experiments on extracting gene and protein interactions from two different data sets.
The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). $$$$$ With this aim, we will integrate the output of a parser (possibly trained on a domain-specific resource such the Genia Treebank).
The former approach can produce higher performance: the evaluation of Giuliano et al (2006) includes both alternatives, and their method achieves an F-score of 63.9% under the former criterion, which they term One Answer per Relation in a given Document (OARD). $$$$$ The objective of the challenge was to evaluate the performance of systems based on machine learning techniques to identify gene/protein interactions and their roles, agent or target.

Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). $$$$$ Table 1 shows the performance of the three kernels defined in Section 3 for protein-protein interactions using the two evaluation methodologies described above.
Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). $$$$$ This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned´ellec, 2005).
Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). $$$$$ In one case (AImed) interactions are considered symmetric, while in the other (LLL challenge) agents and targets of genic interactions have to be identified.
Our method outperforms most studies using similar evaluation methodology, with the exception being the approach of Giuliano et al (2006). $$$$$ A second reason concerns the fact that syntactic parsing is not always robust enough to deal with real-world sentences.

Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%. $$$$$ This data set was used in the Learning Language in Logic (LLL) challenge on Genic Interaction extraction5 (Ned´ellec, 2005).
Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%. $$$$$ By substituting OP into Equation 1, we obtain the n-gram kernel Kn, which counts common uni-grams, bi-grams, ... , n-grams that two patterns have in common2.
Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%. $$$$$ For example, as the sentence shown in Figure 1 contains three entities, the total number of examples generated is 3C2 = 3.
Airola et al. (2008) repeat the method published by Giuliano et al (2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%. $$$$$ We report in Figure 4 the precision-recall curves of ERK and KSL using OARD evaluation methodology (the evaluation performed by Bunescu and Mooney (2005b)).

 $$$$$ Even though the resulting feature space has high dimensionality, an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse.
 $$$$$ First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets.
 $$$$$ (Koster and Seutter, 2003)).
 $$$$$ Formally, given a relation example R, a local context L = t_w, ... , t_1, t0, t+1, ... , t+w is represented as a row vector where fi is a feature function that returns 1 if it is active in the specified position of L, 0 otherwise3.

In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. $$$$$ Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.
In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. $$$$$ The latter also includes manually checked information, such as lemma and syntactic dependencies.
In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. $$$$$ As said at the beginning of this section, this task is simpler than the LLL challenge because there is no distinction between types (all entities are proteins) and roles (the relation is symmetric).
In addition to word features, Giuliano et al (2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction. $$$$$ We performed experiments on extracting gene and protein interactions from two different data sets.

On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). $$$$$ Even though the resulting feature space has high dimensionality, an efficient computation of Equation 1 can be carried out explicitly since the input representations defined below are extremely sparse.
On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). $$$$$ If we adopt the OAOD methodology, all the seven occurrences have to be extracted to achieve the maximum score.
On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). $$$$$ This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear.
On the LLL data set, the LA method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (Giuliano et al, 2006). $$$$$ However, relation identification is more useful from an applicative perspective but it is still a considerable challenge for automatic tools.

Giuliano et al (2006) use no syntactic information. $$$$$ We would like to thank Razvan Bunescu for providing detailed information about the AImed data set and the settings of the experiments.
Giuliano et al (2006) use no syntactic information. $$$$$ The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.
Giuliano et al (2006) use no syntactic information. $$$$$ All the previous approaches have been evaluated on different data sets so that it is not possible to have a clear idea of which approach is better than the other.

 $$$$$ The two data sets used for the experiments concern the same domain (i.e. gene/protein interactions).
 $$$$$ The best results on the LLL challenge were obtained by the group from the University of Edinburgh (Reidel and Klein, 2005), which used Markov Logic, a framework that combines loglinear models and First Order Logic, to create a set of weighted clauses which can classify pairs of gene named entities as genic interactions.
 $$$$$ We would like to thank Razvan Bunescu for providing detailed information about the AImed data set and the settings of the experiments.
 $$$$$ As the type is not known, we use the information provided by the two local contexts of the candidate interacting entities, called left and right local context respectively.

In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. $$$$$ Table 3 shows the best results obtained at the official competition performed in April 2005.
In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. $$$$$ Figure 2 shows the examples generated from the sentence in Figure 1.
In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. $$$$$ In (Bunescu and Mooney, 2005b), the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags.
In contrast, work reported in (Giuliano et al, 2006) does not make use of syntactic information which on the data without coreferences yields higher recall. $$$$$ The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities, similarly to what was done by Bunescu and Mooney (2005b).

For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). $$$$$ For this data set entity recognition is performed using a dictionary of protein and gene names in which the type of the entities is unknown.
For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). $$$$$ The experiments show clearly that our approach consistently improves previous results.
For RE, we use AImed, previously used to train protein interaction extraction systems ((Giuliano et al, 2006)). $$$$$ Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.

We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. $$$$$ Third, we would like to use a named entity recognizer instead of assuming that entities are already extracted or given by a dictionary.
We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. $$$$$ We would like to thank Razvan Bunescu for providing detailed information about the AImed data set and the settings of the experiments.
We use the KGC kernel from (Giuliano et al, 2006), one of the highest-performing systems on AImed to date and perform 10-fold cross validation. $$$$$ The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.

The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). $$$$$ At first glance, it may seem strange that KGC outperforms ERK on AImed, as the latter approach exploits a richer representation: sparse sub-sequences of words, PoS tags, entity and chunk types, or WordNet synsets.
The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). $$$$$ A dictionary of named entities (including typographical variants and synonyms) is associated to the data set.
The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). $$$$$ Claudio Giuliano and Lorenza Romano have been supported by the ONTOTEXT project, funded by the Autonomous Province of Trento under the FUP2004 research program.
The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al, 2006). $$$$$ First, we would like to evaluate the contribution of syntactic information to relation extraction from biomedical literature.

Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. $$$$$ Our long term goal is to populate databases and ontologies by extracting information from large text collections such as Medline.
Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. $$$$$ Other relevant approaches include those that adopt kernel methods to perform relation extraction.
Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. $$$$$ The good results obtained using only shallow linguistic features provide a higher baseline against which it is possible to measure improvements obtained using methods based on deep linguistic processing.
Bunescu and Mooney (2005) and Giuliano et al (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. $$$$$ For example, as the sentence shown in Figure 1 contains three entities, the total number of examples generated is 3C2 = 3.
