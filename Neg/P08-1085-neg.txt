Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008). $$$$$ We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.
Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008). $$$$$ While recent work, such as GG, aim to use the Bayesian framework and incorporate “linguistically motivated priors”, in practice such priors currently only account for the fact that language related distributions are sparse - a very general kind of knowledge.
Thus, an orthogonal line of research can involve inducing classes for words which are more general than single categories, i.e., something akin to ambiguity classes (see, e.g., the discussion of ambiguity class guessers in Goldberg et al, 2008). $$$$$ The method is general, and can be applied to different languages.

This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ As expected, EM-HMM improves results (from 80% to 88%).
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline.
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%.
This efficient and data-driven approach gives the best reported tagging accuracy for type-supervised sequence models, outperforming the minimized model of Ravi and Knight (2009), the Bayesian LDA-based model of Toutanova and Johnson (2008), and an HMM trained with language-specific initialization described by Goldberg et al (2008). $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.

 $$$$$ Note that the context-free tagger based on our p(tjw) estimates is quite accurate.
 $$$$$ Instead of inventing a new learning framework, we go back to the traditional EM trained HMMs.
 $$$$$ We use contexts of the form LR=w_1, w+1 (the neighbouring words).
 $$$$$ We presented a general family of algorithms to compute effective initial conditions: estimation of p(t|w) relying on an iterative process shifting probabilities between words and their contexts.

All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. $$$$$ In particular, (Haghighi and Klein, 2006) presents very strong results using a distributional-similarity module and achieve impressive tagging accuracy while starting with a mere 116 prototypical words.
All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. $$$$$ Because of the wide coverage of the Hebrew lexicon, we take RELc to be C (all available contexts).
All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.
All of the methods to which we compare except Goldberg et al (2008) focus on learning and modeling techniques, while our method only addresses initialization. $$$$$ We have access to a good Hebrew lexicon (and a morphological analyzer), and a fair amount of unlabeled training data, but hardly any annotated corpora.

See Goldberg et al (2008) for details. $$$$$ (2) Our models were designed under the assumption of a relatively complete dictionary.
See Goldberg et al (2008) for details. $$$$$ These are complemented by a linear lexical context model.
See Goldberg et al (2008) for details. $$$$$ Finally, we test effectiveness of the initial conditions with EM-HMM learning.
See Goldberg et al (2008) for details. $$$$$ We initialize the p(tjt_1, t+1) distribution with smoothed ML estimates based on tag trigram and bigram counts (ignoring the tag-word annotations).

Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. $$$$$ While these 3 methods rely on the same feature set (lexical context, spelling features) for the learning stage, the LDA approach bases its predictions entirely on observable features, and excludes the traditional hidden states sequence.
Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.
Goldberg et al (2008) provide a linguistically-informed starting point for EM to achieve 91.4% accuracy. $$$$$ We address the task of unsupervised POS tagging.

Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. $$$$$ A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method).
Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. $$$$$ The tagging accuracy of supervised stochastic taggers is around 96%–97% (Manning and Schutze, 1999).
Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. $$$$$ In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on “diluted dictionaries” – in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ).
Goldberg et al (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. $$$$$ Our ambiguity-class guesser assigns for each word the set of all open-class tags that appeared with the word suffix in the dictionary.

(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs. $$$$$ We have access to a good Hebrew lexicon (and a morphological analyzer), and a fair amount of unlabeled training data, but hardly any annotated corpora.
(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs. $$$$$ In contrast, our method allow the incorporation of much more fine-grained intuitions.
(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs. $$$$$ Such models have other benefits as well: they are simple, robust, and computationally more attractive.
(Goldberg et al, 2008) extend the work of (Adler and Elhadad, 2006) by using an EM algorithm, and achieve an accuracy of 88% for full morphological analysis, but again, this does not include lemma IDs. $$$$$ In English, our model is competitive with recent state-of-the-art results, while using simple and efficient learning methods.

 $$$$$ We then report on the effectiveness of p(tjw) as a context-free tagger that assigns to each word the most likely tag, both for full morphological analysis (3,561 tags) (Full) and for the simpler task of token segmentation and POS tag selection (36 tags) (Seg+Pos).
 $$$$$ Linear-Context-based p(t|w) approximation For the initial p(t|w) we use either a uniform distribution based on the tags allowed in the dictionary, or the estimate obtained by using the modified Levinger et al. algorithm.
 $$$$$ For the complete details, refer to the original paper.
 $$$$$ While recent work, such as GG, aim to use the Bayesian framework and incorporate “linguistically motivated priors”, in practice such priors currently only account for the fact that language related distributions are sparse - a very general kind of knowledge.

Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. $$$$$ In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”).
Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. $$$$$ We presented a general family of algorithms to compute effective initial conditions: estimation of p(t|w) relying on an iterative process shifting probabilities between words and their contexts.
Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. $$$$$ We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline.
Despite the fact that HMM-EM has a poor reputation in POS literature (Goldberg et al, 2008) has shown that with good initialization together with some language specific features and language dependent constraints HMM-EM achieves 91.4% accuracy. $$$$$ We now apply the same technique to English semisupervised POS tagging.

Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. $$$$$ We present a family of algorithms to compute effective estimations We test the method on the task of full morphological disambiguation in Hebrew achieving an error reduction of 25% over a strong uniform distribution baseline.
Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. $$$$$ In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t|w) from unlabeled data, which we describe below.
Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. $$$$$ The 17-tags English tagset allows for V-V transitions.
Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al, 2008) showed that by feeding thee M process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text. $$$$$ The tagging accuracy of supervised stochastic taggers is around 96%–97% (Manning and Schutze, 1999).

As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). $$$$$ We run a series of experiments with 8 distinct initial conditions, as shown in Table 1: our baseline (Uniform) is the uniform distribution over all tags provided by the KC analyzer for each word.
As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). $$$$$ We have access to a good Hebrew lexicon (and a morphological analyzer), and a fair amount of unlabeled training data, but hardly any annotated corpora.
As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). $$$$$ Recent work on unsupervised POS tagging for English has significantly improved the results on this task: GG, SE and most recently TJ report the best results so far on the task of unsupervised POS tagging of the WSJ with diluted dictionaries.
As another baseline, we experimented with a pipeline system in which the input text is automatically segmented and tagged using a state-of-the-art HMMpos-tagger (Goldberg et al, 2008). $$$$$ We use the following 3 context templates: LL=w_2,w_1, LR=w_1,w+1 and RR=w+1,w+2.

Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. $$$$$ While we demonstrate competitive results using a simple suffix-based ambiguity-class guesser which ignores capitalization and hyphenation information, we believe there is much room for improvement in this respect.
Goldberg et al 2008 note that fixing noisy dictionaries by hand is actually quite feasible, and suggest that effort should focus on exploiting human knowledge rather than just algorithmic improvements. $$$$$ The other models are variations excluding the Bayesian components (PLSA+AC) or the ambiguity class.

The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). $$$$$ While we demonstrate competitive results using a simple suffix-based ambiguity-class guesser which ignores capitalization and hyphenation information, we believe there is much room for improvement in this respect.
The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). $$$$$ We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.
The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). $$$$$ We also test the same method on the standard WSJ unsupervised POS tagging task and obtain results competitive with recent state-ofthe-art methods, while using simple and efficient learning methods.
The probability of a lemma was defined as the sum of probabilities for all morphological analyses containing the lemma, using a morpho-lexical context-independent probabilities approximation (Goldberg et al, 2008). $$$$$ As such, they are not very good at assigning ambiguity-classes to OOV tokens when starting with a very small dictionary.

The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). $$$$$ We also report state-of-the-art results for Hebrew full morphological disambiguation.
The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). $$$$$ We have demonstrated that unsupervised POS tagging can reach good results using the robust EMHMM learner when provided with good initial conditions, even with incomplete dictionaries.
The latter had two important characteristics: The first is flexibility This tagger allows adapting the estimates of the prior (context-independent) probability of each morphological analysis in an unsupervised manner, from an unlabeled corpus of the target domain (Goldberg et al, 2008). $$$$$ Brill (1995b) proposed an unsupervised tagger based on transformationbased learning (Brill, 1995a), achieving accuracies of above 95%.

The importance is underscored succinctly by Goldberg et al (2008). $$$$$ However, better initial conditions bring us much over this particular local maximum – with an error reduction of 20%.
The importance is underscored succinctly by Goldberg et al (2008). $$$$$ Such models have other benefits as well: they are simple, robust, and computationally more attractive.
The importance is underscored succinctly by Goldberg et al (2008). $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.
The importance is underscored succinctly by Goldberg et al (2008). $$$$$ While our models are trained on the unannotated text of the entire WSJ Treebank, CE and BHMM use much less training data (only the 24k words of the test-set).

EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). $$$$$ This results in a potentially high number of tags for each token.
EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). $$$$$ They experiment with full dictionaries (containing complete POS information for all the words in the text) as well as “diluted” dictionaries, from which large portions of the vocabulary are missing.
EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008). $$$$$ With (artificially created) good initial conditions, such as a good approximation of the tag distribution for each word, Elworthy reports an improvement to 94.6%, 92.27%, and 94.51% on the same data sets.

Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. $$$$$ Merialdo, on the other hand, reports an improvement to 92.6% and 94.4% for the case where 100 and 2,000 sentences of the training corpus are manually tagged.
Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. $$$$$ In Hebrew, Levinger et al. (1995) introduced the similar-words algorithm for estimating p(t|w) from unlabeled data, which we describe below.
Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. $$$$$ It seems that computing initial conditions is also the right place to capture complex linguistic intuition without fear that over-generalization could lead a learner to diverge.
Goldberg et al (2008) depart from the Bayesian framework and show how EM can be used to learn good POS taggers for Hebrew and English, when provided with good initial conditions. $$$$$ Linear-Context-based p(t|w) approximation For the initial p(t|w) we use either a uniform distribution based on the tags allowed in the dictionary, or the estimate obtained by using the modified Levinger et al. algorithm.

The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. $$$$$ (uniform(oc)) or the allowed tags according to our simple ambiguity-class guesser (uniform(suf)).
The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. $$$$$ CE+spl is the Contrastive-Estimation CRF method of SE.
The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. $$$$$ We address the task of unsupervised POS tagging.
The system achieves a better accuracy than the 88.6% from Smith and Eisner (2005), and even surpasses the 91.4% achieved by Goldberg et al (2008) without using any additional linguistic constraints or manual cleaning of the dictionary. $$$$$ In English, our model is competitive with recent state-of-the-art results, while using simple and efficient learning methods.

Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008). $$$$$ We demonstrate that good results can be obtained using the robust EM-HMM learner when provided with good initial conditions, even with incomplete dictionaries.
Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008). $$$$$ (2) Our models were designed under the assumption of a relatively complete dictionary.
Their models are trained on the entire Penn Treebank data (instead of using only the 24,115-token test data), and so are the tagging models used by Goldberg et al (2008). $$$$$ The parameters of this process (definition of the contexts and initial estimations of p(t|w) can safely encapsulate rich linguistic intuitions.
