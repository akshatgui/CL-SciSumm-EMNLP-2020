ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). $$$$$ Section 6 shows how these measures correlate with human judgments using DUC 2001, 2002, and 2003 data.
ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). $$$$$ We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.
ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). $$$$$ To improve the basic LCS method, we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS.
ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004). $$$$$ The ground truth is based on human assigned scores.

We used ROUGE (Lin, 2004) as an evaluation criterion. $$$$$ Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram-based F-measure as follows: Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y, ß controlling the relative importance of Pskip2 and Rskip2, and C is the combination function.
We used ROUGE (Lin, 2004) as an evaluation criterion. $$$$$ This paper introduces four different included in the summarization evaluation package and their evaluations.

Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004). $$$$$ We can also obtain ROUGE-SU from ROUGE-S by adding a begin-of-sentence marker at the beginning of candidate and reference sentences.
Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004). $$$$$ In this paper, we introduced ROUGE, an automatic evaluation package for summarization, and conducted comprehensive evaluations of the automatic measures included in the ROUGE package using three years of DUC data.
Part of this research takes inspiration from the work on automatic evaluation in machine translation (Papineni et al, 2002) and automatic summarisation (Lin, 2004). $$$$$ It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.

with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. $$$$$ A sequence Z = [z1, z2, ..., zn] is a subsequence of another sequence X = [x1, x2, ..., xm], if there exists a strict increasing sequence [i1, i2, ..., ik] of indices of X such that for all j = 1, 2, ..., k, we have xij = zj (Cormen et al., 1989).
with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. $$$$$ ROUGE-1, ROUGE-S4, ROUGE-SU4, ROUGE-S9, and ROUGESU9 with stopword removal had correlation above 0.70.
with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. $$$$$ In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations.
with the addition of skip-bigram features derived from the ROUGE-S (Lin, 2004) measure. $$$$$ Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.

ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;. $$$$$ In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations.
ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al, 2005), evaluate a summary by computing its overlap with a set of model (human) summaries;. $$$$$ It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.

Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. $$$$$ This effectively gives more weight to matching n-grams occurring in multiple references.
Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. $$$$$ For example, given a reference sequence X and two candidate sequences Y1 and Y2 as follows: Y1 and Y2 have the same ROUGE-L score.
Version 1.5.5 of the ROUGE scoring algorithm (Lin, 2004) is also used for evaluating results. $$$$$ We use the polynomial function of the form ka in the ROUGE evaluation package.

Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. $$$$$ Comparing skip-bigram with LCS, skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence.
Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. $$$$$ We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.
Such metrics have been introduced in other fields, including PAR ADISE (Walker et al, 1997) for spoken dialogue systems, BLEU (Papineni et al, 2002) for machine translation,1 and ROUGE (Lin, 2004) for summarisation. $$$$$ This paper introduces four different included in the summarization evaluation package and their evaluations.

Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries. $$$$$ Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.
Automated evaluation utilizes the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system generated summary against a set of human generated peer summaries. $$$$$ This paper introduces four different included in the summarization evaluation package and their evaluations.

We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ The other advantage is that it automatically includes longest in-sequence common n-grams, therefore no predefined n-gram length is necessary.
We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y.
We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ LCS has many nice properties as we have described in the previous sections.
We considered a variety of tools like ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) but decided they were unsuitable for this task. $$$$$ This paper introduces four different included in the summarization evaluation package and their evaluations.

For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004). $$$$$ LCS∪ (ri, C) is the LCS score of the union longest common subsequence between reference sentence ri and candidate summary C. For example, if ri = w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and c2 = w1 w3 w8 w9 w5, then the longest common subsequence of ri and c1 is “w1 w2” and the longest common subsequence of ri and c2 is “w1 w3 w5”.
For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004). $$$$$ Therefore, 2 (multi or single) x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) = 18 data points were collected for each ROUGE measure and each DUC task.
For the evaluation measure, we used the standard ROUGE suite of automatic evaluation measures (Lin, 2004). $$$$$ The relative performance of ROUGE measures followed the pattern of the 100 words multi-document summarization task.

 $$$$$ However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questions and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts.
 $$$$$ The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004).
 $$$$$ for Recall-Oriented Understudy for Gisting Evaluation.

 $$$$$ However, how to achieve high correlation with human judgments in multi-document summarization tasks as ROUGE already did in single document summarization tasks is still an open research topic.
 $$$$$ Due to the large number of samples (624) in this data set, using multiple references did not improve correlations.
 $$$$$ We found that (1) ROUGE-2, ROUGE-L, ROUGE-W, and ROUGE-S worked well in single document summarization tasks, (2) ROUGE-1, ROUGE-L, ROUGE-W, ROUGE-SU4, and ROUGE-SU9 performed great in evaluating very short summaries (or headline-like summaries), (3) correlation of high 90% was hard to achieve for multi-document summarization tasks but ROUGE-1, ROUGE-2, ROUGE-S4, ROUGE-S9, ROUGE-SU4, and ROUGE-SU9 worked reasonably well when stopwords were excluded from matching, (4) exclusion of stopwords usually improved correlation, and (5) correlations to human judgments were increased by using multiple references.

They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ The extended version is called ROUGE-SU.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ A closely related measure, BLEU, used in automatic evaluation of machine translation, is a precision-based measure.
They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). $$$$$ Due to limitation of space, we only report correlation analysis results based on Pearson’s correlation coefficient.

ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). $$$$$ To check the significance of the results, we estimated confidence intervals of correlations using bootstrap resampling.
ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). $$$$$ Using Equation 15 and f(k)=k2 as the weighting function, the ROUGE-W scores for sequences Y1 and Y2 are 0.571 and 0.286 respectively.
ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). $$$$$ However, in order to normalize the final ROUGE-W score, we also prefer to have a function that has a close form inverse function.
ROUGE-2 (based on bi grams) and ROUGE-SU4 (based on both unigrams and skip-bi grams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). $$$$$ It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.

Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. $$$$$ Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences.
Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. $$$$$ The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.
Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. $$$$$ Note that the number of n-grams in the denominator of the ROUGE-N formula increases as we add more references.
Empirical evaluations using two standard summarization metrics the Pyra mid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004) show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. $$$$$ It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.

Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). $$$$$ Given translations X of length m and Y of length n, assuming X is a reference translation and Y is a candidate translation, we compute skip-bigram-based F-measure as follows: Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y, ß controlling the relative importance of Pskip2 and Rskip2, and C is the combination function.
Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). $$$$$ The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y.
Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). $$$$$ Another possible function family is the polynomial family of the form ka where -a > 1.

However, original measures based on lexical matching, such as BLEU (Papineni et al, 2001a) and ROUGE (Lin, 2004) are still preferred as de facto standards in MT and AS, respectively. $$$$$ Adjusting Equations 16, 17, and 18 to use maximum skip distance limit is straightforward: we only count the skip-bigram matches, SKIP2(X,Y), within the maximum skip distance and replace denominators of Equations 16, C(m,2), and 17, C(n,2), with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively.

In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004). $$$$$ The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.
In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004). $$$$$ In the next section, we introduce the skip-bigram co-occurrence statistics.
In the case of automatic summarization (AS), we have employed the standard variants of ROUGE (Lin, 2004). $$$$$ In this paper, we introduce a package, ROUGE, for automatic evaluation of summaries and its evaluations.

We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures. $$$$$ Acquiring human judgments are usually very expensive; fortunately, we have DUC 2001, 2002, and 2003 evaluation data that include human judgments for the following: Besides these human judgments, we also have 3 sets of manual summaries for DUC 2001, 2 sets for DUC 2002, and 4 sets for DUC 2003.
We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures. $$$$$ Using the example given in Section 3.1: each sentence has C(4,2)1 = 6 skip-bigrams.
We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures. $$$$$ Unfortunately, the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences.
We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGEscore (Lin, 2004) and subjective readability measures. $$$$$ If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams.

We used ROUGE (Lin, 2004) for evaluating the content of summaries. $$$$$ Note that human judges used only one manual summary in all the evaluations although multiple alternative summaries were available.
We used ROUGE (Lin, 2004) for evaluating the content of summaries. $$$$$ The stability and reliability of ROUGE at different sample sizes was reported by the author in (Lin, 2004).
We used ROUGE (Lin, 2004) for evaluating the content of summaries. $$$$$ Therefore, 2 (multi or single) x 3 (CASE, STEM, or STOP) x 3 (Pearson, Spearman, or Kendall) = 18 data points were collected for each ROUGE measure and each DUC task.
