Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ For example, to apply this approach to the 71,520 sentence pairs from the MT evaluation set (described in Section 4.1.2), we had to train 2,380 classifiers.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ In this paper, we explore the use of paraphrasing methods for refinement of automatic evaluation techniques.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.

Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ To train such a classifier, we collect a large corpus of sentences that contain the word wj and an equal number of randomly extracted sentences that do not contain this word.
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.

Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference.
Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ 2b.
Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ The BLEU score is computed as follows: where pn is the n-gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences.

Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ To evaluate the accuracy of different paraphrasing methods, we randomly extracted 200 paraphrasing examples from each method.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ Both techniques are based on distributional similarity.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ As Table 7 shows, the ranking between the accuracy of the different paraphrasing methods mirrors the ranking of the corresponding MT evaluation methods shown in Table 4.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference.

Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ BLEU is the basic evaluation measure that we use in our experiments.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ It is hard to believe that such tremendous changes have taken place for those people and lands that I have never stopped missing while living abroad.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ Incorporating syntactic dependencies and class-based features into the context representation could also increase the accuracy and the coverage of the method.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ We apply our paraphrasing method in the context of machine translation evaluation.

Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.
Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ We focus on automatic evaluation of machine translation due to the availability of human annotated data in that domain.
Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.
Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ This process is repeated for ten different documents created by the same process.

Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ These results suggest that the substitution frequency cannot predict the utility value of the paraphrasing method.
Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ In section 4.3, we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods.

In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ This paper studies the impact of paraphrases on the accuracy of automatic evaluation.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ The BLEU score is computed as follows: where pn is the n-gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ Thanks to Michael Collins, Charles Elkan, Yoong Keok Lee, Philip Koehn, Igor Malioutov, Ben Snyder and the anonymous reviewers for helpful comments and suggestions.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.

In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ To evaluate the accuracy of different paraphrasing methods, we randomly extracted 200 paraphrasing examples from each method.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ Words home and place are paraphrases in the sense of “habitat”, but in the reference sentence “place” occurs in a different sense, being part of the collocation “take place”.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ First, the algorithm identifies pairs of words from the reference and the system output that could potentially form paraphrases.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ The synthetic reference keeps the meaning of the original reference, but has a higher word overlap with the system output.

Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ This process is repeated for ten different documents created by the same process.
Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ These results suggest that the substitution frequency cannot predict the utility value of the paraphrasing method.
Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ We apply our paraphrasing method in the context of machine translation evaluation.
Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ In the future, we would like to incorporate substitutions at the level of phrases and syntactic trees.

We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ These results suggest that the substitution frequency cannot predict the utility value of the paraphrasing method.
We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ We found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.

To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ One source of possible improvement lies in exploring more powerful learning frameworks and more sophisticated linguistic representations.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ The hypothesis is that by using a synthetic reference translation, automatic measures approximate better human evaluation.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ BLEU is the basic evaluation measure that we use in our experiments.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.

For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ This corpus is acquired automatically, and does not require any manual annotations.
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005).
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ Both n-grams and collocations exclude the word wj.
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ A local collocation also takes into account the position of an n-gram with respect to the target word.

Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ Our experiments show improvement of the accuracy of WordNet paraphrasing and we believe that this method can similarly benefit other approaches that use lexicosemantic resources to obtain paraphrases.
Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ Candidate Selection We assume that words from the reference sentence that already occur in the system generated sentence should not be considered for substitution.
Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ In this paper, we report experiments with BLEU due to its wide use in the machine translation community.
Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ We begin by describing relevant background information, including the BLEU evaluation method, the test data set, and the alternative paraphrasing methods considered in our experiments.

In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ In section 4.2, we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ The question is whether such cases are common phenomena or infrequent exceptions.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ Examples of such knowledge sources include stemming and TF-IDF weighting (Babych and Hartley, 2004; Banerjee and Lavie, 2005).
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ For the negative examples, a random position in a sentence is selected for extracting the context.

 $$$$$ Our experiments show improvement of the accuracy of WordNet paraphrasing and we believe that this method can similarly benefit other approaches that use lexicosemantic resources to obtain paraphrases.
 $$$$$ For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.

PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ Consider the candidate pair (home, place) from our example (see Table 2).
PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ Methods for entailment recognition extensively rely on lexico-semantic resources (Haghighi et al., 2005; Harabagiu et al., 2001), and we believe that our method for contextual substitution can be beneficial in that context.
PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ Our experiments show that evaluation based on paraphrased references gives a better approximation of human judgments than evaluation that uses original references.
PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ We apply our paraphrasing method in the context of machine translation evaluation.

This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ We apply our paraphrasing method in the context of machine translation evaluation.
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168) and DARPA (Kauchak; grant HR0011-06-C-0023).
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.

Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation. $$$$$ Clearly, any measure based on word overlap will penalize a system for generating such a sentence.
Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation. $$$$$ Our method for reference paraphrasing can be combined with any of these metrics.
Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation. $$$$$ In our experiments we compute semantic similarity using WordNet, a large-scale lexico-semantic resource employed in many NLP applications for similar pur2a.
Sentence Similarity computation: Kauchak and Barzilay (2006) have tried paraphrasing-based sentence similarity computation. $$$$$ This paper studies the impact of paraphrases on the accuracy of automatic evaluation.
