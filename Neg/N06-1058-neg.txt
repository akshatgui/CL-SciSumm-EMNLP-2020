Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ Thanks to Michael Collins, Charles Elkan, Yoong Keok Lee, Philip Koehn, Igor Malioutov, Ben Snyder and the anonymous reviewers for helpful comments and suggestions.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ Table 6 shows the substitution frequency and the corresponding BLEU score.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
Kauchak and Barzilay (2006) have shown that creating synthetic reference sentences by substituting synonyms from Wordnet into the original reference sentences can increase the number of exact word matches with an MT system's output and yield significant improvements in correlations of BLEU (Papineni et al., 2002) scores with human judgments of translation adequacy. $$$$$ Our results suggest that researchers may find it useful to augment standard measures such as BLEU and ROUGE with paraphrasing information thereby taking more semantic knowledge into account.

Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ Our method for reference paraphrasing can be combined with any of these metrics.
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ We apply our paraphrasing method in the context of machine translation evaluation.
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ We compute the substitution frequency of an automatic paraphrasing method by counting the number of words it rewrites in a set of reference sentences.
Deriving lexical relatedness between terms has been a topic of interest with applications in word sense disambiguation (Patwardhan et al, 2005), paraphrasing (Kauchak and Barzilay, 2006), question answering (Prager et al, 2001), and machine translation (Blatz et al, 2004) to name a few. $$$$$ Applying this step to the two sentences in Table 2, we obtain two candidate pairs (home, place) and (difficult, hard).

Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ For each candidate word wj we train a classifier that models contextual preferences of wj.
Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ The input to our method consists of a reference sentence R = r1 ... rm and a system-generated sentence W = w1 ... wp whose words form the sets R and W respectively.
Banerjee and Lavie (2005) and Chan and Ng (2008) use WordNet, and Zhou et al (2006) and Kauchak and Barzilay (2006) exploit large collections of automatically-extracted paraphrases. $$$$$ Words home and place are paraphrases in the sense of “habitat”, but in the reference sentence “place” occurs in a different sense, being part of the collocation “take place”.

Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ Empirical evidence supports the former.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ We focus on automatic evaluation of machine translation due to the availability of human annotated data in that domain.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference.
Kauchak and Barzilay (2006) used paraphrases of the reference translations to improve automatic MT evaluation. $$$$$ We select these candidates using existing lexico-semantic resources such as WordNet.

Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ We use the Chinese portion of the 2004 NIST MT dataset.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ Our experiments show improvement of the accuracy of WordNet paraphrasing and we believe that this method can similarly benefit other approaches that use lexicosemantic resources to obtain paraphrases.
Some researchers extract synonyms as paraphrases (Kauchak and Barzilay, 2006), while some others use looser definitions, such as hypernyms and holonyms (Barzilay and Elhadad, 1997). $$$$$ Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.

Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ In this paper, we report experiments with BLEU due to its wide use in the machine translation community.
Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ Our work also has interesting connections with research on automatic textual entailment (Dagan et al., 2005), where the goal is to determine whether a given sentence can be inferred from text.
Training a classifier for word paraphrasing, Kauchak and Barzilay (2006) used occurrences of the rule's RHS as positive context examples, and randomly picked negative examples. $$$$$ 5Depending on the experimental setup, correlation values can vary widely.

Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ We begin by describing relevant background information, including the BLEU evaluation method, the test data set, and the alternative paraphrasing methods considered in our experiments.
Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ This corpus is acquired automatically, and does not require any manual annotations.
Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ By analyzing several paraphrasing resources, we found that the accuracy and coverage of a paraphrasing method correlate with its utility for automatic MT evaluation.
Metrics in the Rouge family allow for skip n-grams (Lin and Och, 2004a); Kauchak and Barzilay (2006) take paraphrasing into account; metrics such as METEOR (Banerjee and Lavie, 2005) and GTM (Melamed et al., 2003) calculate both recall and precision; METEOR is also similar to SIA (Liu and Gildea, 2006) in that word class information is used. $$$$$ We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.

In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ Thanks to Michael Collins, Charles Elkan, Yoong Keok Lee, Philip Koehn, Igor Malioutov, Ben Snyder and the anonymous reviewers for helpful comments and suggestions.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.
In text summarization (Zhou et al, 2006) and machine translation (Kauchak and Barzilay, 2006), summaries comparison based on sentence similarity has been applied for automatic evaluation. $$$$$ We apply our paraphrasing method in the context of machine translation evaluation.

In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ For example, Pang et al. (2003) expand a set of reference translations using syntactic alignment, and generate new reference sentences that could be used in automatic evaluation.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ Our primary goal is to investigate the impact of machine-generated paraphrases on the accuracy of automatic evaluation.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) and Owczarzak et al (2006) in adding a number of paraphrases in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ Our secondary goal is to study the relationship between the quality of paraphrases and their contribution to the performance of automatic machine translation evaluation.

Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
Kauchak and Barzilay (2006) and Owczarzak et al (2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al (2006). $$$$$ By analyzing several paraphrasing resources, we found that the accuracy and coverage of a paraphrasing method correlate with its utility for automatic MT evaluation.

We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference.
We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ In section 4.2, we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references.
We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ Any opinions, findings and conclusions expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA or NSF.
We introduced synonyms and paraphrases into the process of evaluation, creating new best-matching references for the translations using either paraphrases derived from the test set itself (following Owczarzak et al (2006)) or WordNet synonyms (as in Kauchak and Barzilay (2006)). $$$$$ In section 4.3, we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods.

To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ Our method for reference paraphrasing can be combined with any of these metrics.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ In this formulation, all substitutions are tested independently.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ The BLEU score is computed as follows: where pn is the n-gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences.
To maximize the number of matches between a translation and a reference, Kauchak and Barzilay (2006) use WordNet synonyms during evaluation. $$$$$ Incorporating syntactic dependencies and class-based features into the context representation could also increase the accuracy and the coverage of the method.

For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ For the example from Table 2, only the pair (difficult, hard) passes this filter, and thus the system produces the following synthetic reference: For someone born here but has been sentimentally attached to a foreign country far from home, it is hard to believe this kind of changes.
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ Our method for reference paraphrasing can be combined with any of these metrics.
For example, (Kauchak and Barzilay, 2006) paraphrase references to make them closer to the system translation in order to obtain more reliable results when using automatic evaluation metrics like BLEU (Papineni et al, 2002). $$$$$ This paper studies the impact of paraphrases on the accuracy of automatic evaluation.

Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.
Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ The question is whether such cases are common phenomena or infrequent exceptions.
Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). $$$$$ Automatic Paraphrasing and Entailment Our work is closely related to research in automatic paraphrasing, in particular, to sentence level paraphrasing (Barzilay and Lee, 2003; Pang et al., 2003; Quirk et al., 2004).

In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ The BLEU score is computed as follows: where pn is the n-gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ While we are not assessing an inference relation between a reference and a system output, the two tasks face similar challenges.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ BLEU is the basic evaluation measure that we use in our experiments.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score. $$$$$ This paper studies the impact of paraphrases on the accuracy of automatic evaluation.

 $$$$$ The input to our method consists of a reference sentence R = r1 ... rm and a system-generated sentence W = w1 ... wp whose words form the sets R and W respectively.
 $$$$$ The authors acknowledge the support of the National Science Foundation (Barzilay; CAREER grant IIS-0448168) and DARPA (Kauchak; grant HR0011-06-C-0023).
 $$$$$ The former category forms positive instances, while the latter represents the negative.
 $$$$$ Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.

PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ BLEU is the basic evaluation measure that we use in our experiments.
PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ In this paper, we report experiments with BLEU due to its wide use in the machine translation community.
PG shows its importance in many areas, such as question expansion in question answering (QA) (Duboueand Chu-Carroll, 2006), text polishing in natural language generation (NLG) (Iordanskaja et al, 1991), text simplification in computer-aided reading (Carroll et al, 1999), and sentence similarity computation in the automatic evaluation of machine translation (MT) (Kauchak and Barzilay, 2006) and summarization (Zhou et al, 2006). $$$$$ In section 4.3, we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods.

This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ In section 4.2, we test this hypothesis by comparing the performance of BLEU scores with and without synthetic references.
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference.
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ For the negative examples, a random position in a sentence is selected for extracting the context.
This application is important for the automatic evaluation of machine translation and summarization, since we can paraphrase the human translations/summaries to make them more similar to the system outputs, which can refine the accuracy of the evaluation (Kauchak and Barzilay, 2006). $$$$$ While we experimented with several parameter settings for LSA and Brown methods, we do not claim that the selected settings are necessarily optimal.

Sentence Similarity computation $$$$$ This paper studies the impact of paraphrases on the accuracy of automatic evaluation.
Sentence Similarity computation $$$$$ An n-gram is a sequence of n adjacent words appearing in r1 ... ri−10ri+1 ... rm.
Sentence Similarity computation $$$$$ Any opinions, findings and conclusions expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA or NSF.
Sentence Similarity computation $$$$$ We focus on automatic evaluation of machine translation due to the availability of human annotated data in that domain.
