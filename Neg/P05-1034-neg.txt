 $$$$$ Currently we only consider the top parse of an input sentence.
 $$$$$ In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne...pas except in the context of specific intervening words.
 $$$$$ The impact of this optimization is explored in Table 5.6.
 $$$$$ All results are based on a training set size of 100,000 sentences and a phrase size of 4, except Table 5.2 which varies the phrase size and Table 5.3 which varies the training set size.

This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ To translate an input sentence, we parse the sentence, producing a dependency tree for that sentence.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ It utilizes a separate sentence realization component (Ringger et al., 04) to turn this into a target sentence.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ We ran experiments on subsets of the training data ranging from 1,000 to 300,000 sentences.

Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ As such, it does not use a target language model during decoding, relying instead on MLE channel probabilities and heuristics such as pattern size.
Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ We used the heuristic combination described in (Och & Ney, 03) and extracted phrasal translation pairs from this combined alignment as described in (Koehn et al., 03).
Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04).

We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ This reordering model is very limited in terms of linguistic generalizations.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ As such, it does not use a target language model during decoding, relying instead on MLE channel probabilities and heuristics such as pattern size.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ Our experience has been that the quality of GIZA++ alignments for such language pairs is inadequate.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ All results are based on a training set size of 100,000 sentences and a phrase size of 4, except Table 5.2 which varies the phrase size and Table 5.3 which varies the training set size.

 $$$$$ This results in the model simply learning a very high level preference regarding how often nodes should switch order without any contextual information.
 $$$$$ We infer that whereas Pharaoh depends heavily on long phrases to encapsulate reordering, our dependency treebased ordering model enables credible performance even with single-word ‘phrases’.
 $$$$$ We infer that whereas Pharaoh depends heavily on long phrases to encapsulate reordering, our dependency treebased ordering model enables credible performance even with single-word ‘phrases’.

Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser developed at Microsoft Research able to produce syntactic analyses at varying levels of depth (Heidorn, 02).
Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ While all the differences are statistically significant at the 99% confidence level, the wide gap at smaller phrase sizes is particularly striking.
Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.

This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.
This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ Thus we try to predict as follows: Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03).
This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ We presented a novel approach to syntacticallyinformed statistical machine translation that leverages a parsed dependency tree representation of the source language via a tree-based ordering model and treelet phrase extraction.
This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ We compared against two radically different types of systems to demonstrate the competitiveness of this approach: We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data (e.g., support articles, product documentation).

For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04).
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ We also add a feature that counts the number of target words; this acts as an insertion/deletion bonus/penalty.
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ Different model structures, machine learning techniques, and target feature representations all have the potential for significant improvements.

We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.
We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ Yet despite this success, statistical machine translation (SMT) has many hurdles to overcome.

(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ Our heuristics differ in that they constrain manyto-one alignments to be contiguous in the source dependency tree.
(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ All results are based on a training set size of 100,000 sentences and a phrase size of 4, except Table 5.2 which varies the phrase size and Table 5.3 which varies the training set size.
(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ Arbitrary reordering of words is allowed within memorized phrases, but typically only a small amount of phrase reordering is allowed, modeled in terms of offset positions at the string level.

 $$$$$ Reported BLEU scores are far below the leading phrasal SMT systems.
 $$$$$ We compared against two radically different types of systems to demonstrate the competitiveness of this approach: We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data (e.g., support articles, product documentation).
 $$$$$ We selected a cleaner subset of this data by eliminating sentences with XML or HTML tags as well as very long (>160 characters) and very short (<40 characters) sentences.

Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ Thus we try to predict as follows: Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03).
Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.

The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ Pharaoh monotone refers to Pharaoh with phrase reordering disabled.
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ Lambdas were trained in the same manner (Och, 03).
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ While all the differences are statistically significant at the 99% confidence level, the wide gap at smaller phrase sizes is particularly striking.
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ Many-to-one alignments project similarly; since the ‘many’ source nodes are connected in the tree, they act as if condensed into a single node.

Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ Dependency analysis, in contrast to constituency analysis, tends to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and is better suited to lexicalized models, such as the ones presented in this paper.
Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ However, an n-best list of even 16,000 translations captures only a tiny fraction of the ordering possibilities of a 20 word sentence; re-ranking provides the syntactic model no opportunity to boost or prune large sections of that search space.
Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ Lin (04) translates dependency trees using paths.

Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ MSR-MT does not use lambdas or a target language model.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ The target dependency tree created in this process may not read off in the same order as the target string, since our alignments do not enforce phrasal cohesion.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.

 $$$$$ Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.
 $$$$$ A phrasal SMT system may learn the internal reordering of specific common phrases, but it cannot generalize to unseen phrases that share the same linguistic structure.
 $$$$$ MSR-MT does not use lambdas or a target language model.
 $$$$$ Further experimentation with such language pairs is necessary to confirm this.

Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ We then projected the dependency trees and used the aligned dependency tree pairs to extract treelet translation pairs and train the order model as described above.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ Our model assigns a probability to the order of a target tree given a source tree.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.

We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ Dependency analysis, in contrast to constituency analysis, tends to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and is better suited to lexicalized models, such as the ones presented in this paper.
We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.
We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ First, we employ treelet translation pairs instead of single word translations.
We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.

For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ The same GIZA++ alignments as above were used in the Pharaoh decoder.
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed surface words.

We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting. $$$$$ Table 5.4 explores different ordering strategies, Table 5.5 looks at the impact of discontiguous phrases and Table 5.6 looks at the impact of decoder optimizations such as treelet pruning and n-best list size.
We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting. $$$$$ This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.
We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting. $$$$$ Noting that the gap widens at smaller corpus sizes, we suggest that our tree-based approach is more suitable than string-based phrasal SMT when translating from English into languages or domains with limited parallel data.
We use all of the Treelet models we described in Quirk et al (2005) namely: Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting. $$$$$ In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne...pas except in the context of specific intervening words.
