 $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.
 $$$$$ This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.
 $$$$$ One means of considering alternate possibilities is to build a packed forest of dependency trees and use this in decoding translations of each input sentence.
 $$$$$ MSR-MT does not use lambdas or a target language model.

This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ MSR-MT does not use lambdas or a target language model.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ Our experience has been that the quality of GIZA++ alignments for such language pairs is inadequate.
This treelet-based SMT system (Quirk et al., 2005) is trained on about 4.6M parallel sentence pairs from diverse sources including bilingual books, dictionaries and web publications. $$$$$ We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.

Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ MSR-MT does not use lambdas or a target language model.
Yamada and Knight (2001) and Galley et al (2004) describe methods that make use of syntactic information in the target language alone; Quirk et al (2005) describe similar methods that make use of dependency representations. $$$$$ Except for the order model (Pharaoh uses its own ordering approach), the same models were used: MLE channel model, Model 1 channel model, target language model, phrase count, and word count.

We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ The target language model was trained using only the French side of the corpus; additional data may improve its performance.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ We attempt to improve on these approaches by incorporating syntactic information.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.
We borrow the term tree let from Quirk et al (2005), who use it to refer to an arbitrary connected subgraph of a tree. $$$$$ It utilizes a separate sentence realization component (Ringger et al., 04) to turn this into a target sentence.

 $$$$$ Different model structures, machine learning techniques, and target feature representations all have the potential for significant improvements.
 $$$$$ Speed numbers are the end-to-end translation speed in sentences per minute.
 $$$$$ We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.

Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ As such, it does not use a target language model during decoding, relying instead on MLE channel probabilities and heuristics such as pattern size.
Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ Following up on ideas introduced by (Cherry & Lin, 03) we plan to explore ways to leverage the dependency tree to improve alignment quality.
Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ In Figure 2c, reattaching démarrage to et suffices to produce the correct order.
Thus we avoid the sparseness problem that other methods based on treelets suffer (Quirk et al, 2005). $$$$$ We compared against two radically different types of systems to demonstrate the competitiveness of this approach: We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data (e.g., support articles, product documentation).

This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ Speed numbers are the end-to-end translation speed in sentences per minute.
This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ We showed that it significantly outperforms a leading phrasal SMT system over a wide range of training set sizes and phrase sizes.
This is a syntactically-informed MT system, designed following (Quirk et al, 2005). $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.

For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04).
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04).
For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al, 2005). $$$$$ We conjecture that in a language pair with large-scale ordering differences, such as English-Japanese, even long phrases are unlikely to capture the necessary reorderings, whereas our tree-based ordering model may prove more robust.

We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ We also ran experiments varying different system parameters.
We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ We presented a novel approach to syntacticallyinformed statistical machine translation that leverages a parsed dependency tree representation of the source language via a tree-based ordering model and treelet phrase extraction.
We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models (Quirk et al, 2005). $$$$$ Arbitrary reordering of words is allowed within memorized phrases, but typically only a small amount of phrase reordering is allowed, modeled in terms of offset positions at the string level.

(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ The order of sur relative to installés is fixed; it remains to place the translated subtrees for the software and is.
(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ As noted above, our approach shows particular promise for language pairs such as EnglishJapanese that exhibit large-scale reordering and have proven difficult for string-based approaches.
(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.
(Quirk et al, 2005) extends paths to treelets, arbitrary connected subgraphs of dependency structures, and propose a model based on tree let pairs. $$$$$ Table 5.4 explores different ordering strategies, Table 5.5 looks at the impact of discontiguous phrases and Table 5.6 looks at the impact of decoder optimizations such as treelet pruning and n-best list size.

 $$$$$ This reordering model is very limited in terms of linguistic generalizations.
 $$$$$ The MLE model effectively captures non-literal phrasal translations such as idioms, but suffers fr om data sparsity.
 $$$$$ Yet despite this success, statistical machine translation (SMT) has many hurdles to overcome.

Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ We conjecture that in a language pair with large-scale ordering differences, such as English-Japanese, even long phrases are unlikely to capture the necessary reorderings, whereas our tree-based ordering model may prove more robust.
Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ Inversion Transduction Grammars (Wu, 97), or ITGs, treat translation as a process of parallel parsing of the source and target language via a synchronized grammar.
Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ This may be because the parser is applied to MT output, which is notoriously unlike native language, and no additional insight is gained via source language analysis.
Quirk et al (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. $$$$$ MSR-MT does not use lambdas or a target language model.

The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ We compared against two radically different types of systems to demonstrate the competitiveness of this approach: We used a parallel English-French corpus containing 1.5 million sentences of Microsoft technical data (e.g., support articles, product documentation).
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ Pharaoh monotone refers to Pharaoh with phrase reordering disabled.
The obvious next step for our framework is to include bilingual rules that include source syntax (Quirk et al, 2005), target syntax (Shen et al,2008), and syntax on both sides. $$$$$ Speed numbers are the end-to-end translation speed in sentences per minute.

Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ Constituents vs. dependencies: Most attempts at syntactic SMT have relied on a constituency analysis rather than dependency analysis.
Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ MSR-MT does not use lambdas or a target language model.
Based on the assumption that constituents generally move as a whole (Quirk et al, 2005), we decompose the sentence reordering probability into the reordering probability for each aligned source word with respect to its head, excluding the root word at the top of the dependency hierarchy which does not have a head word. $$$$$ Results for our system and the comparison systems are presented in Table 5.1.

Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ The target language model was trained using only the French side of the corpus; additional data may improve its performance.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ MSR-MT does not use lambdas or a target language model.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ The difference between Pharaoh and the Treelet system is significant at the 99% confidence level under a two-tailed paired t-test.
Our dependency orientation feature is similar to the order model within dependency tree let translation (Quirk et al, 2005). $$$$$ To eliminate unnecessary ordering operations, we first check that a given set of words has not been previously ordered by the decoder.

 $$$$$ Further experimentation with such language pairs is necessary to confirm this.
 $$$$$ We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.
 $$$$$ The most important contribution of our system is a linguistically motivated ordering approach based on the source dependency tree, yet this paper only explores one possible model.
 $$$$$ While it excels at translating domain-specific terminology and fixed phrases, grammatical generalizations are poorly captured and often mangled during translation (Thurmair, 04).

Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ State-of-the-art phrasal SMT systems such as (Koehn et al., 03) and (Vogel et al., 03) model translations of phrases (here, strings of adjacent words, not syntactic constituents) rather than individual words.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ MSR-MT does not use lambdas or a target language model.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ The most important contribution of our system is a linguistically motivated ordering approach based on the source dependency tree, yet this paper only explores one possible model.
Quirk et al, 2005 demonstrates the success of using fragments of a target language's grammar, what they call treelets, to improve performance in phrasal translation. $$$$$ MSR-MT used its own word alignment approach as described in (Menezes & Richardson, 01) on the same training data.

We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ Under default settings our system tries to decode a sentence with exhaustive ordering until a specified timeout, at which point it falls back to greedy ordering.
We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ Each such interleaving is scored using the models previously described and added to the candidate translation list for that input node.
We previously described (Quirk et al 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases. $$$$$ A phrasal SMT system may learn the internal reordering of specific common phrases, but it cannot generalize to unseen phrases that share the same linguistic structure.

For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser developed at Microsoft Research able to produce syntactic analyses at varying levels of depth (Heidorn, 02).
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ While this is a natural starting point due to its wellunderstood nature and commonly available tools, we feel that this is not the most effective representation for syntax in MT.
For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al (2005). $$$$$ In addition, these systems are limited to phrases contiguous in both source and target, and thus cannot learn the generalization that English not may translate as French ne...pas except in the context of specific intervening words.

We use all of the Treelet models we described in Quirk et al (2005) namely $$$$$ We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.
We use all of the Treelet models we described in Quirk et al (2005) namely $$$$$ Each such interleaving is scored using the models previously described and added to the candidate translation list for that input node.
We use all of the Treelet models we described in Quirk et al (2005) namely $$$$$ We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.
We use all of the Treelet models we described in Quirk et al (2005) namely $$$$$ MSR-MT does not use lambdas or a target language model.
