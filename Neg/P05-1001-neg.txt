Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ Fixing the number of iterations to a constant, it runs in linear tomand the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances.
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ This work uses a linear formulation of structural learning.

 $$$$$ POS tri-grams on the left and right. labels of the two words on the left and their bi-grams. bi-grams of the current word and two labels on the left. use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data.
 $$$$$ By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.

Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ KM01 and CM03 boost performance by classifier combinations.
Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ SP03 trains conditional random fields for NP (noun phrases) only.
Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ In machine learning, whether one can build a more accurate classifier by using data is an important issue.

 $$$$$ As is commonly done, we encode chunk information into word tags to cast the chunking problem to that of sequential word tagging.
 $$$$$ By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
 $$$$$ These auxiliary classifiers can be adequately learned since we have very large amounts of ‘training data’ for them, which we automatically generate from a very large amount of unlabeled data.
 $$$$$ We choosesinstances for each classifier with high confidence while preserving the class distribution observed in the initial labeled data, and add them to the labeled data.

For a more complete description, see (Ando and Zhang, 2005a). $$$$$ •Fixmpredictors{ut}, and find(O,{vt})that minimizes the joint empirical risk (3).
For a more complete description, see (Ando and Zhang, 2005a). $$$$$ The types of auxiliary problems are the same as in the named entity experiments.
For a more complete description, see (Ando and Zhang, 2005a). $$$$$ These auxiliary classifiers can be adequately learned since we have very large amounts of ‘training data’ for them, which we automatically generate from a very large amount of unlabeled data.

 $$$$$ For clarity, the figure only shows words beginning with upper-case letters (i.e., likely to be names in English).
 $$$$$ This paper presents a novel semi-supervised method that employs a learning paradigm which we call The idea is to find “what good classifiers are like” by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.
 $$$$$ By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
 $$$$$ Both types – “Predict words” and “Predict top-2 choices of the classifier” – are useful, producing significant performance improvements over the supervised baseline.

An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.
An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ A number of bootstrapping methods have been proposed for NLP tasks (e.g.
An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ Consider m problems indexed by ` E { 1 each with nt samples (Xti; Yt) indexed by i E {1; ::: ; nt}.

 $$$$$ To gain insights into the information obtained from unlabeled data, we examine the O entries associated with the feature ‘current words’, computed for the English named entity task.
 $$$$$ Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively.
 $$$$$ We first briefly review a standard linear prediction model and then extend it for structural learning.
 $$$$$ In practice, it is desirable to create as many auxiliary problems as possible, as long as there is some reason to believe in their relevancy to the task.

Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. $$$$$ Moreover, the auxiliary problems used in our experiments are merely possible examples.
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. $$$$$ We perform Viterbistyle decoding to choose the word tag sequence that maximizes the sum of tagging confidence values.
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm:. $$$$$ In particular, we do not use any gazetteer information, which was used in all other systems.

This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).
This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ The likely reason is that the number of labeled data is relatively large (>200K), making it hard to benefit from unlabeled data.
This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data.

 $$$$$ This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e. what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data.
 $$$$$ That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated ‘training data’.
 $$$$$ For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001).

 $$$$$ Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
 $$$$$ In NLP applications, features have natural grouping according to their types/origins such as ‘current words’, ‘parts-of-speech on the right’, and so forth.
 $$$$$ SP03 trains conditional random fields for NP (noun phrases) only.
 $$$$$ Unlike named entity chunking, the use of external resources on this task is rare.

 $$$$$ KM01 and CM03 boost performance by classifier combinations.
 $$$$$ It is desirable to perform a localized optimization for each of such natural feature groups.
 $$$$$ •Fixmpredictors{ut}, and find(O,{vt})that minimizes the joint empirical risk (3).

 $$$$$ More generally, given a feature representation of the input data, we may mask some features as unobserved, and learn classifiers to predict these ‘masked’ features based on other features that are not masked.
 $$$$$ •Fixmpredictors{ut}, and find(O,{vt})that minimizes the joint empirical risk (3).
 $$$$$ That is, they should share a certain predictive structure.
 $$$$$ This problem is often referred to as semi-supervised learning.

To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ Under some conditions, it can be shown that features in 'P2 with similar predictive performance tend to map to similar low-dimensional vectors through O.
To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ We use two (or more) distinct feature maps:-P1 and 'P2.
To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ The results we report are obtained by using all the problems in Figure 3 unless otherwise specified. with one of three sets of labeled training examples: a small English set (10K examples randomly chosen), the entire English training set (204K), and the entire German set (207K), tested on either the development set or test set.

Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ The types of auxiliary problems are the same as in the named entity experiments.
Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ Co- and self-training, at their oracle performance, improve recall but often degrade precision; consequently, their F-measure improvements are relatively low:—0.05% to +1.63%.
Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ We first briefly review a standard linear prediction model and then extend it for structural learning.

We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ This is because the risk is relatively minor while the potential gain from relevant problems is large.
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German).
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ Both types – “Predict words” and “Predict top-2 choices of the classifier” – are useful, producing significant performance improvements over the supervised baseline.
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.

For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ In particular, we do not use any gazetteer information, which was used in all other systems.
For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of -P2.
For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.
For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ One example of such auxiliary problems for chunking tasks is to ‘mask’ a word and predict whether it is “people” or not from the context, like language modeling.

Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ For word-prediction problems, we only consider the instances whose current words are either nouns or adjectives since named entities mostly consist of these types.
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ The motivation is that positive weights are usually directly related to the target concept, while negative ones often yield much less specific information representing ‘the others’.
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of -P2.

As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ Matrix O (whose rows are orthonormal) is the common structure parameter shared by all the problems; wt and vt are weight vectors specific to each prediction problem `.
As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ One may select other loss functions such as SVM or logistic regression.
As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ In machine learning, whether one can build a more accurate classifier by using data is an important issue.
