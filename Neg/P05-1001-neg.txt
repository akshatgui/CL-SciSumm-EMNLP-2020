Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of -P2.
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ Under some conditions, it can be shown that features in 'P2 with similar predictive performance tend to map to similar low-dimensional vectors through O.
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ It is desirable to perform a localized optimization for each of such natural feature groups.
Ando and Zhang (2005) independently used this phrase, for a semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner. $$$$$ Comparison with top systems As shown in Figure 5, ASO-semi achieves higher performance than the top systems on both English and German data.

 $$$$$ The process is then repeated.
 $$$$$ The optimization algorithm for this extension is essentially the same as SVD-ASO in Figure 1, but with the SVD step performed separately for each group.
 $$$$$ For instance, predict whether F1 assigns the highest confidence values to CLASS1 and CLASS2 in this order.
 $$$$$ On CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German), the method exceeds the previous best systems (including those which rely on hand-crafted resources) by using unlabeled data as the only additional resource.

Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ This problem is relevant to, for instance, named entity chunking since knowing a word is “Smith” helps to predict whether it is part ofa name.
Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ Though the space constraint precludes providing the detail, we note that ASO-semi outperforms all of the previous top systems in both precision and recall.
Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ Predict the combination of k (a few) classes to which F1 assigns the highest output (confidence) values.
Ando and Zhang (2005) utilized a multi task learner within their semi-supervised algorithm to learn feature representations which were useful across a large number of related tasks. $$$$$ Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.

 $$$$$ Supervised classifier For comparison, we train a classifier using the same features and algorithm, but without unlabeled data (O=0in effect).
 $$$$$ This problem is relevant to, for instance, named entity chunking since knowing a word is “Smith” helps to predict whether it is part ofa name.
 $$$$$ Supervised classifier For comparison, we train a classifier using the same features and algorithm, but without unlabeled data (O=0in effect).

For a more complete description, see (Ando and Zhang, 2005a). $$$$$ Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g.
For a more complete description, see (Ando and Zhang, 2005a). $$$$$ The process is then repeated.
For a more complete description, see (Ando and Zhang, 2005a). $$$$$ Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm.

 $$$$$ Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
 $$$$$ The intuition is as follows.
 $$$$$ The resulting extension, in effect, only uses the positive components ofUin the SVD computation.
 $$$$$ In NLP applications, features have natural grouping according to their types/origins such as ‘current words’, ‘parts-of-speech on the right’, and so forth.

An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.
An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated ‘training data’.
An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ It is possible to develop a general theory to show that the auxiliary problems we use are helpful under reasonable conditions.
An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive theta are not necessarily those problems we are aiming to solve. $$$$$ The taskspecific setup is described in Sections 5 and 6.

 $$$$$ The specific choice is not important for the purpose of this paper.
 $$$$$ Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.

Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm $$$$$ On CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German), the method exceeds the previous best systems (including those which rely on hand-crafted resources) by using unlabeled data as the only additional resource.
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm $$$$$ The taskspecific setup is described in Sections 5 and 6.
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm $$$$$ For word-prediction problems, we only consider the instances whose current words are either nouns or adjectives since named entities mostly consist of these types.
Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of theat can be obtained from (4) by the following algorithm $$$$$ In machine learning, whether one can build a more accurate classifier by using data is an important issue.

This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ In machine learning, whether one can build a more accurate classifier by using data is an important issue.
This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ We use two (or more) distinct feature maps:-P1 and 'P2.
This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same theta is used for all auxiliary problems. $$$$$ By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power.

 $$$$$ We use the POS information provided by the organizer.
 $$$$$ The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 15–18 (212K words) and section 20, respectively.

 $$$$$ Co- and self-training, at their oracle performance, improve recall but often degrade precision; consequently, their F-measure improvements are relatively low:—0.05% to +1.63%.
 $$$$$ The process is then repeated.
 $$$$$ We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.
 $$$$$ Our feature representation is a slight modification of a simpler configuration (without linguistic features) in (Zhang et al., 2002), as shown in Figure 6.

 $$$$$ We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.
 $$$$$ For word predictions, we exclude instances of punctuation symbols.
 $$$$$ For word predictions, we exclude instances of punctuation symbols.
 $$$$$ Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.

 $$$$$ By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
 $$$$$ We use the POS information provided by the organizer.
 $$$$$ We use POS and syntactic chunk information provided by the organizer.
 $$$$$ This is because the risk is relatively minor while the potential gain from relevant problems is large.

To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ On the other hand, potential gains from relevant auxiliary problems can be significant.
To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ Figure 11 plots F-measure in relation to the rowdimension ofO, which shows that the method is relatively insensitive to the change of this parameter, at least in the range which we consider.
To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). $$$$$ The method produces performance higher than the previous best results on CoNLL’00 syntactic chunking and CoNLL’03 named entity chunking (English and German).

Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ Though the space constraint precludes providing the detail, we note that ASO-semi outperforms all of the previous top systems in both precision and recall.
Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ This is the key reason why structural learning is effective.
Pivot features correspond to the auxiliary problems of Ando and Zhang (2005a). $$$$$ The training algorithm is stochastic gradient descent, which is argued to perform well for regularized convex ERM learning formulations (Zhang, 2004).

We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ In machine learning, whether one can build a more accurate classifier by using data is an important issue.
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ In supervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain.
We follow Ando and Zhang (2005a) and use the modified Huber loss. $$$$$ Clearly, auxiliary problems more closely related to the target problem will be more beneficial.

For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ In this sense, our method is robust.
For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems.
For both computational and statistical reasons, though, we follow Ando and Zhang (2005a) and compute a low-dimensional linear approximation to the pivot predictor space. $$$$$ The likely reason is that the number of labeled data is relatively large (>200K), making it hard to benefit from unlabeled data.

Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ We present two general strategies for generating useful auxiliary problems: one in a completely unsupervised fashion, and the other in a partiallysupervised fashion.
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ We use POS and syntactic chunk information provided by the organizer.
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ Intuitively, the optimum O captures the maximal commonality of the m predictors (each derived from ut).
Ando and Zhang (2005a) describe several free paramters and extensions to ASO, and we briefly address our choices for these here. $$$$$ Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of -P2.

As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ Under some conditions, it can be shown that features in 'P2 with similar predictive performance tend to map to similar low-dimensional vectors through O.
As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ Clearly, auxiliary problems more closely related to the target problem will be more beneficial.
As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ First, we present a novel robust semi-supervised method based on a new learning model and its application to chunking tasks.
As in Ando and Zhang (2005a), we observed that setting h between 20 and 100 did not change results significantly, and a lower dimensionality translated to faster run-time. $$$$$ POS tri-grams on the left and right. labels of the two words on the left and their bi-grams. bi-grams of the current word and two labels on the left. use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data.
