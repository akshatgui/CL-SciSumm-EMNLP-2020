 $$$$$ This is the aim of our work.
 $$$$$ This enables them to find sentences from fairly dissimilar documents, and to handle any amount of reordering, which makes the method applicable to truly comparable corpora.

We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. $$$$$ One limitation of all these methods is that they are designed to find only full sentences.
We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. $$$$$ We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.
We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. $$$$$ Our approach can be improved in several aspects.
We employ the same algorithm used in (Munteanu and Marcu, 2006) which first use the GI ZA++ (with grow-diag-final-and heuristic) to obtain the word alignment between source and target words, and then calculate the association strength between the aligned words. $$$$$ However, most of the words from aligned sentences are actually unrelated; therefore, this is a rather weak notion of cooccurrence.

Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. $$$$$ All systems use the same 2 language models: one trained on 800 million English tokens, and one trained on the English side of all our parallel and comparable corpora.
Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. $$$$$ HR001106-C-0022.
Munteanu and Marcu (2006) first extract the candidate parallel sentences from the comparable corpora and further extract the accurate sub-sentential bilingual fragments from the candidate parallel sentences using the in-domain probabilistic bilingual lexicon. $$$$$ Joining both extracted datasets does not improve performance; since most of the parallel data in this corpus exists at sentence level, the extracted fragments cannot bring much additional knowledge.

They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). $$$$$ This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs.
They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). $$$$$ We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
They are neither parallel nor comparable because we cannot even extract a small number of parallel sentence pairs from this monolingual data using the method of (Munteanu and Marcu, 2006). $$$$$ We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.

Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora. $$$$$ This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs.
Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora. $$$$$ On the BBC corpus, the fragment extraction method produces statistically significant improvements over the baseline, while the sentence extraction method does not.
Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora. $$$$$ This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs.
Munteanu and Marcu (2006) proposed a method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora. $$$$$ The fact that the source and target signal are filtered separately is also a weakness; a joint analysis should produce better results.

Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ These approaches are only applicable to corpora which are at most “noisy-parallel”, i.e. contain documents which are fairly similar, both in content and in sentence ordering.
Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ Our methodology is the first effort aimed at detecting sub-sentential correspondences.
Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ We present a novel method for extracting parallel sub-sentential fragments from comparable, non-parallel bilingual corpora.

 $$$$$ This method enables us to extract useful machine translation training data even from very non-parallel corpora, which contain no parallel sentence pairs.
 $$$$$ We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.
 $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
 $$$$$ In addition, since we want to distinguish between source words that have a translation on the target side and words that do not, we also need a measure of the probability that two words are not translations of each other.

Similarly, Munteanu and Marcu (2006) propose a method to extract sub sentential fragments from non-parallel corpora. $$$$$ The other two sources are fairly similar, both in genre and in degree of parallelism, so we group them together and refer to them as the EZZ corpus.
Similarly, Munteanu and Marcu (2006) propose a method to extract sub sentential fragments from non-parallel corpora. $$$$$ We showed that using this lexicon helps improve the precision of our extraction method.
Similarly, Munteanu and Marcu (2006) propose a method to extract sub sentential fragments from non-parallel corpora. $$$$$ And, even if the sentences would be used for MT training, considering the amount of noise they contain, they might do more harm than good for the system’s performance.
Similarly, Munteanu and Marcu (2006) propose a method to extract sub sentential fragments from non-parallel corpora. $$$$$ Despite the better lexicon, the greatest source of errors is still related to false word correspondences, generally involving punctuation and very common, closed-class words.

 $$$$$ Fung and Cheung (2004a) describe corpora ranging from noisy parallel, to comparable, and finally to very non-parallel.
 $$$$$ HR001106-C-0022.
 $$$$$ Both methods extend algorithms designed to perform sentence alignment of parallel texts: they use dynamic programming to do sentence alignment of documents hypothesized to be similar.
 $$$$$ The work of Deng et. al (2006) also deals with sub-sentential fragments.

Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ The research reported by Fung and Cheung (2004a; 2004b), Cheung and Fung (2004) and Wu and Fung (2005) is aimed explicitly at “very non-parallel corpora”.
Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ In LLR-Lex that average is 5, which is a significant decrease.
Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ Comparable corpora exhibit various degrees of parallelism.
Other approaches aim to identify pairs of sentences (Munteanu and Marcu, 2005) or sub sentential fragments (Munteanu and Marcu, 2006) that are parallel within comparable corpora. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. $$$$$ This enables them to find sentences from fairly dissimilar documents, and to handle any amount of reordering, which makes the method applicable to truly comparable corpora.
The approach that is closest to our work is that of Munteanu and Marcu (2006): They use standard information retrieval together with simple word-based translation for CLIR, and extract phrases from the retrieval results using a clean bilingual lexicon and an averaging filter. $$$$$ We evaluate our extracted corpora by measuring their impact on the performance of an SMT system.

Our first technique resembles the technique of Munteanu and Marcu (2006) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction. $$$$$ HR001106-C-0022.
Our first technique resembles the technique of Munteanu and Marcu (2006) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction. $$$$$ One limitation of all these methods is that they are designed to find only full sentences.
Our first technique resembles the technique of Munteanu and Marcu (2006) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction. $$$$$ In particular, they contain no parallel sentence pairs; methods designed to extract full parallel sentences will not find any useful data in them.
Our first technique resembles the technique of Munteanu and Marcu (2006) who also perform phrase extraction by combining clean alignment lexica for initial signals with heuristics to smooth alignments for final fragment extraction. $$$$$ We evaluate our extracted corpora by measuring their impact on the performance of an SMT system.

The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanuand Marcu, 2006). $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
The first attempt to detect sub-sentential fragments from comparable sentences is (Munteanuand Marcu, 2006). $$$$$ The signal filtering function is very simple; more advanced filters might work better, and eliminate the need of applying additional heuristics (such as our requirement that the extracted fragments have at least 3 words).

 $$$$$ All these are part of our second lexicon, LLR-Ler, which we present in detail in Section 2.2.
 $$$$$ The Fragment-noLLR datasets bring no translation performance improvements; moreover, when the initial corpus is small (1M words) and the comparable corpus is noisy (BBC), the data has a negative impact on the BLEU score.
 $$$$$ Our approach can be improved in several aspects.

Munteanu and Marcu (2006) extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. $$$$$ These approaches are only applicable to corpora which are at most “noisy-parallel”, i.e. contain documents which are fairly similar, both in content and in sentence ordering.
Munteanu and Marcu (2006) extract sub sentential translation pairs from comparable corpora using the log-likelihood-ratio of word translation probability. $$$$$ This characteristic is useful for the first two stages of the extraction pipeline, which are not intended to be very precise.

We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006) on translation fragment extraction. $$$$$ By analyzing potentially similar sentence pairs using a signal processinginspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not.
We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006) on translation fragment extraction. $$$$$ In our experiments, we compare our fragment extraction method (which we call FragmentExtract) with the sentence extraction approach of Munteanu and Marcu (2005) (SentenceExtract).
We mainly follow our previous approach (Wang and CallisonBurch, 2011), which is a modified version of an approach by Munteanu and Marcu (2006) on translation fragment extraction. $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.

It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). $$$$$ By analyzing potentially similar sentence pairs using a signal processinginspired approach, we detect which segments of the source sentence are translated into segments in the target sentence, and which are not.
It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). $$$$$ We also presented a method for computing a probabilistic lexicon based on the LLR statistic, which produces a higher quality lexicon.
It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). $$$$$ Several researchers (Zhao and Vogel, 2002; Vogel, 2003; Resnik and Smith, 2003; Fung and Cheung, 2004a; Wu and Fung, 2005; Munteanu and Marcu, 2005) have shown how fairly good-quality parallel sentence pairs can be automatically extracted from comparable corpora, and used to improve the performance of machine translation (MT) systems.
It also can be quantified as the rate of successful extraction of translation equivalents by automated tools, such as proposed in Munteanu and Marcu (2006). $$$$$ All systems use the same 2 language models: one trained on 800 million English tokens, and one trained on the English side of all our parallel and comparable corpora.

At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein). $$$$$ We showed that using this lexicon helps improve the precision of our extraction method.
At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein). $$$$$ This work was partially supported under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein). $$$$$ We downloaded comparable data from three online news sites: the BBC, and the Romanian newspapers “Evenimentul Zilei” and “Ziua”.
At last, the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu (2006) and references therein). $$$$$ The earliest efforts in this direction are those of Zhao and Vogel (2002) and Utiyama and Isahara (2003).
