Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ Note that the extracted consensus translation can be different from the original M translations.
Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ In this work we describe a novel technique for computing a consensus translation from the outputs of multiple machine translation systems.
Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ They are manually adjusted based on the performance of the involved MT systems on a held-out development set.
Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ Combining outputs from different systems was shown to be quite successful in automatic speech recognition (ASR).

Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ For each sentence, we obtain a total of M confusion networks and unite them in a single lattice.
Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ Some state-of-the-art speech translation systems can translate either the first best recognition hypotheses or the word lattices of an ASR system.
Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ We consider this primary hypothesis to have the “correct” word order.

The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ The training corpus for alignment is created from a test corpus of N sentences (usually a few hundred) translated by all of the involved MT engines.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ In this work, we proposed a novel, theoretically well-founded procedure for computing a possibly new consensus translation from the outputs of multiple MT systems.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ The “voting” on the union of confusion networks is straightforward and analogous to the ROVER system.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ The selection is made based on the scores of translation, language, and other models (Nomoto, 2004; Paul et al., 2005).

The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ In contrast to existing approaches, the context of the whole document rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment.
The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ Here, the involved MT systems had used about 60K sentence pairs (420K running words) for training.
The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ Note that the word “have” in translation 2 is aligned to the word “like” in translation 1.
The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.

Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ In particular, we will present details of the enhanced alignment and reordering procedure.
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ Since M confusion networks are used, the lattice may contain two best paths with the same probability, the same words, but different word order.
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.

This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ Thus, the correspondence between words in different hypotheses and, consequently, the consensus translation can be improved overtime.
This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ The data for these tasks come from the Basic Travel Expression corpus (BTEC), consisting of tourism-related sentences.
This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ In Section 2, we will describe the computation of consensus translations with our approach.

 $$$$$ Based on the alignment, we construct a confusion network from the (possibly reordered) translation hypotheses, similarly to the approach of (Bangalore et al., 2001).
 $$$$$ Since all of the hypotheses are in the same language, we count co-occurring equal words, i. e. if en is the same word as em.
 $$$$$ On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.

 $$$$$ This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
 $$$$$ This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
 $$$$$ HR0011-06-C0023.

 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
 $$$$$ The alignment procedure explicitly models word reordering.
 $$$$$ Thus, the decision on how to align two translations of a sentence takes the whole document context into account. plied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.
 $$$$$ The method was also tested in the framework of multi-source and speech translation.

In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ Thus, the decision on how to align two translations of a sentence takes the whole document context into account. plied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ Some state-of-the-art speech translation systems can translate either the first best recognition hypotheses or the word lattices of an ASR system.
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ The alignment procedure explicitly models word reordering.

Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ (Bangalore et al., 2001) used the edit distance alignment extended to multiple sequences to construct a confusion network from several translation hypotheses.
Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ However, given the presence of the word “have” in translation 4, this is not the best alignment.
Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ Alternatively, the N-best hypotheses can be extracted for rescoring by additional models.
Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ The method was also tested in the framework of multi-source and speech translation.

Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ The alignment and voting algorithm was evaluated on both small and large vocabulary tasks.
Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task.
Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ In this work, we proposed a novel, theoretically well-founded procedure for computing a possibly new consensus translation from the outputs of multiple MT systems.

Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ The alignment procedure explicitly models word reordering.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ However, this approach uses many heuristics and is based on the alignment that is performed to calculate a specific MT error measure; the performance improvements are reported only in terms of this measure.

Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ The evaluation (as well as the alignment training) was case-insensitive, without considering the punctuation marks.
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ Thus, the correspondence between words in different hypotheses and, consequently, the consensus translation can be improved overtime.
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.

The basic concept of the approach has been described by Matusov et al (2006). $$$$$ Here, we combined four different systems which performed best in the TC-STAR 2005 evaluation, see Table 3.
The basic concept of the approach has been described by Matusov et al (2006). $$$$$ • A test corpus of translations generated by each of the systems is used for the unsupervised statistical alignment training.

 $$$$$ Table 2 shows the performance of the best and the worst of these systems in terms of the BLEU score.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
 $$$$$ The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.
 $$$$$ Thus, the decision on how to align two translations of a sentence takes the whole document context into account. plied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.

(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ The alignment procedure explicitly models word reordering.
(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994).
(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ Initial experiments were performed on the IWSLT 2004 Chinese-English and Japanese-English tasks (Akiba et al., 2004).

And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ Some research on multi-engine machine translation has also been performed in recent years.
And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ Initial experiments were performed on the IWSLT 2004 Chinese-English and Japanese-English tasks (Akiba et al., 2004).
And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. alignment would|would you|you have|like coffee|coffee or|or tea|tea and would|would you|you like|like your|$ coffee|coffee or|or $|tea reordering I|$ would|would you|you like|like have|$ some|$ coffee|coffee $|or tea|tea $ would you like $ $ coffee or tea confusion $ would you have $ $ coffee or tea network $ would you like your $ coffee or $ I would you like have some coffee $ tea The alignment between En and the primary hypothesis E,,t used for reordering is computed as a function of words in the secondary translation En with minimal costs, with an additional constraint that identical words in En can not be all aligned to the same word in E,,t.
And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ We combined the outputs of several MT systems that had officially been submitted to the IWSLT 2004 evaluation.

Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ In this experiment, the global system probabilities for scoring the confusion networks were tuned manually on a development set.
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ Based on the alignment, we construct a confusion network from the (possibly reordered) translation hypotheses, similarly to the approach of (Bangalore et al., 2001).
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ This constraint is necessary to avoid that reordered hypotheses with e. g. multiple consecutive articles “the” would be produced if fewer articles were used in the primary hypothesis.

 $$$$$ This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
 $$$$$ Table 4 gives examples of improved translation quality by using the consensus translation as derived from the rescored N-best lists.
 $$$$$ They introduce a method that allows non-monotone alignments of words in different translation hypotheses for the same sentence.
