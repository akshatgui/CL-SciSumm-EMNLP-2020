Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ This algorithm produces monotone alignments only (i. e. allows insertion, deletion, and substitution of words); it is not able to align translation hypotheses with significantly different word order.
Combining multiple MT outputs to increase performance is, in general, a difficult task (Matusov et al, 2006) when significantly different engines compete for producing the best outputs. $$$$$ The best performing system was a JapaneseEnglish system with a BLEU score of 44.7%, see Table 5.

Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ HR0011-06-C0023.
Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ Other approaches combine lattices or N-best lists from several different MT systems (Frederking and Nirenburg, 1994).
Note that the approach by Matusov et al (2006) attempts to align synonyms and different morphological forms of words to each other but this is done implicitly, relying on the parallel text to learn word alignments. $$$$$ • A test corpus of translations generated by each of the systems is used for the unsupervised statistical alignment training.

The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ These probabilities are the global probabilities assigned to the different MT systems.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ An important feature of a real-life application of the proposed alignment technique is that the lexicon and alignment probabilities can be updated with each translated sentence and/or text.
The recent approaches used pair-wise alignment algorithms based on symmetric alignments from a HMM alignment model (Matusov et al, 2006) or edit distance alignments allowing shifts (Rosti et al, 2007). $$$$$ The evaluation (as well as the alignment training) was case-insensitive, without considering the punctuation marks.

The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ Thus, the decision on how to align two translations of a sentence takes the whole document context into account. plied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.
The second, sys comb giza, corresponds to the pair-wise symmetric HMM alignments from GIZA++ described in (Matusov et al, 2006). $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).

Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ First, we focused on combining different MT systems which have the same source and target language.
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ Note that the extracted consensus translation can be different from the original M translations.
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
Thus, when Matusov et al (2006) use this procedure, they deterministically reorder each translation prior to the monotone alignment. $$$$$ The better performing word lattice translations were given higher system probabilities.

This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task.
This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
This ties in with recent work on ensemble combinations of SMT systems, which have used alignment techniques (Matusov et al, 2006) or simple heuristics (Eisele, 2005) to guide target sentence selection and generation. $$$$$ We consider the conditional probability Pr(En|Em) of the event that, given Em, another hypothesis En is generated from the Em.

 $$$$$ This consensus translation may be different from the original translations.
 $$$$$ The proposed alignment and voting approach was evaluated on several machine translation tasks, including a large vocabulary task.
 $$$$$ The first-best recognition word error rate on this corpus is 22.3%.

 $$$$$ This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
 $$$$$ The outputs are combined and a possibly new translation hypothesis can be generated.

 $$$$$ The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.
 $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).

In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ This paper is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ To investigate the potential of the proposed approach, we generated the N-best lists (N = 1000) of consensus translations.
In (Matusov et al, 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). $$$$$ After reordering each secondary hypothesis En, we determine M − 1 monotone one-to-one alignments between E,,t and En, n = 1, ... , M; n =6 m. In case of many-toone connections of words in En to a single word in E,,t, we only keep the connection with the lowest alignment costs.

Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ Some state-of-the-art speech translation systems can translate either the first best recognition hypotheses or the word lattices of an ASR system.
Tuning is fully automatic, as opposed to (Matusov et al, 2006) where global system weights were set manually. $$$$$ • A test corpus of translations generated by each of the systems is used for the unsupervised statistical alignment training.

Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ The initialization could be furthermore improved by using word classes, part-of-speech tags, or a list of synonyms.
Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. alignment would|would you|you have|like coffee|coffee or|or tea|tea and would|would you|you like|like your|$ coffee|coffee or|or $|tea reordering I|$ would|would you|you like|like have|$ some|$ coffee|coffee $|or tea|tea $ would you like $ $ coffee or tea confusion $ would you have $ $ coffee or tea network $ would you like your $ coffee or $ I would you like have some coffee $ tea The alignment between En and the primary hypothesis E,,t used for reordering is computed as a function of words in the secondary translation En with minimal costs, with an additional constraint that identical words in En can not be all aligned to the same word in E,,t.
Similar combination of multiple confusion networks was presented in (Matusov et al, 2006). $$$$$ The alignment procedure explicitly models word reordering.

Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ The words of the primary hypothesis are printed in bold.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ In our future work, we plan to improve the scoring of hypotheses in the confusion networks to explore this large potential.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ Thus, the decision on how to align two translations of a sentence takes the whole document context into account. plied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.
Matusov et al (2006) propose using a statistical word alignment algorithm as a more robust way of aligning (monolingual) outputs into a confusion network for system combination. $$$$$ On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.

Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ The context of a whole document of translations rather than a single sentence is taken into account to produce the alignment.
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ To create the confusion network, we produce pairwise word alignments of the original machine translation hypotheses with an enhanced statistical alignment algorithm that explicitly models word reordering.
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ The MT engines for this task had been trained on 1.2M sentence pairs (32M running words).
Experiments combining several kinds of MT systems have been presented in (Matusov et al, 2006), based only on the single best output of each system. $$$$$ Thus, the decision on how to align two translations of a sentence takes the whole document context into account. plied in speech translation in order to cope with the negative impact of speech recognition errors on translation accuracy.

The basic concept of the approach has been described by Matusov et al (2006). $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
The basic concept of the approach has been described by Matusov et al (2006). $$$$$ The words of the primary hypothesis are printed in bold.
The basic concept of the approach has been described by Matusov et al (2006). $$$$$ On all tasks and conditions, we achieved significant improvements in translation quality, increasing e. g. the BLEU score by as much as 15% relative.
The basic concept of the approach has been described by Matusov et al (2006). $$$$$ Here, we propose an alignment procedure that explicitly models reordering of words in the hypotheses.

 $$$$$ This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
 $$$$$ For some other MT systems (e.g. knowledge-based systems), the lattices and/or scores of hypotheses may not be even available.
 $$$$$ HR0011-06-C0023.
 $$$$$ Thus, the negative effect of recognition errors on the translation quality was further reduced.

(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ Using global system probabilities and other statistical models, the voting procedure selects the best consensus hypothesis from the confusion network.
(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ The official IWSLT04 test corpus was used for the IWSLT 04 tasks; the CSTAR03 test corpus was used for the speech translation task.
(Matusov et al, 2006) computes consensus translation by voting on a confusion network, which is created by pairwise word alignment of multiple baseline MT hypotheses. $$$$$ The outputs are combined and a possibly new translation hypothesis can be generated.

And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ However, the scores of most statistical machine translation (SMT) systems are not normalized and therefore not directly comparable.
And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ The outputs are combined and a possibly new translation hypothesis can be generated.
And confusion network generates new hypotheses based on confusion network decoding (Matusov et al, 2006), where the confusion network is built on the original N-best translations. $$$$$ However, given the presence of the word “have” in translation 4, this is not the best alignment.

Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ The symbol $ denotes a null alignment or an e-arc in the corresponding part of the confusion network. alignment would|would you|you have|like coffee|coffee or|or tea|tea and would|would you|you like|like your|$ coffee|coffee or|or $|tea reordering I|$ would|would you|you like|like have|$ some|$ coffee|coffee $|or tea|tea $ would you like $ $ coffee or tea confusion $ would you have $ $ coffee or tea network $ would you like your $ coffee or $ I would you like have some coffee $ tea The alignment between En and the primary hypothesis E,,t used for reordering is computed as a function of words in the secondary translation En with minimal costs, with an additional constraint that identical words in En can not be all aligned to the same word in E,,t.
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ This work was also in part funded by the European Union under the integrated project TCSTAR – Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ This paper describes a novel method for computing a consensus translation from the outputs of multiple machine translation (MT) systems.
Matusov et al (2006) let every hypothesis play the role of the skeleton once and used GIZA++ to get word alignment. $$$$$ Thus, the correspondence between words in different hypotheses and, consequently, the consensus translation can be improved overtime.

 $$$$$ The one-to-one alignments are convenient for constructing a confusion network in the next step of the algorithm.
 $$$$$ • A test corpus of translations generated by each of the systems is used for the unsupervised statistical alignment training.
 $$$$$ Similarly to the well-established ROVER approach of (Fiscus, 1997) for combining speech recognition hypotheses, the consensus translation is computed by voting on a confusion network.
 $$$$$ (Jayaraman and Lavie, 2005) try to overcome this problem.
