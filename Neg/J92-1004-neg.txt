The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ Notice that because &quot;teens&quot; was a relatively rare occurrence, a number of incorrect hypotheses had to be pursued before the correct one was considered.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ Another important feature of TINA is that the same grammar can be run in generation mode, making up random sentences by tossing the dice.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ For instance, the set of complements [from-place], [to-place], and [at-time] are freely ordered following a movement verb such as &quot;leave.&quot; Thus a flight can &quot;leave for Chicago from Boston at nine,&quot; or, equivalently, &quot;leave at nine for Chicago from Boston.&quot; If these complements are each allowed to follow the other, then in TINA an infinite sequence of [from-place's, [to-place]s and [at-time]s is possible.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ This is a problem to be aware of in building grammars from example sentences.

The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.
The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ This plural value gets unified with the singular value that had been retained from &quot;each&quot; during the top-down cycle.
The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.

As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ Thus a terminal verb node contains vocabulary entries that include settings for verb mode, and for person/number if the verb is finite.
As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ The correct solution is now in hand.
As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ An initial set of context-free rewrite rules provided by hand is first converted to a network structure.
As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ Representative systems are described in Boisen et al. (1989), De Mattia and Giachin (1989), Niedermair (1989), Niemann (1990), and Young (1989).

Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ The parser produces a set of next-word candidates dynamically for each partial theory.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ I would also like to thank several anonymous reviewers for their careful critiques, the outcome of which was a substantially improved document.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ In addition, Jim Glass, David Goodine, and Christine Pao have all made significant contributions to the programming of the TINA system, for which I am deeply grateful.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ In addition, Jim Glass, David Goodine, and Christine Pao have all made significant contributions to the programming of the TINA system, for which I am deeply grateful.

The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ Exactly how to get from the parse tree to an appropriate meaning representation is a current research topic in our group.
The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ Probability assignments on all arcs in the network are obtained automatically from a set of example sentences.
The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ This research has benefited significantly from interactions with Lynette Hirschman and Victor Zue.
The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ We also plan to experiment with further reductions in perplexity based on a discourse state.

Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ I would also like to thank several anonymous reviewers for their careful critiques, the outcome of which was a substantially improved document.
Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ There are no separate semantic rules off to the side; rather, the semantic information is encoded directly as names attached to nodes in the tree.
Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ Computation of perplexity, for the phrase, &quot;four fifteen:&quot; These are the three transitions with associated probabilities, following the appropriate paths in Figure A.1: Transition Probability Thus, for this example test sentence: This comes out to about 14 words on average following a given word, for this Stephanie Seneff TINA: A Natural Language System for Spoken Language Applications particular phrase.

In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ This paper describes a new natural language system that addresses issues of concern in building a fully integrated spoken language system.
In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ This research has benefited significantly from interactions with Lynette Hirschman and Victor Zue.
In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ Since there are only five training sentences, a number of the arcs of the original grammar are lost after training.

We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ This has been found to be extremely useful for revealing overgeneralization problems in the grammar, as well as for automatically acquiring a word-pair grammar for a recognizer and producing sentences to test the back-end capability.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ The parser produces a set of next-word candidates dynamically for each partial theory.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ The completed semantic frame is used in ATIS both to generate an SQL (Structured Query Language) command to access the database and to generate a text output to be spoken in the interactive dialog.

The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ This answer will be returned to the parent, &quot;tens-place,&quot; and two new hypotheses will be inserted at the top of the Paths through the parse tree for the phrase &quot;four fifteen&quot; with associated probabilities derived from the training data. stack as follows: ones-place number, tens-place 3/5 end number, tens-place 2/5 After the first one is rejected, the second one finds a completed &quot;number&quot; rule and an empty input stream.
The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ Probability assignments on all arcs in the network are obtained automatically from a set of example sentences.

The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ In practice, one must find all possible ways to extend a word sequence, computing total path probability for each one, and then renormalize to assure that with probability 1.0 there is an advance to some next word.
The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ These pieces of information usually are adequate to pinpoint the problem.
The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ When [subject] reaches the top of the queue, it activates units such as [noun phrase], [gerund], and [noun clause].

Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ In addition, Jim Glass, David Goodine, and Christine Pao have all made significant contributions to the programming of the TINA system, for which I am deeply grateful.
Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ I would also like to thank several anonymous reviewers for their careful critiques, the outcome of which was a substantially improved document.
Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ The parser is currently with MIT's for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.
Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.
