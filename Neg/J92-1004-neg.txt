The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ In addition, Jim Glass, David Goodine, and Christine Pao have all made significant contributions to the programming of the TINA system, for which I am deeply grateful.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ If it is to be useful for constraint, however, it must concern itself not only with coverage but also, and perhaps more importantly, with overgeneralizat ion.
The parsing model is a probabilistic recursive transition network similar to those described in (Miller et ai. 1994) and (Seneff 1992). $$$$$ Some of the generators are adverbial or adjectival parts of speech (Pos).

The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ Finally, &quot;teens&quot; will be popped off and matched, and &quot;endi tens-place, teens&quot; will be inserted at the top with probability 1.0.
The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ Section 4 discusses briefly two application domains involving database access in which the parser provides the link between a speech recognizer and the database queries.
The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ This leads to a very efficient implementation of the constraint mechanism.
The resulting N-best hypotheses are processed by the TINA language understanding component (Seneff, 1992). $$$$$ This had not been viewed as a major concern in the past, since systems were typically presented only with well-formed text strings, as opposed to errorful recognizer outputs.

As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ Furthermore, the same [do-question] grammar node deals with the yes/no question &quot;Did Mike buy the pies?,&quot; except in this case there is no CURRENTFOCUS and hence no gap.
As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ In addition, Jim Glass, David Goodine, and Christine Pao have all made significant contributions to the programming of the TINA system, for which I am deeply grateful.
As another way of bringing contextual information to bear in the process of predicting the meaning the following stochastic models, of unparsed inspired in Miller et al (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. $$$$$ Our current research is directed at a number of different remaining issues.

Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ This research has benefited significantly from interactions with Lynette Hirschman and Victor Zue.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ The absorber [pred-adjective] accepts the available float-object as its subparse, but only after confirming that POS is ADJECTIVE.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ The [do-question] is an activator; it moves the CURRENT-FOCUS into the FLOAT-OBJECT position.
Speech recognition results were parsed by the TINA parser (Seneff, 1992) using a hand-crafted grammar. $$$$$ With this strategy, a partial sentence does not become increasingly improbable as more and more words are added.

The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ The x variable in an x-y relationship is not explicitly mentioned, but rather is assigned to be &quot;whatever was delivered by the relative.&quot; Thus, for example, a node such as [subject] unifies in exactly the same way, regardless of the rule under construction.
The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ A functional block diagram of the control strategy is given in Figure 2.
The problem of over-generalization of speech grammars and related issues is well discussed by Seneff (1992). $$$$$ The process of obtaining a completed semantic frame amounts to passing frames along from node to node through the completed parse tree.

Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept.
Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ Section 2 contains a detailed description of the grammar and the control strategy, including syntactic and semantic constraint mechanisms.
Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ Each instantiation of a rule takes place only at the time that the next sibling is the distinguished [end] node, a special node that signifies a return to the level of the parent.
Example of WFST for LUcepts from user utterances by keyword spotting or heuristic rules has also been proposed (Seneff, 1992) where utterances can be transformed into concepts without major modifications to the rules. $$$$$ I would also like to thank several anonymous reviewers for their careful critiques, the outcome of which was a substantially improved document.

In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ This research has benefited significantly from interactions with Lynette Hirschman and Victor Zue.
In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ Work continues on improving all aspects of these domains.
In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ The CURRENT-FOCUS has a number of other uses besides its role in movement.
In our case, the log files include the output of the TINA Natural Language Understanding module, meaning that all semantically relevant units present in an input sentence are marked explicitly in the output parse frame (Seneff, 1992). $$$$$ new natural language system, been developed for applications involving spoken tasks. key ideas from context free grammars, Augmented Transition (ATN's), and the unification concept. a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.

We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ This also makes it straightforward to extract a semantic frame representation directly from an unannotated parse tree.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ The possible features include person and number, case, determiner (DEFINITE, INDEFINITE, PROPER, etc.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ After the addition of a number of semantically loaded nodes and semantic filters, the VOYAGER version of the grammar is now restricted mainly to sentences that are semantically as well as syntactically legitimate.
We utilized a parser (Seneff, 1992) that is based on an enhanced probabilistic context-free grammar (PCFG), which captures dependencies beyond context-free rules by conditioning on the external left-context parse categories when predicting the first child of each parent node. $$$$$ The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal long-distance movement, agreement, and semantic constraints. an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer.

The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ â€¢ Once a parser has produced an analysis of a particular sentence, the next step is to convert it to a meaning representation form that can be used to perform whatever operations the user intended by speaking the sentence.
The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ We didn't look at the test sentences while designing the grammar, nor have we yet looked at those sentences that failed to parse.
The input utterance is processed through the speech recognizer and language under standing (Seneff, 1992) components, to achieve a simple encoding of its meaning. $$$$$ The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal with long-distance movement, agreement, and semantic constraints.

The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance.
The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ If a match with an appropriate word is found, then the terminal node fills its subparse slot with an entry such as ([article] &quot;the&quot;), and activates all of its possible right-siblings.
The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ When we first integrated this recognizer with TINA, we used a &quot;wire&quot; connection, in that the recognizer produced a single best output, which was then passed to TINA for parsing.
The language understanding system, TINA, described at length in (Seneff, 1992), integrates key ideas context free grammar, augmented transition network and unification concepts. $$$$$ I would also like to thank several anonymous reviewers for their careful critiques, the outcome of which was a substantially improved document.

Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ This appendix walks through a pedagogical example to parse spoken digit sequences up to three long, as in &quot;three hundred and sixteen.&quot; Included is a set of initial contextfree rules, a set of training sentences, an illustration of how to compute the path probabilities from the training sentences, and an illustration of both parsing and perplexity computation for a test sentence.
Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ The SQL pattern is controlled through lists of frame patterns to match and query fragments to generate given the match.
Based on the Galaxyarchitecture (Goddeau et al, 1994), Jupiter recognizes user question over the phone, parses the question with the TINA language understanding system (Seneff,1992). $$$$$ In fact, to be truly effective, many potential applications demand that the system carry on a dialog with the user, using its knowledge base and information gleaned from previous sentences to achieve proper response generation.
