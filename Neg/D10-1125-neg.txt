For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ These models make use of functions gR(i, s, s0, r) where s E S, s0 E S are variables in a set of possible states S, and r is an index of a word in the sentence such that i < r < n. The function gR returns a cost for taking word r as the next dependency, and transitioning from state s to s0.
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ For all but one setting3 over 95% of the test sentences are decoded exactly, with 99% exactness in many cases.
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001).
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ The general approach should be applicable to other lexicalized syntactic formalisms, and potentially also to decoding in syntax-driven translation.

In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ By standard results, argmaxy∈Y h(y) + Ei,j u(k)(i, j)y(i, j).
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ This paper describes algorithms that compute y* for more complex definitions of f(y); in this section, we focus on algorithms for models that capture interactions between sibling dependencies.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Define k∗ to the value for k such that y↑(k, i) = 1.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ The accuracy of our models is higher than previous work on a broad range of datasets.

Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient.
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ We describe experiments on non-projective parsing for a number of languages, and in particular compare the dual decomposition algorithm to approaches based on general-purpose linear programming (LP) or integer linear programming (ILP) solvers (Martins et al., 2009).
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ McDonald et al. (2005) describe MST-based parsing for non-projective dependency parsing models with arc-factored decompositions; McDonald and Pereira (2006) make use of an approximate (hill-climbing) algorithm for parsing with more complex models.
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ This appendix describes details of the algorithm, specifically choice of the step sizes αk, and use of the -y(i, j) parameters.

In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models.
In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ We train the model using the averaged perceptron for structured problems (Collins, 2002).

First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ Here we describe an important optimization in the dual decomposition algorithms.
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ In practice, the updates to u are very sparse, and this condition occurs very often in practice.
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ We have found the following method to be effective.
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ For brevity

Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ We first introduce Lagrange multipliers u = {u(i, j) : (i, j) E Z}, and define the Lagrangian This follows because if y = z, the right term in Eq.
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Again, for a vector y|i define l1 ... lp to be the sequence of left modifiers to word i under y|i, and r1 ... rq to be the set of right modifiers.
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Other work has made extensive use of decomposition approaches for efficiently solving LP relaxations for graphical models (e.g., Sontag et al. (2008)).
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models.

While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata.
While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ 8 is the dual of an LP relaxation of the original problem.
While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ In addition, our dual decomposition approach is well-suited to parallelization.
While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ A.2 Use of the -y(i, j) Parameters The parsing algorithms both consider a generalized problem that includes -y(i, j) parameters.

This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ They provably solve an LP relaxation of the non-projective parsing problem.
This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ 3 can be considered to be an approximation to Eq.
This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ 5 ensure that z = y for some y E Y, and hence that z E Y.

Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ A.2 Use of the -y(i, j) Parameters The parsing algorithms both consider a generalized problem that includes -y(i, j) parameters.
Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ We have described dual decomposition algorithms for non-projective parsing, which leverage existing dynamic programming and MST algorithms.

 $$$$$ We use marginalbased pruning, using marginals calculated from an arc-factored spanning tree model using the matrixtree theorem (McDonald and Satta, 2007; Smith and Smith, 2007; Koo et al., 2007).
 $$$$$ Given the i’th example in the training set, (x(i), y(i)), the perceptron updates are as follows: The first step involves inference over the set Z, rather than Y as would be standard in the perceptron.
 $$$$$ We first introduce Lagrange multipliers u = {u(i, j) : (i, j) E Z}, and define the Lagrangian This follows because if y = z, the right term in Eq.

Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ We have found this method to be effective, very likely because Z is a superset of Y.
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ (Here we use Zi to refer to the set of all possible values for y|i: specifically, Z0 = {0,1}n and for i =� 0, Zi = {0,1}n−1.)
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ Our training approach is closely related to local training methods (Punyakanok et al., 2005).
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ The CertS and CertG columns in Table 1 give the results for the sibling and G+S models respectively.

Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ 11 and 12.
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ These models make use of functions gR(i, s, s0, r) where s E S, s0 E S are variables in a set of possible states S, and r is an index of a word in the sentence such that i < r < n. The function gR returns a cost for taking word r as the next dependency, and transitioning from state s to s0.
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ These models include nonprojective dependency parsing models with higherorder (e.g., sibling and/or grandparent) dependency relations as a special case.

DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ These models include nonprojective dependency parsing models with higherorder (e.g., sibling and/or grandparent) dependency relations as a special case.
DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ The dual problem, which our algorithm optimizes, is to obtain the tightest such upper bound, The dual objective L(u) is convex, but not differentiable.
DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ We have simply shifted the α(i, j) weights from one model to the other.
DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ Eisner (2000) describes extensions of head automata to include word senses; we have not discussed this issue in the current paper, but it is simple to develop dual decomposition algorithms for this case, using similar methods to those used for the grandparent models.

We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ For comparison to Martins et al. (2009), we perform experiments for Danish, Dutch, Portuguese, Slovene, Swedish and Turkish data from the CoNLL-X shared task (Buchholz and Marsi, 2006), and English data from the CoNLL-2008 shared task (Surdeanu et al., 2008).
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ Recall that y|i is a binary vector specifying which words are modifiers to the head-word i.
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations.
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ Riedel and Clarke (2006) describe ILP methods for the problem; Martins et al. (2009) recently introduced alternative LP and ILP formulations.

 $$$$$ We suspect this is important in early iterations of the algorithm, when many values for u(i, j) or v(i, j) will be zero, and where with Q = 0 many spanning tree solutions y(k) would be essentially random, leading to very noisy updates to the u(i, j) and v(i, j) values.
 $$$$$ The CertS and CertG columns in Table 1 give the results for the sibling and G+S models respectively.
 $$$$$ 11 and 12.
 $$$$$ Thus, decoding during training can be achieved by dynamic programming over head automata alone, which is very efficient.

For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ In grandparent models each parse tree y is represented as a vector where we have added a second set of duplicate variables, y↑(i, j) for all (i, j) E Z.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ First, the variables z∗(i, j) may not form a wellformed directed spanning tree.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ Given a vector y, define Hence y|i specifies the set of modifiers to word i; note that the vectors y|i for i = 0 ... n form a partition of the full set of variables.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.

 $$$$$ For example, each of the head-automata could be optimized independently in a multi-core or GPU architecture.
 $$$$$ Without the z(i, j) = y(i, j) constraints, the objective would decompose into the separate maximizations z∗ = argmaxz∈Z f(z), and y∗ = argmaxy∈Y h(y), which can be easily solved using dynamic programming and MST, respectively.
 $$$$$ This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures.
 $$$$$ Eisner (2000) describes extensions of head automata to include word senses; we have not discussed this issue in the current paper, but it is simple to develop dual decomposition algorithms for this case, using similar methods to those used for the grandparent models.

Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ We have found the following method to be effective.
Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ This appendix describes details of the algorithm, specifically choice of the step sizes αk, and use of the -y(i, j) parameters.
Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ Our algorithm differs in that we do not use general-purpose LP or ILP solvers, instead using an MST solver in combination with dynamic programming; thus we leverage the underlying structure of the problem, thereby deriving more efficient decoding algorithms.

There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). $$$$$ We have not tested other values for Q.
There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). $$$$$ The (I)LP experiments were carried out using Gurobi, a high-performance commercial-grade solver. for i = 0 ... n. However, if for some i, u(k)(i, j) = u(k−1)(i, j) for all j, then z(k) |�= z(k−1) |� .
There is a lot of flexibility about how to decompose the model into S components: each set Rs can correspond to a single factor in a factor graph (Smith and Eisner, 2008), or to a entire subgraph enclosing several factors (Koo et al, 2010), or even to a formula in Markov logic (Richardson and Domingos, 2006). $$$$$ The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.

 $$$$$ Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training.
 $$$$$ Finding z∗|i under the definition can be accomplished in O(n3) time, by decoding the model using dynamic programming separately for each of the O(n) possible values of k∗, and picking the value for k∗ that gives the maximum value under these decodings.
