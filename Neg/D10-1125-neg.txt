For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Both dual decomposition and Lagrangian relaxation have a long history in combinatorial optimization.
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ We train the model using the averaged perceptron for structured problems (Collins, 2002).
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ There has been a long history in combinatorial optimization of methods that exploit structure in complex problems, using methods such as dual decomposition or Lagrangian relaxation (Lemar´echal, 2001).
For projective parsing, dynamic programming for this factorization was derived in Koo and Collins (2010) (Model 1 in that paper), and for non projective parsing, dual decomposition was used for this factorization in Koo et al (2010). $$$$$ Meshi et al. (2010).

In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ 10.
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Meshi et al. (2010).
In NLP, Rush et al (2010) and Koo et al (2010) applied dual decomposition to enforce agreement between different sentence-level algorithms for parsing and POS tagging. $$$$$ Finally, to better compare the tightness of our LP relaxation to that of earlier work, we consider randomly-generated instances.

Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ Thus far, however, these methods are not widely used in NLP.
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures.
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ 9 is tight, and (y(k), z(k)) and u(k) are primal and dual optimal.
Recent work has shown that a relaxation based on dual decomposition often produces an exact solution for such problems (Koo et al 2010). $$$$$ Although decoding of full parse structures with non-projective head automata is intractable, we leverage the observation that key components of the decoding can be efficiently computed using combinatorial algorithms.

In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ Note that Y C Z, and in all but trivial cases, Y is a strict subset of Z.
In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ While the focus of the current paper is on nonprojective dependency parsing, the approach opens up new ways of thinking about parsing algorithms for lexicalized formalisms such as TAG (Joshi and Schabes, 1997), CCG (Steedman, 2000), and projective head automata.
In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ 10.
In our dual decomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate following the protocol described by Koo et al (2010). $$$$$ 7).

First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ We define Y to be the set of all well-formed non-projective dependency parses (i.e., the set of directed spanning trees rooted at node 0).
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ A.2 Use of the -y(i, j) Parameters The parsing algorithms both consider a generalized problem that includes -y(i, j) parameters.
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ This allows these functions to model grandparent dependencies such as (k∗, i, lj) and sibling dependencies such as (i, lj−1, lj).
First, we follow Koo et al (2010) and use lazy decoding as part of dual decomposition. $$$$$ Without the z(i, j) = y(i, j) constraints, the objective would decompose into the separate maximizations z∗ = argmaxz∈Z f(z), and y∗ = argmaxy∈Y h(y), which can be easily solved using dynamic programming and MST, respectively.

Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ Note that we use different feature sets from both Martins et al. (2009) and Smith and Eisner (2008).
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ The dual decomposition algorithms are simple and efficient, relying on standard dynamic programming and minimum spanning tree algorithms.
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ In practice, the updates to u are very sparse, and this condition occurs very often in practice.
Approximate parsers have there fore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ As described in section 7.7, the algorithms can be easily modified to consider projective structures by replacing Y with the set of projective trees, and then using first-order dependency parsing algorithms in place of MST decoding.

While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ In our experiments we set Q = 0.001, which puts almost all the weight in the head-automata models, but allows weights on spanning tree edges to break ties in MST inference in a sensible way.
While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ While the optimization problem remains the same, the algorithms in Figure 1 and 2 will converge at different rates depending on the value for Q.
While AD requires solving quadratic subproblems as an intermediate step, recent results (Martins et al, 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al, 2010). $$$$$ Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.

This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ In the bigram sibling models in our experiments, we assume that where as before l1 ... lp and r1 ... rq are left and right modifiers under y|i, and where φL and φR are feature vector definitions.
This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).
This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ In the grandparent models in our experiments, we use a similar definition with feature vectors φL(x, i, k∗, lk−1, lk) and φR(x, i, k∗, rk−1, rk), where k∗ is the parent for word i under y|i.
This opens the door for larger subproblems (such as the combination of trees and head automata in Koo et al, 2010) instead of a many-components approach (Martins et al, 2011), while still enjoying faster convergence. $$$$$ We introduce the following assumption: Again, it follows that we can approxiresulting vector z∗ may be deficient in two respects.

Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ Our work was originally inspired by recent work on dual decomposition for inference in graphical models (Wainwright et al., 2005; Komodakis et al., 2007).
Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ However, we can use a subgradient method to derive an algorithm that is similar to gradient descent, and which minimizes L(u).
Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ Columns give scores for UAS accuracy, percentage of solutions which are integral, and solution speed in seconds per sentence.
Koo et al (2010) used an identical automaton for their second-order model, but leaving out the grand-sibling scores. $$$$$ We define Y to be the set of all well-formed non-projective dependency parses (i.e., the set of directed spanning trees rooted at node 0).

 $$$$$ We will assume that the features decompose in the same way as the siblingdecomposable or grandparent/sibling-decomposable models, that is φ(x, y) = Pni=0 φ(x, y|i) for some feature vector definition φ(x, y|i).
 $$$$$ In our experiments we set Q = 0.001, which puts almost all the weight in the head-automata models, but allows weights on spanning tree edges to break ties in MST inference in a sensible way.
 $$$$$ This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures.

Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences.
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ At each iteration k, the algorithm finds y(k) E Y using an MST algorithm, and z(k) E i through separate decoding of the (n + 1) sibling models.
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ Our training approach is closely related to local training methods (Punyakanok et al., 2005).
Our parser achieves the state-of-art unlabeled accuracy of 93.06% and labeled accuracy of 91.86% on the standard test set for English, at a faster speed than a reimplementation of the third-order model of Koo et al (2010). $$$$$ There are a number of possible areas for future work.

Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ These models make use of functions gR(i, s, s0, r) where s E S, s0 E S are variables in a set of possible states S, and r is an index of a word in the sentence such that i < r < n. The function gR returns a cost for taking word r as the next dependency, and transitioning from state s to s0.
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ In fact, for this setting the algorithm returns the same solution as for K = 5000 on 99.59% of the examples.
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ In our experiments we make use of discriminative linear models, where for an input sentence x, the score for a parse y is f(y) = w ' φ(x, y) where w E Rd is a parameter vector, and φ(x, y) E Rd is a feature-vector representing parse tree y in conjunction with sentence x.
Few of many examples include type constraints between relations and entities (Roth and Yih, 2004), sentential and modifier constraints during sentence compression (Clarke and Lapata,2006), and agreement constraints between word alignment directions (Ganchev et al, 2008) or various parsing models (Koo et al, 2010). $$$$$ The algorithm can be viewed as an instance of Lagrangian relaxation; we describe this connection, and give convergence guarantees for the method.

DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ Thus, an alternative approach would be to use the dual decomposition algorithm for inference during training.
DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ We describe a generalization to models that include grandparent dependencies.
DD has been successfully applied to similar situations for combining local with global models; for example, in dependency parsing (Koo et al, 2010), bilingual sequence tagging (Wang et al, 2013) and word alignment (DeNero and Macherey, 2011). $$$$$ Our approach gets around this difficulty by introducing new variables, u(i, j), that serve to enforce agreement between the y(i, j) and z(i, j) variables.

We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ These models make use of functions gR(i, s, s0, r) where s E S, s0 E S are variables in a set of possible states S, and r is an index of a word in the sentence such that i < r < n. The function gR returns a cost for taking word r as the next dependency, and transitioning from state s to s0.
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ 4 was the z = y constraints.
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ We will make direct use of this approximation in the dual decomposition parsing algorithm.
We adopt a learning rate update rule from Koo et al (2010) where t is defined as 1/N, where N is the number of times we observed a consecutive dual value increase from iteration 1 to t. $$$$$ This paper introduces algorithms for nonparsing based on decomposi- We focus on parsing algorithms for nonhead a generalization of head-automata models to non-projective structures.

 $$$$$ Thus, it is these constraints that complicate the optimization.
 $$$$$ Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98% of test sentences.
 $$$$$ 4), However, we do not necessarily have strong duality—i.e., equality in the above equation— because the sets i and Y are discrete sets.
 $$$$$ 10.

For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ In this section we extend the approach to consider grandparent relations.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ As described in section 7.7, the algorithms can be easily modified to consider projective structures by replacing Y with the set of projective trees, and then using first-order dependency parsing algorithms in place of MST decoding.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ Eisner (2000) describes extensions of head automata to include word senses; we have not discussed this issue in the current paper, but it is simple to develop dual decomposition algorithms for this case, using similar methods to those used for the grandparent models.
For the third-order features (e.g., grand-siblings and tri-siblings) described in (Koo et al, 2010), we will discuss it in future work. $$$$$ In particular, In this paper we first give the definition for nonprojective head automata, and describe the parsing algorithm.

 $$$$$ However, the non-projective parsing problem has a very different structure from these models, and the decomposition we use is very different in nature from those used in graphical models.
 $$$$$ Subgradient optimization methods are iterative algorithms with updates that are similar to gradient descent: we omit the details, except to note that when the LP relaxation is not tight, the optimal primal solution to the LP relaxation could be recovered by averaging methods (Nedi´c and Ozdaglar, 2009). where αk is a step size.
 $$$$$ We have simply shifted the α(i, j) weights from one model to the other.
 $$$$$ Given the i’th example in the training set, (x(i), y(i)), the perceptron updates are as follows: The first step involves inference over the set Z, rather than Y as would be standard in the perceptron.

Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ We have found the following method to be effective.
Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ We will assume that the features decompose in the same way as the siblingdecomposable or grandparent/sibling-decomposable models, that is φ(x, y) = Pni=0 φ(x, y|i) for some feature vector definition φ(x, y|i).
Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ The accuracy of our models is higher than previous work on a broad range of datasets.
Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e.g., dependency parsing (Koo et al, 2010), POS tagging (Rush et al., 2012), etc.). $$$$$ The accuracy of our models is higher than previous work on a broad range of datasets.

There is a lot of flexibility about how to decompose the model into S components $$$$$ For example, each of the head-automata could be optimized independently in a multi-core or GPU architecture.
There is a lot of flexibility about how to decompose the model into S components $$$$$ These models make use of functions gR(i, s, s0, r) where s E S, s0 E S are variables in a set of possible states S, and r is an index of a word in the sentence such that i < r < n. The function gR returns a cost for taking word r as the next dependency, and transitioning from state s to s0.
There is a lot of flexibility about how to decompose the model into S components $$$$$ Although decoding of full parse structures with non-projective head automata is intractable, we leverage the observation that key components of the decoding can be efficiently computed using combinatorial algorithms.
There is a lot of flexibility about how to decompose the model into S components $$$$$ Finally, our approach could be used with other structured learning algorithms, e.g.

 $$$$$ In addition, our dual decomposition approach is well-suited to parallelization.
 $$$$$ As before, we define i = {y : y|i E ii for i = 0 ... n}.
 $$$$$ The accuracy of our models is higher than previous work on a broad range of datasets.
 $$$$$ A.2 Use of the -y(i, j) Parameters The parsing algorithms both consider a generalized problem that includes -y(i, j) parameters.
