Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ In order to build high quality systems for complex NLP tasks, such as question answering and textual entailment, it is essential to first have high quality systems for lower level tasks.
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ In order to build high quality systems for complex NLP tasks, such as question answering and textual entailment, it is essential to first have high quality systems for lower level tasks.
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ Most work on joint parsing and semantic role labeling (SRL) has been disappointing, despite obvious connections between the two tasks.
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ Our model is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task.

We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ Therefore, we compute features over adjacent words/labels when computing the features for the binary rule which joins them.
We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling.
We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser.

 $$$$$ The output from the joint model is shown in part (b), with the named entity information encoded within the parse.
 $$$$$ Examples are work of art, product, and law.
 $$$$$ Most of our features are over one or the other aspects of the structure, but not both.

Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ This paper is based on work funded in part by the Defense Advanced Research Projects Agency through IBM.
Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ However, these models are only used at test time; training of the components is still independent.
Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ We tested the significance of our results (on all the data combined) using Dan Bikel’s randomized parsing evaluation comparator6 and found that both the precision and recall gains were significant at p ≤ 0.01.

Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.
Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ The original NP label was then changed to PossNP.
Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ For parse features, we used the exact same features as described in (Finkel and Manning, 2008).
Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ Indeed, it has proven very difficult to build a joint model of parsing and semantic role labeling, either with PCFG trees (Sutton and McCallum, 2005) or with dependency trees.

We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ The rest of the errors seemed to be due to annotation errors and other random weirdnesses.
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ The joint model corrected 72 of those entities, while incorrectly identifying the boundaries of 37 entities which had previously been correctly identified.
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ To combat this problem, we added extra rules, unseen in the training data.
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ For every rule encountered in the training data which has been augmented with named entity information, we add extra copies of that rule to the grammar.

These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ We defined features over both the parse rules and the named entities.
These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of to absolute F1 for parsing, and up to F1 for named entity recognition.
These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ This paper begins to address this problem by building a joint model of both parsing and named entity recognition.
These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser).

Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ This approach was not followed up on in other work, presumably because around this time nearly all the activity in named entity and relation extraction moved to the use of discriminative sequence models, which allowed the flexible specification of feature templates that are very useful for these tasks.
Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ For example, the word today was usually labeled as a date, but about 10% of the time it was not labeled as anything.
Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ But, unfortunately, it is still common practice to cobble together independent systems for the various types of annotation, and there is no guarantee that their outputs will be consistent.
Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ The first author is supported by a Stanford Graduate Fellowship.

However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling.
However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ Previous work on linguistic annotation pipelines (Finkel et al., 2006; Hollingshead and Roark, 2007) has enforced consistency from one stage to the next.
However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.

It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ We tested the statistical significance of the gains (of all the data combined) using the same sentence-level, stratified shuffling technique as Bikel’s parse comparator and found that both precision and recall gains were significant at p < 10−4.
It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree.
It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ Vapnik has observed (Vapnik, 1998; Ng and Jordan, 2002) that “one should solve the problem directly and never solve a more general problem as an intermediate step,” implying that building a joint model of two phenomena is more likely to harm performance on the individual tasks than to help it.
It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ This paper begins to address this problem by building a joint model of both parsing and named entity recognition.

Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ Most of our features are over one or the other aspects of the structure, but not both.
Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling.
Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling.

For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. $$$$$ These grammar additions sufficed to improve overall performance.
For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. $$$$$ We then discuss in detail how we make use of the recently developed OntoNotes corpus both for training and testing the model, and then finally present the performance of the model and some discussion of what causes its superior performance, and how the model relates to prior work.
For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. $$$$$ There have been other attempts in NLP to jointly model multiple levels of structure, with varying degrees of success.

Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. $$$$$ The CoNLL 2008 shared task (Surdeanu et al., 2008) was intended to be about joint dependency parsing and semantic role labeling, but the top performing systems decoupled the tasks and outperformed the systems which attempted to learn them jointly.

In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ The first author is supported by a Stanford Graduate Fellowship.
In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ For comparison, we also trained the parser without the named entity information (and omitted the NamedEntity nodes), and a linear chain CRF using just the named entity information.
In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ But, unfortunately, it is still common practice to cobble together independent systems for the various types of annotation, and there is no guarantee that their outputs will be consistent.
In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ Sutton and McCallum (2005) attempted to jointly model PCFG parsing and SRL for the CoNLL 2005 shared task, but were unable to improve performance on either task.

Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ Indeed, it has proven very difficult to build a joint model of parsing and semantic role labeling, either with PCFG trees (Sutton and McCallum, 2005) or with dependency trees.
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ The lexicon is augmented in a similar manner to the rules.
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ The one thing that should never happen is for a named entity span to have crossing brackets with any spans in the parse tree.

For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). $$$$$ In order to build high quality systems for complex NLP tasks, such as question answering and textual entailment, it is essential to first have high quality systems for lower level tasks.
For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). $$$$$ These models also have the potential to suffer from search errors and are not guaranteed to find the optimal output.

Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ The full results can be found in Table 2.
Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ In the future, we would like to add other levels of annotation available in the OntoNotes corpus to our model, including word sense disambiguation and semantic role labeling.
Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ This allowed the model to learn that certain types of phrasal nodes, such as NPs are more likely to be named entities, and that certain entities were more likely to occur in certain contexts and have particular types of internal structure.
Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ We also distinguish between the root node of an entity, and the descendent nodes.

To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ We first present the joint, discriminative model that we use, which is a feature-based CRF-CFG parser operating over tree structures augmented with NER information.
To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005).
To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ The OntoNotes data does not contain any nested entities.

The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. $$$$$ The joint representation also allows the information from each type of annotation to improve performance on the other, and, in experiments with the OntoNotes corpus, we found improvements of to absolute F1 for parsing, and up to F1 for named entity recognition.
The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. $$$$$ Because this is a new corpus, still under development, it is not surprising that we found places where the data was inconsistently annotated, namely with crossing brackets between named entity and tree annotations.
The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. $$$$$ See Figure 1 for an illustration.

The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.
The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ Their sentence augmentations were similar to ours, but they did not make use of features due to the generative nature of their model.
The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ There have been other attempts in NLP to jointly model multiple levels of structure, with varying degrees of success.
The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ For named entities, the joint model should help with boundaries.
