Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ The first author is supported by a Stanford Graduate Fellowship.
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ Most of our features are over one or the other aspects of the structure, but not both.
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ We first present the joint, discriminative model that we use, which is a feature-based CRF-CFG parser operating over tree structures augmented with NER information.
Compared with the disappointing results of joint learning on syntactic and semantic parsing, Miller et al (2000) and Finkel and Manning (2009) showed the effectiveness of joint learning on syntactic parsing and some simple NLP tasks, such as information extraction and name entity recognition. $$$$$ After modifying the OntoNotes dataset to ensure consistency, which we will discuss in Section 4, we augment the parse tree with named entity information, for input to our learning algorithm.

We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ As we discussed in Section 2.1, this tree will then be augmented with an additional node for the entity (see Figure 1).
We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ Our model is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task.
We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser.
We built a joint model of parsing and named entity recognition (Finkel and Manning, 2009b), which had small gains on parse performance and moderate gains on named entity performance, when compared with single-task models trained on the same data. $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.

 $$$$$ When computing those features, we removed all of the named entity information from the rules, so that these features were just over the parse information and not at all over the named entity information.
 $$$$$ Additionally, 190 entities were found which the baseline model had missed entirely, and 68 entities were lost.
 $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.

Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ When deciding what tags are allowed for each word, we initially ignore named entity information.
Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ The CoNLL 2008 shared task (Surdeanu et al., 2008) was joint dependency parsing and SRL, but the top performing systems decoupled the tasks, rather than building joint models.
Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ For each word we computed a word shape which encoded information about capitalization, length, and inclusion of numbers and other non-alphabetic characters.
Our base joint model for parsing and named entity recognition is the same as (Finkel and Manning, 2009b), which is also based on the discriminative parser discussed in the previous section. $$$$$ Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree.

Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ Ideally, a named entity should correspond to a phrase in the constituency tree.
Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ For the named entity features, we used a fairly standard feature set, similar to those described in (Finkel et al., 2005).
Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ The first author is supported by a Stanford Graduate Fellowship.
Our baseline experiments were modeled after those in (Finkel and Manning, 2009b), and while our results were not identical (we updated to a newer release of the data), we had similar results and found the same general trends with respect to how the joint model improved on the single models. $$$$$ We then augment the labels of the phrasal node and its descendents with the type of named entity.

We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ For each word we computed a word shape which encoded information about capitalization, length, and inclusion of numbers and other non-alphabetic characters.
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ The insideoutside algorithm is run over the clique potentials to produce the partial derivatives and normalizing constant which are necessary for optimizing the log likelihood.
We used OntoNotes 3.0 (Hovy et al, 2006), and made the same data modifications as (Finkel and Manning, 2009b) to ensure consistency between the parsing and named entity annotations. $$$$$ The lexicon is augmented in a similar manner to the rules.

These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ The original NP label was then changed to PossNP.
These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ See Figure 2 for an illustration from the data.
These approaches have been shown to be successful for tasks such as parsing and named entity recognition in newswire data (Finkel and Manning, 2009) or semantic role labeling in the Penn Treebank and Brown corpus (Toutanova et al, 2008). $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.

Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ These models also have the potential to suffer from search errors and are not guaranteed to find the optimal output.
Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ The first author is supported by a Stanford Graduate Fellowship.
Finkel and Manning (2009) proposed a discriminative feature based constituency parser for joint named entity recognition and parsing. $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.

However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ Therefore, we compute features over adjacent words/labels when computing the features for the binary rule which joins them.
However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ This easily results in inconsistent annotations, which are harmful to the performance of the aggregate system.
However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ Both the named entity and parsing features utilize the words of the sentence, as well as orthographic and distributional similarity information.
However, most of the mentioned approaches are task-specific (e.g., (Toutanova et al, 2008) for semantic role labeling, and (Finkel and Manning, 2009) for parsing and NER), and they can hardly be applicable to other NLP tasks. $$$$$ This paper begins to address this problem by building a joint model of both parsing and named entity recognition.

It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ Our model is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task.
It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ Our model is based on a discriminative constituency parser, with the data, grammar, and features carefully constructed for the joint task.
It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ We also augment each word with a distributional similarity tag, which we discuss in greater depth in Section 3, and allow tags seen with other words which belong to the same distributional similarity cluster.
It has been used in some natural language processing tasks, such as joint parsing and named entity recognition (Finkel and Manning, 2009), and word sense disambiguation (Zhong et al, 2008). $$$$$ For many language technology applications, such as question answering, the overall system runs several independent processors over the data (such as a named entity recognizer, a coreference system, and a parser).

Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ The first place to look for improvements is with the boundaries for named entities.
Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ Adjectives and PPs.
Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ Lastly, we have the joint features.
Alternative approaches are that of Finkel and Manning (2009) on joint parsing and named entity recognition and the work of (Wehrli et al, 2010) which uses collocation information to rank competing hypotheses in a symbolic parser. $$$$$ Our model produces a consistent output, where the named entity spans do not conflict with the phrasal spans of the parse tree.

For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. $$$$$ We begin to address this problem with a joint model of parsing and named entity recognition, based on a discriminative feature-based constituency parser.
For example, constraining a parser to respect the boundaries of known entities is standard practice not only in joint modeling of (constituent) parsing and NER (Finkel and Manning, 2009), but also in higher-level NLP tasks, such as relation extraction (Mintz et al, 2009), that couple chunking with (dependency) parsing. $$$$$ This label was added in the new treebank annotation conventions, so as to identify internal left-branching structure inside previously flat NPs.

Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. $$$$$ For each word we computed a word shape which encoded information about capitalization, length, and inclusion of numbers and other non-alphabetic characters.
Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. $$$$$ The first place to look for improvements is with the boundaries for named entities.

In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ We presented a discriminatively trained joint model of parsing and named entity recognition, which improved performance on both tasks.
In the same spirit, Finkel and Manning (2009) merged the syntactic annotations and the named entity annotations of the OntoNotes corpus (Hovy et al, 2006) and trained a discriminative parsing model for the joint problem of syntactic parsing and named entity recognition. $$$$$ Because of this, for all pairs of adjacent words within an entity, there will be a binary rule applied where one word will be under the left child and the other word will be under the right child.

Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ We also distinguish between the root node of an entity, and the descendent nodes.
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ We also wish to thank the creators of OntoNotes, without which this project would not have been possible.
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ For parse features, we used the exact same features as described in (Finkel and Manning, 2008).
Finkel and Manning (2009b) also proposed a parsing model for the extraction of nested named entity mentions, which, like this work, parses just the corresponding semantic annotations. $$$$$ Because we only allow named entity derivations which we have seen in the data, nested entities are impossible.

For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). $$$$$ The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.
For instance, performing named entity recognition (NER) jointly with constituent parsing has been shown to improve performance on both tasks, but the only aspect of the syntax which is leveraged by the NER component is the location of noun phrases (Finkel and Manning, 2009). $$$$$ Looking at the parsing improvements on a per-label basis, the largest gains came from improved identication of NML consituents, from an F-score of 45.9% to 57.0% (on all the data combined, for a total of 420 NML constituents).

Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ Despite these earlier results, we found that combining parsing and named entity recognition modestly improved performance on both tasks.
Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ When a named entity span has crossing brackets with the spans in the parse tree it is usually impossible to effectively combine these pieces of information, and system performance suffers.
Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ In this case, we removed the terminating period from the entity, to produce a consistent annotation.
Finkel and Manning (2009) modeled the task of named entity recognition together with parsing. $$$$$ The data has been annotated with eighteen types of entities.

To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ The first author is supported by a Stanford Graduate Fellowship.
To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ These are the leaves of the tree, when only the named entity for the current word is known.2 The pairwise features, over adjacent labels, are computed at the same time as features over binary rules.
To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ Their sentence augmentations were similar to ours, but they did not make use of features due to the generative nature of their model.
To avoid overfitting, we employed an implementation from previous literature (Finkel and Manning,2009). $$$$$ Much greater improvements in performance were seen on named entity recognition, where most of the domains saw improvements in the range of 3â€“ 4%, with performance on the VOA data improving by nearly 9%, which is a 45% reduction in error.

The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. $$$$$ Despite these earlier results, we found that combining parsing and named entity recognition modestly improved performance on both tasks.
The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. $$$$$ Most of our features are over one or the other aspects of the structure, but not both.
The spirit of this work more closely resembles that of Finkel and Manning (2009), which improves both parsing and named entity recognition by combining the two tasks. $$$$$ This paper begins to address this problem by building a joint model of both parsing and named entity recognition.

The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ We included as features each augmented rule and each augmented label.
The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ For our experiments we used the LDC2008T04 OntoNotes Release 2.0 corpus (Hovy et al., 2006).
The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkeland Manning, 2009) and machine translation (Burkett et al, 2010). $$$$$ The joint model corrected 72 of those entities, while incorrectly identifying the boundaries of 37 entities which had previously been correctly identified.
