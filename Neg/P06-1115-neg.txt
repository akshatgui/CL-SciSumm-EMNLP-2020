KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ STATEID, [8..9]) (STATEID?
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ In each iteration, the positive examples from the previous iteration are first removed so thatnew positive examples which lead to better cor rect derivations can take their place.
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.

For details please refer to (Kate and Mooney, 2006). $$$$$ ulate this type of noise by substituting a word in the corpus by another word, w, with probability ped(w)?P (w), where p is a parameter, ed(w) isw?s edit distance (Levenshtein, 1966) from the original word and P (w) is w?s probability proportional toits word frequency.
For details please refer to (Kate and Mooney, 2006). $$$$$ The task of semantic parsing then is to find the most probable semantic derivation of an NL sentence.

Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ At this point, for many training sentences, the resulting most-probable semantic derivation may not give the correct MR. Hence, next, the system collects more refined positive and negative examples to improve the result in the next iteration.
Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ Normally, SVM classifiers only predict the classof the test example but one can obtain class probability estimates by mapping the distance of the ex ample from the SVM?s separating hyperplane to the range [0,1] using a learned sigmoid function (Platt, 1999).
Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.
Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ Our extended Ear ley?s algorithm does a beam search and attempts to find the ?

We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ For each of these marked words, the procedure considers all of the productions which cover it in the two derivations.
We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.
We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ This ensures thatevery MR will have a unique parse tree.

We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ Productions: ANSWER ? answer(RIVER) RIVER ? TRAVERSE(STATE) STATE ? NEXT TO(STATE) STATE ? STATEID TRAVERSE ? traverse NEXT TO ? next to STATEID ? stateid(?texas?)
We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ STATEID, [8..9]) (STATEID?

We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ The productions of the formal MRL grammar are treated like semantic concepts.
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ = argmaxd?D&getMR(d)=parse(mi) P (d) COLLECT POSITIVES(d?) // collect positives from maximum probability correct derivation for each d ? D do if P (d) > P (d?) and getMR(d) 6= parse(mi) then // collect negatives from incorrect derivation with larger probability than the correct one COLLECT NEGATIVES(d, d?)
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ We presented a new kernel-based approach to learn semantic parsers.
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ A learn ing system for semantic parsing is given a trainingcorpus of NL sentences paired with their respec tive MRs from which it has to induce a semantic parser which can map novel NL sentences to their correct MRs. Figure 1 shows an example of an NL sentence and its MR from the CLANG domain.

KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ We evaluatedour system on two real-world corpora.
KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ We present a new approach for mappingnatural language sentences to their formal meaning representations using string kernel-based classifiers.
KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ It is difficultfor rule-based methods or even statistical featurebased methods to capture the full range of NL con texts which map to a semantic concept because they tend to enumerate these contexts.

KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ It is difficultfor rule-based methods or even statistical featurebased methods to capture the full range of NL con texts which map to a semantic concept because they tend to enumerate these contexts.
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ We presented a new kernel-based approach to learn semantic parsers.
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ Previous work on learning semantic parsers either employ rule-based algorithms (Tang andMooney, 2001; Kate et al, 2005), or use sta tistical feature-based methods (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006).
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ These classifiers are then used to compositionally build complete meaning representa tions of natural language sentences.

The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ The re sults showed that our system compares favorably to other existing systems and is particularly robust to noise.

To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ The re sults showed that our system compares favorably to other existing systems and is particularly robust to noise.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ Then KRISP does not learn classifiers for these constant productions and instead decides if they cover a substring of the sentence or not by matching it with the provided constant substrings.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ We call the process of mapping natural language (NL) utterances into their computer-executablemeaning representations (MRs) as semantic pars ing.

We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ Our system learns these classifiers for every production in theformal language grammar.
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ In section 3, we describe how KRISP learns the string classifiers that are used to obtainthe probabilities needed in finding the most prob able semantic derivation.
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ Testing involves using these classifiers to generate the most probable derivation of a test sentence as described in the subsection 2.2, and returning its MR. The MRL grammar may contain productions corresponding to constants of the domain, for e.g., state names like ?STATEID ? ?texas??, or river names like ?RIVERID ? ?colorado??

KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ The MR in the functional query language can be readas if processing a list which gets modified by various functions.

Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.
Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ In the next subsection we will de scribe how KRISP obtains these probabilities using string-kernel based SVM classifiers.
Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ Instead of non-terminals, productions are shown in the nodes to emphasize the role of productions in semantic derivations.
Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ Our experiments on two real world data sets show that this approachcompares favorably to other existing sys tems and is particularly robust to noise.

 $$$$$ AcknowledgmentsThis research was supported by Defense Ad vanced Research Projects Agency under grant HR0011-04-1-0007.
 $$$$$ We assume that all MRLs have deter ministic context free grammars, which is true for almost all computer languages.
 $$$$$ Computational systems that learn to transform natural language sentences into formal meaning rep resentations have important practical applicationsin enabling user-friendly natural language com munication with computers.
 $$$$$ The second domain we have considered is the GEOQUERY domain (Zelle and Mooney, 1996) which is a query language for a small database of about 800 U.S. geographical facts.
