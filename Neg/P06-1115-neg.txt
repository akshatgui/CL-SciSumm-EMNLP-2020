KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ We evaluatedour system on two real-world corpora.
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ We evaluatedour system on two real-world corpora.
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.
KRISP (Kate and Mooney, 2006) is a semantic parser learning system which uses word subsequence kernel based SVM (Cristianini and Shawe-Taylor, 2000) classifiers and was shown to be robust to noise compared to other semantic parser learners. $$$$$ Kernel methods (Cristianini and Shawe-Taylor, 2000) are particularly suitable for semantic parsing because it involves mappingphrases of natural language (NL) sentences to semantic concepts in a meaning representation lan guage (MRL).

For details please refer to (Kate and Mooney, 2006). $$$$$ 4.2 Results.
For details please refer to (Kate and Mooney, 2006). $$$$$ For 160 trainingexamples it gave 49.2% precision with 12.67% re call.
For details please refer to (Kate and Mooney, 2006). $$$$$ AcknowledgmentsThis research was supported by Defense Ad vanced Research Projects Agency under grant HR0011-04-1-0007.
For details please refer to (Kate and Mooney, 2006). $$$$$ as the constant substring for the produc tion ?STATEID ? ?texas?.

Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ Figure 8 shows that KRISP performs com petently on other languages as well.
Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ For every test sentence, only thebest MR corresponding to the most probable se mantic derivation is considered for evaluation, and its probability is taken as the system?s confidence in that MR. Since KRISP uses a threshold, ?, toprune low probability derivation trees, it sometimes may fail to return any MR for a test sen tence.
Word subsequence kernel was employed in (Kate and Mooney, 2006) to compute the similarity between two substrings. $$$$$ Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept.

We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ We also compared with the CCG-based semantic parser by Zettlemoyer et al (2005), but their results are available only for the GEO880 corpus and their experimental set-up is also different from ours.
We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ The words are num bered according to their position in the sentence.
We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ KRISP learns a semantic parser it eratively, each iteration improving upon the parserlearned in the previous iteration.
We note that this use of multiple classifiers to determine the most probable parse is similar to the method used in the KRISP semantic parser (Kate and Mooney, 2006). $$$$$ In our experiments, the threshold ? was fixedto 0.05 and the beam size ? was 20.

We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.
We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ Given that natural languages are so flexible, there are various ways in which one can express the same semantic concept.
We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ We evaluatedour system on two real-world corpora.
We describe how these are applied in an error driven manner using the base semantic parsing learning algorithm presented in (Kate and Mooney, 2006) resulting in a better learned semantic parser. $$$$$ next to, [5..7]) the5 states6 bordering7 (STATE?

We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ The re sults showed that our system compares favorably to other existing systems and is particularly robust to noise.
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ The incorrectderivation and the most probable correct deriva tion are traversed simultaneously starting from the root using breadth-first traversal.
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ If a semantic derivation gives the correct MR of the NL sentence, then we call it a correct semantic derivation otherwise it is an incorrect semantic derivation.
We very briefly describe the semantic parser learning system, KRISP (Kate and Mooney, 2006), which we will use as a base system for transforming MRGs, we however note that the MRG transformation methods presented in this paper are general enough to work with any system which learns semantic parser using MRGs. $$$$$ In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex withricher predicates and nested structures.

KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ The re sults showed that our system compares favorably to other existing systems and is particularly robust to noise.
KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ These classifiers are then used to compositionally build complete meaning representa tions of natural language sentences.
KRISP (Kate and Mooney, 2006) is a discriminative approach where meaning representation structures are constructed from the natural language strings hierarchically. $$$$$ AcknowledgmentsThis research was supported by Defense Ad vanced Research Projects Agency under grant HR0011-04-1-0007.

KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ It is difficultfor rule-based methods or even statistical featurebased methods to capture the full range of NL con texts which map to a semantic concept because they tend to enumerate these contexts.
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ These pa rameters were found through pilot experiments.The maximum number of iterations (MAX ITR) re quired was only 3, beyond this we found that the system only overfits the training corpus.
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.
KRISP (Kate and Mooney, 2006) uses string classifiers to label substrings of the NL with entities from the MR. $$$$$ The recursion reaches base cases when the productions which have n on the LHS do not have any non-terminal on the RHS or when the substring s[i..j] becomes smaller than the length t. According to the equation, a production pi ? G and a partition (p1, .., pt) ? partition(s[i..j], t) will be selected in constructing the most probable partial derivation.

The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ Our experiments demonstrate that KRISP compares favor 913 NL: ?If the ball is in our goal area then our player 1 should intercept it.?
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ It is difficultfor rule-based methods or even statistical featurebased methods to capture the full range of NL con texts which map to a semantic concept because they tend to enumerate these contexts.
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ Sometimes, the children of an MR parse tree 914 (ANSWER?
The remaining refined landmarks plans are then treated as supervised training data for a semantic-parser learner, KRISP (Kate and Mooney, 2006). $$$$$ The task of semantic parsing then is to find the most probable semantic derivation of an NL sentence.

To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ KRISP was evaluated on CLANG and GEOQUERY domains as described in section 2.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ Our system learns these classifiers for every production in theformal language grammar.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ From the innermost expression go ing outwards it means: the state of Texas, the list containing all the states next to the state of Texas and the list of all the rivers which flow throughthese states.
To train a semantic parser using KRISP (Kate and Mooney, 2006), they had to supply a MRG, a context-free grammar, for their formal navigation plan language. $$$$$ We present a new approach for mappingnatural language sentences to their formal meaning representations using string kernel-based classifiers.

We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ The productions of the formal MRL grammar are treated like semantic concepts.
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ In this paper, we have tested KRISP on two real-world domains in which meaning representations are more complex withricher predicates and nested structures.
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ After a specified number of MAX ITR iterations, 917 (ANSWER?
We modify KRISP, a supervised learning system for semantic parsing presented in (Kate and Mooney, 2006), to make a semi-supervised system we call SEMISUP-KRISP. $$$$$ Meaning representations for novel natural language sen tences are obtained by finding the most probable semantic parse using these stringclassifiers.

KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ After a specified number of MAX ITR iterations, 917 (ANSWER?
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ We compared our system?s performance with the following existing systems: the string and tree versions of SILT (Kate et al, 2005), a system that learns transformation rules relating NL phrases to MRL expressions; WASP (Wong and Mooney, 2006), a system that learns transformation rules using statistical machine translation techniques; SCISSOR (Ge and Mooney, 2005), a system that learns an integrated syntactic-semantic parser; and CHILL (Tang and Mooney, 2001) an ILP-based semantic parser.
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ SVM classifiers based on string subsequence kernels are trained for each ofthe productions in the meaning representation language.
KRISP (Kernel-based Robust Interpretation for Semantic Parsing) (Kate and Mooney, 2006) is a supervised learning system for semantic parsing which takes NL sentences paired with their MRs as training data. $$$$$ and ?bordering?

Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ Computational systems that learn to transform natural language sentences into formal meaning rep resentations have important practical applicationsin enabling user-friendly natural language com munication with computers.
Experimentally, KRISP compares favorably to other existing semantic parsing systems and is particularly robust to noisy training data (Kate and Mooney, 2006). $$$$$ These classifiers are then used to compositionally build complete meaning representa tions of natural language sentences.

 $$$$$ In theMR of the example, bpos stands for ?ball posi tion?.
 $$$$$ Computational systems that learn to transform natural language sentences into formal meaning rep resentations have important practical applicationsin enabling user-friendly natural language com munication with computers.
 $$$$$ AcknowledgmentsThis research was supported by Defense Ad vanced Research Projects Agency under grant HR0011-04-1-0007.
 $$$$$ In the first iteration, the set P(pi) of positive examples for production pi contains all sentences, si, such thatparse(mi) uses the production pi.
