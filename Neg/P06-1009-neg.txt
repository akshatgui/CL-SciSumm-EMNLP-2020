These models are roughly clustered into two groups $$$$$ The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping features.
These models are roughly clustered into two groups $$$$$ Recently, a number of discriminative word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005).

Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). $$$$$ The tradeoff between recall and precision may affect the quality and number of phrases extracted for a phrase translation table.
Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). $$$$$ Our CRF model allows both the observation and transition components of the model to be jointly optimised from the corpus.
Such approaches have been shown to be effective in log-linear word alignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). $$$$$ Moreover, we showed how the CRF’s inference and estimation methods allowed for efficient processing without sacrificing optimality, improving on previous heuristic based approaches.

The one most similar to ours is the one presented by Blunsom and Cohn (2006). $$$$$ When the source language is English, another could be assigned either index 25 or 26; in these ambiguous situations we take the first index.
The one most similar to ours is the one presented by Blunsom and Cohn (2006). $$$$$ We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).
The one most similar to ours is the one presented by Blunsom and Cohn (2006). $$$$$ The results presented in this paper were evaluated in terms of AER.

Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. $$$$$ The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping features.
Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. $$$$$ We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001).
Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. $$$$$ However, their model is limited to only providing one-to-one alignments.
Labeled alignments are also used by Blunsom and Cohn (2006) to train a CRF word alignment model. $$$$$ As we didn’t have access to a Romanian POS tagger, these features were not used for the Romanian-English language pair.

The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). $$$$$ We derive a feature from both the Dice and Model 1 translation scores to allow competition between sources words for a particular target alignment.
The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). $$$$$ Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting.

Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). $$$$$ Moreover, we showed how the CRF’s inference and estimation methods allowed for efficient processing without sacrificing optimality, improving on previous heuristic based approaches.
Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). $$$$$ The results presented in this paper were evaluated in terms of AER.
Examples include the maximum entropy model of (Ittycheriah and Roukos, 2005) or the conditional random field jointly normalized over the entire sequence of alignments of (Blunsom and Cohn, 2006). $$$$$ Table 4 displays the results on both language pairs when these additional features are used with the refined model.

The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. $$$$$ To discourage all of these French words from aligning with the, the best of these (la) is flagged as the best candidate.
The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. $$$$$ Interestingly, without any features derived from the sentence aligned corpus, our model achieves performance equivalent to Model 3 trained on the full corpus (Och and Ney, 2003).
The model is trained by gradient ascent using the l-BFGS method (Liu and Nocedal, 1989), which has been successfully used for training log linear models (Blunsom and Cohn, 2006) in many natural language tasks, including alignment. $$$$$ These are calculated using forward-backward inference, which yields the partition function, ZA(e, f), required for the log-likelihood, and the pair-wise marginals, pA(at−1, at|e, f), required for its derivatives.

(Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. $$$$$ We show how a large number of highly predictive features can be easily incorporated into the CRF, and demonstrate that even with only a few hundred word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively.
(Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. $$$$$ Callison-Burch et al. (2004) demonstrated that the GIZA++ models could be trained in a semi-supervised manner, leading to a slight decrease in error.
(Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. $$$$$ The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data.
(Blunsom and Cohn, 2006) do word alignment by combining features using conditional random fields. $$$$$ These models allow for the use of arbitrary and overlapping features over the source and target sentences, making the most of small supervised training sets.

Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). $$$$$ The tradeoff between recall and precision may affect the quality and number of phrases extracted for a phrase translation table.
Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). $$$$$ The tradeoff between recall and precision may affect the quality and number of phrases extracted for a phrase translation table.
Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). $$$$$ Both the forward-backward and Viterbi algorithm are dynamic programs which make use of the Markov assumption to calculate efficiently the exact marginal distributions.
Reported work includes improved model variants (e.g., Jiao et al, 2006) and applications such as web data extraction (Pinto et al, 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourse level chunking (Feng et al, 2007). $$$$$ The CRF is conditioned on both the source and target texts, and thus allows for the use of arbitrary and overlapping features over these data.

Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). $$$$$ The tradeoff between recall and precision may affect the quality and number of phrases extracted for a phrase translation table.
Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). $$$$$ We follow Taskar et al. (2005) by using the first 100 test sentences for training and the remaining 347 for testing.
Our work is heavily influenced by the bilingual alignment literature, especially the discriminative model proposed by Blunsom and Cohn (2006). $$$$$ The effect of removing the Markov features can be seen from comparing Figures 2 (a) and (b).
