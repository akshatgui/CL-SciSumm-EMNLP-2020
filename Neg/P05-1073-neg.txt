Reranking has previously been applied to semantic role labeling by Toutanova et al (2005), from which we use several features. $$$$$ One reason for this is the lower upper bound, due largely to the the much poorer performance of the identification model on automatic parses.
Reranking has previously been applied to semantic role labeling by Toutanova et al (2005), from which we use several features. $$$$$ For our experiments we used the February 2004 release of PropBank.
Reranking has previously been applied to semantic role labeling by Toutanova et al (2005), from which we use several features. $$$$$ For ARGM, we identify and label core as well as modifier arguments.
Reranking has previously been applied to semantic role labeling by Toutanova et al (2005), from which we use several features. $$$$$ This sequence of labeled nodes is defined with respect to the left-to-right order of constituents in the parse tree.

Toutanova et al (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. $$$$$ The important difference is that their template extracts contextual information from noun phrases surrounding the predicate, rather than from the sequence of argument nodes.
Toutanova et al (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. $$$$$ We build both local and joint models for semantic role labeling, and evaluate the gains achievable by incorporating joint information.
Toutanova et al (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. $$$$$ For approximately 5.6% of the argument constituents in the test set, we could not find exact matches in the automatic parses.
Toutanova et al (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicate argument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. $$$$$ Reflecting linguistic intuition and in line with current work, we have shown that there are substantial gains to be had by jointly modeling the argument frames of verbs.

 $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
 $$$$$ For example, there are hard constraints – that arguments cannot overlap with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument.
 $$$$$ As can be seen from Table 4, our joint models achieve error reductions of 32% and 22% over our local models in FMeasure on CORE and ARGM respectively.

Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al, 2005). $$$$$ The probability (or score) of an assignment L according to this re-ranking model is defined as: The score of an assignment L not in the top N is zero.
Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al, 2005). $$$$$ Reflecting linguistic intuition and in line with current work, we have shown that there are substantial gains to be had by jointly modeling the argument frames of verbs.
Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al, 2005). $$$$$ This is especially true when we model the dependencies with discriminative models capable of incorporating long-distance features.
Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al, 2005). $$$$$ This was confirmed in our experiments and we redefined the whole label sequence features to exclude modifying arguments.

To enforce this constraint, we employ the approach presented by Toutanova et al (2005). $$$$$ In addition to reporting the standard results on individual argument F-Measure, we also report Frame Accuracy (Acc.
To enforce this constraint, we employ the approach presented by Toutanova et al (2005). $$$$$ The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies.
To enforce this constraint, we employ the approach presented by Toutanova et al (2005). $$$$$ As can be seen from Table 3, for relatively small values of N, our re-ranking approach does not present a serious bottleneck to performance.
To enforce this constraint, we employ the approach presented by Toutanova et al (2005). $$$$$ As can be seen from Table 4, our joint models achieve error reductions of 32% and 22% over our local models in FMeasure on CORE and ARGM respectively.

 $$$$$ As our results show, the error reduction of our joint model with respect to the local model is more modest in this setting.
 $$$$$ The most direct way to use trained local identification and classification models in testing is to select a labeling L of the parse tree that maximizes the product of the probabilities according to the two models as in Equation 1.
 $$$$$ Consider the pair of sentences, Despite the different syntactic positions of the labeled phrases, we recognize that each plays the same role – indicated by the label – in the meaning of this sense of the verb give.
 $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.

We implemented a global reranker following Toutanova et al (2005). $$$$$ Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004).
We implemented a global reranker following Toutanova et al (2005). $$$$$ Reflecting linguistic intuition and in line with current work, we have shown that there are substantial gains to be had by jointly modeling the argument frames of verbs.
We implemented a global reranker following Toutanova et al (2005). $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
We implemented a global reranker following Toutanova et al (2005). $$$$$ We report results for local and joint models on argument identification, argument classification, and the complete identification and classification pipeline.

Statistical parsers are major components in NLP applications such as QA (Kwok et al, 2001), MT (Marcu et al, 2006) and SRL (Toutanova et al, 2005). $$$$$ The authors would like to thank the reviewers for their helpful comments and Dan Jurafsky for his insightful suggestions and useful discussions.
Statistical parsers are major components in NLP applications such as QA (Kwok et al, 2001), MT (Marcu et al, 2006) and SRL (Toutanova et al, 2005). $$$$$ Since these models are local, this is equivalent to independently maximizing the product of the probabilities of the two models for the label li of each parse tree node ni as shown below in Equation 2. a product of local identification and classification models.
Statistical parsers are major components in NLP applications such as QA (Kwok et al, 2001), MT (Marcu et al, 2006) and SRL (Toutanova et al, 2005). $$$$$ We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models.
Statistical parsers are major components in NLP applications such as QA (Kwok et al, 2001), MT (Marcu et al, 2006) and SRL (Toutanova et al, 2005). $$$$$ One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other.

We employ this decomposition mainly for efficiency in training $$$$$ In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees (Marcus et al., 1993).
We employ this decomposition mainly for efficiency in training $$$$$ By slightly modifying this procedure, we obtain the most likely assignment according to and classification models can be chained in a principled way, as in Equation 1.
We employ this decomposition mainly for efficiency in training $$$$$ Let 4b(t, v, L) E R' denote a feature map from a tree t, target verb v, and joint assignment L of the nodes of the tree, to the vector space R'.

Toutanova et al (2005) report a substantial improvement in performance on the semantic role labeling task by building a joint classifier, which takes the labels of other phrases into account when classifying a given phrase. $$$$$ Reflecting linguistic intuition and in line with current work, we have shown that there are substantial gains to be had by jointly modeling the argument frames of verbs.
Toutanova et al (2005) report a substantial improvement in performance on the semantic role labeling task by building a joint classifier, which takes the labels of other phrases into account when classifying a given phrase. $$$$$ The labeling of the tree in Figure 1 is a specific example of the kind of errors fixed by the joint models.
Toutanova et al (2005) report a substantial improvement in performance on the semantic role labeling task by building a joint classifier, which takes the labels of other phrases into account when classifying a given phrase. $$$$$ This stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments.

We applied the joint classifiers in the framework of N-best re ranking (Collins, 2000), following Toutanova et al (2005). $$$$$ (Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution.
We applied the joint classifiers in the framework of N-best re ranking (Collins, 2000), following Toutanova et al (2005). $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
We applied the joint classifiers in the framework of N-best re ranking (Collins, 2000), following Toutanova et al (2005). $$$$$ The authors would like to thank the reviewers for their helpful comments and Dan Jurafsky for his insightful suggestions and useful discussions.
We applied the joint classifiers in the framework of N-best re ranking (Collins, 2000), following Toutanova et al (2005). $$$$$ We do stress however that the joint framework and features proposed here can also be used when only a shallow parse (chunked) representation is available as in the CoNLL-2004 shared task (Carreras and M`arquez, 2004).

Our system, on the other hand, follows a joint approach in the spirit of Toutanova et al (2005) and performs the above steps collectively. $$$$$ We build both local and joint models for semantic role labeling, and evaluate the gains achievable by incorporating joint information.
Our system, on the other hand, follows a joint approach in the spirit of Toutanova et al (2005) and performs the above steps collectively. $$$$$ Several semantic role labeling systems have successfully utilized joint information.

In contrast to the work of Toutanova et al (2005) our system applies on line learning to train its parameters and exact inference to predict a collective role labelling. $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
In contrast to the work of Toutanova et al (2005) our system applies on line learning to train its parameters and exact inference to predict a collective role labelling. $$$$$ (Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels.
In contrast to the work of Toutanova et al (2005) our system applies on line learning to train its parameters and exact inference to predict a collective role labelling. $$$$$ Our local models use the features listed in Table 1 and the technique for enforcing the non-overlapping constraint discussed in Section 3.1.

In addition, while the system described here is based on pipelined classification, recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al., 2005). $$$$$ Frame Features: Another very effective class of features we defined are features that look at the label of a single argument node and internal features of other argument nodes.
In addition, while the system described here is based on pipelined classification, recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al., 2005). $$$$$ For simplicity, we describe the dynamic program for the case where only two classes are possible – ARG and NONE.
In addition, while the system described here is based on pipelined classification, recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al., 2005). $$$$$ As can be seen from Table 4, our joint models achieve error reductions of 32% and 22% over our local models in FMeasure on CORE and ARGM respectively.

In a recent paper on the SRL on verbal predicates for English, (Toutanova et al, 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label. $$$$$ When classifying the NP in object position, it is useful to know whether the following argument is a PP.
In a recent paper on the SRL on verbal predicates for English, (Toutanova et al, 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label. $$$$$ Training a model which has such huge number of classes is infeasible if the model does not factorize due to strong independence assumptions.
In a recent paper on the SRL on verbal predicates for English, (Toutanova et al, 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label. $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.

The system, introduced in (Toutanova et al, 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. $$$$$ The local classifier labeled the first argument in the tree as ARG0 instead of ARG1, probably because an ARG0 label is more likely for the subject position.
The system, introduced in (Toutanova et al, 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. $$$$$ For simplicity, we describe the dynamic program for the case where only two classes are possible – ARG and NONE.
The system, introduced in (Toutanova et al, 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. $$$$$ The authors would like to thank the reviewers for their helpful comments and Dan Jurafsky for his insightful suggestions and useful discussions.
The system, introduced in (Toutanova et al, 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. $$$$$ We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models.

The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005). $$$$$ ), the fraction of sentences for which we successfully label all nodes.
The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005). $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005). $$$$$ For ARGM, the local identification model achieves 85.9 F-Measure and 59.4 Frame Accuracy; the local classification model achieves 92.3 F-Measure and 83.1 Frame Accuracy.
The ones denoted with asterisks (*) were not present in (Toutanova et al, 2005). $$$$$ The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies.

We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005). $$$$$ The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies.
We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005). $$$$$ Reflecting linguistic intuition and in line with current work, we have shown that there are substantial gains to be had by jointly modeling the argument frames of verbs.
We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005). $$$$$ Also, 19 of the propositions in the test set were discarded because Charniak’s parser altered the tokenization of the input sentence and tokens could not be aligned.
We find the exact top N consistent most likely local model labelings using a simple dynamic program described in (Toutanova et al, 2005). $$$$$ Several semantic role labeling systems have successfully utilized joint information.

Most of the features we use are described in more detail in (Toutanova et al, 2005). $$$$$ We start by introducing our local models, and later build on them to define joint models.
Most of the features we use are described in more detail in (Toutanova et al, 2005). $$$$$ This work was supported in part by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.
Most of the features we use are described in more detail in (Toutanova et al, 2005). $$$$$ This stands in stark contrast to the linguistic observation that a core argument frame is with strong dependencies between arguments.
Most of the features we use are described in more detail in (Toutanova et al, 2005). $$$$$ We distinguish between models that learn to label nodes in the parse tree independently, called local models, and models that incorporate dependencies among the labels of multiple nodes, called joint models.

The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as part of-speech (POS) tagging (Lafferty et al, 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al, 2005). $$$$$ The dynamic program works from the leaves of the tree up and finds a best assignment for each tree, using already computed assignments for its children.
The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as part of-speech (POS) tagging (Lafferty et al, 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al, 2005). $$$$$ We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative loglinear models.
The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as part of-speech (POS) tagging (Lafferty et al, 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al, 2005). $$$$$ Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1).
