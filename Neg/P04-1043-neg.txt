In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). $$$$$ Arg0 or ArgM in PropBank or Agent and Goal in FrameNet.
In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). $$$$$ Several linguistic theories, e.g.
In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). $$$$$ We considered all PropBank arguments6 from Arg0 to Arg9, ArgA and ArgM for a total of 122,774 and 7,359 arguments in training and testing respectively.
In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). $$$$$ Third, additional work is needed to design kernels suitable to learn the deep semantic contained in FrameNet as it seems not sensible to both PAF and SCF information.

The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ The discovery of relevant features is, as usual, a complex task, nevertheless, there is a common consensus on the basic features that should be adopted.
The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ 82.0% in (Gildea and Palmer, 2002).
The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ The discovery of relevant features is, as usual, a complex task, nevertheless, there is a common consensus on the basic features that should be adopted.

 $$$$$ Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify Prop- Bank predicate arguments with accuracy higher the current argument classification state- Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement.
 $$$$$ Experiments on Support Vector Machines using the above kernels show an improvement of the state-of-the-art for PropBank argument classification.
 $$$$$ On the contrary, FrameNet semantic parsing seems to not take advantage of the structural information provided by our kernels.

We used support vector machines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification. $$$$$ Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify Prop- Bank predicate arguments with accuracy higher the current argument classification state- Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement.
We used support vector machines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification. $$$$$ For example, in Figure 3, KPAK (0(Fflush), 0(Fbuckle)) is quite high as the two verbs have the same syntactic realization of their arguments.
We used support vector machines (Vapnik, 1995) with (a) polynomial kernels to learn the semantic role classification and (b) Tree Kernels (Moschitti, 2004) for learning both frame and ILC classification. $$$$$ Both A (see Section 3.3) and -y parameters were evaluated in a similar way by maximizing the performance of SVM using KPAF and -y KSCF tively.

 $$$$$ In particular, these problems affect the processing of predicate argument structures annotated in PropBank (Kingsbury and Palmer, 2002) or FrameNet (Fillmore, 1982).
 $$$$$ In case the node a exactly covers Paul, a lecture or in Rome, it will be a positive instance otherwise it will be a negative one, e.g.
 $$$$$ The above property also holds for the SCF structures.
 $$$$$ The simplest mapping that we can apply is O(Fz) = z� = (z1,..., zn) where zi = 1 if fi E Fz otherwise zi = 0, i.e. the characteristic vector of the set Fz with respect to F. If we choose as a kernel function the scalar product we obtain the linear kernel KL(Fx, Fz) = x�· z.

The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ Their main property is the ability to process structured representations.
The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ For example, the Parse Tree Path feature represents the path in the parse-tree between a predicate node and one of its argument nodes.
The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ The above object space aims to capture all the information between a predicate and one of its arguments.
The kernel that we employed in our experiments is based on the SCF structure devised in (Moschitti, 2004). $$$$$ Without loss of generality we can assume: (a) Voice=1 if active and 0 if passive, and (b) Position=1 when the argument is after the predicate and 0 otherwise.

 $$$$$ The same is not true for SCF since it does not contain information about a specific argument.
 $$$$$ To study the impact of our structural kernels we firstly derived the maximal accuracy reachable with standard features along with polynomial kernels.
 $$$$$ Their main property is the ability to process structured representations.

As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. $$$$$ The remainder of this paper is organized as follows: Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it.
As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. $$$$$ FrameNet also describes predicate/argument structures but for this purpose it uses richer semantic structures called frames.
As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. $$$$$ It is expressed as a sequence of nonterminal labels linked by direction symbols (up or down), e.g. in Figure 1, VTVPINP is the path between the predicate to give and the argument 1, a lecture.
As shown in (Moschitti, 2004), we can label semantic roles by classifying the smallest subtree that includes the predicate with one of its arguments, i.e. the so called PAF structure. $$$$$ The results have shown that: First, SVMs using the above kernels are appealing for semantically parsing both corpora.

 $$$$$ For example, the Parse Tree Path feature represents the path in the parse-tree between a predicate node and one of its argument nodes.
 $$$$$ The remainder of this paper is organized as follows: Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it.
 $$$$$ For example, if we change the tense of the verb to deliver (Figure 2) in delivered, the [VP [V delivers] [NP]] subtree will be transformed in [VP [VBD delivered] [NP]], where the NP is unchanged.

For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). $$$$$ Second, PAF and SCF can be used to improve automatic classification of PropBank arguments as they provide clues about the predicate argument structure of the target verb.
For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). $$$$$ In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.
For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). $$$$$ It is worth noting that even if the above equations define a kernel function similar to the one proposed in (Collins and Duffy, 2002), the substructures on which it operates are different from the parse-tree kernel.

In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). $$$$$ Note that each predicate/argument pair is associated with only one structure, i.e.
In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). $$$$$ Frame elements or semantic roles are arguments of predicates called target words.
In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). $$$$$ Note that each predicate/argument pair is associated with only one structure, i.e.
In (Moschitti, 2004), an alternative to the SCF extraction was proposed, i.e. the SCF kernel (SK). $$$$$ Given a vector space in Rn and a set of positive and negative points, SVMs classify vectors according to a separating hyperplane, H(x) = w�x x�+ b = 0, where w� E Rn and b E Rare learned by applying the Structural Risk Minimization principle (Vapnik, 1995).

A preliminary study on the benefit of such kernels was measured on the classification accuracy of semantic arguments in (Moschitti, 2004). $$$$$ This different outcome is due to a different task (we classify different roles) and a different classification algorithm.
A preliminary study on the benefit of such kernels was measured on the classification accuracy of semantic arguments in (Moschitti, 2004). $$$$$ For example, SCF improves (a) the classification state-of-theart (i.e. the polynomial kernel) of about 3 percent points and (b) the best literature result of about 5 percent points.

The convolution kernel that we have experimented was devised in (Moschitti, 2004) and is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities. $$$$$ (PAF) We consider the predicate argument structures annotated in PropBank or FrameNet as our semantic space.
The convolution kernel that we have experimented was devised in (Moschitti, 2004) and is characterized by two aspects: the semantic space of the subcategorization structures and the kernel function that measure their similarities. $$$$$ In the future we plan to design other structures and combine them with SCF, PAF and standard features.

The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http: //ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999). $$$$$ The second relates to the sub-categorization frame of verbs.
The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http: //ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999). $$$$$ The simplest mapping that we can apply is O(Fz) = z� = (z1,..., zn) where zi = 1 if fi E Fz otherwise zi = 0, i.e. the characteristic vector of the set Fz with respect to F. If we choose as a kernel function the scalar product we obtain the linear kernel KL(Fx, Fz) = x�· z.
The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http: //ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999). $$$$$ In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.

Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling. $$$$$ In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.
Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling. $$$$$ Their main property is the ability to process structured representations.

In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). $$$$$ Both the above aims can be carried out by combining PAF and SCF with the standard features.
In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). $$$$$ In the future we plan to design other structures and combine them with SCF, PAF and standard features.
In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). $$$$$ PAF or SCF objects) to the set of all their possible sub-structures element in Fp,a with an abuse of notation we use it to indicate the objects themselves. where nc(nx) is the number of the children of nx and ch(n, i) is the i-th child of the node n. Note that as the productions are the same ch(nx, i) = ch(nz, i).
In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004). $$$$$ On the contrary, convolution kernels aim to capture structural information in term of sub-structures, providing a viable alternative to flat features.

For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). $$$$$ In this way, an individual ONE-vs-ALL classifier for each argument i can be trained.
For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). $$$$$ These standard features, firstly proposed in (Gildea and Jurasfky, 2002), refer to a flat information derived from parse trees, i.e.
For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). $$$$$ The results have shown that: First, SVMs using the above kernels are appealing for semantically parsing both corpora.
For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004). $$$$$ On 10Unfortunately the use of a polynomial kernel on top the tree fragments to generate the XOR functions seems not successful. the contrary, the performance decreases, suggesting that the classifier is confused by this syntactic information.

Moschitti (2004) and Che et al (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. $$$$$ Moreover, we have combined them with the polynomial kernel of standard features.
Moschitti (2004) and Che et al (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. $$$$$ Finally, an analysis of SVMs using polynomial kernels over standard features has explained why they largely outperform linear classifiers based-on standard features.
Moschitti (2004) and Che et al (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. $$$$$ This problem can be divided into two subtasks: (a) the detection of the argument boundaries, i.e. all its compounding words and (b) the classification of the argument type, e.g.
Moschitti (2004) and Che et al (2006) used a convolution tree kernel (Collins and Duffy, 2001) for semantic role classification. $$$$$ In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments.

Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. $$$$$ Experiments on Support Vector Machines using the above kernels show an improvement of the state-of-the-art for PropBank argument classification.
Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. $$$$$ Fflush whereas the dashed line tailors the SCF of the predicate buckle, i.e.
Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. $$$$$ Finally, an analysis of SVMs using polynomial kernels over standard features has explained why they largely outperform linear classifiers based-on standard features.
Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. $$$$$ Two pairs <p1, a1> and <p2, a2> have two different Path features even if the paths differ only for a node in the parse-tree.

In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one. $$$$$ These latter are schematic representations of situations involving various participants, properties and roles in which a word may be typically used.
In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one. $$$$$ The main drawback is that structures may not be properly represented by flat features.
In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one. $$$$$ The results have shown that: First, SVMs using the above kernels are appealing for semantically parsing both corpora.
In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one. $$$$$ These parameters were adopted also for all the other kernels.
