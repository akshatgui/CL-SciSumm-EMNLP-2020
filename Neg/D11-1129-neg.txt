We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ The implementation is based on building matrices for words with relational types (adjectives, verbs), and vectors for words with atomic types (nouns), based on data from the BNC.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ As we have seen above, in practice we do not need to build this tensor space, as the computations thereof reduce to point-wise multiplications and summations.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ Support from EPSRC grant EP/F042728/1 is gratefully acknowledged by M. Sadrzadeh.

In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ We leave a thorough study of these phenomena, which fall under providing a modular representation of passiveactive similarities, to future work.
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ We wish to thank P. Blunsom, S. Clark, B. Coecke, S. Pulman, and the anonymous EMNLP reviewers for discussions and comments.
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ Our model matches the results of its competitors in the first experiment, and betters them in the second.
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ Start with an r-dimensional vector space N with basis {−→n i}i, in which meaning vectors of atomic words, such as nouns, live.

The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ As mentioned by one of the reviewers, our pregroup approach to grammar flattens the sentence representation, in that the verb is applied to its subject and object at the same time; whereas in other approaches such as CCG, it is first applied to the object to produce a verb phrase, then applied to the subject to produce the sentence.
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ We are naturally good at understanding ambiguous words given a context, and forming the meaning of a sentence from the meaning of its parts.
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ Preliminary work on integration of the two has been presented by Preller (2007) and more recently also by Preller and Sadrzadeh ( 2009).

Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ However, such models fail to shine when it comes to processing the semantics of phrases and sentences.
Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ For any word vector word, the scalar weight cword iassociated with each context basis vector →−ni is a function of the number of times the word has appeared in that context.
Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005).
Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ This two-sortedness of defining properties of meaning: ‘logical form’ versus ‘contextual use’, has left the quest for ‘what is the foundational structure of meaning?’ even more of a challenge.

While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ The advantages and disadvantages of this method and comparisons with other systems, in particular CCG, constitutes ongoing work.
While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ Furthermore it shows better results in experiments involving higher syntactic complexity.
While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ The findings thereof will increase our understanding of cognition and intelligence and shall assist in applications to automating language-related tasks such as document search.

 $$$$$ Each section was given to a group of evaluators, with a total of 25, who were asked to form simple transitive sentence pairs from the verbs, subject and object provided in each entry; for instance ‘the table showed the result’ from ‘table show result’.
 $$$$$ It is not yet entirely clear how existing set-theoretic approaches, for example that of discourse representation and generalised quantifiers, apply to our setting.
 $$$$$ In the transitive case, 5 = N ® N, hence →−s t = →−n i ® →−n j.

Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. $$$$$ We wish to thank P. Blunsom, S. Clark, B. Coecke, S. Pulman, and the anonymous EMNLP reviewers for discussions and comments.
Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. $$$$$ The advantages and disadvantages of this method and comparisons with other systems, in particular CCG, constitutes ongoing work.
Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. $$$$$ Similar computations yield meanings of sentences with adjectives and adverbs.
Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects: "table show result", "priest say mass", etc. $$$$$ We expect these facts to be reflected in a sufficiently large corpus: the words ‘beer’ and ‘sherry’ occur within the 1E.g. words which appear in the same sentence or n-word window, or words which hold particular grammatical or dependency relations to the word being learned. context of identifying words such as ‘drink’, ‘alcoholic’ and ‘hangover’ more frequently than they occur with other content words.

Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ This will build alongside the general guidelines of Coecke et al. (2010) and concrete insights from the work of Widdows (2005).
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ Our model matches the results of its competitors in the first experiment, and betters them in the second.
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ We wish to thank P. Blunsom, S. Clark, B. Coecke, S. Pulman, and the anonymous EMNLP reviewers for discussions and comments.

 $$$$$ The advantages and disadvantages of this method and comparisons with other systems, in particular CCG, constitutes ongoing work.
 $$$$$ Such models consist of a pairing of syntactic interpretation rules (in the form of a grammar) with semantic interpretation rules, as exemplified by the simple model presented in Figure 1.
 $$$$$ Whereas semantic compositional mechanisms for set-theoretic constructions are well understood, there are no obvious corresponding methods for vector spaces.
 $$$$$ Treatment of function words such as ‘that’, ‘who’, as well as logical words such as quantifiers and conjunctives are left to future work.

Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a ρ quasiidentical to the multiplicative model, indicating significant correlation with the annotator scores.
Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ We use this second evaluation principally to show that there is a strong case for the development of more complex experiments measuring not only the disambiguating qualities of compositional models, but also their syntactic sensitivity, which is not directly measured in the existing experiments.
Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ It is assumed that inter-annotator agreement provides the theoretical maximum p for any model for this experiment.
Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ The advantages and disadvantages of this method and comparisons with other systems, in particular CCG, constitutes ongoing work.
