We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ We then show how to apply verbs to their subject/object, in order to compute the meaning of intransitive and transitive sentences.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ We wish to thank P. Blunsom, S. Clark, B. Coecke, S. Pulman, and the anonymous EMNLP reviewers for discussions and comments.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ We expect these facts to be reflected in a sufficiently large corpus: the words ‘beer’ and ‘sherry’ occur within the 1E.g. words which appear in the same sentence or n-word window, or words which hold particular grammatical or dependency relations to the word being learned. context of identifying words such as ‘drink’, ‘alcoholic’ and ‘hangover’ more frequently than they occur with other content words.
We discuss three ways of constructing such matrices, and evaluate each method in a disambiguation task developed by Grefenstette and Sadrzadeh (2011). $$$$$ However, our approach is sensitive to grammatical structure, leading us to develop a second experiment taking this into account and differentiating it from models with commutative composition operations.

In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable.
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ Here also, Spearman’s ρ is deemed a more rigorous way of determining how well a model tracks difference in meaning.
In (Grefenstette and Sadrzadeh, 2011), we developed and implemented one such method on the data from the British National Corpus. $$$$$ The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.

The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ The correlation of the model’s similarity judgements with the human judgements is also calculated using Spearman’s p, a metric which is deemed to be more scrupulous, and ultimately that by which models should be ranked, by Mitchell and Lapata (2008).
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing exponentially with sentence length hence quickly becoming computationally intractable.
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ The categorical morphism corresponding to it is denoted by the tensor product of 3 components: eV ®1S ®eW, where V and W are subject and object spaces, S is the sentence space, the E’s are the cups, and 1S is the straight line in the diagram.
The experiment is on the dataset developed in (Grefenstette and Sadrzadeh, 2011). $$$$$ In a nutshell, pregroup types are either atomic or compound.

Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ This setting offers geometric means to reason about semantic similarity (e.g. via cosine measure or k-means clustering), as discussed in Widdows (2005).
Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.
Furthermore, the success of both of these methods relative to the others examined in Table 1 shows that it is the extra information provided in the matrix (rather than just the diagonal, representing the lexical vector) that encodes the relational nature of transitive verbs, thereby validating in part the requirement suggested in Coecke et al (2010) and Grefenstette and Sadrzadeh (2011) that relational word vectors live in a space the dimensionality of which be a function of the arity of the relation. $$$$$ The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments.

While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ For any word vector word, the scalar weight cword iassociated with each context basis vector →−ni is a function of the number of times the word has appeared in that context.
While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.
While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic composition by generic algebraic operations. $$$$$ Evaluating such a framework is no easy task.

 $$$$$ Support from EPSRC grant EP/F042728/1 is gratefully acknowledged by M. Sadrzadeh.
 $$$$$ We evaluated our model in two ways: first against the word disambiguation task of Mitchell and Lapata (2008) for intransitive verbs, and then against a similar new experiment for transitive verbs, which we developed.
 $$$$$ We observe a significant (according to p < 0.0.5) improvement in the alignment of our categorical model with the human judgements, from 0.17 to 0.21.

Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects $$$$$ Search engines such as Google either fall back on bag of words models—ignoring syntax and lexical relations—or exploit superficial models of lexical semantics to retrieve pages with terms related to those in the query (Manning et al., 2008).
Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only Tarzan-style statements with determiner-less subjects and objects $$$$$ Such context distributions can be encoded as vectors in a high dimensional space with contexts as −−→ basis vectors.

Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ Other work uses matrices to model meaning (Baroni and Zamparelli, 2010; Guevara, 2010), but only for adjective-noun phrases.
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ Finally, very similar functions, for example a verb with argument alternations such as ‘break’ in ‘Y breaks’ and ‘X breaks Y’, are not treated as unrelated.
Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. $$$$$ Our findings in the first experiment show that the categorical method performs on par with the leading existing approaches.

 $$$$$ As mentioned by one of the reviewers, our pregroup approach to grammar flattens the sentence representation, in that the verb is applied to its subject and object at the same time; whereas in other approaches such as CCG, it is first applied to the object to produce a verb phrase, then applied to the subject to produce the sentence.
 $$$$$ There is not a large difference between the mean High score and mean Low score, but the distribution in Figure 6 shows that our model makes a non-negligible distinction between high similarity phrases and low similarity phrases, despite the absolute scores not being different by more than a few percentiles.
 $$$$$ In a distributional setting, the weights, which are natural or real numbers, will represent more: ‘the extent according to which x and y are related’.
 $$$$$ The left argument is the Kronecker product of subject and object vectors and the right argument is the vector of the verb, so we obtain Since O is commutative, this provides us with a distributional version of the type-logical meaning of the sentence: point-wise multiplication of the meaning of the verb to the Kronecker product of its subject and object: −−−−−−−−→ −−→sub verb obj = verb O (−→ sub ® obj) This mathematical operation can be informally described as a structured ‘mixing’ of the information of the subject and object, followed by it being ‘filtered’ through the information of the verb applied to them, in order to produce the information of the sentence.

Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ In this section, we briefly describe the evaluation of our model against this dataset.
Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ We wish to thank P. Blunsom, S. Clark, B. Coecke, S. Pulman, and the anonymous EMNLP reviewers for discussions and comments.
Grefenstette and Sadrzadeh (2011) learn matrices for verbs in a categorical model. $$$$$ We provide a general algorithm for building (or indeed learning) these matrices from the corpus.
