 $$$$$ In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm.
 $$$$$ Besides tags for frequent words, our baseline POS tagger also uses the tag dictionary to store closed-set tags (Xia, 2000) – those associated only with a limited number of Chinese words.
 $$$$$ The overall tagging accuracy also increased slightly, consistent with observations from the pure POS tagger.

Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ The joint system takes features only from the baseline segmentor and the baseline POS tagger to allow a fair comparison.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ However, the accuracy loss from the beam decoder, as well as alternative decoding algorithms, are worth further exploration.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ In comparison, our model handles character and word information simultaneously in a single perceptron model.

Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ Since tagging is restricted to characters, the search space is reduced to O((4T)'), and beam search decoding is effective with a small beam size.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve the accuracy of segmentation and POS tagging.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.

We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ However, the accuracy loss from the beam decoder, as well as alternative decoding algorithms, are worth further exploration.
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ To reduce overfitting and increase the decoding speed, templates 4, 5, 6 and 7 only include words with less than 3 characters.
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ Ng and Low (2004) and Shi and Wang (2007) were described in the Introduction.

Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ Score(y) is computed by a feature-based linear model.
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ Both models reduced the large search space by imposing strong restrictions on the form of search candidates.
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.

Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ We calculate the w� value by supervised learning, using the averaged perceptron algorithm (Collins, 2002), given in Figure 1.
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ Since Chinese sentences do not contain explicitly marked word boundaries, word segmentation is a necessary step before POS tagging can be performed.
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ Word information is used to process known-words, and character information is used for unknown words in a similar way to Ng and Low (2004).

We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ This work is supported by the ORS and Clarendon Fund.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Chinese word segmentation is a preliminary step.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Shi and Wang (2007) introduced POS information to segmentation by reranking.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy.

Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ However, the application of beam search was far from trivial because of the size of the combined search space.
Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ The algorithm is shown in Figure 2.
Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach.

 $$$$$ We built a two-stage baseline system, using the perceptron segmentation model from our previous work (Zhang and Clark, 2007) and the perceptron POS tagging model from Collins (2002).
 $$$$$ For a sentence with n characters, the number of possible output sequences is O(2n−1 · Tn), where T is the size of the tag set.
 $$$$$ Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space.
 $$$$$ All input characters are processed in the same way, and the final output is the best candidate in the final agenda.

Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ There may be additional features that are particularly useful to the joint system.
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ This work is supported by the ORS and Clarendon Fund.
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ The baseline feature templates for Chinese segmentation and POS tagging, when added together, makes exact inference for the proposed joint model very hard.
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ Using this method, POS features are allowed to interact with segmentation.

Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ To avoid error propagation and improve segmentation by utilizing segmentation and tagging can be performed simultaneously.
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ This work is supported by the ORS and Clarendon Fund.
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ Ng and Low (2004) and Shi and Wang (2007) were described in the Introduction.
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ Shi and Wang (2007) introduced POS information to segmentation by reranking.

Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ The algorithm is shown in Figure 2.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ Other choices are available for the decoding of a joint linear model, such as exact inference with dynamic programming, provided that the range of features allows efficient processing.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ Ng and Low (2004) and Shi and Wang (2007) were described in the Introduction.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ The joint system takes features only from the baseline segmentor and the baseline POS tagger to allow a fair comparison.

 $$$$$ Lastly, segmentation errors contribute around half (51.47%) of all the errors.
 $$$$$ The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.
 $$$$$ Moreover, the most commonly mistaken tags are NN and VV, while among the most frequent tags in the corpus, PU, DEG and M had comparatively less errors.

We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ In the development test, the use of the tag dictionary improves the decoding speed of the joint model, reducing the decoding time from 416 seconds to 256 seconds.
We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach.
We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ Besides tags for frequent words, our baseline POS tagger also uses the tag dictionary to store closed-set tags (Xia, 2000) – those associated only with a limited number of Chinese words.

In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.
In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ If we assume that the beam is not large enough for all the candidates at at each stage, then, from the newly generated candidates, the baseline POS tagger can keep 1/T for the next processing stage, while the joint model can keep only 1/2T, and has to discard the rest.
In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ However, the application of beam search was far from trivial because of the size of the combined search space.

In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ Besides tags for frequent words, our baseline POS tagger also uses the tag dictionary to store closed-set tags (Xia, 2000) – those associated only with a limited number of Chinese words.
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ Chinese word segmentation is a preliminary step.
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve the accuracy of segmentation and POS tagging.

Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ In this section, we build a joint word segmentation and POS tagging model that uses exactly the same source of information as the baseline system, by applying the feature templates from the baseline word segmentor and POS tagger.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ In comparison, our joint model does not impose any hard limitations on the interaction between segmentation and POS information.4 Fast decoding speed is achieved by using a novel multiple-beam search algorithm.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ In the nth test, the nth group is used as the testing data.

We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ This work is supported by the ORS and Clarendon Fund.
We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ Chinese word segmentation is a preliminary step.
We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ Since Chinese sentences do not contain explicitly marked word boundaries, word segmentation is a necessary step before POS tagging can be performed.

For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ Like the baseline segmentor, the baseline tagger also normalizes word length features.
For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ Experiments with the standard beam-search decoder described in (Zhang and Clark, 2007) resulted in low accuracy.
For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ Despite the higher accuracy improvement from the baseline, the joint system did not give higher overall accuracy.

Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ In comparison, our model handles character and word information simultaneously in a single perceptron model.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Other morphological features from Tseng et al. (2005) are not used because they require extra web corpora besides the training data.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Two types of tags are assigned to each character to represent its segmentation and POS.
