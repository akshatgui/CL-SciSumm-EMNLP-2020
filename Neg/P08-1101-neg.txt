 $$$$$ To avoid error propagation and improve segmentation by utilizing segmentation and tagging can be performed simultaneously.
 $$$$$ The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.
 $$$$$ To reduce overfitting and increase the decoding speed, templates 4, 5, 6 and 7 only include words with less than 3 characters.

Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ The learning curves of the baseline and joint models are shown in Figure 3, Figure 4 and Figure 5, respectively.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ Ng and Low (2004) and Shi and Wang (2007) were described in the Introduction.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm.
Zhang and Clark (2008) generated CTB 3.0 from CTB 4.0. $$$$$ Therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and POS tagger.

Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ There may be additional features that are particularly useful to the joint system.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ This work is supported by the ORS and Clarendon Fund.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings. $$$$$ Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space.

We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ To reduce overfitting and increase the decoding speed, templates 4, 5, 6 and 7 only include words with less than 3 characters.
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ Due to the large accuracy gain from the baseline, our joint model performed much better.
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.
We decided to follow the experimental settings of Jiang et al (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora. $$$$$ In comparison, our model handles character and word information simultaneously in a single perceptron model.

Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ Therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and POS tagger.
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ We used a single linear model for combined word segmentation and POS tagging, and chose the generalized perceptron algorithm for joint training. and beam search for efficient decoding.
Following Zhang and Clark (2008), we first generated CTB 3.0 from CTB 4.0 using sentence IDs. $$$$$ For example, the tag “b NN” indicates a character at the beginning of a noun.

Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ Both models reduced the large search space by imposing strong restrictions on the form of search candidates.
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ Moreover, the comparatively rare POS pattern “number word” + “number word” can help to prevent segmenting a long number word into two words.
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ The features used by the baseline segmentor are shown in Table 1.
Table 8 compares the F1 results of our baseline model with Nakagawa and Uchimoto (2007) and Zhang and Clark (2008) on CTB 3.0. $$$$$ The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.

We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Open features, such as knowledge of numbers and European letters, and relationships from semantic networks (Shi and Wang, 2007), have been reported to improve the accuracy of segmentation and POS tagging.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ This beam search algorithm processes an input sentence incrementally.
We relax the max operation by beam-search, resulting in a segment-based decoder similar to the multiple-beam algorithm in (Zhang and Clark, 2008). $$$$$ Various decoding approaches have been used to reduce the combined search space.

Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ The algorithm is simple and efficient, with a linear time complexity of O(BTn), where n is the size of input sentence, and T is the size of the tag set (T = 1 for pure word segmentation).
Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).
Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ We chose to follow Ng and Low (2004) and split the sentences evenly to facilitate further comparison.
Zhang and Clark (2008) used a segment-based decoder for word segmentation and pos tagging. $$$$$ Recent research has explored integration of segmentation and tagging, by decoding under restricted versions of the full combined search space.

 $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
 $$$$$ Word information is used to process known-words, and character information is used for unknown words in a similar way to Ng and Low (2004).
 $$$$$ To solve the above problems, we developed a multiple beam search algorithm, which compares candidates only with complete tagged words, and enables the size of the search space to scale with the input size.

Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ Nakagawa and Uchimoto (2007) proposed a hybrid model for word segmentation and POS tagging using an HMM-based approach.
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard.
Zhang and Clark (2008) built a perceptron-based joint segmenter and part-of-speech (POS) tagger for Chinese, and Toutanova and Cherry (2009) learned a joint model of lemmatization and POS tagging which outperformed a pipelined model. $$$$$ We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).

Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ To reduce overfitting and increase the decoding speed, templates 4, 5, 6 and 7 only include words with less than 3 characters.
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ Therefore, given the flexibility of the feature-based linear model, an obvious next step is the study of open features in the joint segmentor and POS tagger.
Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b). $$$$$ In the development test, the use of the tag dictionary improves the decoding speed of the joint model, reducing the decoding time from 416 seconds to 256 seconds.

Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ The joint model gives an error reduction in segmentation accuracy of an error reduction in tagging acof compared to the traditional pipeline approach.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ This work is supported by the ORS and Clarendon Fund.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ In comparison, our model handles character and word information simultaneously in a single perceptron model.
Zhang and Clark (2008) employed the generalized perceptron algorithm to train a statistical model for joint segmentation and POS tagging, and applied multiple-beam search algorithm for fast decoding. $$$$$ In comparison, our model handles character and word information simultaneously in a single perceptron model.

 $$$$$ There may be additional features that are particularly useful to the joint system.
 $$$$$ The POS tagging features are based on contextual information from the tag trigram, as well as the neighboring three-word window.
 $$$$$ Candidate ranking is based on a discriminative joint model, with features being extracted from segmented words and POS tags simultaneously.
 $$$$$ It can be seen from the table that the NN-VV and VV-NN mistakes were the most commonly made by the decoder, while the NR-NN mistakes are also frequent.

We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ Here an error is counted when a word in the standard output is not produced by the decoder, due to incorrect segmentation or tag assignment.
We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ The joint model gives an error reduction in segmentation accuracy of an error reduction in tagging acof compared to the traditional pipeline approach.
We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ For each word occurring more than N times in the training data, the decoder can only assign a tag the word has been seen with in the training data.
We show that the standard beam-search algorithm can be used as an efficient decoder for the global linear model of Zhang and Clark (2008) for joint word segmentation and POS-tagging, achieving a significant speed improvement. $$$$$ The features used by the POS tagger, some of which are different to those from Collins (2002) and are specific to Chinese, are shown in Table 2.

In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ Here an error is counted when a word in the standard output is not produced by the decoder, due to incorrect segmentation or tag assignment.
In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ 5000 is a rough figure to control the number of frequent words, set according to Zipf’s law.
In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ Though our model is built specifically for Chinese word segmentation and POS tagging, the idea of using the perceptron model to solve multiple tasks simultaneously can be generalized to other tasks.
In our 10-fold cross validation experiments with the Chinese Tree bank, our system performed over 10 times as fast as Zhang and Clark (2008) with little accuracy loss. $$$$$ The system uses a discriminative statistical model, trained using the generalized perceptron algorithm.

In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ We used a single linear model for combined word segmentation and POS tagging, and chose the generalized perceptron algorithm for joint training. and beam search for efficient decoding.
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ These curves are used to show the convergence of perceptron and decide the number of training iterations for the test.
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ However, for the joint problem, candidates in the beam are segmented and tagged sequences up to the current character, where the last word can be a complete word or a partial word.
In our previous work (Zhang and Clark, 2008), which we refer to as Z&C08 from now on, we used an approximate decoding algorithm that keeps track of a set of partially built structures for each character, which can be seen as a dynamic programming chart which is greatly reduced by pruning. $$$$$ In comparison, our model handles character and word information simultaneously in a single perceptron model.

Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ We thank Hwee-Tou Ng and Mengqiu Wang for their helpful discussions and sharing of experimental data, and the anonymous reviewers for their suggestions.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ The baseline feature templates for Chinese segmentation and POS tagging, when added together, makes exact inference for the proposed joint model very hard.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ Chinese word segmentation is a preliminary step.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ However, the comparison is indirect because our partitions of the CTB corpus are different.

We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ They have been shown to improve the accuracy of a Chinese POS tagger (Tseng et al., 2005).
We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ Moreover, the most commonly mistaken tags are NN and VV, while among the most frequent tags in the corpus, PU, DEG and M had comparatively less errors.
We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. $$$$$ We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).

For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ A challenge for this joint approach is the large combined search space, which makes efficient decoding very hard.
For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ Chinese word segmentation is a preliminary step.
For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ In this paper, propose a joint segmentation and tagging model that does not impose any hard constraints on the interaction between word and Fast decoding is achieved by using a novel multiple-beam search algorithm.
For POS tagging features, we follow the work of Zhang and Clark (2008a). $$$$$ We use baseline system to refer to the system which performs segmentation first, followed by POS tagging (using the single-best segmentation); baseline segmentor to refer to the segmentor from (Zhang and Clark, 2007) which performs segmentation only; and baseline POStagger to refer to the Collins tagger which performs POS tagging only (given segmentation).

Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Chinese word segmentation is a preliminary step.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Here the category of a character is the set of tags seen on the character during training.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ The character category features (templates 15 and 16 in Table 2) represent a Chinese character by all the tags associated with the character in the training data.
Following the setup of Duan et al (2007), Zhang and Clark (2008b) and Huang and Sagae (2010), we split CTB5 into training (secs 001 815 and 1001-1136), development (secs 886-931and 1148-1151), and test (secs 816-885 and 1137 1147) sets. $$$$$ Different methods have been proposed to reduce error propagation between pipelined tasks, both in general (Sutton et al., 2004; Daum´e III and Marcu, 2005; Finkel et al., 2006) and for specific problems such as language modeling and utterance classification (Saraclar and Roark, 2005) and labeling and chunking (Shimizu and Haas, 2006).
