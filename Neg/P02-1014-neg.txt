See Ng and Cardie (2002) for a detailed description of the features. $$$$$ Still other grammatical features encode general linguistic preferences either for or against coreference.
See Ng and Cardie (2002) for a detailed description of the features. $$$$$ Thanks to three anonymous reviewers for their comments and, in particular, for suggesting that we investigate data fragmentation issues.
See Ng and Cardie (2002) for a detailed description of the features. $$$$$ For the proposed best-first clustering to be successful, however, a different method for training instance selection would be needed: rather than generate a positive training example for each anaphoric NP and its closest antecedent, we instead generate a positive training examples for its most confident antecedent.
See Ng and Cardie (2002) for a detailed description of the features. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.

We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. $$$$$ Thanks to three anonymous reviewers for their comments and, in particular, for suggesting that we investigate data fragmentation issues.
We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. $$$$$ For the proposed best-first clustering to be successful, however, a different method for training instance selection would be needed: rather than generate a positive training example for each anaphoric NP and its closest antecedent, we instead generate a positive training examples for its most confident antecedent.
We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. $$$$$ Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.
We now show that the search problem in (2) can equivalently be solved by the more intuitive best first decoder (Ng and Cardie, 2002), rather than using the CLE decoder. $$$$$ Thanks to three anonymous reviewers for their comments and, in particular, for suggesting that we investigate data fragmentation issues.

The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. $$$$$ Our results indicate that coreference resolution systems can improve by effectively exploiting the interaction between the classification algorithm, training instance selection, and the clustering algorithm.
The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. $$$$$ Sidner (1979), Harabagiu et al. (2001)) as a means of improving common noun phrase resolution, which remains a challenge for state-of-the-art coreference resolution systems.
The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining meaningful antecedents for mentions can be tracedback to Ng and Cardie (2002) who used a rule based approach. $$$$$ Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.

One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). $$$$$ Our results provide direct evidence for the claim (Mitkov, 1997) that the extra-linguistic strategies employed to combine the available linguistic knowledge sources play an important role in computational approaches to coreference resolution.
One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). $$$$$ We evaluate the Duplicated Soon Baseline system using the standard MUC-6 (1995) and MUC7 (1998) coreference corpora, training the coreference classifier on the 30 “dry run” texts, and applying the coreference resolution algorithm on the 20–30 “formal evaluation” texts.
One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). $$$$$ More specifically, it employs the standard combination of classification and clustering described above.
One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). $$$$$ As noted above, for example, it is important to automate the precision-oriented feature selection procedure as well as to investigate other methods for feature selection.

In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). $$$$$ We demonstrate empirically that the degradation in performance can be attributed, at least in part, to poor performance on common noun resolution.
In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). $$$$$ Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).
In the testing phase, we used the best-first clustering as in Ng and Cardie (2002). $$$$$ Soon’s string match feature (SOON STR) tests whether the two NPs under consideration are the same string after removing determiners from each.

We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported $$$$$ For instance, a rule induced by RIPPER classifies two NPs as coreferent if the first NP is a proper name, the second NP is a definite NP in the subject position, and the two NPs have the same semantic class and are at most one sentence apart from each other.
We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported $$$$$ We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively.
We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported $$$$$ In current work, we are automating this feature selection process, which currently employs a fair amount of user discretion, e.g. to determine a precision cut-off.
We test on the ANC Test set (1291 instances) also used in Bergsma (2005) (highest resolution accuracy reported $$$$$ Exact string match is likely to be a better coreference predictor for proper names than it is for pronouns, for example.

Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.
Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. $$$$$ The features were not derived empirically from the corpus, but were based on common-sense knowledge and linguistic intuitions regarding coreference.
Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. $$$$$ Thanks to three anonymous reviewers for their comments and, in particular, for suggesting that we investigate data fragmentation issues.
Ng and Cardie (2002) expanded the feature set of Soon et al (2001) from 12 to 53 features. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.

Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.
Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. $$$$$ Next, eight features encode traditional linguistic (hard) constraints on coreference.
Ng and Cardie (2002) split this feature into several primitive features, depending on the type of noun phrases. $$$$$ (See the indented Learning Framework results in Table 2.)

Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ Thanks to three anonymous reviewers for their comments and, in particular, for suggesting that we investigate data fragmentation issues.
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ First, we propose and evaluate three extra-linguistic modifications to the machine learning framework, which together provide substantial and statistically significant gains in coreference resolution precision.
Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. $$$$$ Still other grammatical features encode general linguistic preferences either for or against coreference.

Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. $$$$$ We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively.
Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. $$$$$ We also plan to investigate previous work on common noun phrase interpretation (e.g.
Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. $$$$$ For example, the decision tree induced from the MUC-6 data set using the Soon feature set (Learning Framework results) has 16 leaves, each of which contains 1728 instances on average; the tree induced from the same data set using all of the 53 features, on the other hand, has 86 leaves with an average of 322 instances per leaf.
Although there is empirical evidence (e.g. Ng and Cardie 2002a, 2004) that coreference resolution might be further improved with proper anaphoricity information, its contribution is still somewhat disappointing and lacks systematic evaluation. $$$$$ Additional analysis is required to determine the reason for this.

Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. $$$$$ Relational features test whether some property P holds for the NP pair under consideration and indicate whether the NPs are COMPATIBLE or INCOMPATIBLE w.r.t.
Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. $$$$$ Overall, the learning framework and linguistic knowledge source modifications boost performance of Soon’s learning-based coreference resolution approach from an F-measure of 62.6 to 70.4, and from 60.4 to 63.4 for the MUC-6 and MUC-7 data sets, respectively.
Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. $$$$$ Results on the learning framework modifications are shown in Table 2 (third block of results).
Ng and Cardie (2002a) employed various domain-independent features in identifying anaphoric NPs and showed how such information can be incorporated into a coreference resolution system. $$$$$ Perhaps surprisingly, this was accomplished in a decidedly knowledge-lean manner — the learning algorithm has access to just 12 surface-level features.

In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ Results on the learning framework modifications are shown in Table 2 (third block of results).
In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.
In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ Best-first clustering.
In contrast to Ng (2005), Ng and Cardie (2002a) proposed a rule-induction system with rule pruning. $$$$$ Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.

It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). $$$$$ Thanks to three anonymous reviewers for their comments and, in particular, for suggesting that we investigate data fragmentation issues.
It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). $$$$$ We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.
It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). $$$$$ As a result, we modify the coreference clustering algorithm to select as the antecedent of NP✄ the NP with the highest coreference likelihood value from among preceding NPs with coreference class values above 0.5.
It is the same as most prior work in the literature, including Soon et al (2001) and Ng and Cardie (2002b). $$$$$ After training, the decision tree is used by a clustering algorithm to impose a partitioning on all NPs in the test texts, creating one cluster for each set of coreferent NPs.

In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. $$$$$ Second, in an attempt to understand whether incorporating additional knowledge can improve the performance of a corpus-based coreference resolution system, we expand the Soon et al. feature set from 12 features to an arguably deeper set of 53.
In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.
In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. $$$$$ Soon’s string match feature (SOON STR) tests whether the two NPs under consideration are the same string after removing determiners from each.
In the literature, besides the training instance extraction methods proposed by Soon et al (2001) and Ng and Cardie (2002b) as discussed in Section 2, McCarthy and Lehnert (1995) used all possible pairs of training instances. $$$$$ We investigate two methods to improve existing machine learning approaches to the problem of noun phrase coreference resolution.

Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. $$$$$ These and all subsequent results also incorporate the learning framework changes from Section 3.
Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. $$$$$ As a result, F-measure drops precipitously for both learning algorithms and both data sets.
Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjorkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem. $$$$$ Lappin and Leass (1994)) and NP coreference resolution (e.g.

At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). $$$$$ Although the use of similar knowledge sources has been explored in the context of both pronoun resolution (e.g.
At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). $$$$$ First, we find that performance drops significantly when using the full feature set, even though the learning algorithms investigated have built-in feature selection mechanisms.
At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). $$$$$ We also plan to investigate previous work on common noun phrase interpretation (e.g.
At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). $$$$$ We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC- 6 and MUC-7 coreference resolution data sets — F-measures of 70.4 and 63.4, respectively.

Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). $$$$$ For the proposed best-first clustering to be successful, however, a different method for training instance selection would be needed: rather than generate a positive training example for each anaphoric NP and its closest antecedent, we instead generate a positive training examples for its most confident antecedent.
Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). $$$$$ In particular, our tree makes use of many of the features that are not present in the original Soon feature set.
Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.
Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al (2003), McCallum and Wellner (2003)). $$$$$ Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.

Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). $$$$$ As a result, we next evaluate a version of the system that employs manual feature selection: for each classifier/data set combination, we discard features used primarily to induce low-precision rules for common noun resolution and re-train the coreference classifier using the reduced feature set.
Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). $$$$$ Aone and Bennett (1995), McCarthy and Lehnert (1995)).
Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.
Our general approach to source coreference resolution is inspired by the state-of-the-art performance of one such approach to coreference resolution, which relies on a rule learner and single-link clustering as described in Ng and Cardie (2002). $$$$$ *’d features are in the hand-selected feature set (see Section 4) for at least one classifier/data set combination.

We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. $$$$$ Lappin and Leass (1994)) and NP coreference resolution (e.g.
We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. $$$$$ Our baseline coreference system attempts to duplicate both the approach and the knowledge sources employed in Soon et al. (2001).
We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. $$$$$ This work was supported in part by DARPA TIDES contract N66001-00-C-8009, and NSF Grants 0081334 and 0074896.

We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002). $$$$$ We demonstrate empirically that the degradation in performance can be attributed, at least in part, to poor performance on common noun resolution.
We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002). $$$$$ When used in combination, the modifications consistently provide statistically significant gains in precision over the Baseline system 5This new method of training set creation slightly alters the class value distribution in the training data: for the MUC-6 corpus, there are now 27654 training instances of which 5.2% are positive; for the MUC-7 corpus, there are now 37870 training instances of which 4.2% are positive. without any loss in recall.6 As a result, we observe reasonable increases in F-measure for both classifiers and both data sets.
We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002). $$$$$ Currently, only tests for clausal subjects are made.
