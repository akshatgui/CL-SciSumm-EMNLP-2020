Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ We have performed various experiments to assess the effect of different alignment models, training schemes, and knowledge sources.
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ In the literature, it is often claimed that the refined alignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993) are not suitable for small corpora because of data sparseness problems.
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ The use of models that explicitly deal with the hierarchical structures of natural language is very promising (Wu 1996; Yamada and Knight 2001).
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ Then, we extend the alignment A iteratively by adding alignments (i, j) occurring only in the alignment A1 or in the alignment A2 if neither fj nor ei has an alignment in A, or if both of the following conditions hold: Obviously, the intersection of the two alignments yields an alignment consisting of only one-to-one alignments with a higher precision and a lower recall than either one separately.

The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ In this article, we have discussed in detail various statistical and heuristic word alignment models and described various modifications and extensions to models known in the literature.
The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ The union of the two alignments yields a higher recall and a lower precision of the combined alignment than either one separately.
The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.
The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ The elements of this intersection result from both Viterbi alignments and are therefore very reliable.

We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ By generating the reference alignment in this way, we obtain an alignment error rate of 0 percent when we compare the S alignments of every single annotator with the combined reference alignment.
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ The key results of these experiments are as follows: Further improvements in alignments are expected to be produced through the adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment models based on word groups rather than single words (Och, Tillmann, and Ney 1999).
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ The core idea is to enumerate only a small subset of good alignments in the E-step of the EM algorithm instead of enumerating all (I + 1)J alignments.

It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ This is expected to allow an easy integration of more dependencies, such as in a second-order alignment model, without running into the problem of the number of alignment parameters getting unmanageably large.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ To produce the reference alignment, we have used a refined annotation scheme that reduces the problems and ambiguities associated with the manual construction of a word alignment.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.

However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ All work for this paper was done at RWTH Aachen.
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ This is expected to allow an easy integration of more dependencies, such as in a second-order alignment model, without running into the problem of the number of alignment parameters getting unmanageably large.
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ .
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ We plan to develop structured models for the lexicon, alignment, and fertility probabilities using maximum-entropy models.

The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ There are O0! ways to order the O0 words produced by the empty word, and hence, the alignment model of the empty word is nondeficient.
The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ To solve this problem, we perform training in both translation directions (source to target, target to source).
The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ Therefore, lexical correspondences like that of the German compound word Zahnarzttermin with the English dentistâ€™s appointment cause problems, because a single source word must be mapped to two or more target words.

The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ The neighborhood N(a) of an alignment a is then defined as the set of all alignments that differ by one move or one swap from alignment a: For one step of the greedy search algorithm, we define the following hill-climbing operator (for Model 3), which yields for an alignment a the most probable alignment b(a) in the neighborhood N(a): Similarly, we define a hill-climbing operator for the other alignment models.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements.

Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra, Della Pietra, and Mercer 1993) have a significantly more complicated structure than the simple Models 1 and 2.
Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project (project number 30268) by the European Union.
Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ In this Appendix, we describe some methods for efficient training of fertility-based alignment models.

To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ The persons conducting the annotation are asked to specify alignments of two different kinds: an S (sure) alignment, for alignments that are unambiguous, and a P (possible) alignment, for ambiguous alignments.
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ To produce the reference alignment, we have used a refined annotation scheme that reduces the problems and ambiguities associated with the manual construction of a word alignment.
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ In principle, the sum over all (I+ 1)J alignments has to be calculated in the E-step.

The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ This can be accomplished by forming the intersection of the sure alignments (S = S1âˆ©S2) and the union of the possible alignments (P = P1âˆªP2), respectively.
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ All work for this paper was done at RWTH Aachen.
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ Brown, Della Pietra, Della Pietra, and Mercer (1993) describe a method for obtaining Pr(a' | e, f) incrementally from Pr(a  |e, f) if alignment a differs only by moves or swaps from alignment a'.
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ The relationship between the translation model and the alignment model is given by The alignment aJ 1 may contain alignments aj = 0 with the empty word e0 to account for source words that are not aligned with any target word.

All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ (Mutual errors of the two annotators A and B are the errors in the alignment of annotator A if we assume the alignment of annotator B as reference and the errors in the alignment of annotator B if we assume the alignment of annotator A as reference.)
All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ Furthermore, it will be important to verify the applicability of the statistical alignment models examined in this article to less similar language pairs such as ChineseEnglish and Japanese-English.
All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ Typically, the annotation is performed by two human annotators, producing sets S1, P1, S2, P2.

Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. $$$$$ These statistical models are compared with two heuristic models based on the Dice coefficient.
Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. $$$$$ In addition, this work has been partially supported by the National Science Foundation under grant no.
Then the procedure is quite standard: We run GIZA++ (Och and Ney, 2003) for bi-directional word alignment, and then obtain the lexical translation table and phrase table. $$$$$ As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.

Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). $$$$$ From this association score matrix, the word alignment is then obtained by applying suitable heuristics.
Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). $$$$$ In lexicography applications, we might be interested in alignments with a very high precision obtained by performing an alignment intersection.
Word alignment scores: source-target and target-source MGIZA++ (Gao and Vogel, 2008) force-alignment scores using IBM Model 4 (Och and Ney, 2003). $$$$$ In our experiments, we use Model 4 instead of Model 5, as it is significantly more efficient in training and obtains better results.

The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ This can be accomplished by forming the intersection of the sure alignments (S = S1âˆ©S2) and the union of the possible alignments (P = P1âˆªP2), respectively.
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ In the following, we present an annotation scheme for single-word-based alignments and a corresponding evaluation criterion.
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project (project number 30268) by the European Union.

GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ By generating the reference alignment in this way, we obtain an alignment error rate of 0 percent when we compare the S alignments of every single annotator with the combined reference alignment.
GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ , ei, ... , eI that have to be aligned.
GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ All work for this paper was done at RWTH Aachen.

In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ (Mutual errors of the two annotators A and B are the errors in the alignment of annotator A if we assume the alignment of annotator B as reference and the errors in the alignment of annotator B if we assume the alignment of annotator A as reference.)
In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ We present different methodsfor combining word alignments to perform a symmetrization of directed statistical alignment models.
In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ We use one matrix for the scores of a move aj := i: and an additional matrix for the scores of a swap of aj and ajï¿½: During the hill climbing, it is sufficient, after making a move or a swap, to update only those rows or columns in the matrix that are affected by the move or swap.

Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ It is important to keep this in mind in the evaluation of word alignment quality.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ This is expected to allow an easy integration of more dependencies, such as in a second-order alignment model, without running into the problem of the number of alignment parameters getting unmanageably large.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ In our experiments, we use Model 4 instead of Model 5, as it is significantly more efficient in training and obtains better results.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ The resulting alignment contains only one-to-one alignments and typically has a higher precision than the heuristic model defined in equation (7). tage of the heuristic models is their simplicity.

The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ The persons conducting the annotation are asked to specify alignments of two different kinds: an S (sure) alignment, for alignments that are unambiguous, and a P (possible) alignment, for ambiguous alignments.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ All work for this paper was done at RWTH Aachen.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ In addition, these models are the starting point for refined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999) or example-based translation systems (Brown 1997).

Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ This can be accomplished by forming the intersection of the sure alignments (S = S1âˆ©S2) and the union of the possible alignments (P = P1âˆªP2), respectively.
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ We have presented two methods for including a conventional bilingual dictionary in training and described heuristic symmetrization algorithms that combine alignments in both translation directions possible between two languages, producing an alignment with a higher precision, a higher recall, or an improved alignment error rate.
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.

It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ We observe that the alignment quality obtained with a specific model heavily depends on the training scheme that is used to bootstrap the model.
It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.
It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ All work for this paper was done at RWTH Aachen.
