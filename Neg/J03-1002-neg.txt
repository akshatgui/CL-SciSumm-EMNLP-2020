Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ As a result, only dictionary entries that indeed occur in the training corpus have a large effect in training.
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
Brown et al (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model. $$$$$ Let A1 = {(aj, j)  |aj > 01 and A2 = {(i, bi)  |bi > 01 denote the sets of alignments in the two Viterbi alignments.

The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ We present and compare various methods for computing word alignments using statistical or heuristic models.
The trial and test data had been manually aligned at the word level, noting particular pairs of words either as 'sure' or 'possible' alignments, as described by Och and Ney (2003). $$$$$ From these alignments, we finally generate a reference alignment that contains only those S connections on which both annotators agree and all P connections from both annotators.

We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ We use the slightly modified versions of Model 3 and Model 4 described in Section 3.2 and smooth the fertility and the alignment parameters.
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ For deficient models, however, as the amount of deficiency in the model is reduced, the likelihood increases.
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ Typically, the alignment models presented in the literature impose additional constraints on the alignment representation.
We report the performance of our different versions of Model 1 in terms of precision, recall, and alignment error rate (AER) as defined by Och and Ney (2003). $$$$$ As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.

It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ The different models are trained in succession on the same data; the final parameter values of a simpler model serve as the starting point for a more complex model.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ We have suggested measuring the quality of an alignment model using the quality of the Viterbi alignment compared to that achieved in a manually produced reference alignment.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ Therefore, we combine HMM and Model 4 in a log-linear way and call the resulting model Model 6: Here, the interpolation parameter α is employed to weigh Model 4 relative to the hidden Markov alignment model.
It is interesting to contrast our heuristic model with the heuristic models used by Och and Ney (2003) as baselines in their comparative study of alignment models. $$$$$ It is instructive to analyze the alignment quality obtained in the EM training of Model 1.

However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ From these alignments, we finally generate a reference alignment that contains only those S connections on which both annotators agree and all P connections from both annotators.
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ These definitions of precision, recall and the AER are based on the assumption that a recall error can occur only if an S alignment is not found and a precision error can occur only if the found alignment is not even P. The set of sentence pairs for which the manual alignment is produced is randomly selected from the training corpus.
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ Let A1 = {(aj, j)  |aj > 01 and A2 = {(i, bi)  |bi > 01 denote the sets of alignments in the two Viterbi alignments.
However, it is not clear that AER as defined by Och and Ney (2003) is always the appropriate way to evaluate the quality of the model, since the Viterbi word alignment that AER is based on is seldom used in applications of Model 1. $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project (project number 30268) by the European Union.

The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ In applications such as statistical machine translation (Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000), so an alignment union would probably be chosen.
The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ In practice, we observe that the resulting training is 10–20 times faster.
The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ The alignment mapping in such models consists of associations j → i = aj from source position j to target position i = aj.
The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al,2005). $$$$$ The straightforward algorithm given for performing the count collection has the disadvantage of requiring that all alignments in the neighborhood of alignment a be enumerated explicitly.

The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ We believe that this is due to the deficiency of Model 3 and Model 4.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ Every model has a specific set of free parameters.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ This means that the alignment mapping aJ1 must be injective for all word positions aj > 0.
The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al, 1977). $$$$$ For both tasks, we manually aligned a randomly chosen subset of the training corpus.

Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ We present different methodsfor combining word alignments to perform a symmetrization of directed statistical alignment models.
Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ This method results in a constant number of operations that is sufficient to calculate the score of a move or the score of a swap.
Our translation models were trained using GIZA++ (Och and Ney, 2003), which we modified as necessary for the morpheme-based experiments. $$$$$ These statistical models are compared with two heuristic models based on the Dice coefficient.

To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ In practice, we observe that the resulting training is 10–20 times faster.
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ By making this assumption, the model can learn that the longer words usually have a higher fertility than shorter words.
To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.

The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ .
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ We plan to develop structured models for the lexicon, alignment, and fertility probabilities using maximum-entropy models.
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ The software used to train the statistical alignment models described in this article is publicly available (Och 2000).
The main tools are Moses (Koehn et al 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. $$$$$ Using a larger set of alignments increases the training time for Model 4 and Model 5 significantly.

All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ In lexicography applications, we might be interested in alignments with a very high precision obtained by performing an alignment intersection.
All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ This small subset of alignments is the set of neighboring alignments of the best alignment that can be found by a greedy search algorithm.
All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ Assuming a binomial distribution for the number of words aligned with the empty word, we obtain the following distribution for B0: The free parameter p1 is associated with the number of words that are aligned with the empty word.
All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al 2011) for resampling and significance testing. $$$$$ We have suggested measuring the quality of an alignment model using the quality of the Viterbi alignment compared to that achieved in a manually produced reference alignment.

Then the procedure is quite standard $$$$$ In lexicography applications, we might be interested in alignments with a very high precision obtained by performing an alignment intersection.
Then the procedure is quite standard $$$$$ We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.
Then the procedure is quite standard $$$$$ To produce the reference alignment, we have used a refined annotation scheme that reduces the problems and ambiguities associated with the manual construction of a word alignment.
Then the procedure is quite standard $$$$$ For example, when performing a move aj := i, it is necessary to Similar updates have to be performed after a swap.

Word alignment scores $$$$$ It is instructive to analyze the alignment quality obtained in the EM training of Model 1.
Word alignment scores $$$$$ By restricting in this way the number of matrix entries that need to be updated, it is possible to reduce the number of operations in hill climbing by about one order of magnitude.
Word alignment scores $$$$$ Using a dynamic programming approach, it is possible to obtain the Viterbi alignment for the HMM with a complexity of O(I2 ·J) (Vogel, Ney, and Tillmann 1996).
Word alignment scores $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project (project number 30268) by the European Union.

The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ All work for this paper was done at RWTH Aachen.
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project (project number 30268) by the European Union.
The Parallel data is aligned in both directions using the MGIZA++ (Gao and Vogel, 2008) implementation of IBM Model 4 and symmetrized with the grow-diag-final heuristic (Och and Ney, 2003). $$$$$ The key results of these experiments are as follows: Further improvements in alignments are expected to be produced through the adoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment models based on word groups rather than single words (Och, Tillmann, and Ney 1999).

GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.
GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ Tables 4 and 5 show that the statistical alignment models significantly outperform the heuristic Dice coefficient and the heuristic Dice coefficient with competitive linking (Dice+C).
GIZA++ (Och and Ney, 2003) is the most widely used implementation of IBM models and HMM (Vogel et al, 1996) where EM algorithm is employed to estimate the model parameters. $$$$$ In the following, we present an annotation scheme for single-word-based alignments and a corresponding evaluation criterion.

In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ Although there exist simple polynomial algorithms for the baseline Models 1 and 2, we are unaware of any efficient algorithm for computing the Viterbi alignment for the fertility-based alignment models.
In a first step, the corpus is aligned at the word level, by using alignment tools such as Giza++ (Och and Ney, 2003) and some symmetrisation heuristics; phrases are then extracted by other heuristics (Koehn et al, 2003) and assigned numerical weights. $$$$$ Hence, the EM algorithm can increase likelihood by simply aligning more and more words with the empty word.3 Therefore, we modify Models 3 and 4 slightly, such that the empty word also has a deficient alignment model.

Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ We have suggested measuring the quality of an alignment model using the quality of the Viterbi alignment compared to that achieved in a manually produced reference alignment.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ We present different methodsfor combining word alignments to perform a symmetrization of directed statistical alignment models.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ In this Appendix, we describe some methods for efficient training of fertility-based alignment models.
Our method is to use human alignment as the oracle of supervised learning and compare its performance against that of GIZA++ (Och and Ney 2003), a state of the art unsupervised aligner. $$$$$ We have performed various experiments to assess the effect of different alignment models, training schemes, and knowledge sources.

The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ Analyzing the training program reveals that most of the time is spent on the computation of the costs of moves and swaps.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ These definitions of precision, recall and the AER are based on the assumption that a recall error can occur only if an S alignment is not found and a precision error can occur only if the found alignment is not even P. The set of sentence pairs for which the manual alignment is produced is randomly selected from the training corpus.
The study of the relation between alignment quality and MT performance can be traced as far as to Och and Ney, 2003. $$$$$ An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.

Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ On the Verbmobil task, the best alignment error rate is obtained in the second iteration.
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ 2.3.3 Overview of Models.
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.
Based on the GIZA++ package (Och and Ney, 2003), we implemented a MWA tool for collocation detection. $$$$$ Figure 2 shows an example of a manually aligned sentence with S and P labels.

It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ We plan to develop structured models for the lexicon, alignment, and fertility probabilities using maximum-entropy models.
It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ The elements of this intersection result from both Viterbi alignments and are therefore very reliable.
It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ This is one order of magnitude faster than the straightforward algorithm described above.
It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). $$$$$ We have suggested measuring the quality of an alignment model using the quality of the Viterbi alignment compared to that achieved in a manually produced reference alignment.
