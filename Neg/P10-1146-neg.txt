Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. $$$$$ For Chinese, we first tried increasing the distortion limit from 10 words to 20.
Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and DARPA contract HR0011-09-1-0028.
Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. $$$$$ We can strike a compromise by continuing to allow SAMT-style complex categories, but committing to a single analysis by requiring all phrases to nest.
Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. $$$$$ Though exact tree-to-tree translation tends to hamper translation quality by imposing too many constraints during both grammar extraction and decoding, we have shown that using both source and target syntax improves translation accuracy when the model is given the opportunity to learn from data how strongly to apply syntactic constraints.

Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions. $$$$$ But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language.
Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions. $$$$$ This augmented model is able to learn from data whether to rely on syntax or not, or to revert back to monotone phrase-based translation.
Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions. $$$$$ This augmented model is able to learn from data whether to rely on syntax or not, or to revert back to monotone phrase-based translation.

 $$$$$ In experiments on Chinese-English and ArabicEnglish translation, we find that when both source and target syntax are made available to the model in an unobtrusive way, the model chooses to build structures that are more syntactically well-formed and yield significantly better translations than a nonsyntactic hierarchical phrase-based model.
 $$$$$ These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language.
 $$$$$ Indeed, we have found that the model learns on its own to choose syntactically richer and more wellformed structures, demonstrating that source- and target-side syntax can be used together profitably as long as they are not allowed to overconstrain the translation model.

SAMT extension with source and target-side syntax described by Chiang (2010). $$$$$ Table 3 shows the scores on our development sets and test sets, which are about 3000 and 2000 sentences, respectively, of newswire drawn from NIST MT evaluation data and GALE development data and disjoint from the tuning data.
SAMT extension with source and target-side syntax described by Chiang (2010). $$$$$ Indeed, we have found that the model learns on its own to choose syntactically richer and more wellformed structures, demonstrating that source- and target-side syntax can be used together profitably as long as they are not allowed to overconstrain the translation model.
SAMT extension with source and target-side syntax described by Chiang (2010). $$$$$ Some syntactic categories are similar enough to be considered compatible: for example, if a rule rooted in VBD (pasttense verb) could substitute into a site labeled VBZ (present-tense verb), it might still generate correct output.
SAMT extension with source and target-side syntax described by Chiang (2010). $$$$$ We optimized feature weights on 90% of this and held out the other 10% to determine when to stop.

In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set. $$$$$ The label “entity” stands for handwritten rules for named entities and numbers.
In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set. $$$$$ However, it is not possible to represent this restructuring with a single tree (see Figure 4).
In future work, the problem could be addressed by reconsidering our naming scheme for virtual nodes, by allowing fuzzy matching of labels at translation time (Chiang, 2010), or by other techniques aimed at reducing the size of the overall nonterminal set. $$$$$ The systems were trained using MIRA (Crammer and Singer, 2003; Chiang et al., 2009) on a tuning set of about 3000 sentences of newswire from NIST MT evaluation data and GALE development data, disjoint from the training data.

Although MT systems that employ syntactic or hierarchical information have recently shown improvements over phrase-based approaches (Chiang, 2010), our initial investigation with syntactically driven approaches showed poorer performance on the text simplification task and were less robust to noise in the training data. $$$$$ These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language.

The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ Thanks to Steve DeNeefe, Adam Lopez, Jonathan May, Miles Osborne, Adam Pauls, Richard Schwartz, and the anonymous reviewers for their valuable help.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ This suggests that the strength of fuzzy tree-to-tree extraction lies in its ability to break up flat structures and to reconcile the source and target trees with each other, rather than multiple restructurings of the training trees.
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) methods have also been the subject of experimentation, as well as other formalisms such as Dependency Trees (Shen et al, 2008). $$$$$ The label “entity” stands for handwritten rules for named entities and numbers. rewrites in Chinese-English translation between string-to-string (s-to-s) and fuzzy tree-to-tree (tto-t) grammars.

The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). $$$$$ We therefore propose to move these constraints from the formalism into the model, implemented as features in the hierarchical phrasebased model Hiero (Chiang, 2005).
The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). $$$$$ This suggests that the strength of fuzzy tree-to-tree extraction lies in its ability to break up flat structures and to reconcile the source and target trees with each other, rather than multiple restructurings of the training trees.
The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). $$$$$ We therefore propose to move these constraints from the formalism into the model, implemented as features in the hierarchical phrasebased model Hiero (Chiang, 2005).
The reported results show that while utilizing linguistic information helps, the coverage is more important (Chiang, 2010). $$$$$ In other words, exact tree-to-tree extraction commits to a single structural analysis but fuzzy tree-to-tree extraction pursues many restructured analyses at once.

Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match. $$$$$ “Loose source/target” is the maximum number of unaligned source/target words at the endpoints of a phrase. limit, above which the glue rule must be used.
Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match. $$$$$ Several changes appear to have to do with definiteness of NPs: on the English side, adding the syntax features encourages matching substitutions of type DT \ NP-C (anarthrous NP), but discourages DT \ NP-C and NN from substituting into NP-C and vice versa.
Chiang (2010) extended SAMT-style labels to both source and target-side parses, also introducing a mechanism by which SCFG rules may apply at runtime even if their labels do not match. $$$$$ But in some cases we may want to soften the matching constraint itself.

However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. $$$$$ We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.
However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. $$$$$ One confounding factor in such a comparison is that some methods generate many new syntactic categories, making it more difficult to satisfy syntactic constraints at decoding time.
However, as discussed by Chiang (2010), while tree-to-tree translation is indeed promising in theory, in practice it usually ends up over-constrained. $$$$$ First, Table 4 shows that the system using the tree-to-tree grammar used the glue rule much less and performed more matching substitutions.

Inspired by Chiang (2010), we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). $$$$$ Because this heuristic produces a set of nesting phrases, we can represent them all in a single restructured tree.
Inspired by Chiang (2010), we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). $$$$$ But with fuzzy tree-to-tree extraction, we obtained an improvement of +0.6 on both Chinese-English sets, and +0.7/+0.8 on the ArabicEnglish sets.
Inspired by Chiang (2010), we adopt a fuzzy way to label every source string with the complex syntactic categories of SAMT (Zollmann and Venugopal, 2006). $$$$$ This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and DARPA contract HR0011-09-1-0028.

However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive. $$$$$ Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years.
However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive. $$$$$ Thanks to Steve DeNeefe, Adam Lopez, Jonathan May, Miles Osborne, Adam Pauls, Richard Schwartz, and the anonymous reviewers for their valuable help.
However, as noted by Lavie et al. (2008), Liu et al. (2009), and Chiang (2010), the integration of syntactic information on both sides tends to decrease translation quality because the systems be come too restrictive. $$$$$ We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for significant improvements in translation accuracy.

Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). $$$$$ Though exact tree-to-tree translation tends to hamper translation quality by imposing too many constraints during both grammar extraction and decoding, we have shown that using both source and target syntax improves translation accuracy when the model is given the opportunity to learn from data how strongly to apply syntactic constraints.
Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). $$$$$ Table 3 shows the scores on our development sets and test sets, which are about 3000 and 2000 sentences, respectively, of newswire drawn from NIST MT evaluation data and GALE development data and disjoint from the tuning data.
Fuzzy constituency constraints can solve this problem with a combination of product categories and slash categories (Chiang, 2010). $$$$$ But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009).

Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). $$$$$ Though exact tree-to-tree translation tends to hamper translation quality by imposing too many constraints during both grammar extraction and decoding, we have shown that using both source and target syntax improves translation accuracy when the model is given the opportunity to learn from data how strongly to apply syntactic constraints.
Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). $$$$$ Tables 5 and 6 show how the new syntax features affected particular substitutions.
Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). $$$$$ The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006).
Using both source and target syntax, but relaxing on rule extraction and substitution enables HPBMT to produce more well-formed and syntactically richer derivations (Chiang, 2010). $$$$$ We trained two 5-gram language models: one on the combined English halves of the bitexts, and one on two billion words of English.

Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical. $$$$$ All rewrites occurring more than 1% of the time in either system are shown.
Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical. $$$$$ Indeed, we have found that the model learns on its own to choose syntactically richer and more wellformed structures, demonstrating that source- and target-side syntax can be used together profitably as long as they are not allowed to overconstrain the translation model.

Tellingly, in the entire proceedings of ACL 2010 (Hajic et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT. $$$$$ The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006).
Tellingly, in the entire proceedings of ACL 2010 (Hajic et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT. $$$$$ The label “entity” stands for handwritten rules for named entities and numbers.
Tellingly, in the entire proceedings of ACL 2010 (Hajic et al., 2010), only one paper describing a statistical MT system cited the use of MIRA for tuning (Chiang, 2010), while 15 used MERT. $$$$$ All rewrites occurring more than 1% of the time in either system are shown, plus a few more of interest.
