Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. $$$$$ First, unsupervised segmentation approaches offer a very compelling alternative to the manually crafted segmentation lattices that we created.
Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. $$$$$ The training, development, and test data for German-English and Hungarian-English systems used were distributed as part of the 2009 EACL Workshop on Machine Translation,4 and the Turkish-English data corresponds to the training and test sets used in the work of Oflazer and Durgar ElKahlout (2007).
Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. $$$$$ Special thanks to Kemal Oflazar and Reyyan Yeniterzi of Sabancı University for providing the Turkish-English corpus and to Philip Resnik, Adam Lopez, Trevor Cohn, and especially Phil Blunsom for their helpful suggestions.
Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. $$$$$ Figure 3 shows the effect of manipulating the density parameter on the precision and recall of the German lattices.

To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. $$$$$ In contrast, wiederaufnahme can only be translated correctly using the unsegmented form, even though in German the meaning of the full form is a composition of the meaning of the individual morphemes.1 It should be noted that phrase-based models can translate multiple words as a unit, and therefore capture non-compositional meaning.
To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. $$$$$ Thus, we wish to minimize the following objective (which can be computed using the forward algorithm over the unpruned hypothesis lattices): To compute these values, the first expectation is computed using forward-backward inference over the full lattice.
To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. $$$$$ Since we permit these characters to be deleted, then an edge where they are deleted will have fewer characters than the coverage indicated by the edge’s starting and ending vertices.
To construct a segmentation dictionary, I used the 1-best segmentations from a supervised MaxEnt compound splitter (Dyer, 2009) run on all token types in bitext. $$$$$ We show that these lattices significantly improve translation quality when translating into English from three languages exhibiting productive compounding: German, Turkish, and Hungarian.

Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). $$$$$ The value of the denominator can be computed using the forward algorithm.
Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). $$$$$ For the experiments in this paper, we generated a development and test set by randomly choosing 19 German newspaper articles, identifying all words greater than 6 characters is length, and segmenting each word so that the resulting units could be translated compositionally into English.
Despite their simplicity, unigram weights have been shown as an effective feature in segmentation models (Dyer, 2009). $$$$$ Word alignment was carried out by running Giza++ implementation of IBM Model 4 initialized with 5 iterations of Model 1, 5 of the HMM aligner, and 3 iterations of Model 4 (Och and Ney, 2003) in both directions and then symmetrizing using the grow-diag-final-and heuristic (Koehn et al., 2003).

The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). $$$$$ A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model.
The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). $$$$$ Furthermore, since these parameters were chosen to have valid interpretation across a variety of languages, we find that the weights estimated for one apply quite well to another.
The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). $$$$$ The cost of any path from the start to the goal vertex will be equal to the numerator in equation (4).

These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). $$$$$ Thus, with virtually no cost, this model can be used with a variety of diverse languages.
These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). $$$$$ Figure 3 shows the effect of manipulating the density parameter on the precision and recall of the German lattices.
These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). $$$$$ But into what units should a compound word be segmented?
These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). $$$$$ The paper is structured as follows.

We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ While these results are already quite satisfying, there are a number of compelling extensions to this work that we intend to explore in the future.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ However, much of this work has relied on multiple segmenters that perform differently on the same input to generate sufficiently diverse source segmentation lattices.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ The WER on the held-out test set for a system tuned using MERT is 9.9%, compared to 11.1% for maximum likelihood training.
We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al, 2010), which learns word segmentation lattices from raw text in an unsupervised manner. $$$$$ For example, one class of error we frequently observe is that the one-best segmenter splits an OOV proper name into two pieces when a portion of the name corresponds to a known word in the source language (e.g. tom tancredo→tom tan credo which is then translated as tom tan belief).6 Figure 4 shows the effect of manipulating the density parameter (cf.

Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. $$$$$ Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al., 2005) or hand-crafted rules (DeNeefe et al., 2008).
Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. $$$$$ The approaches suggested thus far have been mostly of a heuristic nature tailored to Chinese-English translation (Bai et al., 2008; Ma et al., 2007).
Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. $$$$$ In this work, we describe a maximum entropy model of compound word splitting that relies on a few general features that can be used to generate segmentation lattices for most languages with productive compounding.
Then Dyer (2009) employ a single Maximum Entropy segmentation model to generate more diverse lattice, they test their model on the hierarchical phrase-based system. $$$$$ We thus desire lattices containing as little oversegmentation as possible.

Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). $$$$$ Using a model optimized for German translation, we present results showing significant improvements in translation quality in German-English, Hungarian-English, and Turkish-English translation over state-ofthe-art baselines.
Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). $$$$$ The intuition behind using lattices in both approaches is to avoid the error propagation effects that are found when a one-best guess is used.
Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). $$$$$ A number of researchers have demonstrated the value of using lattices to encode segmentation alternatives as input to a machine translation system (Dyer et al., 2008; DeNeefe et al., 2008; Xu et al., 2004), but this is the first work to do so using a single segmentation model.
Since German is a language that makes productive use of "closed" compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). $$$$$ This research was supported by the Army Research Laboratory.

These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). $$$$$ Section 3.2) ranging from α = 0 to α = 5.
These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). $$$$$ For each language pair, the corpus was aligned twice, once in its non-segmented variant and once using the single-best segmentation variant.
These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). $$$$$ Section 3.2) ranging from α = 0 to α = 5.
These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). $$$$$ In this paper, we have presented a maximum entropy model for compound word segmentation and used it to generate segmentation lattices for input into a statistical machine translation system.

All data was tokenized and lowercased; German compounds were split (Dyer, 2009). $$$$$ Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008).
All data was tokenized and lowercased; German compounds were split (Dyer, 2009). $$$$$ However, this is not a requirement.
All data was tokenized and lowercased; German compounds were split (Dyer, 2009). $$$$$ Previous approaches to generating segmentation lattices have been quite laborious, relying either on the existence of multiple segmenters (Dyer et al., 2008; Xu et al., 2005) or hand-crafted rules (DeNeefe et al., 2008).
All data was tokenized and lowercased; German compounds were split (Dyer, 2009). $$$$$ By carrying a certain amount of uncertainty forward in the processing pipeline, information contained in the translation models can be leveraged to help resolve the upstream ambiguity.

Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011). $$$$$ By carrying a certain amount of uncertainty forward in the processing pipeline, information contained in the translation models can be leveraged to help resolve the upstream ambiguity.
Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al, 2011). $$$$$ The intuition behind using lattices in both approaches is to avoid the error propagation effects that are found when a one-best guess is used.

An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. $$$$$ Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of the sponsors.
An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. $$$$$ Aside from improving the vocabulary coverage of machine translation systems (Koehn et al., 2008; Yang and Kirchhoff, 2006; Habash and Sadat, 2006), compound word segmentation (also referred to as decompounding) has been shown to be helpful in a variety of NLP tasks including mono- and crosslingual IR (Airio, 2006) and speech recognition (Hessen and Jong, 2003).
An extended version of the lattice approach that does not require the use (and existence) of monolingual segmentation tools was proposed in (Dyer, 2009) where a maximum entropy model is used to assign probabilities to the segmentations of an input word to generate diverse segmentation lattices from a single automatically learned model. $$$$$ Additionally, if the unsegmented form of the word was removed from the lattice during pruning, it was restored to the lattice with zero weight.

Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. $$$$$ We have now have a concept of a “gold standard” segmentation lattice for translation: it should contain all linguistically motivated segmentations that also correspond to plausible word-for-word translations into English.
Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. $$$$$ However, while these words are structurally quite similar, translating them into English would seem to require different amounts of segmentation.
Dyer (2009) applied this to German using a lattice encoding different segmentations of German words. $$$$$ In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word.

Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. $$$$$ Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of the sponsors.
Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. $$$$$ While these results are already quite satisfying, there are a number of compelling extensions to this work that we intend to explore in the future.
Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. $$$$$ Special thanks to Kemal Oflazar and Reyyan Yeniterzi of Sabancı University for providing the Turkish-English corpus and to Philip Resnik, Adam Lopez, Trevor Cohn, and especially Phil Blunsom for their helpful suggestions.
Dyer (2009) applies a maximum entropy model of compound splitting to generate segmentation lattices that serve as input to a translation system. $$$$$ However, using a strategy of “over segmentation” and relying on phrase models to learn the non-compositional translations has been shown to degrade translation quality significantly on several tasks (Xu et al., 2004; Habash and Sadat, 2006).

Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input. $$$$$ Second, incorporating target language information into a segmentation model holds considerable promise for inducing more effective translation models that perform especially well for segmentation lattice inputs.
Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input. $$$$$ In most of our experiments, s will be identical to the substring of w that the edge is designated to cover.
Dyer (2009) introduces a maximum entropy model for compound word splitting, which he uses to create word lattices for translation input. $$$$$ The paper is structured as follows.
