 $$$$$ We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.
 $$$$$ One weakness of all recent weighted grammar induction work—including Klein and Manning (2004), Smith and Eisner (2005b), and the present paper—is a sensitivity to hyperparameters, including smoothing values, choice of N (for CE), and annealing schedules—not to mention initialization.
 $$$$$ We seek here to capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements in accuracy of the learned model (§3).
 $$$$$ Further, the new models are not getting better by merely reversing the direction of links made by EM; undirected accuracy also improved significantly under a sign test (p < 10−6), across all six languages.

(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. $$$$$ Now the total probability of an n-length sentence x, marginalizing over its hidden structures, sums up not only over trees, but over segmentations of x.
(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. $$$$$ The parameters O are the conditional distributions pstop and pchild.
(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. $$$$$ When Q = 0, every value of T is equally likely.
(Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. $$$$$ We found that the number of corrected attachments always outnumbered the number of new errors by a factor of two or more.

For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. $$$$$ At first glance broadening the hypothesis space to entertain all 2n−1 possible segmentations may seem expensive.
For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. $$$$$ By the last iteration the objective is the same as in EM, but the annealed search process has acted like a good initializer.
For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. $$$$$ Neighborhoods that can be expressed as finitestate lattices built from xi were shown to give significant improvements in dependency parser quality over EM.
For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. $$$$$ While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover.

We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. $$$$$ At first glance broadening the hypothesis space to entertain all 2n−1 possible segmentations may seem expensive.
We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. $$$$$ first show how a structural bias improve the accuracy of state-of-the-art dependency grammar induction models trained by EM from unannotated examples (Klein and Manning, 2004).
We follow the idea of annealing proposed in Rose et al (1990) and Smith and Eisner (2006) for the ? by gradually loosening hard constraints on ? as the variational EM algorithm proceeds. $$$$$ Then δ is iteratively increased and training repeated, using the last learned model to initialize.

This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). $$$$$ In this work, instead of imposing constraints on the entropy of the model, we manipulate bias toward local hypotheses.
This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). $$$$$ We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.
This is a strict model reminiscent of the successful application of structural bias to grammar induction (Smith and Eisner, 2006). $$$$$ At each step the optimization task becomes more difficult, but the initializer is given by the previous step and, in practice, tends to be close to a good local maximum of the more difficult objective.

These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). $$$$$ Our method, is a general technique with broad applicability to hidden-structure discovery problems.
These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). $$$$$ A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979).
These include the constituent-context model (CCM) (Klein and Manning, 2002), its extension using a dependency model (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar-based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of (Seginer, 2007). $$$$$ An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection.

Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. $$$$$ EM’s mediocre performance (Table 1) reflects two problems.
Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. $$$$$ Supervised model selection, which uses a small annotated development set, performs almost as well as the oracle, but unsupervised model selection, which selects the model that maximizes likelihood on an unannotated development set, is often much worse.
Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. $$$$$ We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.
Smith and Eisner (2006) propose structural annealing (SA), in which a strong bias for local dependency attachments is enforced early in learning, and then gradually relaxed. $$$$$ We found that the number of corrected attachments always outnumbered the number of new errors by a factor of two or more.

Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). $$$$$ A related way to focus on local structure early in learning is to broaden the set of hypotheses to include partial parse structures.
Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). $$$$$ We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.
Finally, note that structural annealing (Smith and Eisner, 2006) provides 66.7% accuracy on WSJ10 when choosing the best performing annealing schedule (Smith, 2006). $$$$$ In unsupervised learning, DA iteratively re-estimates parameters like EM, but begins by requiring that the entropy of the posterior pp(y  |x) be maximal, then gradually relaxes this entropy constraint.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ All this means is that O (specifically, pstop and pchild in Eq.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U)DOP based models (Bod, 2006a; Bod, 2006b), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) that we use in this work. $$$$$ In fact the dynamic programming computation is almost the same as summing or maximizing over connected dependency trees.

In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). $$$$$ In structural annealing, the final objective would be the same as EM if our final δ, δf = 0, but we found that annealing farther (δf > 0) works much better.4 Experiment: Annealing δ.
In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). $$$$$ We found that the number of corrected attachments always outnumbered the number of new errors by a factor of two or more.
In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). $$$$$ For (language, N) pairs where CE was effective, we trained models using CE with a fixedQ segmentation model.
In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). $$$$$ An obstacle for unsupervised learning in general is the need for automatic, efficient methods for model selection.

These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ By injecting a bias (S 7� 0 or Q > −oc) among tree hypotheses, however, we have gone beyond the CE framework.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ The probability P(yi  |xi) of generating this subtree, given its head word xi, is defined recursively: where firsty(j) is a predicate defined to be true iff xj is the closest child (on either side) to its parent xi.
These include CCM (Klein and Manning, 2002), the DMV and DMV+CCM models (Klein and Manning, 2004), (U) DOP based models (Bod, 2006a; Bod, 2006b; Bod, 2007), an exemplar based approach (Dennis, 2005), guiding EM using contrastive estimation (Smith and Eisner, 2006), and the incremental parser of Seginer (2007) which we use here. $$$$$ We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.

Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). $$$$$ In supervised dependency parsing, Eisner and Smith (2005) showed that imposing a hard constraint on the whole structure— specifically that each non-$ dependency arc cross fewer than k words—can give guaranteed O(nk2) runtime with little to no loss in accuracy (for simple models).
Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). $$$$$ 1) is now a set of nonnegative weights rather than probabilities.
Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). $$$$$ Further, the new models are not getting better by merely reversing the direction of links made by EM; undirected accuracy also improved significantly under a sign test (p < 10−6), across all six languages.
Analogously, Baby Steps induces an early structural locality bias (Smith and Eisner, 2006), then relaxes it, as if annealing (Smith and Eisner, 2004). $$$$$ We relate this approach to contrastive estimation (Smith and Eisner, 2005a), apply the latter to grammar induction in six languages, and show that our new approach improves accuracy by 1–17% (absolute) over CE (and 8–30% over EM), achieving to our knowledge the best results on this to date.

Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. $$$$$ We call this structural annealing, since we are varying the strength of a soft constraint (bias) on structural hypotheses.
Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. $$$$$ In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).
Smith and Eisner (2006) used a structural locality bias, experimenting on five languages. $$$$$ Second, the likelihood surface is not globally concave, and learners such as the EM algorithm can get trapped on local maxima (Charniak, 1993).

Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. $$$$$ Time progresses from left to right.
Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. $$$$$ 1) is now a set of nonnegative weights rather than probabilities.
Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. $$$$$ Performance with unsupervised and supervised model selection across different λ values in add-λ smoothing and three initializers O(0) is reported in Table 1.
Notable examples are (Clark, 2003) for unsupervised POS tagging and (Smith and Eisner, 2006) for unsupervised dependency parsing. $$$$$ After EM training, β was diminished by 10;this was repeated down to a value of βf = −3.

Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length $$$$$ Note that structural annealing does not always outperform fixed-δ training (English and Portuguese).
Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length $$$$$ In structural annealing, the final objective would be the same as EM if our final δ, δf = 0, but we found that annealing farther (δf > 0) works much better.4 Experiment: Annealing δ.
Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length $$$$$ Next, by annealing the free parameter that controls this bias, we achieve further improvements.
Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length $$$$$ We have presented a new unsupervised parameter estimation method, structural annealing, for learning hidden structure that biases toward simplicity and gradually weakens (anneals) the bias over time.

Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). $$$$$ The EM baseline corresponds to S = 0.
Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). $$$$$ This method, then, is not robust to the choice of λ, β0, or βf, nor does it always do as well as annealing δ, although considerable gains are possible; see the fifth column of Table 3.
Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). $$$$$ A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2,..., n} --, 211,2,...,n1) that map each word to its sets of left and right dependents, respectively.
Our model is thus a form of quasi-synchronous grammar (QG) (Smith and Eisner, 2006a). $$$$$ Our method, is a general technique with broad applicability to hidden-structure discovery problems.

These alignment classes are called configurations (Smith and Eisner, 2006a, and following). $$$$$ Following common practice, we always replace words by part-ofspeech (POS) tags before training or testing.
These alignment classes are called configurations (Smith and Eisner, 2006a, and following). $$$$$ For (language, N) pairs where CE was effective, we trained models using CE with a fixedQ segmentation model.
These alignment classes are called configurations (Smith and Eisner, 2006a, and following). $$$$$ We compared errors made by the selected EM condition with the best overall condition, for each language.
These alignment classes are called configurations (Smith and Eisner, 2006a, and following). $$$$$ We compared errors made by the selected EM condition with the best overall condition, for each language.

Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a). $$$$$ We then describe an alternative kind of structural bias, toward “broken” hypotheses consisting of partial structures over segmented sentences, and show a similar pattern of improvement.
Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a). $$$$$ We then explore how gradually changing S over time affects learning (§4): we start out with a strong preference for short dependencies, then relax the preference.
Thus, our generative model is a quasi-synchronous grammar, exactly as in (Smith and Eisner, 2006a). $$$$$ 1.

 $$$$$ While the most common corrections were to nouns, these account for only 25–41% of corrections, indicating that corrections are not “all of the same kind.” Finally, since more than half of corrections in every language involved reattachment to a noun or a verb (content word), we believe the improved models to be getting closer than EM to the deeper semantic relations between words that, ideally, syntactic models should uncover.
 $$$$$ We applied the technique to weighted dependency grammar induction and achieved a significant gain in accuracy over EM and CE, raising the state-of-the-art across six languages from 42– 54% to 58–73% accuracy.

Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. $$$$$ Segmented analyses are intermediate representations that may be helpful for a learner to use to formulate notions of probable local structure, without committing to full trees.5 We only allow unobserved breaks, never positing a hard segmentation of the training sentences.
Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. $$$$$ First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure.
Smith and Eisner (2005) use contrastive estimation instead of EM, while Smith and Eisner (2006) use structural annealing which penalizes long-distance dependencies initially, gradually weakening the penalty during training. $$$$$ Each has a nonzero probability.
