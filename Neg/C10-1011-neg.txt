In this paper, we use the following baseline parsers $$$$$ extr.-features-&-calc-arrays(i,??w ) ; tee,k tsp,k; yp?
In this paper, we use the following baseline parsers $$$$$ For a transition based parser, Gesmundo et al (2009) reported run times between 2.2 days for English and 4.7 days forCzech for the joint training of syntactic and se mantic dependencies.
In this paper, we use the following baseline parsers $$$$$ Parsing and training times can be improved by methods that maintain the accuracy level, or methods that trade accuracy against better parsing times.
In this paper, we use the following baseline parsers $$$$$ The second order parsing algorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling.

Parser $$$$$ This has lead to a higher accuracy.
Parser $$$$$ Parsing algo rithms can use several cores.

 $$$$$ For the parallel algorithms, Table 5 shows the elapsed times depend on the number of 94 # Standard Features # Linear Features Linear G. Features Sibling Features 1 l,hf ,hp,d(h,d) 14 l,hp,h+1p,dp,d(h,d) 44 l,gp,dp,d+1p,d(h,d) 99 l,sl,hp,d(h,d)?r(h,d) 2 l,hf ,d(h,d) 15 l,hp,d-1p,dp,d(h,d) 45 l,gp,dp,d-1p,d(h,d) 100 l,sl,dp,d(h,d)?r(h,d) 3 l,hp,d(h,d) 16 l,hp,dp,d+1p,d(h,d) 46 l,gp,g+1p,d-1p,dp,d(h,d) 101 l,hl,dp,d(h,d)?r(h,d) 4 l,df ,dp,d(h,d) 17 l,hp,h+1p,d-1p,dp,d(h,d) 47 l,g-1p,gp,d-1p,dp,d(h,d) 102 l,dl,sp,d(h,d)?r(h,d) 5 l,hp,d(h,d) 18 l,h-1p,h+1p,d-1p,dp,d(h,d) 48 l,gp,g+1p,dp,d+1p,d(h,d) 75 l,?dm,?sm,d(h,d) 6 l,dp,d(h,d) 19 l,hp,h+1p,dp,d+1p,d(h,d) 49 l,g-1p,gp,dp,d+1p,d(h,d) 76 l,?hm,?sm,d(h,s) 7 l,hf ,hp,df ,dp,d(h,d) 20 l,h-1p,hp,dp,d-1p,d(h,d) 50 l,gp,g+1p,hp,d(h,d) Linear S. Features 8 l,hp,df ,dp,d(h,d) Grandchild Features 51 l,gp,g-1p,hp,d(h,d) 58 l,sp,s+1p,hp,d(h,d) 9 l,hf ,df ,dp,d(h,d) 21 l,hp,dp,gp,d(h,d,g) 52 l,gp,hp,h+1p,d(h,d) 59 l,sp,s-1p,hp,d(h,d) 10 l,hf ,hp,df ,d(h,d) 22 l,hp,gp,d(h,d,g) 53 l,gp,hp,h-1p,d(h,d) 60 l,sp,hp,h+1p,d(h,d) 11 l,hf ,df ,hp,d(h,d) 23 l,dp,gp,d(h,d,g) 54 l,gp,g+1p,h-1p,hp,d(h,d) 61 l,sp,hp,h-1p,d(h,d) 12 l,hf ,df ,d(h,d) 24 l,hf ,gf ,d(h,d,g) 55 l,g-1p,gp,h-1p,hp,d(h,d) 62 l,sp,s+1p,h-1p,d(h,d) 13 l,hp,dp,d(h,d) 25 l,df ,gf ,d(h,d,g) 56 l,gp,g+1p,hp,h+1p,d(h,d) 63 l,s-1p,sp,h-1p,d(h,d) 77 l,hl,hp,d(h,d) 26 l,gf ,hp,d(h,d,g) 57 l,g-1p,gp,hp,h+1p,d(h,d) 64 l,sp,s+1p,hp,d(h,d) 78 l,hl,d(h,d) 27 l,gf ,dp,d(h,d,g) Sibling Features 65 l,s-1p,sp,hp,h+1p,d(h,d) 79 l,hp,d(h,d) 28 l,hf ,gp,d(h,d,g) 30 l,hp,dp,sp,d(h,d) ?r(h,d) 66 l,sp,s+1p,dp,d(h,d) 80 l,dl,dp,d(h,d) 29 l,df ,gp,d(h,d,g) 31 l,hp,sp,d(h,d)?r(h,d) 67 l,sp,s-1p,dp,d(h,d) 81 l,dl,d(h,d) 91 l,hl,gl,d(h,d,g) 32 l,dp,sp,d(h,d)?r(h,d) 68 sp,dp,d+1p,d(h,d) 82 l,dp,d(h,d) 92 l,dp,gp,d(h,d,g) 33 l,pf ,sf ,d(h,d)?r(h,d) 69 sp,dp,d-1p,d(h,d) 83 l,dl,hp,dp,hl,d(h,d) 93 l,gl,hp,d(h,d,g) 34 l,pp,sf ,d(h,d)?r(h,d) 70 sp,s+1p,d-1p,dp,d(h,d) 84 l,dl,hp,dp,d(h,d) 94 l,gl,dp,d(h,d,g) 35 l,sf ,pp,d(h,d)?r(h,d) 71 s-1p,sp,d-1p,dp,d(h,d) 85 l,hl,dl,dp,d(h,d) 95 l,hl,gp,d(h,d,g) 36 l,sf ,dp,d(h,d)?r(h,d) 72 sp,s+1p,dp,d+1p,d(h,d) 86 l,hl,hp,dp,d(h,d) 96 l,dl,gp,d(h,d,g) 37 l,sf ,dp,d(h,d)?r(h,d) 73 s-1p,sp,dp,d+1p,d(h,d) 87 l,hl,dl,hp,d(h,d) 74 l,?dm,?gm,d(h,d) 38 l,df ,sp,d(h,d)?r(h,d) Special Feature 88 l,hl,dl,d(h,d) Linear G. Features 97 l,hl,sl,d(h,d)?r(h,d) 39 ?l,hp,dp,xpbetween h,d 89 l,hp,dp,d(h,d) 42 l,gp,g+1p,dp,d(h,d) 98 l,dl,sl,d(h,d)?r(h,d) 41 l,?hm,?dm,d(h,d) 43 l,gp,g-1p,dp,d(h,d) Table 4: Features Groups.
 $$$$$ This has lead to a higher accuracy.
 $$$$$ The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.

This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ For English a thresh old between 0.3 and about 2.0 would work well.
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ We could further increase theparsing and training speed with a paral lel feature extraction and a parallel parsing algorithm.
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ However, pars ing and training times are still relatively long.
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ In Section 6, we determine the opti mal setting for the Non-Projective ApproximationAlgorithm.

We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.
We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ In addition to a high accuracy, short parsing and training times are the most important properties of a parser.
We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ To gain even faster parsing times, it may be possible to trade accuracy against speed.

It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ Hy per threading can improve the parsing times again and we get with hyper threading 4.6 faster parsingtimes.
It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ It consists of the second order parsing algorithm of Carreras(2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component.
It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ In section 3, we analyze the time usage of the components of 89the parser.
It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild.

we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ (2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.
we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ The second order parsing algorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling.
we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ Ta ble 1 shows the elapsed times in 11000 seconds (milliseconds) of the selected languages for the procedure calls in the loops of Algorithm 1.
we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ It updates ??v as well, whereby the algorithm additionally weights the updates by ?.

Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. $$$$$ In section 3, we analyze the time usage of the components of 89the parser.
Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. $$$$$ We have developed a very fast parser with ex cellent attachment scores.

The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. $$$$$ We selected for all three lan guages a threshold of 0.3.
The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. $$$$$ 1 to c Tt ? create-array-thread(t, xi,data-list) start array-thread Tt// start thread t for t?
The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. $$$$$ The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.

For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ It increases a bit until about 0.3and remains relative stable before it slightly decreases.
For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ Transition based parsers typically have a linear or quadratic complexity (Nivre et al, 2004; Attardi, 2006).Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst casequadratic complexity and an expected linear pars ing time.
For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ The edge labeling is an integral part of the algorithm which requires an additional loop over the labels.
For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ The main method waits until all threads are completed and returns the result.

It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). $$$$$ (n? 1) ? I + i ? ?
It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). $$$$$ 1 to c Tt ? create-array-thread(t, xi,data-list) start array-thread Tt// start thread t for t?
It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). $$$$$ The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training.

The three dependency parsers are $$$$$ The improvement is due solely to theHash Kernel.
The three dependency parsers are $$$$$ We have developed a very fast parser with ex cellent attachment scores.
The three dependency parsers are $$$$$ The curves for all languages are a bit volatile.
The three dependency parsers are $$$$$ For these experiment, we set the clock speed to 3.46 Ghz in order to have the same clock speed for all experiments.

For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ The second order parsing algorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling.
For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ The English curve is rather flat.
For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ Current CPUs have up to 12 cores and we will see soon CPUs with more cores.

The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ This procedure is similar to randomization of weights (features), which aims to save space by sharing values in the weight vector (Blum., 2006; Rahimi and Recht, 2008).
The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.
The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ Figure 2 showsthe labeled attachment scores for the Czech, En glish and German development set in relation to the rearrangement threshold.
The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ Most of the numbers in Table2 are primes, since they are frequently used to obtain a better distribution of the content in hash ta 3>> n shifts n bits right, and % is the modulo operation.

For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ Transition based parsers typically have a linear or quadratic complexity (Nivre et al, 2004; Attardi, 2006).Nivre (2009) introduced a transition based non projective parsing algorithm that has a worst casequadratic complexity and an expected linear pars ing time.
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ This algorithmfrequently reaches very good, or even the best la beled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajic?
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training.
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ The numbers in bold face mark the top scores.

Please refer to Table 4 of Bohnet (2010) for the complete feature list. $$$$$ (n? 1) ? I + i ? ?
Please refer to Table 4 of Bohnet (2010) for the complete feature list. $$$$$ We are convinced thatthe Hash Kernel can be applied successful to tran sition based dependency parsers, phrase structure parsers and many other NLP applications.
Please refer to Table 4 of Bohnet (2010) for the complete feature list. $$$$$ In Section 6, we determine the opti mal setting for the Non-Projective ApproximationAlgorithm.

We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. $$$$$ The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006).
We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. $$$$$ In Section 2, we describe related work.
We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. $$$$$ maps the observations X to a feature space.
We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. $$$$$ In the experiment,we use the Hash Kernel and increase the thresh 95 System Average Catalan Chinese Czech English German Japanese Spanish Top CoNLL 09 85.77(1) 87.86(1) 79.19(4) 80.38(1) 89.88(2) 87.48(2) 92.57(3) 87.64(1) Baseline Parser 85.10 85.70 76.88 76.93 90.14 87.64 92.26 86.12 this work 86.33 87.45 76.99 80.96 90.33 88.06 92.47 88.13 Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al (2009), (2) Bohnet (2009), (3) Che et al.

 $$$$$ old at the beginning in small steps by 0.1 and later in larger steps by 0.5 and 1.0.
 $$$$$ We choose a weightvector size of 115911564 values for further exper iments since we get more non zero weights and therefore fewer collisions.
 $$$$$ The parsing speed is 16 times fasterfor the English test set than the conventional ap proach.
 $$$$$ We traced down the high time consumption to the access of the key and the access of the value.

Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers $$$$$ The curves for all languages are a bit volatile.
Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers $$$$$ The scores for Catalan, Chinese and Japanese are still lower than the top scores.
Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers $$$$$ Figure 2 showsthe labeled attachment scores for the Czech, En glish and German development set in relation to the rearrangement threshold.

As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ A parser build out of these two algorithms provides a good baseline and starting point to improve upon the parsing and training times.
As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ This algorithmfrequently reaches very good, or even the best la beled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajic?
As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ The factors of the first order maximum spanning tree parsing algorithm are edges consisting of the head, the dependent (child) and the edge label.
As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ The edge labeling is an integral part of the algorithm which requires an additional loop over the labels.
