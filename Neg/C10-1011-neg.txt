In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing. $$$$$ By using parallel algorithms, we could further increase the parsing time by a factor of 3.4 on a 4 core CPU and including hyper threading by a factor of 4.6.
In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing. $$$$$ (?(xi, yi)?
In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing. $$$$$ This procedure is up to 5 times faster than computing the features each time anew.
In this paper, we use the following baseline parsers: MaltParser (Nivre et al, 2007) for transition-based parsing; MSTParser (McDonald et al, 2005) (with sibling 2-edge factors) and Bohnet Parser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and BerkeleyParser (Petrov et al, 2006) for constituency-based parsing. $$$$$ 1 to c join Tt// wait until thread t is finished A?

Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010). $$$$$ In Section 6, we determine the opti mal setting for the Non-Projective ApproximationAlgorithm.
Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010). $$$$$ For instance, dialog systems only have a few hundred milliseconds toanalyze a sentence and machine translation sys tems, have to consider in that time some thousandtranslation alternatives for the translation of a sen tence.
Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010). $$$$$ Consequently, we have to consider atfirst methods to improve a parser, which do not in volve an accuracy loss, such as faster algorithms,faster implementation of algorithms, parallel al gorithms that use several CPU cores, and feature selection that eliminates the features that do not improve accuracy.

 $$$$$ We counted about twice the number of nonzero weights in the weight vector of the Hash Kernel compared to the baseline parser.For instance, we counted for English 17.34 Mil lions nonzero weights in the Hash Kernel and 8.47 Millions in baseline parser and for Chinese 18.28 Millions nonzero weights in the Hash Kernel and 8.76 Millions in the baseline parser.
 $$$$$ This algorithm has a quadratic complexity.
 $$$$$ This has lead to a higher accuracy.
 $$$$$ We choose a weightvector size of 115911564 values for further exper iments since we get more non zero weights and therefore fewer collisions.

This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ To resolve this problem, we implemented the passive-aggressive percep tron algorithm as a Hash Kernel.
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average For different j, the hash function h(j) might generate the same value k. This means that the hash function maps more than one feature to thesame weight.
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ For German and Czech, a threshold of about 0.3is the best choice.
This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). $$$$$ Figure 1 shows the difference between the labeled attachment score of the parser with MIRA and the Hash Kernel for Spanish.

We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ For these experiment, we set the clock speed to 3.46 Ghz in order to have the same clock speed for all experiments.
We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ We define ?(x, y) as the numeric fea ture representation indexed by J . Let ?k(x, y) = ?j(x, y) the hash based feature?index mapping,where h(j) = k. The process of parsing a sen tence xi is to find a parse tree yp that maximizes a scoring function argmaxyF (xi, y).
We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ Nevertheless, the reported run times inthe last shared tasks were similar to the maxi mum spanning tree parsers.
We also implement the feature mapping function as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudo code. $$$$$ data-list ?{(w1, w2)} c?

It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ For instance, dialog systems only have a few hundred milliseconds toanalyze a sentence and machine translation sys tems, have to consider in that time some thousandtranslation alternatives for the translation of a sen tence.
It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ The English curve is rather flat.
It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). $$$$$ In a pilot experiment, we have shown that it is possible to reduce the parsing time in this way to as little as 9 milliseconds.

we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ The idea for the custom hash function h2 is not to overlap the values of the feature sequence number and the edge label with other values.
we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ number of CPU cores for t?
we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ Johansson and Nugues (2008) reduced the needed number of loops over the edge labels by using only the edges that existed in the training corpus for a distinct head and child part-of-speech tag combination.The transition based parsers have a lower com plexity.
we compare our system with the MATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al, 2005) with hash kernels, using the MELT part-of-speech tagger (Denis and Sagot, 2009) and our own lemma tiser. $$$$$ The parsing time is 1.9 times fasteron two cores and 3.4 times faster on 4 cores.

Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. $$$$$ This algorithmfrequently reaches very good, or even the best la beled attachment scores, and was one of the most used parsing algorithms in the shared task 2009 of the Conference on Natural Language Learning (CoNLL) (Hajic?
Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. $$$$$ A Hash Kernel for structured data uses a hash function h : J ? {1...n} to index ?, cf.
Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. $$$$$ ThresholdFor non-projective parsing, we use the NonProjective Approximation Algorithm of McDon ald and Pereira (2006).
Bohnet (2010) showed that the Hash Kernel improves parsing speed and accuracy since the parser uses additionally negative features. $$$$$ By using parallel algorithms, we could further increase the parsing time by a factor of 3.4 on a 4 core CPU and including hyper threading by a factor of 4.6.

The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. $$$$$ Each time, we use the feature represen tation ?, the hash function h maps the features to integer numbers between 1 and |??w |.
The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. $$$$$ However, pars ing and training times are still relatively long.
The dependency labels were provided by the Bohnet parser (Bohnet, 2010) for English and by magyarlanc 2.0 (Zsibrita et al, 2013) for Hungarian. $$$$$ The outer loop iterates over the number of training epochs, while the innerloop iterates over all training examples.

For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ This procedure is relatively slow.
For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ The last row shows the times for 8 threads on a 4 core CPU with Hyper-threading.
For POS tagging and lemmatization, we use TreeTagger (Schmid, 1994) and determine grammatical gender with the morphological layer of the MATE Tools (Bohnet, 2010). $$$$$ The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild.

It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). $$$$$ number of CPU cores for t?
It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). $$$$$ Nevertheless, the reported run times inthe last shared tasks were similar to the maxi mum spanning tree parsers.
It uses an approximate exhaustive search for unlabeled parsing, then a separate arc label classifier is applied to label each arc. The Mateparser (Bohnet, 2010) is an efficient second order dependency parser that models the interaction between siblings as well as grandchildren (Carreras, 2007). $$$$$ The parsing times were about one word per second, which speeds upquickly with a smaller beam-size, although the ac curacy of the parser degrades a bit.

The three dependency parsers are: MaltParser (Nivre et al, 2006), Mate (Bohnet, 2010) 2 and MSTParser (McDonald and Pereira, 2006). $$$$$ We could also successful parallelize theprojective parsing and the non-projective approximation algorithm.
The three dependency parsers are: MaltParser (Nivre et al, 2006), Mate (Bohnet, 2010) 2 and MSTParser (McDonald and Pereira, 2006). $$$$$ The features are listed in Table 4.
The three dependency parsers are: MaltParser (Nivre et al, 2006), Mate (Bohnet, 2010) 2 and MSTParser (McDonald and Pereira, 2006). $$$$$ We counted 5.8 timesmore access to negative features than positive fea tures.We now look more into the implementation details of the used hash table to answer the pre viously asked question.

For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007).
For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ The idea for the custom hash function h2 is not to overlap the values of the feature sequence number and the edge label with other values.
For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ We il lustrate that the mapping of the features onto their weights in the support vectormachine is the major factor in time complexity.
For training the Berkeley Parser, we used Chinese Treebank (CTB) 7.0.We conducted our dependency-based pre ordering experiments on the Berkeley Parser and the Mate Parser (Bohnet, 2010), which were shown to be the two best parsers for Stanford typed dependencies (Che et al, 2012). $$$$$ This algorithm therefore has a complexity of O(n4).

The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ However, pars ing and training times are still relatively long.
The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ The curves for all languages are a bit volatile.
The three methods outperformed the baseline (the state of the art parser for French which is a second order graph based method) (Bohnet, 2010). $$$$$ We used for Catalan, Chinese, Japanese and Spanish the projective parsing algorithm.

For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ The second order parsing algorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling.
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ These techniquesdid not help much.
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ 1 379 21.3 18.2 1.5 420 354 45.8h 2 196 11.7 9.2 2.1 219 187 23.9h 3 138 8.9 6.5 1.6 155 126 16.6h 4 106 8.2 5.2 1.6 121 105 13.2h 4+4h 73.3 8.8 4.8 1.3 88.2 77 9.6hTable 5: Elapsed times in milliseconds for differ ent numbers of cores.
For syntactic features, we adopt those of Bohnet (2010) which include two categories corresponding to the two types of scoring subtrees in Fig. $$$$$ In Section 6, we determine the opti mal setting for the Non-Projective ApproximationAlgorithm.

Please refer to Table 4 of Bohnet (2010) for the complete feature list. $$$$$ te tp ta r total par.
Please refer to Table 4 of Bohnet (2010) for the complete feature list. $$$$$ Parsing algo rithms can use several cores.
Please refer to Table 4 of Bohnet (2010) for the complete feature list. $$$$$ Finally, the method stores the feature vectors on the hard disc.

We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. $$$$$ l represents the label, h the head, d the dependent, s a sibling, and g a grandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance.
We add for each verb in VerbNet and for each noun in CoreLex its base class or basic type as an additional feature where words tagged by the mate tagger (Bohnet, 2010) as NN.* are treated as nouns and words tagged as VB.* as verbs. $$$$$ The edge labeling is an integral part of the algorithm which requires an additional loop over the labels.

 $$$$$ A parser build out of these two algorithms provides a good baseline and starting point to improve upon the parsing and training times.
 $$$$$ The parsing time is approximately 20% faster, since some of the values did not have to be recalculated.
 $$$$$ It updates ??v as well, whereby the algorithm additionally weights the updates by ?.

Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al, 2013) Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency. $$$$$ In a pilot experiment, we have shown that it is possible to reduce the parsing time in this way to as little as 9 milliseconds.
Our system not only out performs the best single system (Bjorkelundetal., 2013) by 1.4%, but it also tops the ensemble system that combines three powerful parsers: the Mate parser (Bohnet, 2010), the Easy-First parser (Goldberg and Elhadad, 2010) and the Turbo parser (Martins et al, 2013) Impact of Sampling Methods We compare two sampling methods introduced in Section 3.2 with respect to their decoding efficiency. $$$$$ The parsing time is 3.5 times faster on a sin gle CPU core than the baseline parser which has an typical architecture for a maximum spanning tree parser.

As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ For a transition based parser, Gesmundo et al (2009) reported run times between 2.2 days for English and 4.7 days forCzech for the joint training of syntactic and se mantic dependencies.
As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ 1 to c join Tt// wait until thread t is finished A?
As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ The Hash Kernel substantially improves the parsing times and takes into account thefeatures of negative examples built dur ing the training.
As a result, the hash kernel often improves accuracy as well as efficiency compared to traditional techniques that only make use of features that occur in gold standard parses (Bohnet, 2010). $$$$$ 93bles.
