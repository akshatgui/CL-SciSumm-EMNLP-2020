We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ Here, FP means the number of output nodes that are not in the correct path, and FN means the number of nodes in the correct path that cannot be recognized by the system.
We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework.
We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ For the backward search, we use A*style decoding to generate the top k-best paths.

Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ Finally, we obtain the artificial unknown words that combine the unidentified unknown words in cross validation and infrequent words for learning unknown words.
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ Zhang and Clark (2008) indicated that their results cannot directly compare to the results of Shi and Wang (2007) due to different experimental settings.
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words.

Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ However, versions of CTB and experimental settings vary across different studies.

The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ As a result, the correct path yt can contain both wordlevel and character-level nodes (marked with asterisks (*)).
The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word segmentation and POS tagging.
The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors.

Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ Bigram features: Table 3 shows our bigram features.
Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors.
Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ We only aim to capture the characteristics of unknown words and augment their representatives.

 $$$$$ Nakagawa and Uchimoto (2007) provided empirical evidence that the characterbased model is not always better than the wordbased model.
 $$$$$ For the baseline policy, we varied r in the range of [1, 5] and found that setting r = 3 yielded the best performance on the development set for both the small and large training corpus experiments.
 $$$$$ Our approach has two important advantages.

K2009 is the result of Kruengkrai et al (2009). $$$$$ We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.
K2009 is the result of Kruengkrai et al (2009). $$$$$ Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words.
K2009 is the result of Kruengkrai et al (2009). $$$$$ We introduced a simple scheme based on the error-driven concept to effectively learn the characteristics of known and unknown words from the training corpus.

 $$$$$ Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation.

Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b).
Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ Table 6 shows the results of our word-character hybrid model using the error-driven and baseline policies.
Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ We represent the search space with a lattice based on the word-character hybrid model (Nakagawa and Uchimoto, 2007).
Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ F1 =

For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. $$$$$ In this paper, we presented a discriminative wordcharacter hybrid model for joint Chinese word segmentation and POS tagging.
For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. $$$$$ Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems.
For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. $$$$$ In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.
For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. $$$$$ In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance.

Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). $$$$$ We describe k-best decoding for our hybrid model and design its loss function and the features appropriate for our task.
Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). $$$$$ In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.

"Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). $$$$$ Note that the unidentified unknown words in the cross validation are not necessary to be infrequent words, but some overlap may exist.
"Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). $$$$$ Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b).
"Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). $$$$$ Table 5 provides the statistics of our experimental settings on the small and large training data.
"Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). $$$$$ The third and fourth columns indicate the numbers of known and artificial unknown words in the training phase.

We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i).
We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ We represent the search space with a lattice based on the word-character hybrid model (Nakagawa and Uchimoto, 2007).
We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ Our baseline model outperforms all prior approaches for both Seg and Seg & Tag, and we hope that our error-driven model can further improve performance.
We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i).

We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. $$$$$ We decided to follow the experimental settings of Jiang et al. (2008a; 2008b) on CTB 5.0 and Zhang and Clark (2008) on CTB 4.0 since they reported the best performances on joint word segmentation and POS tagging using the training materials only derived from the corpora.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. $$$$$ Let us introduce some notation.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. $$$$$ Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems.

Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ Our word-character hybrid model offers high performance since it can handle both known and unknown words.
Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ For example, (TE(w−1), TB(w0)) captures the change of character types from the end character in the previous word to the beginning character in the current word.
Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.
Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ Our word-character hybrid model offers high performance since it can handle both known and unknown words.

On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. $$$$$ Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006).
On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. $$$$$ Note that some nodes and state transitions are not allowed.
On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. $$$$$ Note that some nodes and state transitions are not allowed.

 $$$$$ Let us introduce some notation.
 $$$$$ We introduced a simple scheme based on the error-driven concept to effectively learn the characteristics of known and unknown words from the training corpus.

 $$$$$ Another potential algorithm is MIRA, which integrates the notion of the large-margin classifier (Crammer, 2004).
 $$$$$ For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b).

 $$$$$ The first is robust search space representation based on a hybrid model in which word-level and characterlevel nodes are used to identify known and unknown words, respectively.
 $$$$$ Table 7 compares the F1 results with previous studies on CTB 5.0.
 $$$$$ Let us introduce some notation.
 $$$$$ F1 =

We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ Optimal balances were selected using the development set.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004).
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ Using just the surface forms can overfit the training data and lead to poor predictions on the test data.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ As for search space representation, Ng and Low (2004) found that for Chinese, the characterbased model yields better results than the wordbased model.
