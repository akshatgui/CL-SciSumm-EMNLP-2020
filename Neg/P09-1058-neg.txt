We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ However, these features only slightly improve performance over using simple POS bigrams.
We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ Nakagawa and Uchimoto (2007) provided empirical evidence that the characterbased model is not always better than the wordbased model.
We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ Another potential algorithm is MIRA, which integrates the notion of the large-margin classifier (Crammer, 2004).
We follow Kruengkrai et al (2009) and split the CTB 5 into training, development testing and testing sets, as shown in Table 3. $$$$$ In each iteration, MIRA updates the weight vector w by keeping the norm of the change in the weight vector as small as possible.

Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ Note that some nodes and state transitions are not allowed.
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem.
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ Another potential algorithm is MIRA, which integrates the notion of the large-margin classifier (Crammer, 2004).
Kruengkrai et al (2009) made use of character type knowledge for spaces, numerals, symbols, alphabets, Chinese and other characters. $$$$$ The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model.

Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ With regard to error-driven learning, Brill (1995) proposed a transformation-based approach that acquires a set of error-correcting rules by comparing the outputs of an initial tagger with the correct annotations on a training corpus.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ We only aim to capture the characteristics of unknown words and augment their representatives.
Kruengkrai et al (2009) and Zhang and Clark (2008) are the most similar to our system among related work. $$$$$ In other words, we use word-level nodes to identify known words and character-level nodes to identify unknown words.

The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model.
The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ We propose the following simple scheme: Several types of errors are produced by the baseline model, but we only focus on those caused by unidentified unknown words, which can be easily collected in the evaluation process.
The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ We propose an errordriven policy that delivers this balance by acquiring examples of unknown words from particular errors in a training corpus.
The work of Kruengkrai et al (2009) is based on Nakagawa and Uchimoto (2007), which separates the processing of known words and unknown words, and uses a set of segmentation tags to represent the segmentation of characters. $$$$$ We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned.

Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ The goal of our learning algorithm is to learn a mapping from inputs (unsegmented sentences) x E X to outputs (segmented paths) y E Y based on training samples of input-output pairs S = {(xt, yt)}t1.
Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ In the training phase, since several paths (marked in bold) can correspond to the correct analysis in the annotated corpus, we need to select one correct path yt as a reference for training.2 The next section describes our strategies for dealing with this issue.
Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b).
Our learning and decoding algorithms are also different from Kruengkrai et al (2009). $$$$$ In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance.

 $$$$$ For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes.
 $$$$$ Our approach overcomes the limitation of the original hybrid model by a discriminative online learning algorithm for training.

K2009 is the result of Kruengkrai et al (2009). $$$$$ Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words.
K2009 is the result of Kruengkrai et al (2009). $$$$$ We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.

 $$$$$ In particular, we use a generalized version of MIRA (Crammer et al., 2005; McDonald, 2006) that can incorporate k-best decoding in the update procedure.
 $$$$$ Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006).
 $$$$$ Our word-character hybrid model offers high performance since it can handle both known and unknown words.

Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ Figure 1 shows an example of a lattice for a Chinese sentence: “ ” (Chongming is China’s third largest island).
Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ Our word-character hybrid model offers high performance since it can handle both known and unknown words.
Therefore, it's natural that we evaluate the accuracy of joint Chinese word segmentation and part of speech tagging, as reported in previous literature (Kruengkrai et al 2009). $$$$$ Nakagawa and Uchimoto (2007) provided empirical evidence that the characterbased model is not always better than the wordbased model.

For ordinary word segmentation, the best result is reported to be around 97% F1 on CTB 5.0 (Kruengkrai et al, 2009), while our parser performs at 97.3%, though we should remember that the result concerns flat words only. $$$$$ We represent the search space with a lattice based on the word-character hybrid model (Nakagawa and Uchimoto, 2007).

Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). $$$$$ However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model.
Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). $$$$$ We used recall (R), precision (P), and F1 as evaluation metrics.
Because the tasks of word segmentation and POS tagging have strong interactions, many studies have been devoted to the task of joint word segmentation and POS tagging for languages such as Chinese (e.g. Kruengkrai et al (2009)). $$$$$ Our baseline model outperforms all prior approaches for both Seg and Seg & Tag, and we hope that our error-driven model can further improve performance.

"Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). $$$$$ In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.
"Kruengkrai+ '09" is a lattice-based model by Kruengkrai et al (2009). $$$$$ We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.

We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ With regard to error-driven learning, Brill (1995) proposed a transformation-based approach that acquires a set of error-correcting rules by comparing the outputs of an initial tagger with the correct annotations on a training corpus.
We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ Let us introduce some notation.
We applied morphological analysis and part-of-speech tagging by using TreeTagger (Schmid, 1994) for English, JUMAN for Japanese, and mma (Kruengkrai et al, 2009) for Chinese, respectively. $$$$$ For the backward search, we use A*style decoding to generate the top k-best paths.

We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. $$$$$ The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. $$$$$ In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline parser to parse all the sentences in the data. $$$$$ Ideally, we need to build a wordcharacter hybrid model that effectively learns the characteristics of unknown words (with characterlevel nodes) as well as those of known words (with word-level nodes).

Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model.
Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ Shi and Wang (2007) used CTB that was distributed in the SIGHAN Bakeoff.
Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ We introduced a simple scheme based on the error-driven concept to effectively learn the characteristics of known and unknown words from the training corpus.
Firstly, the Chinese sentences are segmented, POS tagged and parsed by the tools described in Kruengkrai et al (2009) and Cao et al (2007), both of which are trained on the Penn Chinese Treebank 6.0. $$$$$ We describe an efficient framework for training our model based on the Margin Infused Relaxed Algorithm (MIRA), evaluate our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature.

On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. $$$$$ Character-level nodes have special tags where position-of-character (POC) and POS tags are combined (Asahara, 2003; Nakagawa, 2004).
On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. $$$$$ Another potential algorithm is MIRA, which integrates the notion of the large-margin classifier (Crammer, 2004).
On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al, 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. $$$$$ The second is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model.

 $$$$$ Our approach overcomes the limitation of the original hybrid model by a discriminative online learning algorithm for training.
 $$$$$ We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiring examples of unknown words from particular errors in a training corpus.
 $$$$$ In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese.
 $$$$$ Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature.

 $$$$$ In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese.
 $$$$$ Experimental results indicate that our approach can achieve state-of-the-art performance.
 $$$$$ We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned.

 $$$$$ We design our feature templates to capture various levels of information about words and POS tags.
 $$$$$ Jiang et al. (2008a; 2008b) (Jiang08a, Jiang08b) used CTB 5.0.
 $$$$$ We propose a new framework for training the wordcharacter hybrid model based on the Margin Infused Relaxed Algorithm (MIRA) (Crammer, 2004; Crammer et al., 2005; McDonald, 2006).

We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ The first is robust search space representation based on a hybrid model in which word-level and characterlevel nodes are used to identify known and unknown words, respectively.
We used the MMA system (Kruengkrai et al, 2009) trained on the training data to perform word segmentation and POS tagging and used the Baseline Parser to parse all the sentences in the data. $$$$$ Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance.
