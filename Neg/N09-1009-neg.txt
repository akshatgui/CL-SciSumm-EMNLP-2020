Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data.
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ We provide a variational EM algorithm for inference.

To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution.
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ Because we wish to learn the values of µ and E, we embed variational inference as the E step within a variational EM algorithm, shown schematically in Fig.
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ For a vector in v E RN and a set I C_ 1:N, we denote by vI to be the vector created from v by using the coordinates in I.

Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ The θ are the multinomial distributions 0s(· , ·,·) and 0c(· 1 ·, ·).
Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ Instead of using a single normal vector for all of the multinomials, we use several normal vectors, partition each one and then recombine parts which correspond to the same multinomial, as a mixture.
Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.

 $$$$$ We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.
 $$$$$ An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data.
 $$$$$ We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ Using the notation above, DMV defines x = (x1,x2, ..., xn) to be a sentence. x0 is a special “wall” symbol, $, on the left of every sentence.
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ Bayesian methods offer an elegant framework for combining prior knowledge with data.
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.

 $$$$$ Note that there may be many derivations y for a given string x—perhaps even infinitely many in some kinds of grammars.
 $$$$$ This research was supported by NSF IIS-0836431.
 $$$$$ The partitioned LN distribution in Aitchison (1986) can be formulated as a shared LN distribution where N = 1.

 $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
 $$$$$ PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of “child” symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM).
 $$$$$ The probability of the entire tree is given by p(x, y θ) = P(y(0) 1$, θ).
 $$$$$ In §2, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to Klein and Manning (2004).

Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ The rest of this paper is organized as follows.
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ Taking the Bayesian approach, we wish to place a prior on those multinomials, and the Dirichlet family is a natural candidate for such a prior because of its conjugacy, which makes inference algorithms easier to derive.
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ This research was supported by NSF IIS-0836431.
While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data.
While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ This research was supported by NSF IIS-0836431.

While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ The errors being governed by such attachments are the direct result of nouns and verbs being the most common parents in these data sets.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al. Klein and Manning (2004) learned the DMV probabilities 0 from a corpus of part-of-speech-tagged sentences using the EM algorithm.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ The LN draws a multinomial 0 as follows: Blei and Lafferty (2006) defined correlated topic models by replacing the Dirichlet in latent Dirichlet allocation models (Blei et al., 2003) with a LN distribution.

Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results.
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution.

The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ For each direction D (left or right), the set of multinomials of the form Oc(·  |v, D), for v E V , all share a normal expert.
The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y).
The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.
The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ In general, a probabilistic grammar defines the joint probability of a string x and a grammatical where fk,i is a function that “counts” the number of times the kth distribution’s ith event occurs in the derivation.

These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Let y(i) denote the subtree rooted at position i.
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ DMV defines a probabilistic grammar for unlabeled, projective dependency structures.
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Each “step” of the walk and each symbol emission corresponds to one derivation step.

Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ In fact, variational inference in this case takes an action similar to smoothing the counts using the exp-Ψ function during the E-step.
Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of “child” symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM).
Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.

 $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
 $$$$$ This is in principle what is required for arbitrary covariance between grammar probabilities, except that DMV has O(t2) weights for a part-of-speech vocabulary of size t, requiring a very large multivariate normal distribution with O(t4) covariance parameters.

This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Each step or emission of an HMM and each rewriting operation of a PCFG is conditionally independent of the other rewriting operations given a single structural element (one HMM or PCFG state); this Markov property permits efficient inference for the probability distribution defined by the probabilistic grammar.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Here, the graph is constrained to be a projective tree rooted at x0 = $: each word except $ has a single parent, and there are no cycles or crossing dependencies. yleft(0) is taken to be empty, and yright(0) contains the sentence’s single head.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Bayesian methods offer an elegant framework for combining prior knowledge with data.

Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. $$$$$ We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only: both Headen et al., (2009) and Cohen and Smith (2009) train on WSJ10 even when the test set includes longer sentences. $$$$$ This research was supported by NSF IIS-0836431.

Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state.
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ Parts of speech are matched through the single coarse tagset (footnote 4).
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.

The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ 2 gives an example of a non-trivial case of using a SLN distribution, where three multinomials are generated from four normal experts.
The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.
The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ For each treebank, we divide the tags into twelve disjoint tag families.4 The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0.5 between tags which belong to the same family, and 0 otherwise.
The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ This research was supported by NSF IIS-0836431.

Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ We provide a variational EM algorithm for inference.
