Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words.
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ The probability P(y(i) I xi, θ) of generating this subtree, given its head word xi, is defined recursively, as described in Fig.
Cohen et al (2008) and Cohen and Smith (2009) employed the logistic normal prior to model the correlations between grammar symbols. $$$$$ Nonparametric models (Teh, 2006) may be appropriate.

To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ For Chinese, we train on §1–270, use §301–1151 for development and report testing results on §271–300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus.
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ This research was supported by NSF IIS-0836431.
To our knowledge, the only grammar induction work on non-parallel corpora is (Cohen and Smith, 2009), but their method does not model a common grammar, and requires prior information such as part-of-speech tags. $$$$$ The probability P(y(i) I xi, θ) of generating this subtree, given its head word xi, is defined recursively, as described in Fig.

Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ This research was supported by NSF IIS-0836431.
Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al. (2008), showing how to extend the use of the LN prior to tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary).
Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ Recall that K is the number of multinomials in the probabilistic grammar, and Nk is the number of events in the kth multinomial.
Cohen and Smith (2009) use more complicated algorithms (variational EM and MBR decoding) and stronger linguistic hints (tying related parts of speech and syntactically similar bilingual data). $$$$$ This research was supported by NSF IIS-0836431.

 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
 $$$$$ We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learnand with a non-parallel, multilingual corpus.
 $$$$$ We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ We present a family of priors over probabilistic grammar weights, called the shared logistic normal distribution.
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).
Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. $$$$$ We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration.

 $$$$$ After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately.
 $$$$$ 2).
 $$$$$ We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar.
 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.

 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
 $$$$$ Each “step” of the walk and each symbol emission corresponds to one derivation step.
 $$$$$ Def.
 $$$$$ 2).

Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ Note that there may be many derivations y for a given string x—perhaps even infinitely many in some kinds of grammars.
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ Because we wish to learn the values of µ and E, we embed variational inference as the E step within a variational EM algorithm, shown schematically in Fig.
Cohen and Smith (2009) present a model for jointly learning English and Chinese dependency grammars without bitexts. $$$$$ Bayesian methods offer an elegant framework for combining prior knowledge with data.

While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).
While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.
While some choices of prior structure can greatly complicate inference (Cohen and Smith, 2009), we choose a hierarchical Gaussian form for the drift term, which allows the gradient of the observed data likelihood to be easily computed using standard dynamic programming methods. $$$$$ Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability.

While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ This is in principle what is required for arbitrary covariance between grammar probabilities, except that DMV has O(t2) weights for a part-of-speech vocabulary of size t, requiring a very large multivariate normal distribution with O(t4) covariance parameters.
While this progression of model structure is similar to that explored in Cohen and Smith (2009), Cohen and Smith saw their largest improvements from tying together parameters for the varieties of coarse parts-of-speech monolinugally, and then only moderate improvements from allowing cross-linguistic influence on top of monolingual sharing. $$$$$ Klein and Manning (2004) achieved their best results with a combination of DMV with a model known as the “constituent-context model” (CCM).

Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ In this paper, we use the “dependency model with valence” (DMV), due to Klein and Manning (2004).
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ Using the notation above, DMV defines x = (x1,x2, ..., xn) to be a sentence. x0 is a special “wall” symbol, $, on the left of every sentence.
Evaluating our LINGUISTIC model on the same test sets as (Cohen and Smith, 2009), sentences of length 10 or less in section 23 of PTB and sections 271 - 300 of CTB, we achieved an accuracy of 56.6 for Chinese and 60.3 for English. $$$$$ This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.

The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ The prior encodes information about which grammar rules’ weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold.
The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ Using the notation above, DMV defines x = (x1,x2, ..., xn) to be a sentence. x0 is a special “wall” symbol, $, on the left of every sentence.
The best models of Cohen and Smith (2009) achieved accuracies of 52.0 and 62.0 respectively on these same test sets. $$$$$ We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar.

These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ The probability of the entire tree is given by p(x, y θ) = P(y(0) 1$, θ).
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008).
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ Nonparametric models (Teh, 2006) may be appropriate.
These are the same training, development, and test sets used by Cohen and Smith (2009). $$$$$ To follow the general setting of Eq.

Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors.
Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. $$$$$ Let y(i) denote the subtree rooted at position i.

 $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
 $$$$$ Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to Dirichlet priors for probabilistic grammars, based on the logistic normal (LN) distribution over the probability simplex.
 $$$$$ This research was supported by NSF IIS-0836431.

This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ We begin our experiments with a monolingual setting, where we learn grammars for English and Chinese (separately) using the settings described above.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Headden et al. (2009) extended DMV so that the distributions θe condition on the valence as well, with smoothing, and showed significant improvements for short sentences.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Using the notation above, DMV defines x = (x1,x2, ..., xn) to be a sentence. x0 is a special “wall” symbol, $, on the left of every sentence.
This has been done successfully in multilingual settings (Cohen and Smith, 2009). $$$$$ Recall that K is the number of multinomials in the probabilistic grammar, and Nk is the number of events in the kth multinomial.

Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only $$$$$ Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes.
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only $$$$$ The rest of this paper is organized as follows.
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only $$$$$ In particular, as noted by Blei and Lafferty (2006), there is no explicit flexible way for the Dirichlet’s parameters to encode beliefs about covariance between the probabilities of two events.
Today's best unsupervised dependency parsers, which are rooted in this model, train on short sentences only $$$$$ A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process.

Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ The authors thank the anonymous reviewers and Sylvia Rebholz for helpful comments.
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ This family extends the partitioned logistic normal distribution, enabling factored covariance between the probabilities of different derivation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar.
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable.
Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al, 2008, and Cohen and Smith, 2009). $$$$$ We discuss future work (§5) and conclude in §6.

The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ Def.
The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar.
The performance of Cohen and Smith (2009), like the performance of Headden et al (2009), is greater than what we report, but those developments are orthogonal to the contributions of this paper. $$$$$ The θ are the multinomial distributions 0s(· , ·,·) and 0c(· 1 ·, ·).

Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ Bayesian methods offer an elegant framework for combining prior knowledge with data.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ Probabilistic grammars have become an important tool in natural language processing.
Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from well chosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al, 2009). $$$$$ Instead of using a single normal vector for all of the multinomials, we use several normal vectors, partition each one and then recombine parts which correspond to the same multinomial, as a mixture.
