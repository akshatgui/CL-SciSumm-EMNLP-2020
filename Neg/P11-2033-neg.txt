The present paper deals with five parsers evaluated within the translation frame work $$$$$ A set of shiftreduce actions are defined, which consume words from the queue and build the output parse.
The present paper deals with five parsers evaluated within the translation frame work $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
The present paper deals with five parsers evaluated within the translation frame work $$$$$ The distance between S0 and N0 will correspond to the distance between a pair of head and modifier when an LeftArc action is taken, for example, but not when a Shift action is taken.
The present paper deals with five parsers evaluated within the translation frame work $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.

ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ With all new features added, the speed dropped to 29 sentences per second.
ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ Their results are relevant although not directly comparable with ours.
ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall.
ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ Valency of S0 and N0 The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al. (2009) and Sagae and Tsujii (2007).

The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010). $$$$$ We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row ‘Z&C08 transition’), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row ‘H&S10’), and graphbased models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row ‘K08 baeline’), and the two models of Koo and Collins (2010).

 $$$$$ In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall.
 $$$$$ In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.
 $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
 $$$$$ An open source release of our parser is freely available.

 $$$$$ Moreover, we will see that label information can actually improve link accuracy.
 $$$$$ Each group of new feature templates improved the accuracies over the previous system, and the final accuracy with all new features was 93.14% in unlabeled attachment score.
 $$$$$ An open source release of our parser is freely available.

Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). $$$$$ Recent research has addressed two potential disadvantages of systems like MaltParser.
Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). $$$$$ In particular, we calculate the number of left and right modifiers separately, calling them left valency and right valency, respectively.

Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. $$$$$ For the Chinese Treebank, they give a signficant improvement of the state of the art.
Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.
Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. $$$$$ For example, S0pN0wp represents the feature template that takes the word and POS-tag of N0, and combines it with the word of S0.

ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations.
ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall.
ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.
ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ Unigram label information has been used in MaltParser (Nivre et al., 2006a; Nivre, 2006).

Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.
Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row ‘Z&C08 transition’), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row ‘H&S10’), and graphbased models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row ‘K08 baeline’), and the two models of Koo and Collins (2010).
Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010).
Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ Unlike both earlier globallearning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels.

More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). $$$$$ In this short paper, we study a third aspect in a statistical system: feature definition.
More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). $$$$$ Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy.
More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). $$$$$ In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.

Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.
Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ The new templates include unigram word, POS-tag and dependency labels of S0h2, S0l2, S0r2 and N0l2, as well as POS-tag combinations with S0 and N0.
Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ Such use is exemplified by the feature templates “from three words” in Table 1.

The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1 $$$$$ Our experiments were performed on a Linux platform with a 2GHz CPU.
The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1 $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.

We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ Unlike both earlier globallearning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels.
We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ Their results are relevant although not directly comparable with ours.
We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ In this short paper, we study a third aspect in a statistical system: feature definition.

We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. $$$$$ For all experiments, we set the beam size to 64 for the parser, and report unlabeled and labeled attachment scores (UAS, LAS) and unlabeled exact match (UEM) for evaluation.

Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. $$$$$ A set of shiftreduce actions are defined, which consume words from the queue and build the output parse.
Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.

Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ These features are mostly taken from Zhang and Clark (2008) and Huang and Sagae (2010), and our parser reproduces the same accuracies as reported by both papers.
Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ At each step during a parsing process, the parser configuration can be represented by a tuple (S, N, A), where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built.
Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.

Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ Table 3 shows the effect of new features on the development test data for English.
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transitionbased dependency parsers incorporate some nonlocal features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a).
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ Our experiments were performed using the Penn Treebank (PTB) and Chinese Treebank (CTB) data.
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.

Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. $$$$$ Our experiments were performed on a Linux platform with a 2GHz CPU.
Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.
Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. $$$$$ In this short paper, we study a third aspect in a statistical system: feature definition.
Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.

For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ A set of shiftreduce actions are defined, which consume words from the queue and build the output parse.
For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ Valency of S0 and N0 The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al. (2009) and Sagae and Tsujii (2007).
For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ A set of shiftreduce actions are defined, which consume words from the queue and build the output parse.
For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ In this short paper, we extend the baseline feature templates with the following: Distance between S0 and N0 Direction and distance between a pair of head and modifier have been used in the standard feature templates for maximum spanning tree parsing (McDonald et al., 2005).

Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model. $$$$$ In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall.
Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model. $$$$$ Third-order features of S0 and N0 Higher-order context features have been used by graph-based dependency parsers to improve accuracies (Carreras, 2007; Koo and Collins, 2010).
Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model. $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.
