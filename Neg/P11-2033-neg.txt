The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). $$$$$ A set of shiftreduce actions are defined, which consume words from the queue and build the output parse.
The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). $$$$$ Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010).
The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). $$$$$ Our parser gave 93.5% UAS, 91.9% LAS and 52.1% UEM when trained and evaluated on Stanford basic dependencies, which are projective dependency trees.
The present paper deals with five parsers evaluated within the translation frame work: three genuine dependency parsers, namely the parsers described in (McDonald et al, 2005), (Nivre et al, 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs we reconverted to dependency structures by Penn Converter (Johansson and Nugues, 2007). $$$$$ A set of shiftreduce actions are defined, which consume words from the queue and build the output parse.

ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transitionbased dependency parsers incorporate some nonlocal features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a).
ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ In this short paper, we extend the baseline feature templates with the following: Distance between S0 and N0 Direction and distance between a pair of head and modifier have been used in the standard feature templates for maximum spanning tree parsing (McDonald et al., 2005).
ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ In the aspect of training, global structural learning has been used to replace local learning on each decision (Zhang and Clark, 2008; Huang et al., 2009), although the effect of global learning has not been separated out and studied alone.
ZPar - Zpar parser which is basically an alternative implementation of the Malt parser, employing a richer set of non-local features as described by Zhang and Nivre (2011). $$$$$ Each group of new feature templates improved the accuracies over the previous system, and the final accuracy with all new features was 93.14% in unlabeled attachment score.

The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010). $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010). $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.
The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010). $$$$$ The distance between S0 and N0 will correspond to the distance between a pair of head and modifier when an LeftArc action is taken, for example, but not when a Shift action is taken.
The dependency parsing features are taken from the work of Zhang and Nivre (2011), and the features for joint word segmentation and POS-tagging are taken from Zhang and Clark (2010). $$$$$ As an alternative to Penn2Malt, bracketed sentences can also be transformed into Stanford dependencies (De Marneffe et al., 2006).

 $$$$$ We explore considerably richer feature representations and show that they improve parsing accuracy significantly.
 $$$$$ Bracketed sentences from PTB were transformed into dependency formats using the Penn2Malt tool.2 Following Huang and Sagae (2010), we assign POS-tags to the training data using ten-way jackknifing.
 $$$$$ Denoting the top of stack w – word; p – POS-tag; vl, vr – valency; l – dependency label, sl, sr – labelset. with S0, the front items from the queue with N0, N1, and N2, the head of S0 (if any) with S0h, the leftmost and rightmost modifiers of S0 (if any) with S0l and S0r, respectively, and the leftmost modifier of N0 (if any) with N0l, the baseline features are shown in Table 1.
 $$$$$ Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations.

 $$$$$ With all new features added, the speed dropped to 29 sentences per second.
 $$$$$ We start with the baseline features in Table 1, and incrementally add the distance, valency, unigram, third-order and label set feature templates in Table 2.
 $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.
 $$$$$ For the Chinese Treebank, they give a signficant improvement of the state of the art.

Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). $$$$$ Transitionbased parsing, by contrast, can easily accommodate arbitrarily complex representations involving nonlocal features.
Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). $$$$$ In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.
Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). $$$$$ Complex non-local features, such as bracket matching and rhythmic patterns, are used in transition-based constituency parsing (Zhang and Clark, 2009; Wang et al., 2006), and most transitionbased dependency parsers incorporate some nonlocal features, but current practice is nevertheless to use a rather restricted set of features, as exemplified by the default feature models in MaltParser (Nivre et al., 2006a).
Our results are better than most systems on this data split, except Zhang and Nivre (2011), Li et al (2012) and Chen et al (2009). $$$$$ We take the standard split of CTB and use gold segmentation and POS-tags for the input.

Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. $$$$$ Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy.
Python implementation for the labeled arc-eager system with the rich feature set of Zhang and Nivre (2011) is available on the first author's homepage. $$$$$ We further use their word and POS-tag information as “unigram” features in Table 2.

ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ An open source release of our parser is freely available.
ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
ArcS use feature set of Huang and Sagae (2010) (50 templates), and ArcE that of Zhang and Nivre (2011) (72 templates). $$$$$ In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon greedy one-best search, and positive results were reported.

Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ Their results are relevant although not directly comparable with ours.
Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ This information is combined with the word and POS-tag of S0 and N0 to make feature templates.
Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon greedy one-best search, and positive results were reported.
Li11 refers to the second-order graph-based model of Li et al (2011), whereas Z&N11 is the feature-rich transition-based model of Zhang and Nivre (2011). $$$$$ Unlike both earlier globallearning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels.

More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). $$$$$ As an alternative to Penn2Malt, bracketed sentences can also be transformed into Stanford dependencies (De Marneffe et al., 2006).
More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). $$$$$ Each group of new feature templates improved the accuracies over the previous system, and the final accuracy with all new features was 93.14% in unlabeled attachment score.
More efficient decoding would also allow the use of the look-ahead features (Hatori et al, 2011) and richer parsing features (Zhang and Nivre, 2011). $$$$$ Valency of S0 and N0 The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al. (2009) and Sagae and Tsujii (2007).

Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ Moreover, we will see that label information can actually improve link accuracy.
Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ Recent research have focused on action sets that build projective dependency trees in an arc-eager (Nivre et al., 2006b; Zhang and Clark, 2008) or arc-standard (Yamada and Matsumoto, 2003; Huang and Sagae, 2010) process.
Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.
Additionally, we look at higher-order dependency arc label features, which is novel to graph-based parsing, though commonly exploited in transition-based parsing (Zhang and Nivre, 2011). $$$$$ We used our implementation of the Collins (2002) tagger (with 97.3% accuracy on a standard Penn Treebank test) to perform POS-tagging.

The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. $$$$$ In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon greedy one-best search, and positive results were reported.
The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. $$$$$ In standard experiments using the Penn Treebank, our parser gets an unlabeled attachment score of 92.9%, which is the best result achieved with a transition-based parser and comparable to the state of the art.
The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. $$$$$ Representing the type of information a statistical system uses to make predictions, feature templates can be one of the most important factors determining parsing accuracy.
The speed of our parser is 220 tokens per second, which is over 4 times faster than an exact third-order parser that at Figure 1: Example Sentence.tains UAS of 92.81% and comparable to the state-of the-art transition-based system of Zhang and Nivre (2011) that employs beam search. $$$$$ An open source release of our parser is freely available.

We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations.
We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack.
We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ Unlike both earlier globallearning parsers, which only perform unlabeled parsing, we perform labeled parsing by augmenting the LeftArc and RightArc actions with the set of dependency labels.
We compare our method to a state-of-the-art graph-based parser (Koo and Collins, 2010) as well as a state-of-the-art transition-based parser that uses a beam (Zhang and Nivre, 2011) and the dynamic programming transition-based parser of Huang and Sagae (2010). Additionally, we compare to our own implementation of exact first to third-order graph-based parsing and the transition-based system of Zhang and Nivre (2011) with varying beam sizes. $$$$$ We explore considerably richer feature representations and show that they improve parsing accuracy significantly.

We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. $$$$$ Left and right valencies are represented by vl and vr in Table 2, respectively.
We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. $$$$$ It is worth noticing that the use of distance information in our transition-based model is different from that in a typical graph-based parser such as MSTParser.
We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. $$$$$ In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.
We also report results for a re-implementation of exact first to third-order graph-based parsing and a re-implementation of Zhang and Nivre (2011) in order to compare parser speed. $$$$$ Table 3 shows the effect of new features on the development test data for English.

Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. $$$$$ In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall.
Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. $$$$$ We adopt the arc-eager system1, for which the actions are: Further, we follow Zhang and Clark (2008) and Huang et al. (2009) and use the generalized perceptron (Collins, 2002) for global learning and beamsearch for decoding.
Second, at a similar toks/sec parser speed, our method achieves better performance than the transition-based model of Zhang and Nivre (2011) with a beam of 256. $$$$$ The speed of our baseline parser was 50 sentences per second.

Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ We explore considerably richer feature representations and show that they improve parsing accuracy significantly.
Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems.
Here we use the identical training/validation/evaluation splits and experimental set-up as Zhang and Nivre (2011). $$$$$ Our experiments were performed using the Penn Treebank (PTB) and Chinese Treebank (CTB) data.

Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ In the aspect of decoding, beam-search (Johansson and Nugues, 2007; Zhang and Clark, 2008; Huang et al., 2009) and partial dynamic-programming (Huang and Sagae, 2010) have been applied to improve upon greedy one-best search, and positive results were reported.
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ In this short paper, we study a third aspect in a statistical system: feature definition.
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ Hence our work is in line with Titov and Henderson (2007) in using labeled transitions with global learning.
Here we compare to our re-implementations of Zhang and Nivre (2011), exact first to third-order parsing and Rush and Petrov (2012) for the data sets in which they reported results. $$$$$ Cer et al. (2010) report results on Stanford collapsed dependencies, which allow a word to have multiple heads and therefore cannot be produced by a regular dependency parser.

Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. $$$$$ We add the distance between S0 and N0 to the feature set by combining it with the word and POS-tag of S0 and N0, as shown in Table 2.
Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. $$$$$ Our experiments were performed on a Linux platform with a 2GHz CPU.

For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ Denoting the top of stack w – word; p – POS-tag; vl, vr – valency; l – dependency label, sl, sr – labelset. with S0, the front items from the queue with N0, N1, and N2, the head of S0 (if any) with S0h, the leftmost and rightmost modifiers of S0 (if any) with S0l and S0r, respectively, and the leftmost modifier of N0 (if any) with N0l, the baseline features are shown in Table 1.
For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ Various recent attempts have been made to include non-local features into graph-based dependency parsing (Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010).
For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ Moreover, we will see that label information can actually improve link accuracy.
For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. $$$$$ In our case, valency information is put into the context of the shift-reduce process, and used together with each action to give a score to the local decision.

Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model. $$$$$ Distance information has also been used in the easy-first parser of (Goldberg and Elhadad, 2010).
Our feature template is an extended version of the feature template of Zhang and Nivre (2011), originally developed for the arc-eager model. $$$$$ In Table 2, sl and sr stands for the set of labels on the left and right of the head, respectively.
