In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. $$$$$ Besides, we will extend this work to translating the top k parse trees, instead of committing to the 1-best tree, as parsing errors certainly affect translation quality.
In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. $$$$$ In addition, our translator also enjoys a speedup by this decoupling, with each of the two stages having a smaller search space.
In this framework, decoding without language model (LM decoding) is simply a linear-time depth-first search with memoization (Huang et al, 2006), since a tree of n words is also of size O (n) and we visit every node only once. $$$$$ We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.

Huang et al (2006) study a TSG-based tree-to-string alignment model. $$$$$ With the extended LHS of our transducer, there may be many different rules applicable at one tree node.
Huang et al (2006) study a TSG-based tree-to-string alignment model. $$$$$ Here we use the rule r3 for the passive construction: which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice.
Huang et al (2006) study a TSG-based tree-to-string alignment model. $$$$$ Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems.
Huang et al (2006) study a TSG-based tree-to-string alignment model. $$$$$ The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models.

Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance. $$$$$ We use a standard trigram model for Pr(c).
Liu et al (2006) and Huang et al (2006) then used the TTS transducer on the task of Chinese-to-English and English-to-Chinese translation, respectively, and achieved decent performance. $$$$$ However, the structural divergence across languages often results in nonisomorphic parse-trees that is beyond the power of SCFGs.

The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). $$$$$ The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models.
The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). $$$$$ There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005).
The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). $$$$$ We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus.
The formal description of a TTS transducer is describe din Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined in (Huang et al, 2006). $$$$$ Note that positive A prefers longer translations.

The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). $$$$$ Initial experimental results on English-to-Chinese translation are presented.
The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). $$$$$ The relationship among these concepts is illustrated in Fig.
The implementation of a TTS transducer can be done either top down with memoization to the visited subtrees (Huang et al, 2006), or with a bottom-up dynamic programming (DP) algorithm (Liu et al, 2006). $$$$$ We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.

To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template. $$$$$ Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation.
To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template. $$$$$ SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.
To speed up the decoding ,standard beam search is used. In Figure 3, BinaryCombine denotes the target size binarization (Huang et al, 2006) combination. The translation candidates of the template's variables, as well as its terminals, are combined pair wise in the order they appear in the RHS of the template. $$$$$ Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005).

Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Besides, we will extend this work to translating the top k parse trees, instead of committing to the 1-best tree, as parsing errors certainly affect translation quality.
Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length.
Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.
Huang et al (2006) used character-based BLEU as a way of normalizing in consistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).

Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.
Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ We apply stochastic operations at each node of the source-language parse-tree and search for the best derivation (a sequence of translation steps) that converts the whole tree into some target-language string with the highest probability.
Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side.
Huang et al (2006) used character-based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese.

Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. $$$$$ We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility.
Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. $$$$$ Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously.
Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. $$$$$ Rule r2, for instance, can be thought of as a phrase pair (the gunman, qiangshou).
Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al, 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. $$$$$ Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation.

The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ The rescored translations are better than the 1-best results from the direct model, but still slightly worse than Pharaoh.
The formal description of a TTS transducer is given by Graehl and Knight (2004), and our baseline approach follows the Extended Tree-to-String Transducer defined by Huang et al (2006). $$$$$ So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: This method should work in general for any equivalence relation (say, same derived tree) that can be defined on derivations.

It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). $$$$$ For example, rules r1 and r3 in Section 1 are both of rank 2.
It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). $$$$$ Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference.
It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). $$$$$ The results are summarized in Table 1.
It is straightforward to generalize the algorithm for larger n-gram models and TTS templates with any number of children in the bottom using target-side binarized combination (Huang et al, 2006). $$$$$ We now marginalize over all English parse trees T (e) that yield the sentence e: Rather than taking the sum, we pick the best tree T* and factors the search into two separate steps: parsing (4) (a well-studied problem) and tree-to-string translation (5) (Section 5): In this sense, our approach can be considered as a Viterbi approximation of the computationally expensive joint search using (3) directly.

Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT.
Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.
Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work).
Huang et al (2006) used character based BLEU as a way of normalizing inconsistent Chinese word segmentation, but we avoid this problem as the training, development, and test data are from the same source. $$$$$ Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules.

We push the idea behind this method further and make the following contributions in this paper $$$$$ We are also investigating more principled algorithms for integrating n-gram language models during the search, rather than k-best rescoring.
We push the idea behind this method further and make the following contributions in this paper $$$$$ We use a standard trigram model for Pr(c).

We test our methods on two large-scale English-to-Chinese translation systems $$$$$ Initial experimental results on English-to-Chinese translation are presented.
We test our methods on two large-scale English-to-Chinese translation systems $$$$$ Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive converter, so that the latter can have an extended domain of locality.
We test our methods on two large-scale English-to-Chinese translation systems $$$$$ 3 (c).
We test our methods on two large-scale English-to-Chinese translation systems $$$$$ Under the log-linear model, one still prefers to search for the globally best derivation d*: However, integrating the n-gram model with the translation model in the search is computationally very expensive.

Our data preparation follows Huang et al (2006) $$$$$ In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation.
Our data preparation follows Huang et al (2006) $$$$$ Figure 3 shows how the translator works.
Our data preparation follows Huang et al (2006) $$$$$ We devise a simple-yet-effective algorithm to non-duplicate translations rescoring.
Our data preparation follows Huang et al (2006) $$$$$ SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language.

We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. $$$$$ Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6.
We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. $$$$$ We devise a simple-yet-effective algorithm to non-duplicate translations rescoring.
We use the same test set as (Huang et al, 2006), which is a 140-sentence sub set of the NIST 2003 test set with 9-36 words on the English side. $$$$$ The relationship among these concepts is illustrated in Fig.

For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. $$$$$ Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference.
For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. $$$$$ This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT.
For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. $$$$$ Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference.
For cube growing, we use a non-duplicate k-best method (Huang et al, 2006) to get 100-best unique translations according to LM to estimate the lower-boundheuristics. This preprocessing step takes on aver age 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. $$$$$ Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules.

Compared with its string-based counterparts, tree-based decoding is simpler and faster $$$$$ In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable.
Compared with its string-based counterparts, tree-based decoding is simpler and faster $$$$$ Note that positive A prefers longer translations.
Compared with its string-based counterparts, tree-based decoding is simpler and faster $$$$$ 2.
Compared with its string-based counterparts, tree-based decoding is simpler and faster $$$$$ We use a standard trigram model for Pr(c).

Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). $$$$$ Initial experimental results on English-to-Chinese translation are presented.
Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). $$$$$ This paper presents an adaptation of the classic syntax-directed translation with linguisticallymotivated formalisms for statistical MT.
Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al, 2006), the source side (Huang et al, 2006), or both (Liu et al, 2009). $$$$$ Then we extract the top 5000 non-duplicate translated strings from this forest and rescore them with the trigram model and the length penalty.

The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006). $$$$$ Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c  |e).
The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006). $$$$$ Parameters a, Q, and A are the weights of relevant features.
The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006). $$$$$ The resulting rule set has 24.7M xRs rules.
The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al (2006). $$$$$ For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length.
