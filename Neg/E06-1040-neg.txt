In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. $$$$$ We plan to further investigate the performance of automatic evaluation measures in NLG in the future: (i) performing similar experiments to the one described here in other domains, and with more subjects and larger test sets; (ii) investigating whether automatic corpus-based techniques can evaluate content determination; (iii) investigating how well both human ratings and corpus-based measures correlate with extrinsic evaluations of the effectiveness of generated texts.
In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. $$$$$ We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.
In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. $$$$$ However, forecast readers dislike this use of later (because later is used to mean something else in a different type of forecast), and also dislike variants of by evening, because they are unsure how to interpret them (Reiter et al., 2005); this is why SUMTIME uses by midnight.
In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. $$$$$ Many thanks to John Carroll, Roger Evans and the anonymous reviewers for very helpful comments.

As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. $$$$$ Automatic metrics can be expected to correlate very highly with human judgments only if the reference texts used are of high quality, or rather, can be expected to be judged high quality by the human evaluators.
As we showed previously (Belz and Reiter, 2006) that there are significant inter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results. $$$$$ Ultimately, we would like to move beyond critiques of existing corpus-based metrics to proposing (and validating) new metrics which work well for NLG.

A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. $$$$$ We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.
A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. $$$$$ Anja Belz’s part of the research reported in this paper was supported under UK EPSRC Grant GR/S24480/01.
A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. $$$$$ We used approximate randomisation (AR) as our significance test, as recommended by Riezler and Maxwell III (2005).
A previous study by Belz and Reiter (2006) has demonstrated that automatic metrics can correlate highly with human ratings if the training dataset is of high quality. $$$$$ The SUMTIME system builders believe deviating from corpus frequency in such cases makes SUMTIME texts better from the reader’s perspective, and it does appear to increase human ratings of the system; but deviating from the corpus in such a way decreases the system’s score under corpus-similarity metrics.

Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006). $$$$$ Expert opinions can vary considerably, as shown by the low correlation among experts in our study (and as seen in corpus studies, e.g.
Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006). $$$$$ If an imperfect corpus is used as the gold standard for the automatic metrics, then high correlation with human judgments is less likely, and this may explain the difference in human and automatic scores for SUMTIME-Hybrid.
Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006). $$$$$ In our experiments, we have found NIST a more reliable evaluation metric than BLEU and in particular ROUGE which did not seem to offer any advantage over simple string-edit distance.
Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al, 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al, 2005), and persuasive effectiveness (Carenini and Moore, 2006). $$$$$ Anja Belz’s part of the research reported in this paper was supported under UK EPSRC Grant GR/S24480/01.

The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). $$$$$ We computed another correlation statistic (shown in brackets in Table 3) which measures how well scores by an arbitrary single human or run of a metric correlate with the average scores by a set of humans or runs of a metric.
The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). $$$$$ Using more reference texts does counteract this tendency, but only up to a point: no matter how many reference texts are used, there will still be one, or a small number of, most frequent variants, and using anything else will still worsen corpussimilarity scores.
The two automatic metrics used in the evaluations, NIST and BLEU have been shown to correlate highly with expert judgments (Pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006). $$$$$ The exceptions were the differences in NIST and SE scores for SUMTIMEHybrid/pCRU-roulette, and the difference in BLEU scores for SUMTIME-Hybrid/pCRU-2gram.

The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects. $$$$$ SUMTIME is a knowledge-based NLG system.
The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects. $$$$$ Pair-wise tests between results in Table 2 showed all but three differences to be significant with the likelihood of incorrectly rejecting the null hypothesis p < 0.05 (the standard threshold in NLP).
The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects. $$$$$ This is especially important when the generated texts are of similar quality to human-written texts.
The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects. $$$$$ These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination.

However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006). $$$$$ These give similar results, except that BLEU-1 and BLEU-2 rank pCRU-roulette as highly as the human judges.
However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006). $$$$$ As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable.
However, the important thing to keep in mind is that compression of a given sentence is a problem for which there are usually multiple solutions (Belz and Reiter, 2006). $$$$$ We conthat automatic evaluation of systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available.

This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. $$$$$ Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts.
This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. $$$$$ In the shorter term, we recommend that automatic evaluations of NLG systems be supported by conventional large-scale human-based evaluations.
This suggests that, for this generation task, the data in the corpus can indeed be treated as a gold standard - unlike, for example, the corpus used by Belz and Reiter (2006), where the human judges sometimes preferred generated output to the corpus data. $$$$$ Ultimately, we would like to move beyond critiques of existing corpus-based metrics to proposing (and validating) new metrics which work well for NLG.

Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). $$$$$ However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.
Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). $$$$$ Metrics that do not exclusively reward similarity with reference texts (such as NIST) are more likely to correlate well with human judges, but all of the existing metrics that we looked at still penalised generators that do not always choose the most frequent variant.
Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). $$$$$ In both Tables 2 and 4, there are two major differences between the rankings assigned by human and automatic evaluation: (i) Human evaluators prefer SUMTIME-Hybrid over pCRU-greedy, whereas all the automatic metrics have it the other way around; and (ii) human evaluators score pCRU-roulette highly (second and third respectively), whereas the automatic metrics score it very low, second worst to random generation (except for NIST which puts it second).
Belz and Reiter (2006) and Reiter and Belz (2009) describe comparison experiments between the automatic evaluation of system output and human (expert and non-expert) evaluation of the same data (English weather forecasts). $$$$$ This base generator was then trained on 9/10 of the corpus (the training data).

While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. $$$$$ At the start, subjects were shown two practice examples.
While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. $$$$$ Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems.
While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. $$$$$ Experts are better predictors for each other’s judgments (0.799) than non-experts (0.609).
While it is found that BLEU and NIST correlate quite well with human judgments in evaluating NLG systems (Belz and Reiter, 2006), it is best to support these figures with human evaluation, which we did on a small scale. $$$$$ We plan to further investigate the performance of automatic evaluation measures in NLG in the future: (i) performing similar experiments to the one described here in other domains, and with more subjects and larger test sets; (ii) investigating whether automatic corpus-based techniques can evaluate content determination; (iii) investigating how well both human ratings and corpus-based measures correlate with extrinsic evaluations of the effectiveness of generated texts.

The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson's r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006). $$$$$ Anja Belz’s part of the research reported in this paper was supported under UK EPSRC Grant GR/S24480/01.
The two automatic metrics used in the evaluations, NIST2 and BLEU3, have been shown to correlate well with expert judgments (Pearson's r= 0.82 and 0.79 respectively) in the SUMTIME domain (Belz and Reiter, 2006). $$$$$ Automatic evaluations: We used NIST2, BLEU3, and ROUGE4 to automatically evaluate the above systems and texts.

When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. $$$$$ The strongest correlation with experts and nonexperts is achieved by NIST-5 (0.82 and 0.83), with ROUGE-4 and SE showing especially poor correlation.
When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. $$$$$ We also found individual experts’ judgments are not likely to correlate highly with average expert opinion, in fact less likely than NIST scores.
When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. $$$$$ Properly calculated BLEU scores have been shown to correlate reliably with human judgments (Papineni et al., 2002).
When Belz and Reiter (2006) performed a similar study comparing natural-language generation systems that used different text-planning strategies, they also found similar results: automated measures tended to favour majority-choice strategies, while human judges preferred those that made weighted choices. $$$$$ If an imperfect corpus is used as the gold standard for the automatic metrics, then high correlation with human judgments is less likely, and this may explain the difference in human and automatic scores for SUMTIME-Hybrid.

it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). $$$$$ The experts rank the corpus forecasts fourth, the non-experts second.
it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). $$$$$ All ranks are shown in brackets behind the absolute scores.
it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). $$$$$ Although the automatic evaluation metrics generally replicated human judgments fairly well when comparing different statistical NLG systems, there was a discrepancy in the ranking of pCRUroulette (ranked high by humans, low by several of the automatic metrics). pCRU-roulette differs from the other statistical generators because it does not always try to make the most common choice (maximise the likelihood of the corpus), instead it tries to vary choices.
it may be that subjects actually prefer the generated facial displays to the displays in the corpus, as was found by Belz and Reiter (2006). $$$$$ Automatic metrics can be expected to correlate very highly with human judgments only if the reference texts used are of high quality, or rather, can be expected to be judged high quality by the human evaluators.

Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. $$$$$ We consider the evaluation problem in Language Generation and results for evaluating several systems with similar functionality, including a knowledge-based generator and several statistical systems.
Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. $$$$$ We consider the evaluation problem in Language Generation and results for evaluating several systems with similar functionality, including a knowledge-based generator and several statistical systems.
Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. $$$$$ Ultimately, we would like to move beyond critiques of existing corpus-based metrics to proposing (and validating) new metrics which work well for NLG.
Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements. $$$$$ 5 different random divisions of the corpus into training and testing data were used (i.e. all results were validated by 5-fold hold-out cross-validation).

The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005). $$$$$ Human scores are normalised to range 0 to 1.
The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005). $$$$$ BLEU’s ability to detect subtle but important differences in translation quality has been questioned, some research showing NIST to be more sensitive (Doddington, 2002; Riezler and Maxwell III, 2005).
The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005). $$$$$ Evaluation is becoming an increasingly important topic in Natural Language Generation (NLG), as in other fields of computational linguistics.
The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al, 2005). $$$$$ We also computed NIST-5 and ROUGE-4.
