Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. $$$$$ We also compare our performance against other published results that use different training and testing sections.
Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. $$$$$ The rest of this section considers various combinations of feature cutoffs and Gaussian smoothing.
Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. $$$$$ The algorithm is as follows, where E p f, is the empirical expected value of J and E p fi is the expected value according to model p: In practice C is maximised over the (x, y) pairs in the training data, although in theory C can be any constant greater than or equal to the figure in (8).
Implementations of GIS typically use a correction feature, but following Curran and Clark (2003) we do not use such a feature, which simplifies the algorithm. $$$$$ We also performed 10-fold cross-validation using MXPOST and TNT, a publicly available Markov model PO S tagger (Brants, 2000).

Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003). $$$$$ The tagger returns the most probable sequence for the sentence.
Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003). $$$$$ Note that the unseen word-tag pairs do not include the previously unseen words.
Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003). $$$$$ The parameters o-, are usually collapsed into one parameter which can be set using heldout data.
Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases, using the C & C POS tagger (Curran and Clark, 2003). $$$$$ Achieving optimal performance with Gaussian smoothing and without cutoffs demonstrates that low frequency features can contribute to good performance.

 $$$$$ For the supertagger, t, is the lexical category of the ith word.
 $$$$$ We also investigate how the use of a correction feature affects the performance of ME taggers.
 $$$$$ Several methods have been proposed for smoothing ME models (see Chen and Rosenfeld (1999)).
 $$$$$ This paper investigates two elements of Maximum Entropy tagging: the use of a correction feature in the Generalised Iterative Scaling (Gis) estimation algorithm, and techniques for model smoothing.

When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). $$$$$ The best performance (in row 1) is obtained when the cutoffs are eliminated entirely.
When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). $$$$$ Finding the maximum entropy model that satisfies these constraints is a constrained optimisation problem, which can be solved using the method of Lagrange multipliers, and leads to the form in (1) where the Ai are the Lagrange multipliers.
When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). $$$$$ When using a Gaussian prior, the objective function is no longer the likelihood, L(A), but has the form: 2oMaximising this function is a form of maximum a posteriori estimation, rather than maximum likelihood estimation.
When compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). $$$$$ The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &quot;supertagger&quot;, which assigns tags from the much larger set of lexical types from Combinatory Categorial Grammar (ccG) (Clark, 2002).

Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al, 1994). $$$$$ The obvious cost associated with retaining all the features is the significant increase in model size, which slows down both the training and tagging and requires more memory.
Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al, 1994). $$$$$ We also investigate how the use of a correction feature affects the performance of ME taggers.
Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al, 1994). $$$$$ We would like to thank Joshua Goodman, Miles Osborne, Andrew Smith, Hanna Wallach, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.

Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger. $$$$$ We would like to thank Joshua Goodman, Miles Osborne, Andrew Smith, Hanna Wallach, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.
Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger. $$$$$ ).
Table 1 lists the contextual predicates used in our baseline system, which are based on those used in the Curran and Clark (2003) CCG supertagger. $$$$$ CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).

The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). $$$$$ This feature does not contribute to the model and can be ignored during weight update.
The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). $$$$$ We test this for ME taggers by replacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999).
The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). $$$$$ The supertagger assigns CCG lexical categories (Steedman, 2000) which encode subcategorisation information.
The tagger is very similar to the Maximum Entropy POS tagger described in Curran and Clark (2003). $$$$$ We also explore the use of a Gaussian prior and a simple cutoff for smoothing.

Here we use the Maximum Entropy models described in Curran and Clark (2003). $$$$$ The results on section 00 and section 23 are given in Tables 11 and 12.4 c&c outperforms Clark's supertagger by 0.43% on the test set, a reduction in error rate of 4.9%.
Here we use the Maximum Entropy models described in Curran and Clark (2003). $$$$$ However, the current word must have appeared at least 5 times with any tag for the current word feature to be included; otherwise the word is considered rare and morphological features are included instead.
Here we use the Maximum Entropy models described in Curran and Clark (2003). $$$$$ The experiments are performed with two tagsets: standard Penn Treebank and the larger set of lexical types from
Here we use the Maximum Entropy models described in Curran and Clark (2003). $$$$$ Introducing this null feature still satisfies Jensen's inequality, which is used to provide a lower bound on the change in likelihood between iterations, and the existing GIS weight update (7) can still be derived analytically.

Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. $$$$$ Introducing this null feature still satisfies Jensen's inequality, which is used to provide a lower bound on the change in likelihood between iterations, and the existing GIS weight update (7) can still be derived analytically.
Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. $$$$$ Note that this new update rule is still significantly simpler than that required for 11s.
Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. $$$$$ Since this is not the case for many applications, the standard method is to add a &quot;correction&quot;, or &quot;slack&quot;, feature to each event Improved Iterative Scaling (us) (Berger et al., 1996; Della Pietra et al., 1997) eliminated the correction feature to improve the convergence rate of the algorithm.
Curran and Clark (2003) describes the model and explains how Generalised Iterative Scaling, together with a Gaussian prior for smoothing, can be used to set the weights. $$$$$ As well as evaluating the overall accuracy of the taggers (Acc), we also calculate the accuracy on previously unseen words (UwoRD), previously unseen word-tag pairs (UTAG) and ambiguous words (AMB), that is, those with more than one tag over the testing, training and development datasets.

The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. $$$$$ This result is shown in the first row of Table 5.
The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. $$$$$ We report optimal results with respect to the smoothing parameter a, where a = No-2 and N is the number of training instances.
The supertagger in Curran and Clark (2003) finds the single most probable category sequence given the sentence, and uses additional features defined in terms of the previously assigned categories. $$$$$ This paper has demonstrated, both analytically and empirically, that GIS does not require a correction feature Eliminating the correction feature simplifies further the already very simple estimation algorithm.

The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger. $$$$$ We also explore the use of a Gaussian prior and a simple cutoff for smoothing.
The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger. $$$$$ Using maximum entropy for text classification.
The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger. $$$$$ This may further boost supertagger performance.
The table gives results for gold standard POS tags and, in the final 2 columns, for POS tags automatically assigned by the Curran and Clark (2003) tagger. $$$$$ We have also shown that using a Gaussian prior on the parameters of the ME model improves performance over a simple frequency cutoff.

The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. $$$$$ We reimplemented Ratnaparkhi's publicly available POS tagger MXPOST (Ratnaparkhi, 1996; Ratnaparkhi, 1998) and Clark's CCG supertagger (Clark, 2002) as a starting point for our experiments.
The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. $$$$$ This form can be derived by choosing the model with maximum entropy (i.e. the most uniform model) from a set of models that satisfy a certain set of constraints.
The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. $$$$$ We have also shown that using a Gaussian prior on the parameters of the ME model improves performance over a simple frequency cutoff.
The CCG parser results are based on automatically assigned POS tags, using the Curran and Clark (2003) tagger. $$$$$ Supertagging has the potential to benefit more from Gaussian smoothing than POS tagging because the feature space is sparser by virtue of the much larger tagset.

We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers $$$$$ CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).
We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers $$$$$ We also explore the use of a Gaussian prior and a simple cutoff for smoothing.
We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers $$$$$ We test this for ME taggers by replacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999).
We investigate whether co-training based upon directly maximising agreement can be successfully applied to a pair of part-of-speech (POS) taggers $$$$$ For the remainder of the experiments we use neither the correction nor the default features.

The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches. $$$$$ This research is supported by a Commonwealth scholarship and a Sydney University Travelling scholarship to the first author, and EPSRC grant GR/M96889.
The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches. $$$$$ Our supertagger used the same configuration as our best performing POS tagger, except that the a parameter was again optimised on the development set.
The modern Bible is tagged using the C & C maximum entropy tagger (Curran and Clark, 2003), and these tags are transferred from source to target through high-confidence alignments aquired from two alignment approaches. $$$$$ Table 3 shows the number of sentences and words in the training, development and test datasets.

The C & C tagger (Curran and Clark, 2003) was trained on the Wall Street Journal texts in the Penn Treebank and then used to tag the NET Bible (the source text). $$$$$ CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).
The C & C tagger (Curran and Clark, 2003) was trained on the Wall Street Journal texts in the Penn Treebank and then used to tag the NET Bible (the source text). $$$$$ We test this for ME taggers by replacing the cutoff with the use of a Gaussian prior, a technique which works well for language models (Chen and Rosenfeld, 1999).
The C & C tagger (Curran and Clark, 2003) was trained on the Wall Street Journal texts in the Penn Treebank and then used to tag the NET Bible (the source text). $$$$$ Our supertagger used the same configuration as our best performing POS tagger, except that the a parameter was again optimised on the development set.

However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate $$$$$ (12) word(x) = the is an example of what Ratnaparkhi calls a contextual predicate.
However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate $$$$$ The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant.
However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate $$$$$ A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998).
However, it is unclear whether multi-POS tagging will be useful in this context, since our single-tagger POS tagger is highly accurate $$$$$ The Gaussian prior effectively relaxes the constraints on the ME model, which allows the model to use low frequency features without overfitting.

Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). $$$$$ Table 1 gives some examples.
Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). $$$$$ This research is supported by a Commonwealth scholarship and a Sydney University Travelling scholarship to the first author, and EPSRC grant GR/M96889.
Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). $$$$$ In doing so, we discovered a number of minor variations from Ratnaparkhi (1998): MXPOST uses a cutoff of 1 for the current word feature and 5 for other features.
Part-of-speech (POS) tagging is done using the C & C tagger (Curran and Clark, 2003a) and lemmatisation is done using morpha (Minnen et al, 2000). $$$$$ We would like to thank Joshua Goodman, Miles Osborne, Andrew Smith, Hanna Wallach, Tara Murphy and the anonymous reviewers for their comments on drafts of this paper.

We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b). $$$$$ The improvement is 0.22% overall (a reduction in error rate of 7.5%) and 1.58% for unknown words (a reduction in error rate of 9.7%).
We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b). $$$$$ The best performance (in row 1) is obtained when the cutoffs are eliminated entirely.
We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b). $$$$$ The experiments are performed with two tagsets: standard Penn Treebank and the larger set of lexical types from
We use both rule-based and machine-learning named entity recognition (NER) components, the former implemented using LT-TTT2 and the latter using the C & C maximum entropy NER tagger (Curran and Clark, 2003b). $$$$$ Using maximum entropy for text classification.

We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003). $$$$$ The features used by each tagger are binary valued, and pair a tag with various elements of the context; for example: fi(x ) = { 1 if word(x)= the & y = DT ,y
We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003). $$$$$ ).
We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003). $$$$$ This research is supported by a Commonwealth scholarship and a Sydney University Travelling scholarship to the first author, and EPSRC grant GR/M96889.
We determine weights for the features with a modified version of the Generative Iterative Scaling algorithm (Curran and Clark, 2003). $$$$$ The results on section 00 and section 23 are given in Tables 11 and 12.4 c&c outperforms Clark's supertagger by 0.43% on the test set, a reduction in error rate of 4.9%.

It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003). $$$$$ The original formulation of GIS (Darroch and Ratcliff, 1972) required the sum of the feature values for each event to be constant.
It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003). $$$$$ Note that the unseen word-tag pairs do not include the previously unseen words.
It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003). $$$$$ Table 3 shows the number of sentences and words in the training, development and test datasets.
It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003). $$$$$ CCG supertagging is more difficult than POS tagging because the set of &quot;tags&quot; assigned by the supertagger is much larger (398 in this implementation, compared with 45 POS tags).
