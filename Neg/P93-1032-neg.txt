Following Manning (1993), we empirically determined the value of p. $$$$$ Initial experiments suggest that this technique works at least as well as previously tried techniques, and yields a method that can learn all the possible subcategorization frames of verbs.
Following Manning (1993), we empirically determined the value of p. $$$$$ The learner presented here finds a subcategorization involving from for all but one of these 7 verbs (the exception being ferry which was fairly rare in the training corpus).
Following Manning (1993), we empirically determined the value of p. $$$$$ Next to each verb, listing just a subcategorization frame means that it appears in both the OALD and my subcategorization dictionary, a subcategorization frame preceded by a minus sign (—) means that the subcategorization frame only appears in the OALD, and a subcategorization frame preceded by a plus sign (+) indicates one listed only in my program's subcategorization dictionary (i.e., one that is probably wrong).15 The numbers are the number of cues that the program saw for each sub catframes.
Following Manning (1993), we empirically determined the value of p. $$$$$ The parser includes a simple NP recognizer (parsing determiners, possessives, adjectives, numbers and compound nouns) and various other rules to recognize certain cases that appeared frequently (such as direct quotations in either a normal or inverted, quotation first, order).

Our local language model approach also bears some resemblance to statistical approaches to modeling sub categorization frames (Manning, 1993). $$$$$ More recently, Brent (1992) substituted a very simple heuristic method to detect verbs (anything that occurs both with and without the suffix -ing in the text is taken as a potential verb, and every potential verb token is taken as an actual verb unless it is preceded by a determiner or a preposition other than to.4 This is a rather simplistic and inadequate approach to verb detection, with a very high error rate.
Our local language model approach also bears some resemblance to statistical approaches to modeling sub categorization frames (Manning, 1993). $$$$$ This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora.
Our local language model approach also bears some resemblance to statistical approaches to modeling sub categorization frames (Manning, 1993). $$$$$ Initial experiments suggest that this technique works at least as well as previously tried techniques, and yields a method that can learn all the possible subcategorization frames of verbs.
Our local language model approach also bears some resemblance to statistical approaches to modeling sub categorization frames (Manning, 1993). $$$$$ The learner presented here finds a subcategorization involving from for all but one of these 7 verbs (the exception being ferry which was fairly rare in the training corpus).

Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus. $$$$$ The method used for filtering is that suggested by Brent (1992).
Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus. $$$$$ Since the system now makes integral use of a high-error-rate component,6 it makes little sense for other components to be exceedingly selective about which data they use in an attempt to avoid as many errors as possible.
Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus. $$$$$ Nevertheless, this size already compares favorably with the size of some production MT systems (for example, the English dictionary for Siemens' METAL system lists about 2500 verbs (Adriaens and de Braekeleer 1992)).
Our work differs from corpus-based work such as Manning (1993) or Kawahara and Kurohashi (2001) in that we are using existing lexical resources rather than a corpus. $$$$$ For a parsing system, the current subcategorization dictionary could probably be incorporated as is, since the utility of the increase in coverage would almost undoubtedly outweigh problems arising from the incorrect subcategorization frames in the dictionary.

 $$$$$ Rule-based parsers use subcategorization information to constrain the number of analyses that are generated.
 $$$$$ No error rates for running the system on untagged text were given and no recall figures were given for either system.
 $$$$$ Many of the more obscure subcategorizations for less common verbs never appeared in the modest-sized learning corpus, so the model had no chance to master them.'

Techniques for the automatic acquisition of subcategofization dictionaries have been developed by Manning (1993), Bfiscoe and Carroll (1997) and Carroll and Rooth (1998). $$$$$ In sentence (2), John is an argument and in the bathroom is an adjunct: (2) Mary berated John in the bathroom.
Techniques for the automatic acquisition of subcategofization dictionaries have been developed by Manning (1993), Bfiscoe and Carroll (1997) and Carroll and Rooth (1998). $$$$$ The aim in choosing error bounds for the filtering procedure was to get a highly accurate dictionary at the expense of recall, and the lower bound precision figure of 90% suggests that this goal was achieved.
Techniques for the automatic acquisition of subcategofization dictionaries have been developed by Manning (1993), Bfiscoe and Carroll (1997) and Carroll and Rooth (1998). $$$$$ NPINF Direct object and infinitive clause ING Takes a participial VP complement P (prep) Prepositional phrase headed by prep NP_P (prep) Direct object and PP headed by prep subcategorization frame was not listed in the Cobuild dictionary (Sinclair 1987).

This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al 1993), from parsed corpora such as Marcus et al's (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994). $$$$$ Hence I have used high bounds on the probability of cues being false cues for certain triggers (the used values range from 0.25 (for Tv-P( of )) to 0.02).
This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al 1993), from parsed corpora such as Marcus et al's (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994). $$$$$ Brent and Berwick (1991) took the approach of trying to generate very high precision data.2 The input was hand-tagged text from the Penn Treebank, and they used a very simple finite state parser which ignored nearly all the input, but tried to learn from the sentences which seemed least likely to contain false triggers — mainly sentences with pronouns and proper names.3 This was a consistent strategy which produced promising initial results.
This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al 1993), from parsed corpora such as Marcus et al's (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994). $$$$$ 1719), and are thus higher than the true recall rates from the corpus (observe in Table 3 that no cues were generated for infrequent verbs or subcategorization patterns).
This can be done automatically with unparsed corpora (Briscoe and Carroll 1997, Manning 1993, Ushioda et al 1993), from parsed corpora such as Marcus et al's (1993) Treebank (Merlo 1994, Framis 1994) or manually as was done for COMLEX (Macleod and Grishman 1994). $$$$$ I have done some preliminary work to answer these questions.

Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998). $$$$$ Arguments fill semantic slots licensed by a particular verb, while adjuncts provide information about sentential slots (such as time or place) that can be filled for any verb (of the appropriate aspectual type).
Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998). $$$$$ However, for many subcategorizations there just are no highly accurate cues.7 For example, some verbs subcategorize for the preposition in, such as the ones shown in (3): There just is no high accuracy cue for verbs that subcategorize for in.
Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998). $$$$$ Some idea of the growth of the subcategorization dictionary can be had from Table 1.
Our eventual goal is to develop a set of regular expressions that work on fiat tagged corpora instead of TreeBank parsed structures to allow us to gather information from larger corpora than have been done by the TreeBank project (see Manning 1993 and Gahl 1998). $$$$$ A subcategorization frame is a statement of what types of syntactic arguments a verb (or adjective) takes, such as objects, infinitives, thatclauses, participial clauses, and subcategorized prepositional phrases.

This shows to which extent the range of arguments is fine grained, in contrast to other works where the range is at the categorial level, such as NP or PP (M. Brent 1993, C. Manning 1993, P. Merlo & M. Leybold 2001). $$$$$ The program acquired a dictionary of 4900 subcategorizations for 3104 verbs (an average of 1.6 per verb).
This shows to which extent the range of arguments is fine grained, in contrast to other works where the range is at the categorial level, such as NP or PP (M. Brent 1993, C. Manning 1993, P. Merlo & M. Leybold 2001). $$$$$ Rule-based parsers use subcategorization information to constrain the number of analyses that are generated.
This shows to which extent the range of arguments is fine grained, in contrast to other works where the range is at the categorial level, such as NP or PP (M. Brent 1993, C. Manning 1993, P. Merlo & M. Leybold 2001). $$$$$ Further, it is argued that this method can be used to learn all subcategorization frames, whereas previous methods are not extensible to a general solution to the problem.
This shows to which extent the range of arguments is fine grained, in contrast to other works where the range is at the categorial level, such as NP or PP (M. Brent 1993, C. Manning 1993, P. Merlo & M. Leybold 2001). $$$$$ The program doesn't yet have a good way of representing classes of prepositions.

C. Manning (1993) presents the acquisition of sub categorization frames from unlabelled text corpora. $$$$$ After establishing that it is desirable to be able to automatically induce the subcategorization frames of verbs, this paper examined a new technique for doing this.
C. Manning (1993) presents the acquisition of sub categorization frames from unlabelled text corpora. $$$$$ Thus there is a need for a program that can acquire a subcategorization dictionary from on-line corpora of unrestricted text: and easily as different usages develop.
C. Manning (1993) presents the acquisition of sub categorization frames from unlabelled text corpora. $$$$$ For a parsing system, the current subcategorization dictionary could probably be incorporated as is, since the utility of the increase in coverage would almost undoubtedly outweigh problems arising from the incorrect subcategorization frames in the dictionary.
C. Manning (1993) presents the acquisition of sub categorization frames from unlabelled text corpora. $$$$$ A major bottleneck in the production of highcoverage parsers is assembling lexical information, °Thanks to Julian Kupiec for providing the tagger on which this work depends and for helpful discussions and comments along the way.

 $$$$$ In the mezzanine, a man came with two sons and one baseball glove, like so many others there, in case, [p (with)] OK iv of course, a foul ball was hit to them.
 $$$$$ In the mezzanine, a man came with two sons and one baseball glove, like so many others there, in case, [p (with)] OK iv of course, a foul ball was hit to them.
 $$$$$ Arguments fill semantic slots licensed by a particular verb, while adjuncts provide information about sentential slots (such as time or place) that can be filled for any verb (of the appropriate aspectual type).

The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus,), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. $$$$$ Languages that have free word order employ either case markers or agreement affixes on the head to mark arguments.
The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus,), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. $$$$$ It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.
The same year, (Manning, 1993) used 4 million words of the New York Times (Sandhaus,), selected only clauses with auxiliary verbs and automatically analyzed them with a finite-state parser. $$$$$ Dictionaries produced by hand always substantially lag real language use.

Increasingly, tools are also becoming available for acquiring sub categorization information from corpora, i.e. for inferring the sub categorization frames of a given lemma (e.g. Manning 1993). $$$$$ The applications of this system are fairly obvious.
Increasingly, tools are also becoming available for acquiring sub categorization information from corpora, i.e. for inferring the sub categorization frames of a given lemma (e.g. Manning 1993). $$$$$ One month (approximately 4 million words) of the New York Times newswire was tagged using a version of Julian Kupiec's stochastic part-of-speech tagger (Kupiec 1992).1° Subcategorization learning was then performed by a program that processed the output of the tagger.
Increasingly, tools are also becoming available for acquiring sub categorization information from corpora, i.e. for inferring the sub categorization frames of a given lemma (e.g. Manning 1993). $$$$$ In this work I will use a stochastic part-of-speech tagger to detect verbs (and the part-of-speech of other words), and will suggest that this gives much better results.6 Leaving this aside, moving to either this last approach of Brent's or using a stochastic tagger undermines the consistency of the initial approach.
Increasingly, tools are also becoming available for acquiring sub categorization information from corpora, i.e. for inferring the sub categorization frames of a given lemma (e.g. Manning 1993). $$$$$ However, using hand-tagged text is clearly not a solution to the knowledge acquisition problem (as hand-tagging text is more laborious than collecting subcategorization frames), and so, in more recent papers, Brent has attempted learning subcategorizations from untagged text.

Both Brcnt (1993) and Manning (1993), who attempt to induce a lexicon of sub categorization features do so by completely discarding all preexisting knowledge; both systems are stand-ahmc, without a parsing engine to test or use the "learned" information. $$$$$ After establishing that it is desirable to be able to automatically induce the subcategorization frames of verbs, this paper examined a new technique for doing this.
Both Brcnt (1993) and Manning (1993), who attempt to induce a lexicon of sub categorization features do so by completely discarding all preexisting knowledge; both systems are stand-ahmc, without a parsing engine to test or use the "learned" information. $$$$$ Initial experiments suggest that this technique works at least as well as previously tried techniques, and yields a method that can learn all the possible subcategorization frames of verbs.
Both Brcnt (1993) and Manning (1993), who attempt to induce a lexicon of sub categorization features do so by completely discarding all preexisting knowledge; both systems are stand-ahmc, without a parsing engine to test or use the "learned" information. $$$$$ I am also indebted for comments on an earlier draft to Marti Hearst (whose comments were the most useful!

 $$$$$ For disambiguating whether a PP is subcategorized by a verb in the V NP PP environment, Hindle and Rooth (1991) used a t-score to determine whether the PP has a stronger association with the verb or the preceding NP.
 $$$$$ This paper presents a new method for producing a dictionary of subcategorization frames from unlabelled text corpora.
 $$$$$ It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.
 $$$$$ NPINF Direct object and infinitive clause ING Takes a participial VP complement P (prep) Prepositional phrase headed by prep NP_P (prep) Direct object and PP headed by prep subcategorization frame was not listed in the Cobuild dictionary (Sinclair 1987).

(Manning, 1993) observes that Brent's recognition technique is a rather simplistic and inadequate approach to verb detection, with a very high error rate. $$$$$ After establishing that it is desirable to be able to automatically induce the subcategorization frames of verbs, this paper examined a new technique for doing this.
(Manning, 1993) observes that Brent's recognition technique is a rather simplistic and inadequate approach to verb detection, with a very high error rate. $$$$$ The paper showed that the technique of trying to learn from easily analyzable pieces of data is not extendable to all subcategorization frames, and, at any rate, the sparseness of appropriate cues in unrestricted texts suggests that a better strategy is to try and extract as much (noisy) information as possible from as much of the data as possible, and then to use statistical techniques to filter the results.
(Manning, 1993) observes that Brent's recognition technique is a rather simplistic and inadequate approach to verb detection, with a very high error rate. $$$$$ I will assume without discussion a fairly standard categorization of subcategorization frames into 19 classes (some parameterized for a preposition), a selection of which are shown below:

(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates. $$$$$ The learner presented here finds a subcategorization involving from for all but one of these 7 verbs (the exception being ferry which was fairly rare in the training corpus).
(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates. $$$$$ For a parsing system, the current subcategorization dictionary could probably be incorporated as is, since the utility of the increase in coverage would almost undoubtedly outweigh problems arising from the incorrect subcategorization frames in the dictionary.
(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates. $$$$$ The paper showed that the technique of trying to learn from easily analyzable pieces of data is not extendable to all subcategorization frames, and, at any rate, the sparseness of appropriate cues in unrestricted texts suggests that a better strategy is to try and extract as much (noisy) information as possible from as much of the data as possible, and then to use statistical techniques to filter the results.
(Briscoe and Carroll, 1997) observe that in the work of (Brent, 1993), (Manning, 1993) and (Ushioda et al, 1993), the maximum number of distinct sub categorization classes recognized is sixteen, and only Ushioda et al attempt to derive relative sub cat egorization frequency for individual predicates. $$$$$ It is shown that statistical filtering of the results of a finite state parser running on the output of a stochastic tagger produces high quality results, despite the error rates of the tagger and the parser.
