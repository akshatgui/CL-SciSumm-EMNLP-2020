Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. $$$$$ To evaluate this performance we use two different measurements: Firstly, we record how many correct word-pairs we have constructed.
Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.
Koehn and Knight (2002) combine a vector-space approach with other clues such as orthographic similarity and frequency. $$$$$ Combining the context and the spelling clues yields a significantly better result than using each clue by itself.

Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. $$$$$ However, they span different time periods and have a different orientation: the World Street Journal covers mostly business news, the German news wire mostly German politics.
Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. $$$$$ The important point here is that we have generated a similarity matrix, which we will use now to find new translation word pairs.
Unlike the noun-only test sets used in other studies, (e.g., Koehn and Knight (2002), Haghighi et al (2008)), TS1000 also contains adjectives and verbs. $$$$$ They might also provide useful context information that is beneficial to building a noun lexicon.

Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. $$$$$ From these, some high-quality lexical entries can be learned, but there will always be many words that are missing.
Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. $$$$$ This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora.
Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. $$$$$ These methods may be also useful given a different starting point: For efforts in building machine translation systems, some small parallel text should be available.
Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. $$$$$ Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.

This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. $$$$$ Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.
This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. $$$$$ The frequent financial terms in the English WSJ corpus (stock, bank, sales, etc.) are rather rare in the German corpus.
This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. $$$$$ The similarity and frequency clues, however, seem to be too imprecise to pinpoint the search to the correct translations.
This setting has been considered before, most notably in Koehn and Knight (2002) and Fung (1995), but the current paper is the first to use a probabilistic model and present results across a variety of language pairs and data conditions. $$$$$ For all experiments the starting point was the seed lexicon of 1339 identical spelled words described in Section 2.1. which achieve 15.8% Corpus score.

Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. $$$$$ Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.
Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. $$$$$ This is done by checking the generated wordpairs against an existing bilingual lexicon.4 In essence, we try to recreate this lexicon, which contains 9,206 distinct German and 10,645 distinct English nouns and 19,782 lexicon entries.

Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. $$$$$ Some of this work has been re-implemented and is freely available for research purposes [AlOnaizan et al., 1999].
Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. $$$$$ For instance, Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag.
Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. $$$$$ Context, as we understand it here, is defined by the frequencies of context words in surrounding positions.
Also, as in Koehn and Knight (2002), we make use of a seed lexicon, which consists of a small, and perhaps incorrect, set of initial translation pairs. $$$$$ Table 4 illustrates the situation with our corpora.

The second method is to heuristically induce, where applicable, a seed lexicon using edit distance, as is done in Koehn and Knight (2002). $$$$$ Again, as in Section 2.2, we have to compute all possible word – or context vector – matches.

In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. $$$$$ Intuitively it is obvious that pairs of words that are similar in one language should have translations that are similar in the other language.
In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. $$$$$ According to Google,2 the word directory occurs 61 million times, empathy 383,000 times, and reflex 787,000 times.
In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. $$$$$ In all other cases, these scores are not helpful.
In order to explore system robustness to heuristically chosen seed lexicons, we automatically extracted a seed lexicon similarly to Koehn and Knight (2002): we ran EDITDIST on EN-ES-D and took the top 100 most confident translation pairs. $$$$$ This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora.

However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl. $$$$$ However, they span different time periods and have a different orientation: the World Street Journal covers mostly business news, the German news wire mostly German politics.
However, we attempted to run an experiment as similar as possible in setup to Koehn and Knight (2002), using English Gigaword and German Europarl. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.

In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). $$$$$ They might also provide useful context information that is beneficial to building a noun lexicon.
In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). $$$$$ For experiments on training probabilistic translation lexicons from parallel corpora and similar tasks on the same test corpus, refer to our earlier work [Koehn and Knight, 2000, 2001].
In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). $$$$$ Various parallel texts have recently become available, mostly from government sources such as parliament proceedings (the Canadian Hansard, the minutes of the European parliament1) or law texts (from Hong Kong).
In this setting, our MCCA system yielded 61.7% accuracy on the 186 most confident predictions compared to 39% reported in Koehn and Knight (2002). $$$$$ This measure is trained, however, on parallel sentence-aligned text, which is not available here.

Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. $$$$$ We have attempted to learn a one-to-one translation lexicon purely from unrelated monolingual corpora.
Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. $$$$$ This local context has to be translated into the other language, and we can search the word with the most similar context.
Koehn and Knight (2002) describe few potential clues that may help in extracting bilingual lexicon from two monolingual corpora such as identical words, similar spelling, and similar context features. $$$$$ However, they span different time periods and have a different orientation: the World Street Journal covers mostly business news, the German news wire mostly German politics.

Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. $$$$$ The starting point for the corpus score is the 15.8% that are already achieved with the seed lexicon from Section 2.1.
Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. $$$$$ Specifically, we want to automatically generate a one-to-one mapping of German and English nouns.
Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. $$$$$ Both corpora are news sources in the general sense.
Koehn and Knight (2002) map 976 identical word pairs that are found in their two monolingual German-English corpora and report that 88.0 percent of them are correct. $$$$$ For a machine translation system, it is often more important to get more frequently used words right than obscure ones.

Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). $$$$$ Verbs, adjectives, adverbs and other part of speech may be tackled in a similar way.
Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). $$$$$ According to Google,2 the word directory occurs 61 million times, empathy 383,000 times, and reflex 787,000 times.
Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). $$$$$ We are testing our mappings against a bilingual lexicon of 9,206 German and 10,645 English nouns.
Koehn and Knight (2002) mention few related works that use different measurement to compute the similarity, such as longest common subsequence ratio (Melamed, 1995) and string edit distance (Mann 10 and Yarowski, 2001). $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.

However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.
However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.
However, Koehn and Knight (2002) point out that majority of their word pairs do not show much resemblance at all since they use German-English language pair. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.

Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. $$$$$ Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.
Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. $$$$$ Still, for most language pairs, parallel texts are hard to come by.
Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
Koehn and Knight (2002) present one interesting idea of using extracted cognate pairs from corpus as the seed words in order to alleviate the need of huge, initial bilingual lexicon. $$$$$ In the Hansard, each of these words occurs only once.

 $$$$$ This paper presents work on the task of constructing a word-level translation lexicon purely from unrelated monolingual corpora.
 $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
 $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.

Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. $$$$$ Due to computational constraints,6 we focus on the additional mapping of only 1,000 German and English words.
Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. $$$$$ Roughly speaking, SMT divides the task of translation into two steps: a word-level translation model and a model for word reordering during the translation process.
Koehn and Knight (2002) derived such a seed lexicon from German-English cognates which were selected by using string similarity criteria. $$$$$ A closer look of the spelling and context scores reveals that while the spelling clue allows to learn more correct lexicon entries (140 opposed to 107), the context clue does better with the more frequently used lexicon entries, as found in the test corpus (accuracy of 31.9% opposed to 25.4%).

Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. $$$$$ Experimental results for the construction of a German-English noun lexicon are reported.
Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. $$$$$ Both corpora are news sources in the general sense.
Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. $$$$$ Due to computational constraints,6 we focus on the additional mapping of only 1,000 German and English words.
Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. $$$$$ Noun translation accuracy of 39% scored against a parallel test corpus could be achieved.

Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. $$$$$ The seminal work by Brown et al. [1990] at IBM on the Candide system laid the foundation for much of the current work in Statistical Machine Translation (SMT).
Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. $$$$$ Beyond this, we examined four different clues.
Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. $$$$$ For each occurrence of a target word, counts are collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions.
Following Koehn and Knight (2002), we have also employed simple transformation rules for the adoption of words from one language to another. $$$$$ Using identically spelled words proved to be a good starting point.

The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage. $$$$$ They might also provide useful context information that is beneficial to building a noun lexicon.
The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage. $$$$$ They might also provide useful context information that is beneficial to building a noun lexicon.
The previous approach relying on work from Koehn and Knight (2002) has been outperformed in terms of precision and coverage. $$$$$ We combine various clues such as cognates, similar context, preservation of word similarity, and word frequency.
