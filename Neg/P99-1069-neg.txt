In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ As sample size grows, maximizing likelihood amounts to minimizing divergence.
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ Abney proposes using a Metropolis acceptancerejection method to adjust the distribution of this stream of feature structures to achieve detailed balance, which then produces a stream of feature structures distributed according to Pei (w).
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.

The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ Section 5 describes an experiment with a small LFG corpus provided to us by Xerox PARC.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ If a sentence has 1 most likely parses (i.e., all 1 parses have the same conditional probability) and one of these parses is the correct parse, then we score 1// for this sentence.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ Indeed, the number and choice of features strongly influences the performance of the model.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ In fact, in this case any value of 0i gives the same conditional distribution Po (w ) y(w)); 0i is irrelevant to the problem of choosing good parses.

The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ This metric seems more relevant to applications where the system needs to estimate how likely it is that the correct analysis lies in a certain set of possible parses; e.g., ambiguitypreserving translation and human-assisted disambiguation.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ C9(c73) is a highly discontinuous function of 0, and most conventional optimization algorithms perform poorly on it.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ Log-linear models are well-suited for lexical dependencies, but because of the large number of such dependencies substantially larger corpora will probably be needed to estimate such mo dels .1 'Alternatively, it may be possible to use a simpler non-SUBG model of lexical dependencies estimated from a much larger corpus as the reference distribution with parses of the test corpus that were the correct parses, and — log PL(E5test) is the negative logarithm of the pseudo-likelihood of the test corpus.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.

For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ This paper described a log-linear model for SUBGs and evaluated two estimators for such models.

To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ The pseudo-likelihood estimator described in the last section finds parameter values which maximize the conditional probabilities of the observed parses (syntactic analyses) given the observed sentences (yields) in the training corpus.
To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ Such infinite parameter values indicate that the model treats pseudo-maximal features categorically; i.e., any parse with a non-maximal feature value is assigned a zero conditional probability.
To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ In fact, in this case any value of 0i gives the same conditional distribution Po (w ) y(w)); 0i is irrelevant to the problem of choosing good parses.

The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ This procedure is much more computationally intensive than the gradient-based pseudo-likelihood procedure.
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ It is easy to see that if h is pseudo-maximal on each sentence of the training corpus then the parameter assignment 03 = oo maximizes the corpus pseudo-likelihood.

It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ As sample size grows, maximizing likelihood amounts to minimizing divergence.
It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.

A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ The simple &quot;relative frequency&quot; estimator for PCFGs yields the maximum likelihood parameter estimate, which is to say that it minimizes the Kulback-Liebler divergence between the training and estimated distributions.
A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ In fact, in this case any value of 0i gives the same conditional distribution Po (w ) y(w)); 0i is irrelevant to the problem of choosing good parses.
A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ Suppose = wi, , wr, is a training corpus of n syntactic analyses.
A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ Thus models that use production features alone predict that there should not be a systematic preference for one of these analyses over the other, contrary to standard psycholinguistic results.

For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ For any given UBG there are a large (usually infinite) number of SUBGs that can be constructed from it, differing only in the features that each SUBG uses.
For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ The pseudolikelihood estimator performed better than the correct-parses estimator on both corpora under both evaluation metrics.
For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ C9(c73) is a highly discontinuous function of 0, and most conventional optimization algorithms perform poorly on it.

Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ The unification constraints create non-local dependencies among the productions and the dependency graph of a SUBG is usually not a tree.
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ Log-linear models are well-suited for lexical dependencies, but because of the large number of such dependencies substantially larger corpora will probably be needed to estimate such mo dels .1 'Alternatively, it may be possible to use a simpler non-SUBG model of lexical dependencies estimated from a much larger corpus as the reference distribution with parses of the test corpus that were the correct parses, and — log PL(E5test) is the negative logarithm of the pseudo-likelihood of the test corpus.
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ The pseudo-likelihood estimator described in the last section finds parameter values which maximize the conditional probabilities of the observed parses (syntactic analyses) given the observed sentences (yields) in the training corpus.
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ Extending a PCFG model by adding additional features not associated with productions will in general add additional dependencies, destroy the tree structure, and substantially complicate maximum likelihood estimation.

As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ If a sentence has 1 most likely parses (i.e., all 1 parses have the same conditional probability) and one of these parses is the correct parse, then we score 1// for this sentence.
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Table 1 summarizes the basic properties of these corpora.
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ But the resulting estimation procedures (discussed in detail, shortly), albeit more complicated, have the virtue of applying to essentially arbitrary features—of the production or non-production type.

As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Because of the small size of our corpora we evaluated our estimators using a 10-way crossvalidation paradigm.
As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Extending a PCFG model by adding additional features not associated with productions will in general add additional dependencies, destroy the tree structure, and substantially complicate maximum likelihood estimation.
As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.

 $$$$$ They can provide a systematic treatment of preferences in parsing.
 $$$$$ We evaluated our estimators using held-out test corpus (7)test .
 $$$$$ For large n: where 00 is the true (and unknown) parameter vector.
 $$$$$ Its computational difficulty grows (and the quality of solutions degrade) rapidly with the number of features.

The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ They can provide a systematic treatment of preferences in parsing.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).

L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ Suppose there is a feature L which depends only on yields: f2(w) = fz(y(w)).
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ Specifying the features of a SUBG is as much an empirical matter as specifying the grammar itself.

When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ The estimated models are able to identify the correct parse from the set of all possible parses approximately 50% of the time.
When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear (1995).
When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.

 $$$$$ Linguistically there is no particular reason for assuming that productions are the best features to use in a stochastic language model.
 $$$$$ However, the Monte-Carlo parameter estimation procedure that Abney proposes seems to be computationally impractical for reasonable-sized grammars.
 $$$$$ If a sentence has 1 most likely parses (i.e., all 1 parses have the same conditional probability) and one of these parses is the correct parse, then we score 1// for this sentence.
 $$$$$ Log-linear models are models in which the log probability is a linear combination of feature values (plus a constant).

The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ However, the Monte-Carlo parameter estimation procedure that Abney proposes seems to be computationally impractical for reasonable-sized grammars.
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.

As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ For example, in a machine-assisted translation system a model like ours could be used to order possible translations so that more likely alternatives are presented before less likely ones.
As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ But the resulting estimation procedures (discussed in detail, shortly), albeit more complicated, have the virtue of applying to essentially arbitrary features—of the production or non-production type.
As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ These corpora contain packed c/fstructure representations (Maxwell III and Kaplan, 1995) of the grammatical parses of each sentence with respect to Lexical-Functional grammars.
As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.

This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ Because of the small size of our corpora we evaluated our estimators using a 10-way crossvalidation paradigm.
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ We would have liked to introduce features corresponding to dependencies between lexical items.
