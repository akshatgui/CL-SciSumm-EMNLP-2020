In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ The estimated models are able to identify the correct parse from the set of all possible parses approximately 50% of the time.
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ The evaluation scores from each subcorpus were summed in order to provide the scores presented here.
In previous work on log-linear models for LFG by Johnson et al (1999), pseudo likelihood estimation from annotated corpora has been introduced and experimented with on a small scale. $$$$$ This paper described a log-linear model for SUBGs and evaluated two estimators for such models.

The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ That is, since estimators capable of finding maximum-likelihood parameter estimates for production features in a SUBG will also find maximum-likelihood estimates for non-production features, there is no motivation for restricting features to be of the production type.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ However, there may be applications which can benefit from a model that performs even at this level.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ Samples are generated as follows: Given a SUBG, Abney constructs a covering PCFG based upon the SUBG and 0, the current estimate of 0.
The basic properties employed in our models are similar to the properties of Johnson et al (1999) which incorporate general linguistic principles into a log-linear model. $$$$$ What are the asymptotics of optimizing a pseudo-likelihood function?

The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ Its computational difficulty grows (and the quality of solutions degrade) rapidly with the number of features.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Johnson and Riezler (2000). $$$$$ We would have liked to introduce features corresponding to dependencies between lexical items.

For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ The simple &quot;relative frequency&quot; estimator for PCFGs yields the maximum likelihood parameter estimate, which is to say that it minimizes the Kulback-Liebler divergence between the training and estimated distributions.
For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ This is the situation for a SUBG, even if the features are production occurences.
For these we use the 13 feature schemas described in Charniak and Johnson (2005), which were inspired by earlier work in discriminative estimation techniques, such as Johnson et al (1999) and Collins (2000). $$$$$ In the ambiguity-preserving translation framework, a model like this one could be used to choose between sets of analyses whose ambiguities cannot be preserved in translation.

To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.
To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
To overcome this problem, we penalize the objective function by adding a Gaussian prior (a term proportional to the squared norm as suggested in (Johnson et al, 1999)). $$$$$ We randomly assigned sentences of each corpus into 10 approximately equal-sized subcorpora, each of which was used in turn as the test corpus.

The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ While it is possible to construct UBGs for which the number of possible parses is unmanageably high, for many grammars it is quite manageable to enumerate the set of possible parses and thereby directly evaluate Eo (f3(w)iy (w ) = yi ) .
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ Therefore, we propose replacing the gradient, (3), by and performing a gradient ascent.
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ We describe two computationally-tractable ways of estimating the parameters of such grammars from a training corpus of syntactic analyses, and apply these to estimate a stochastic version of Lexical
The method is related to the boosting approach to ranking problems (Freund et al. 1998), the Markov Random Field methods of (Johnson et al 1999), and the boosting approaches for parsing in (Collins 2000). $$$$$ Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.

It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ For example, the features of a PCFG are indexed by productions, i.e., the value f(w) of feature f, is the number of times the ith production is used in the derivation w. This set of features induces a tree-structured dependency graph on the productions which is characteristic of Markov Branching Processes (Pearl, 1988; Frey, 1998).
It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.
It does this using the re ranking methodology described in Collins (2000), using a Maximum Entropy model with Gaussian regularization as described in Johnson et al (1999). $$$$$ Sections 3 and 4 describe two new estimation procedures which are computationally tractable.

A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ C9(c73) is a highly discontinuous function of 0, and most conventional optimization algorithms perform poorly on it.
A Gaussian prior also handles the problem of "pseudo-maximal" features (Johnson et al, 1999). $$$$$ Table 1 summarizes the basic properties of these corpora.

For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ As for pseudo-likelihood: So that maximizing pseudo-likelihood (at large samples) amounts to minimizing the average (over yields) divergence between the true and estimated conditional distributions of analyses given yields.
For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.
For example, Johnson et al (1999) and Riezler et al (2002) use all parses generated by an LFG parser as input to an MRF approach given the level of ambiguity in natural language, this set can presumably become extremely large. $$$$$ Log-linear models are well-suited for lexical dependencies, but because of the large number of such dependencies substantially larger corpora will probably be needed to estimate such mo dels .1 'Alternatively, it may be possible to use a simpler non-SUBG model of lexical dependencies estimated from a much larger corpus as the reference distribution with parses of the test corpus that were the correct parses, and — log PL(E5test) is the negative logarithm of the pseudo-likelihood of the test corpus.

Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ But the gradient (in particular, E9(f3)) is intractable.
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ Berger et al. (1996) and Jelinek (1997) make this same point and arrive at the same estimator, albeit through a maximum entropy argument.
Following (Johnson et al, 1999), a conditional ME model of the probabilities of trees {t1 ... tn} for a string s, and assuming a set of feature functions {f1 ... fm} with corresponding weights {λ1 ... λm}, is defined as. $$$$$ The derivation trees of the PCFG can be mapped onto a set containing all of the SUBG's syntactic analyses.

As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ To make the numbers more manageable, we actually present the negative logarithm of the pseudo-likelihood rather than the pseudo-likelihood itself—so smaller is better.
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ These corpora contain packed c/fstructure representations (Maxwell III and Kaplan, 1995) of the grammatical parses of each sentence with respect to Lexical-Functional grammars.
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ In an actual parsing application a SUBG might be used to identify the correct parse from the set of grammatical parses, so ouorfitrestevaluation measure counts the number c(.5st) of sentences in the test corpus CA-hest whose maximum likelihood parse under the estimated model 0 is actually the correct parse.
As is now standard for feature-based grammars, we use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.

As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Every step of the proposed procedure (corresponding to a single step of gradient ascent) requires a very large number of PCFG samples: samples must be found that correspond to well-formed SUBGs; many such samples are required to bring the Metropolis algorithm to (near) equilibrium; many samples are needed at equilibrium to properly estimate E(f3).
As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ The estimated models are able to identify the correct parse from the set of all possible parses approximately 50% of the time.
As is now standard for feature-based grammars, we mainly use log-linear models for parse selection (Johnson et al, 1999). $$$$$ Its computational difficulty grows (and the quality of solutions degrade) rapidly with the number of features.

 $$$$$ In addition to production features, the stochastic LFG models evaluated below used the following kinds of features, guided by the principles proposed by Hobbs and Bear (1995).
 $$$$$ We would have liked to introduce features corresponding to dependencies between lexical items.

The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ However, there may be applications which can benefit from a model that performs even at this level.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ However, there may be applications which can benefit from a model that performs even at this level.
The most direct points of comparison of our method are the approaches of Johnson et al (1999) and Riezler et al (2000), esp. since they use the same evaluation criteria than we use. $$$$$ Abney (1997) proposes a Markov Random Field or log linear model for SUBGs, and the models described here are instances of Abney's general framework.

L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ With a small number of features the correct-parses estimator typically scores better than the pseudo-likelihood estimator on the correct-parses evaluation metric, but the pseudo-likelihood estimator always scores better on the pseudo-likelihood evaluation metric.
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ This procedure is much more computationally intensive than the gradient-based pseudo-likelihood procedure.
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ With a small number of features the correct-parses estimator typically scores better than the pseudo-likelihood estimator on the correct-parses evaluation metric, but the pseudo-likelihood estimator always scores better on the pseudo-likelihood evaluation metric.
L(w) is the 'conditionalized' likelihood of the training data X p (Johnson et al, 1999), computed as L (w)= lQ Ni=1 p w (r i js i). $$$$$ The corpora also indicate which of these parses is in fact the correct parse (this information was manually entered).

When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ For example, for a PCFG co would be parse tree, for a LFG co would be a tuple consisting of (at least) a c-structure, an fstructure and a mapping from c-structure nodes to f-structure elements, and for a Chomskyian transformational grammar co would be a derivation.
When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ However, the Monte-Carlo parameter estimation procedure that Abney proposes seems to be computationally impractical for reasonable-sized grammars.
When statistically modelling linguistic phenomena of one sort or another, researchers typically train log linear models to the data (for example (Johnson et al., 1999)). $$$$$ We evaluated on each subcorpus the parameters that were estimated from the 9 remaining subcorpora that served as the training corpus for this run.

 $$$$$ We would have liked to have included features concerning specific lexical items (to capture head-to-head dependencies), but we felt that our corpora were so small that the associated parameters could not be accurately estimated.
 $$$$$ Regardless of the pragmatic (computational) motivation, one could perhaps argue that the conditional probabilities Po(wly) are as useful (if not more useful) as the full probabilities Po(w), at least in those cases for which the ultimate goal is syntactic analysis.
 $$$$$ Probabilistic methods have revolutionized computational linguistics.
 $$$$$ On the other hand, as Abney (1997) points out, the context-sensitive dependencies that &quot;unification-based&quot; constraints introduce render the relative frequency estimator suboptimal: in general it does not maximize the likelihood and it is inconsistent.

The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ Because estimators that can estimate rule features for SUBGs can also estimate other kinds of features, there is no particular reason to limit attention to rule features in a SUBG.
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ There is a feature for non-parallel coordinate structures (where parallelism is measured in constituent structure terms).
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ The point is that Eo(f3(w)ly(w) yi) is generally computable.
The results we report are with the Gaussian prior regularization term described in (Johnson et al, 1999). $$$$$ We had the most success with a slightly modified version of the simulated annealing optimizer described in Press et al. (1992).

As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ One of the empirical evaluation measures we use in the next section measures the number of correct parses selected from the set of all possible parses.
As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ Because slightly different grammars were used for each corpus we chose not to combine the two corpora, although we used the set of features described in section 2 for both in the experiments described below.
As in Johnson et al (1999) we trained the model by maximizing the conditional likelihood of the preferred analyses and using a Gaussian prior for smoothing (Chen and Rosenfeld, 1999). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.

This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ Table 3 presents the results of the empirical evaluation.
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ The idea is to generate random samples of feature structures from the distribution PO(w), where 0 is the current parameter estimate, and to use these to estimate Eo(f3), and hence the gradient of the likelihood.
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ Log-linear models provide a statistically sound framework for Stochastic &quot;Unification-Based&quot; Grammars (SUBGs) and stochastic versions of other kinds of grammars.
This disambiguation decision seems to require common world knowledge or it might be addressable with addition of knowledge about parallel structures ((Johnson et al, 1999) add features measuring parallelism). $$$$$ The evaluation scores from each subcorpus were summed in order to provide the scores presented here.
