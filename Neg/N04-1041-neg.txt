We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). $$$$$ Passage retrieval is used in QA to supply relevant information to an answer pinpointing module.
We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). $$$$$ Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships.
We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). $$$$$ The passage retrieval module can make use of the hyponym relationships that are discovered by our system.
We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). $$$$$ Our method begins to address this thorny issue by quantifying the name assigned to a class and by simultaneously assigning a number that can be interpreted to reflect the strength of membership of each element to the class.

Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ The current state of the art discovers many semantic classes but fails to label their concepts.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ The authors wish to thank the reviewers for their helpful comments.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Our system shows small gains in the performance of the IR output.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Once labels are assigned to concepts, we can extract a hyponym relationship between each instance of a concept and its label.

Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. $$$$$ Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.
Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. $$$$$ We then added the mutual information scores for each extracted relationship among the 50 concepts.
Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. $$$$$ One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).
Our co-occurrence model (Pantel and Ravichandran 2004) makes use of semantic classes like those generated by CBC. $$$$$ We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.

These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. $$$$$ The current state of the art discovers many semantic classes but fails to label their concepts.
These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. $$$$$ A committee is a set of representative elements that unambiguously describe the members of a possible class.
These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. $$$$$ The authors wish to thank the reviewers for their helpful comments.
These relationships, automatically learned in (Pantel and Ravichandran 2004), include appositions, nominal subjects, such as relationships, and like relationships. $$$$$ Our system shows small gains in the performance of the IR output.

The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). $$$$$ For each expected semantic answer type corresponding to a given question (e.g. band and color), we indexed the entire TREC-2002 IR collection with our system's hyponyms.
The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). $$$$$ The Kappa statistic (Siegel and Castellan Jr. 1988) measures the agreements between a set of judges' assessments correcting for chance agreements: where P(A) is the probability of agreement between the judges and P(E) is the probability that the judges agree by chance on an assessment.
The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). $$$$$ The authors wish to thank the reviewers for their helpful comments.
The syntactical co-occurrence approach has worst-case time complexity O (n2k), where n is the number of words in the corpus and k is the feature space (Pantel and Ravichandran 2004). $$$$$ Such rare senses make it difficult for a coreference resolution system to use WordNet to enforce the constraint that personal pronouns (e.g. he or she) must refer to a person.

Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. $$$$$ The current state of the art discovers many semantic classes but fails to label their concepts.
Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. $$$$$ One approach constructs automatic thesauri by computing the similarity between words based on their distribution in a corpus (Hindle 1990; Lin 1998).
Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. $$$$$ This research was partly supported by NSF grant #EIA-0205111.
Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for low frequency observations. $$$$$ Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989).

Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. $$$$$ For each concept, we added to the list of names a human generated name (obtained from an annotator looking at only the concept instances).
Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. $$$$$ We propose here an algorithm for automatically labeling concepts that searches for syntactic patterns within a grammatical template for a class.
Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. $$$$$ We first construct a frequency count vector C(e) = (ce1, ce2, ï¿½, cem), where m is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in a grammatical context f. For example, if the word wave occurred 217 times as the object of the verb catch, then the feature vector for wave will have value 217 for its &quot;object-of catch&quot; feature.
Pantel and Ravichandran (2004) addressed the more specific task of labelling a semantic class by applying Hearst-style lexico-semantic patterns to each member of that class. $$$$$ For example, Figure 1 shows an excerpt of the grammatical signature for concept (B) in Section 1.

Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work. $$$$$ This allows CBC to discover the less frequent senses of a word and to avoid discovering duplicate senses.
Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work. $$$$$ This research was partly supported by NSF grant #EIA-0205111.
Pantel and Ravichandran (2004) have used a method that is not related to query expansion, but yet very related to our work. $$$$$ This research was partly supported by NSF grant #EIA-0205111.

 $$$$$ For example, in (D), the color and fruit senses of orange are mixed up.
 $$$$$ For each concept to which a word belongs, we extracted up to 3 hyponyms, one for each of the top-3 labels for the concept.

Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. $$$$$ Girju, Badulescu and Moldovan (2003) improved upon this work by using a machine learning filter.
Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. $$$$$ Table 5 shows that the hyponyms involving proper nouns are much more reliable than common nouns.
Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. $$$$$ Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.
Lin et al (2003) and Pantel and Ravichandran (2004) have proposed to classify the output of systems based on feature vectors using lexico-syntactic patterns, respectively in order to remove antonyms from a related words list and to name clusters of related terms. $$$$$ This may lead to better answer selections.

We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). $$$$$ Some applications such as question answering would benefit from class labels.
We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). $$$$$ Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.
We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). $$$$$ The authors wish to thank the reviewers for their helpful comments.
We also adopt the 'discount score' to penalize low occuring words (Pantel and Ravichandran, 2004). $$$$$ The output of the system is a ranked list of concept names for each semantic class.

Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. $$$$$ In Section 4.1, we describe how we obtain these features.
Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. $$$$$ This research was partly supported by NSF grant #EIA-0205111.
Few recent attempts on related (though different) tasks were made to classify (Lin et al, 2003) and label (Pantel and Ravichandran, 2004) distributional similarity output using lexical-syntactic patterns, in a pipeline architecture. $$$$$ In this paper, we propose an algorithm for automatically inducing names for semantic classes and for finding instance/concept (is-a) relationships.

First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. $$$$$ For each expected semantic answer type corresponding to a given question (e.g. band and color), we indexed the entire TREC-2002 IR collection with our system's hyponyms.
First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. $$$$$ For example: &quot;Who is Aaron Copland?&quot; and &quot;What is the Kama Sutra?&quot; For each question we looked for at most five corresponding concepts in our hyponym list.
First, instead of separately addressing the tasks of collecting unlabeled sets of instances (Lin, 1998), assigning appropriate class labels to a given set of instances (Pantel and Ravichandran, 2004), and identifying relevant attributes for a given set of classes (Pasca, 2007), our integrated method from Section 2 enables the simultaneous extraction of class instances, associated labels and attributes. $$$$$ It then proceeds by assigning elements to their most similar committees.

Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. $$$$$ With the advent of the Web, we have access to enormous amounts of text.
Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. $$$$$ The authors wish to thank the reviewers for their helpful comments.
Given pre-existing sets of instances, (Pantel and Ravichandran, 2004) investigates the task of acquiring appropriate class labels to the sets from unstructured text. $$$$$ Without being able to automatically name a cluster and extract hyponym/hypernym relationships, the utility of automatically generated clusters or manually compiled lists of terms is limited.

In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. $$$$$ The research discussed above on discovering hyponym relationships all take a bottom up approach.
In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. $$$$$ This research was partly supported by NSF grant #EIA-0205111.
In (Pantel and Ravichandran, 2004), given a collection of news articles that is both cleaner and smaller than Web document collections, a syntactic parser is applied to document sentences in order to identify and exploit syntactic dependencies for the purpose of selecting candidate class labels. $$$$$ The authors wish to thank the reviewers for their helpful comments.

Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ Systems that automatically discover semantic classes have emerged in part to address the limitations of broad-coverage lexical resources such as WordNet and Cyc.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ For example, if we labeled concept (A) from Section 1 with disease, then we could extract is-a relationships such as: diabetes is a disease, cancer is a disease, and lupus is a disease.
Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. $$$$$ There is a need for (semi-) automatic approaches to building and extending ontologies as well as for validating the structure and content of existing ones.

We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). $$$$$ We propose an algorithm labeling semantic and for leveraging them to extract relationships using a top-down approach.
We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). $$$$$ We propose here an algorithm for automatically labeling concepts that searches for syntactic patterns within a grammatical template for a class.
We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). $$$$$ WordNet also contains a very poor coverage of proper nouns.
We thus multiply pmi (i, p) with the discounting factor suggested in (Pantel and Ravichandran 2004). $$$$$ Passage retrieval is used in QA to supply relevant information to an answer pinpointing module.

(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. $$$$$ The research discussed above on discovering hyponym relationships all take a bottom up approach.
(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. $$$$$ There have also been several approaches to discovering hyponym (is-a) relationships from text.
(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. $$$$$ After assigning an element to a cluster, CBC removes their overlapping features from the element before assigning it to another cluster.
(Pantel and Ravichandran, 2004) have proposed an algorithm for labeling semantic classes, which can be viewed as a form of cluster. $$$$$ Using WordNet to expand queries to an information retrieval system, the expansion of computer will include words like estimator and reckoner.

Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. $$$$$ Moreover, once such knowledge is discovered, mechanisms must be in place to enrich current ontologies with this new knowledge.
Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. $$$$$ It then proceeds by assigning elements to their most similar committees.
Pantel and Ravichandran (2004) note that the nouns computer and company both have a WordNet sense that is a hyponym of person, falsely indicating these nouns would be compatible with pronouns like he or she. $$$$$ Section 3 describes our algorithm for labeling concepts and for extracting hyponym relationships.

We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. $$$$$ For each expected semantic answer type corresponding to a given question (e.g. band and color), we indexed the entire TREC-2002 IR collection with our system's hyponyms.
We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. $$$$$ The two columns of numbers indicate the frequency and mutual information score for each feature respectively.
We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. $$$$$ The higher the performance of the passage retrieval module, the higher will be the performance of the answer pinpointing module.
We use PMI (point-wise mutual information) of hyponymy relation candidate (hyper, hypo) as a collocation feature (Pantel and Ravichandran, 2004), where we assume that hyper and hypo in candidates would frequently co-occur in the same sentence if they hold a hyponymy relation. $$$$$ Our method begins to address this thorny issue by quantifying the name assigned to a class and by simultaneously assigning a number that can be interpreted to reflect the strength of membership of each element to the class.
