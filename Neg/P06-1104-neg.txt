To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ Many feature-based learning algorithms involve only the dot-product between feature vectors.
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. previous work from the viewpoint of feature exploration.

For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ The main difference is the different feature spaces.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination.

For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ However, the above issues can be handled by allowing grammar-driven partial rule matching and other approximate matching mechanisms in the parse tree kernel calculation.
For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ By comparing the performance of T1 and T2, we can evaluate the effect of sub-trees with partial production rules as shown in T2 and the necessity of keeping the whole left and right context sub-trees as shown in T1 in relation extraction.
For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.

For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.
For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ To some degree, it could possibly lead to over-fitting and compromise the performance.
For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous bestreported feature-based methods on the ACE corpus.

We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ The rest of the paper is organized as follows.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ It shows that 83.5%(198+115/198+115+62) / 85.8%(416 +171/416+171+96) of the errors result from relation detection and only 16.5%/14.2% of the errors result from relation characterization.

(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. $$$$$ Kp(•,•) = (K(•,•)+1)2 , and α is the coefficient.
(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. $$$$$ Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.4.
(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data: 72.1% F measure. $$$$$ Our study demonstrates that the composite kernel is very effective for relation extraction.

For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.
For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ As a result, using the PC with Intel P4 3.0G CPU and 2G RAM, our system only takes about 110 minutes and 30 minutes to do training on the ACE 2003 (~77k training instances) and 2004 (~33k training instances) data, respectively.
For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ Finally, it is worth noting that by introducing more individual kernels our method can easily scale to cover more features from a multitude of sources (e.g.

In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous bestreported feature-based methods on the ACE corpus.
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ However, the path does not maintain the tree structure information.
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ (1) Compared with Feature-based Methods: The basic difference lies in the relation instance representation (parse tree vs. feature vector) and the similarity calculation mechanism (kernel function vs. dot-product).

For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ The time complexity for computing this kernel is O( |N1  |⋅  |N2|) .
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ The time complexity for computing this kernel is O( |N1  |⋅  |N2|) .
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ Our study demonstrates that the composite kernel is very effective for relation extraction.

Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ Therefore, we design a linear kernel to explicitly capture such features: where R1 and R2 stands for two relation instances, Ei means the ith entity of a relation instance, and KE(•,•) is a simple kernel function over the features of entities: where fi represents the ith entity feature, and the function C(•,•) returns 1 if the two feature values are identical and 0 otherwise.
Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.

AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ (2) Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004).
AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ One may ask: how can we make full use of the nice properties of kernel methods and define an effective kernel for relation extraction?
AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ Kp(•,•) = (K(•,•)+1)2 , and α is the coefficient.
AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ Table 1 compares the performance of 5 tree kernel setups on the ACE 2003 data using the tree structure information only.

The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ Acknowledgements: We would like to thank Dr. Alessandro Moschitti for his great help in using his Tree Kernel Toolkits and fine-tuning the system.
The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.
The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ The rest of the paper is organized as follows.
The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ Kernel functions have nice properties.

Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ Wordnet, gazetteers, etc) that can be brought to bear on the task of relation extraction.
Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.
Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination.

Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ The polynomial expansion aims to explore the entity bi-gram features, esp. the combined features from the first and second entities, respectively.
Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.
Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ This paper proposes a novel composite kernel for relation extraction.

Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes.
Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.

Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two relation instances.
Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ In this section, we define the composite kernel and study the effective representation of a relation instance.

Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ ∆(n1, n2) = ∑i Isubtreei (n1 ) ⋅ Isubtreei (n2 ) ∆(n1 , n2) can be computed by the following recursive rules: where nc(n1) is the child number of n1, ch(n,j) is the jth child of node n andλ (0<λ <1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes.
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ However, the path does not maintain the tree structure information.
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ The most immediate extension of our work is to improve the accuracy of relation detection.
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.

Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.
Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.
Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.

In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ In the future work, we will design a more flexible tree kernel for more accurate similarity measure.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ In addition, the recursive rule (3) holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring.

Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ Our composite kernel consists of an entity kernel and a convolution parse tree kernel.
