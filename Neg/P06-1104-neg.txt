To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compare these methods exactly and fairly.
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.
To benefit from both views, a composite kernel (Zhanget al, 2006) integrates the flat features from entities and structured features from parse trees. $$$$$ In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances.

For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ This paper proposes a novel composite kernel for relation extraction.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.
For the similarity matrix W in section 4.1 and the kernel K in section 4.2, we used the composite kernel function (Zhang et al, 2006), which is based on structured features and entity-related features. $$$$$ Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.

For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION.
For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ Kernel functions have nice properties.
For example, by combining tree kernels and convolution string kernels, (Zhang et al, 2006) achieved the state of the art performance on ACE (ACE, 2004), which is a benchmark dataset for relation extraction. $$$$$ This is to explain more about why our method performs better and significantly outperforms the previous two dependency tree kernels from the theoretical viewpoint.

For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ Kernel functions have nice properties.
For comparison, we use the same setting as (Zhang et al, 2006), by applying a 5-fold cross-validation. $$$$$ This paper proposes a novel composite kernel for relation extraction.

We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ We also would like to thank the three anonymous reviewers for their invaluable suggestions.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.
We also compare our approaches to the other state-of-the-art approaches including Convolution Tree kernel (Collinsand Duffy, 2001), Syntactic kernel (Zhao and Grishman, 2005), Composite kernel (linear) (Zhang et al, 2006) and the best kernel in (Nguyen et al, 2009). $$$$$ This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.

(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data $$$$$ In this paper, we have designed a composite kernel for relation extraction.
(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data $$$$$ In the future work, we will design a more flexible tree kernel for more accurate similarity measure.
(Zhang et al, 2006) showed that by carefully choosing the weight of each component and using a polynomial expansion, they could achieve the best performance on this data $$$$$ Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus.

For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 19871998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005).
For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ Kernel functions have nice properties.
For example, by combining tree kernels and convolution string kernels, (Zhang et al., 2006) achieved the state of the art performance on ACE data (ACE, 2004). $$$$$ This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints.

In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ Our study demonstrates that the composite kernel is very effective for relation extraction.
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.
In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al, 2006a) syntactic features are employed for relation extraction. $$$$$ The kernel requires the two paths to have the same length; otherwise the kernel value is zero.

For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ By comparing the performance of T1 and T2, we can evaluate the effect of sub-trees with partial production rules as shown in T2 and the necessity of keeping the whole left and right context sub-trees as shown in T1 in relation extraction.
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.
For the first time, in (Zhang et al, 2006a), this convolution tree kernel was used for relation extraction. $$$$$ In Section 2, we review the previous work.

Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling.
Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ We also would like to thank the three anonymous reviewers for their invaluable suggestions.
Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.
Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al, 2006b). $$$$$ This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.

AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively.
AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.
AAP is computed over the MCT tree portion which is also proposed by (Zhang et al, 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments. $$$$$ Many feature-based learning algorithms involve only the dot-product between feature vectors.

The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.
The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels.
The parameter of CD'01 kernel is set to 0.4 according to (Zhang et al., 2006a). $$$$$ The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples.

Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ This paper proposes a novel composite kernel for relation extraction.
Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ 1) with “NP”.
Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ Finally, the parse tree kernel requires exact match between two subtrees, which normally does not occur very frequently.
Although the path-enclosed tree portion (PT) (Zhang et al, 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features). $$$$$ Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance.

Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ To some degree, it could possibly lead to over-fitting and compromise the performance.
Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ This is to evaluate whether the limited context information in CPT can boost performance.
Motivated by the work of (Zhang et al, 2006), we here examine four cases that contain different sub-structures as shown in Fig. $$$$$ The kernel requires the two paths to have the same length; otherwise the kernel value is zero.

Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola, 2001).
Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ In the future work, we will design a more flexible tree kernel for more accurate similarity measure.
Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance.
Zhang et al (2006) discover that the Shortest Path enclosed Tree (SPT) achieves the best performance. $$$$$ This paper proposes a novel composite kernel for relation extraction.

Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ In this paper, two composite kernels are defined by combing the above two individual kernels in the following ways: is the coefficient.
Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous bestreported feature-based methods on the ACE corpus.
Zhang et al (2006) describe a convolution tree kernel (CTK, Collins and Duffy, 2001) to investigate various structured information for relation extraction and find that the Shortest Path enclosed Tree (SPT) achieves the F-measure of 67.7 on the 7 relation types of the ACE RDC 2004 corpus. $$$$$ Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features.

Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ The relation instance is excerpted from the ACE 2003 corpus, where a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and “workers” (PER).
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features.
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ Our statistics on the ACE data reveals that the entity features impose a strong constraint on relation types.
Zhang et al (2006) design a composite kernel consisting of an entity linear kernel and a standard CTK, obtaining the F-measure of 72.1 on the 7 relation types in the ACE RDC 2004 corpus. $$$$$ The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction.

Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ In this paper, we study how relation extraction can benefit from the elegant properties of kernel methods: 1) implicitly exploring (structured) features in a high dimensional space; and 2) the nice mathematical properties, for example, the sum, product, normalization and polynomial expansion of existing kernels is a valid kernel (Schölkopf and Smola, 2001).
Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner.
Zhang et al (2006) explore five kinds of tree spans and find that the Shortest Path-enclosed Tree (SPT) achieves the best performance. $$$$$ Kernel functions have nice properties.

In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 19871998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005).
In fact, SPT (Zhang et al, 2006) can be arrived at by carrying out part of the above removal operations using a single rule (i.e. all the constituents outside the linking path should be removed) and CS-CSPT (Zhou et al, 2007) further recovers part of necessary context-sensitive information outside SPT, this justifies that SPT performs well, while CS-SPT outperforms SPT. $$$$$ Section 4 reports the experimental results and our observations.

Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ Finally, the parse tree kernel requires exact match between two subtrees, which normally does not occur very frequently.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.23.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ We conclude our work and indicate the future work in Section 6.
Experiments by Zhang et al (2006) show that linear kernel using only entity features contributes much when combined with the convolution parse tree kernel. $$$$$ We also demonstrate how our composite kernel effectively captures the diverse knowledge for relation extraction.
