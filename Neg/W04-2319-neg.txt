Following the work of Shriberg et al (2004), we use the 5 general tags in our experiments: Disruption indicates the current Dialogue Act is interrupted. $$$$$ For example, within only half a minute, speaker c5 has interacted with speakers c3 and c6, and speaker c6 has interacted with speakers c2 and c5.
Following the work of Shriberg et al (2004), we use the 5 general tags in our experiments: Disruption indicates the current Dialogue Act is interrupted. $$$$$ Values are shown by labeler pair.
Following the work of Shriberg et al (2004), we use the 5 general tags in our experiments: Disruption indicates the current Dialogue Act is interrupted. $$$$$ Also, of the 19 utterances within the excerpt, 9 are incomplete due to interruption by another talker, as is typical of many regions in the corpus showing high speaker overlap.
Following the work of Shriberg et al (2004), we use the 5 general tags in our experiments: Disruption indicates the current Dialogue Act is interrupted. $$$$$ We suggest various ways to group the large set of labels into a smaller set of classes, depending on the research focus.

Like the Switchboard corpus, the ICSI Meeting Room DA (MRDA) corpus (Shriberg et al, 2004) was annotated using a variant of the DAMSL tag set, similar but not identical to the Switchboard DAMSL annotation. $$$$$ We describe a new corpus of hand-annotated dialog acts and adjacency pairs for roughly 72 hours of naturally occurring multi-party meetings.
Like the Switchboard corpus, the ICSI Meeting Room DA (MRDA) corpus (Shriberg et al, 2004) was annotated using a variant of the DAMSL tag set, similar but not identical to the Switchboard DAMSL annotation. $$$$$ (2) Look at each other labeler’s words.
Like the Switchboard corpus, the ICSI Meeting Room DA (MRDA) corpus (Shriberg et al, 2004) was annotated using a variant of the DAMSL tag set, similar but not identical to the Switchboard DAMSL annotation. $$$$$ For example, within only half a minute, speaker c5 has interacted with speakers c3 and c6, and speaker c6 has interacted with speakers c2 and c5.

The differences (and a translation between the two sets) can be seen in Shriberg et al (2004). $$$$$ Table 3 shows the distribution of the tags in more detail.
The differences (and a translation between the two sets) can be seen in Shriberg et al (2004). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
The differences (and a translation between the two sets) can be seen in Shriberg et al (2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
The differences (and a translation between the two sets) can be seen in Shriberg et al (2004). $$$$$ Match every word in the utterance, and then mark the matched utterance in the reference so it cannot be matched again (this prevents felicitous matches due to identical repeated words).

To facilitate cross-corpus classification, we will cluster these labels as described in Shriberg et al (2004). $$$$$ We computed interlabeler reliability among the three labelers for both segmentation (into DA units) and DA labeling, using randomly selected excerpts from the 75 labeled meetings.
To facilitate cross-corpus classification, we will cluster these labels as described in Shriberg et al (2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
To facilitate cross-corpus classification, we will cluster these labels as described in Shriberg et al (2004). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
To facilitate cross-corpus classification, we will cluster these labels as described in Shriberg et al (2004). $$$$$ If instead we look at only the 11 obligatory general tags, for which there is one per DA, and if we split labels at the pipe bar, the total is 113,560 (excluding tags that only include a disruption label).

We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). $$$$$ If we ignore the tag marking rising intonation (rt), since this is not a DA tag, we find 180,218 total tags.
We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). $$$$$ Furthermore, 6 of the 19 total utterances express some form of agreement or disagreement (arp, aa, and nd) with previous utterances.
We also used additional annotation that has been developed to support higher-level analyses of meeting structure, in particular the ICSI Meeting Recorder Dialog act (MRDA) corpus (Shriberg et al., 2004). $$$$$ We find in related work that regions of high overlap correlate with high speaker involvement, or “hot spots” [15].

The following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (Shriberg et al, 2004). $$$$$ We provide basic statistics based on the dialog act labels for the 75 meetings.
The following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (Shriberg et al, 2004). $$$$$ Match every word in the utterance, and then mark the matched utterance in the reference so it cannot be matched again (this prevents felicitous matches due to identical repeated words).
The following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (Shriberg et al, 2004). $$$$$ We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.
The following experiments used manual meeting transcripts and relied on manual dialogue act segmentation (Shriberg et al, 2004). $$$$$ We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.

All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al, 2005). $$$$$ We include other useful information with the corpus.
All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al, 2005). $$$$$ There are a28 native English speakers, although many of the nonnative English speakers are quite fluent.
All of them have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and abstractive and extractive summaries in the AMI project (Murray et al, 2005). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.

While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically. $$$$$ Thus meetings pose interesting challenges to descriptive and theoretical models of discourse, as well as to researchers in the speech recognition community [4,7,9,13,14,15].
While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically. $$$$$ 2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.
While there are related tags for dialogue act tagging schema? like DAMSL (Core and Allen, 1997), which includes tags such as Action-Directive and Commit, and the ICSI MRDA schema (Shriberg et al, 2004) which includes a committag these classes are too general to allow identification of action items specifically. $$$$$ 2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.

All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.
All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ The views are those of the authors and do not represent the views of the funding agencies.

First, it is rare to have sub DAs labeled in training data, and indeed this is true of the corpus (Shriberg et al, 2004) that we use. $$$$$ We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.
First, it is rare to have sub DAs labeled in training data, and indeed this is true of the corpus (Shriberg et al, 2004) that we use. $$$$$ If instead we look at only the 11 obligatory general tags, for which there is one per DA, and if we split labels at the pipe bar, the total is 113,560 (excluding tags that only include a disruption label).
First, it is rare to have sub DAs labeled in training data, and indeed this is true of the corpus (Shriberg et al, 2004) that we use. $$$$$ Meetings contain regions of high speaker overlap, affective variation, complicated interaction structures, abandoned or interrupted utterances, and other interesting turn-taking and discourse-level phenomena.
First, it is rare to have sub DAs labeled in training data, and indeed this is true of the corpus (Shriberg et al, 2004) that we use. $$$$$ We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.

We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al, 2004) corpus, and find that our novel hidden back off model can significantly improve dialog tagging accuracy. $$$$$ We found that SWBD-DAMSL [11], a system adapted from DAMSL [6], provided a fairly good fit.
We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al, 2004) corpus, and find that our novel hidden back off model can significantly improve dialog tagging accuracy. $$$$$ If we ignore the tag marking rising intonation (rt), since this is not a DA tag, we find 180,218 total tags.
We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al, 2004) corpus, and find that our novel hidden back off model can significantly improve dialog tagging accuracy. $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
We evaluate our methods on the ICSI meeting recorder dialog act (MRDA) (Shriberg et al, 2004) corpus, and find that our novel hidden back off model can significantly improve dialog tagging accuracy. $$$$$ Annotator comments are also provided.

In all our models, to simplify we assume that the sentence change information is known (as is common with this corpus (Shriberg et al, 2004)). $$$$$ 2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.
In all our models, to simplify we assume that the sentence change information is known (as is common with this corpus (Shriberg et al, 2004)). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.

We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004). $$$$$ Word-level time information is available, based on alignments from an automatic speech recognizer.
We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004). $$$$$ We provide basic statistics based on the dialog act labels for the 75 meetings.
We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004). $$$$$ We suggest various ways to group the large set of labels into a smaller set of classes, depending on the research focus.
We evaluated our hidden back off model on the ICSI meeting recorder dialog act (MRDA) corpus (Shriberg et al, 2004). $$$$$ This allows the researcher to either split or not split at the bar, depending on the research goals.

All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ 2 Data We describe a new corpus of over 180,000 handannotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings.
All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.
All the meetings have been transcribed and annotated with dialog acts (DA) (Shriberg et al, 2004), topics, and extractive summaries (Murray et al, 2005). $$$$$ We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.

In our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the ICSI corpus (Shriberg et al, 2004). $$$$$ We include other useful information with the corpus.
In our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the ICSI corpus (Shriberg et al, 2004). $$$$$ An example from one of the meetings is shown in Figure 2 as an illustration of some of the types of interactions we observe in the corpus.
In our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the ICSI corpus (Shriberg et al, 2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
In our future work, we plan to examine initiative conflicts in face-to-face multi-party conversation, such as the ICSI corpus (Shriberg et al, 2004). $$$$$ Excluding nonlabelable cases, there are 11 general tags and 39 specific tags.

This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al, 2004). $$$$$ Finally, the corpus contains information that may be useful in for developing automatic modeling of prosody, such as hand-marked annotation of rising intonation.
This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al, 2004). $$$$$ Word-level time information is available, based on alignments from an automatic speech recognizer.
This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al, 2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
This GUI showed both their textual summary and the orthographic transcription, without topic segmentation but with one line per dialogue act based on the pre-existing MRDA coding (Shriberg et al, 2004). $$$$$ If we ignore the tag marking rising intonation (rt), since this is not a DA tag, we find 180,218 total tags.

We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al 2004). $$$$$ In addition to the obvious high degree of overlap—roughly one third of all words are overlapped—note the explicit struggle for the floor indicated by the two failed floor grabbers (fg) by speakers c5 and c6.
We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al 2004). $$$$$ The distribution of general tags is shown in Table 4.
We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al 2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
We use only the forced alignments of these meetings, available in the accompanying MRDA Corpus (Shriberg et al 2004). $$$$$ If we ignore the tag marking rising intonation (rt), since this is not a DA tag, we find 180,218 total tags.

Experiments are performed using all train/test pairs among three conversational speech corpora: the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch board DAMSL (Swbd) (Jurafsky et al, 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al, 1998). $$$$$ The views are those of the authors and do not represent the views of the funding agencies.
Experiments are performed using all train/test pairs among three conversational speech corpora: the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch board DAMSL (Swbd) (Jurafsky et al, 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al, 1998). $$$$$ We thank Chuck Wooters, Don Baron, Chris Oei, and Andreas Stolcke for software assistance, Ashley Krupski for contributions to the annotation scheme, Andrei Popescu-Belis for analysis and comments on a release of the 50 meetings, and Barbara Peskin and Jane Edwards for general advice and feedback.
Experiments are performed using all train/test pairs among three conversational speech corpora: the Meeting Recorder Dialog Actcorpus (MRDA) (Shriberg et al, 2004), Switch board DAMSL (Swbd) (Jurafsky et al, 1997), and the Spanish Callhome dialog act corpus (SpCH) (Levin et al, 1998). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.

Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al, 2004). $$$$$ We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.
Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al, 2004). $$$$$ This work was supported by an ICSI subcontract to the University of Washington on a DARPA Communicator project, ICSI NSF ITR Award IIS-0121396, SRI NASA Award NCC2-1256, SRI NSF IRI-9619921, an SRI DARPA ROAR project, an ICSI award from the Swiss National Science Foundation through the research network IM2, and by the EU Framework 6 project on Augmented Multi-party Interaction (AMI).
Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al, 2004). $$$$$ We describe a new corpus of hand-annotated dialog acts and adjacency pairs for roughly 72 hours of naturally occurring multi-party meetings.
