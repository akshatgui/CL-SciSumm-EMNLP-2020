A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank.

Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its &quot;progenitive&quot; features here, describing only how each of the two models of this paper differ in the subsequent two sections.
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ This research was funded in part by NSF grant SBR-89-20230-15.
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank.

The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy.
The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ As discussed in the introduction, lexical items' idiosyncratic parsing preferences are modeled by lexicalizing the grammar formalism, using a lexicalized PCFG in one case and a lexicalized stochastic TAG in the other.
The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ Furthermore, when testing on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with &quot;-all&quot;, and models trained with the reduced English training set with &quot;-small&quot;.

The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ Furthermore, when testing on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with &quot;-all&quot;, and models trained with the reduced English training set with &quot;-small&quot;.
The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ See Table 1 for results.
The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ While more investigation is required, we suspect part of the difference may be due to the fact that currently, the BBN model uses language-specific rules to guess part of speech tags for unknown words.

We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ In order to put the new Chinese Treebank results into context with the unmodified (English) parsing models, we present results on two test sets from the Wall Street Journal: WSJ-all, which is the complete Section 23 (the de facto standard test set for English parsing), and WSJ-small, which is the first 400 sentences of Section 23 and which is roughly comparable in size to the Chinese test set.
We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.

Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We use p to denote the unlexicalized nonterminal corresponding to P in (1), and similarly for i, ri and h. We now present the toplevel generation probabilities, along with examples from Figure 1.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank.

Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ We modified the head rules described in (Xia, 1999) for the Xinhua corpus and substituted these new rules into both models.

 $$$$$ Pi(a) is the probability of beginning a derivation with a; Ps(a j ) is the probability of substituting a at 71; Pa(O I n) is the probability of adjoining 13 at 77; finally, Pa(NONE j 7) is the probability of nothing adjoining at n. Our variant adds another set of parameters: This is the probability of sister-adjoining -y between the ith and i + 1 th children of 77 (allowing for two imaginary children beyond the leftmost and rightmost children).
 $$$$$ The Chinese Treebank consists of 4185 sentences of Xinhua newswire text.
 $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
 $$$$$ We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing.

This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ The rules are interpreted as follows: a head is kept in the same elementary tree in its parent, an argument is broken off into a separate initial tree, leaving a substitution node, and an adjunct is broken off into a separate modifier tree.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ While results for the two languages are far from equal, we believe that further tuning of the head rules, and analysis of development test set errors will yield significant performance gains on Chinese to close the gap.

Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ Linguistically-reasonable independence assumptions are made, such as the independence of grammar productions in the case of the PCFG model, or the independence of the composition operations in the case of the LTAG model, and we would argue that these assumptions are no less reasonable for the Chinese grammar than they are for that of English.
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen.

 $$$$$ In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank.
 $$$$$ In both languages, the nouns, adjectives, adverbs, and verbs have preferences for certain arguments and adjuncts, and these preferences—in spite of the potentially vastlydifferent configurations of these items—are effectively modeled.
 $$$$$ We obtain maximum-likelihood estimates of the parameters of this model using frequencies gathered from the training data.

Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy.
Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ However, there are definite similarities between the grammars of English and Chinese, especially when viewed through the lens of the statistical models we employed here.
Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen.
Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993).

Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ See (Chiang, 2000) for more details.
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ This increase has of course been due to their proven efficacy in many tasks, but also to their engineering efficacy.
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ As discussed in the introduction, lexical items' idiosyncratic parsing preferences are modeled by lexicalizing the grammar formalism, using a lexicalized PCFG in one case and a lexicalized stochastic TAG in the other.
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ Figure 2 illustrates all three of these operations, al is an initial tree which substitutes at the leftmost node labeled NP,I.

 $$$$$ Finally, we fully expect that absolute performance will increase greatly as additional highquality Chinese parse data becomes available.
 $$$$$ The model of (Chiang, 2000) is based on stochastic TAG (Resnik, 1992; Schabes, 1992).
 $$$$$ We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description of the TAG model, see (Chiang, 2000)).
 $$$$$ See Table 1 for results.

The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ As discussed in the introduction, lexical items' idiosyncratic parsing preferences are modeled by lexicalizing the grammar formalism, using a lexicalized PCFG in one case and a lexicalized stochastic TAG in the other.
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ The reader will note that the modified BBN model does significantly poorer than (Chiang, 2000) on Chinese.
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ For example, almost all statistical parsers make use of lexicalized nonterminals in some way, which allows lexical items' indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determined almost entirely by the training data, thereby making this feature—which models preferences of particular words of a particular language—almost entirely languageindependent.

 $$$$$ We blindly separated this into training, devtest and test sets, with a roughly 80/10/10 split, putting files 001-270 (3484 sentences, 84,873 words) into the training set, 301-325 (353 sentences, 6776 words) into the development test set and reserving 271-300 (348 sentences, 7980 words) for testing.
 $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.
 $$$$$ This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.
 $$$$$ This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.
