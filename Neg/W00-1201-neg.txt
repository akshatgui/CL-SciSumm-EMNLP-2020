A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ Finally, we fully expect that absolute performance will increase greatly as additional highquality Chinese parse data becomes available.
A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.
A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ See (Joshi and Schabes, 1997) for a more detailed explanation.
A decade of Chinese parsing research, enabled by the Penn Chinese Treebank (PCTB; Xue et al,2005), has seen Chinese parsing performance improve from 76.7 F1 (Bikel and Chiang, 2000) to 84.1 F1 (Qian and Liu, 2012). $$$$$ In this paper, we will explore the use of two parsing models, which were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese IYeebank.

Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ This research was funded in part by NSF grant SBR-89-20230-15.
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ Ever since the success of HMMs' application to part-of-speech tagging in (Church, 1988), machine learning approaches to natural language processing have steadily become more widespread.
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing.
Xiong et al (2005) used a similar model to the BBN's model in (Bikel and Chiang, 2000), and augmented the model by semantic categorical information and heuristic rules. $$$$$ A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that &quot;linguistically-motivated&quot; does not necessarily mean language-dependent&quot;--often, it means just the opposite.

The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ We blindly separated this into training, devtest and test sets, with a roughly 80/10/10 split, putting files 001-270 (3484 sentences, 84,873 words) into the training set, 301-325 (353 sentences, 6776 words) into the development test set and reserving 271-300 (348 sentences, 7980 words) for testing.
The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ See (Joshi and Schabes, 1997) for a more detailed explanation.
The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.
The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000). $$$$$ The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Collins, 2000) on the now-standard English test set of the Penn rkeebank (Marcus et al., 1993).

The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ Finally, we fully expect that absolute performance will increase greatly as additional highquality Chinese parse data becomes available.
The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that &quot;linguistically-motivated&quot; does not necessarily mean language-dependent&quot;--often, it means just the opposite.
The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000).
The English sentences in the 2-best lists were parsed using the Collins parser (Collins, 1999), and the Chinese sentences were parsed using a Chinese parser provided to us by D. Bikel (Bikel and Chiang, 2000). $$$$$ We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing.

We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.
We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ Furthermore, when testing on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with &quot;-all&quot;, and models trained with the reduced English training set with &quot;-small&quot;.
We used the Bikel Chinese head finder (Bikel and Chiang, 2000) and the Collins English head finder (Collins, 1999) to transform the gold constituency parses into gold dependency parses. $$$$$ There is no question that a great deal of care and expertise went into creating the Chinese Treebank, and that it is a source of important grammatical information that is unique to the Chinese language.

Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We would also like to thank Mike Collins and our advisors Aravind Joshi and Mitch Marcus.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ In both languages, the nouns, adjectives, adverbs, and verbs have preferences for certain arguments and adjuncts, and these preferences—in spite of the potentially vastlydifferent configurations of these items—are effectively modeled.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al, 2002) operate at word-level with the assumption that input sentences are pre-segmented. $$$$$ We also discuss directions for future work.

Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ While results for the two languages are far from equal, we believe that further tuning of the head rules, and analysis of development test set errors will yield significant performance gains on Chinese to close the gap.
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
Bikel and Chiang (2000) and Xu et al (2002) construct word-based statistical parsers on the first release of Chinese Treebank, which has about 100K words, roughly half of the training data used in this study. $$$$$ We also discuss directions for future work.

 $$$$$ We blindly separated this into training, devtest and test sets, with a roughly 80/10/10 split, putting files 001-270 (3484 sentences, 84,873 words) into the training set, 301-325 (353 sentences, 6776 words) into the development test set and reserving 271-300 (348 sentences, 7980 words) for testing.
 $$$$$ In both languages, the nouns, adjectives, adverbs, and verbs have preferences for certain arguments and adjuncts, and these preferences—in spite of the potentially vastlydifferent configurations of these items—are effectively modeled.
 $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.
 $$$$$ We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen.

This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ This paper presents the first-ever results of applying statistical parsing models to the newly-available Chinese Treebank.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000).
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ Linguistically-reasonable independence assumptions are made, such as the independence of grammar productions in the case of the PCFG model, or the independence of the composition operations in the case of the LTAG model, and we would argue that these assumptions are no less reasonable for the Chinese grammar than they are for that of English.
This two-dimensional parametrization has been instrumental in devising parsing models that improve disambiguation capabilities for English as well as other languages, such as German (Dubey and Keller, 2003) Czech (Collins et al, 1999) and Chinese (Bikel and Chiang, 2000). $$$$$ In both languages, the nouns, adjectives, adverbs, and verbs have preferences for certain arguments and adjuncts, and these preferences—in spite of the potentially vastlydifferent configurations of these items—are effectively modeled.

Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ Therefore, by comparing the WSJ-small results with the Chinese results, one can reasonably gauge the performance gap between English parsing on the Penn Treebank and Chinese parsing on the Chinese Treebank.
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ In order to put the new Chinese Treebank results into context with the unmodified (English) parsing models, we present results on two test sets from the Wall Street Journal: WSJ-all, which is the complete Section 23 (the de facto standard test set for English parsing), and WSJ-small, which is the first 400 sentences of Section 23 and which is roughly comparable in size to the Chinese test set.
Neither Collins et al (1999) nor Bikel and Chiang (2000) compare the lexicalized model to an unlexicalized baseline model, leaving open the possibility that lexicalization is useful for English, but not for other languages. $$$$$ See (Chiang, 2000) for more details.

 $$$$$ While results for the two languages are far from equal, we believe that further tuning of the head rules, and analysis of development test set errors will yield significant performance gains on Chinese to close the gap.
 $$$$$ As discussed in the introduction, lexical items' idiosyncratic parsing preferences are modeled by lexicalizing the grammar formalism, using a lexicalized PCFG in one case and a lexicalized stochastic TAG in the other.
 $$$$$ See Table 1 for results.

Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ The probability of an entire parse tree is the product of the probabilities of generating all of the elements of that parse tree, The hidden nonterminal +BEGIN+ is used to provide a convenient mechanism for determining the initial probability of the underlying Markov process generating the modifying nonterminals; the hidden nonterminal +END+ is used to provide consistency to the underlying Markov process, i.e., so that the probabilities of all possible nonterminal sequences sum to 1.
Collins et al (1999) and Bikel and Chiang (2000) do not compare their models with an unlexicalized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages. $$$$$ There is no question that a great deal of care and expertise went into creating the Chinese Treebank, and that it is a source of important grammatical information that is unique to the Chinese language.

Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ We also discuss directions for future work.
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ While results for the two languages are far from equal, we believe that further tuning of the head rules, and analysis of development test set errors will yield significant performance gains on Chinese to close the gap.
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ However, there are definite similarities between the grammars of English and Chinese, especially when viewed through the lens of the statistical models we employed here.
Besides, Bikel and Chiang (2000) applied two lexicalized parsing models developed for English to Penn Chinese Treebank. $$$$$ While more investigation is required, we suspect part of the difference may be due to the fact that currently, the BBN model uses language-specific rules to guess part of speech tags for unknown words.

 $$$$$ We would greatly like to acknowledge the researchers at BBN who allowed us to use their model: Ralph Weischedel, Scott Miller, Lance Ramshaw, Heidi Fox and Sean Boisen.
 $$$$$ This research was funded in part by NSF grant SBR-89-20230-15.
 $$$$$ We have employed two models, one extracted and adapted from BBN's SIFT System (Miller et al., 1998) and a TAGbased parsing model, adapted from (Chiang, 2000).
 $$$$$ On sentences with <40 words, the former model performs at 69% precision, 75% recall, and the latter at 77% precision and 78% recall.

The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ This research was funded in part by NSF grant SBR-89-20230-15.
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ We also discuss directions for future work.
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ Linguistically-reasonable independence assumptions are made, such as the independence of grammar productions in the case of the PCFG model, or the independence of the composition operations in the case of the LTAG model, and we would argue that these assumptions are no less reasonable for the Chinese grammar than they are for that of English.
The third experiment was on the Chinese Tree bank, starting with the same head rules used in (Bikel and Chiang, 2000). $$$$$ Many machine learning approaches let the data speak for itself (data ipsa loguuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems.

 $$$$$ We blindly separated this into training, devtest and test sets, with a roughly 80/10/10 split, putting files 001-270 (3484 sentences, 84,873 words) into the training set, 301-325 (353 sentences, 6776 words) into the development test set and reserving 271-300 (348 sentences, 7980 words) for testing.
 $$$$$ For brevity, we omit the smoothing details of BBN's model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997).
 $$$$$ Furthermore, when testing on WSJ-small, we trained on a subset of our English training data roughly equivalent in size to our Chinese training set (Sections 02 and 03 of the Penn Treebank); we have indicated models trained on all English training with &quot;-all&quot;, and models trained with the reduced English training set with &quot;-small&quot;.
 $$$$$ We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing.
