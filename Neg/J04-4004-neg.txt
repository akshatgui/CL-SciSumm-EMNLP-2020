Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. $$$$$ Model Mtw,tw shows our baseline, and Model Mφ,φ shows the effect of removing all dependence on the headword and its part of speech, with the other models illustrating varying degrees of removing elements from the two parameter classes’ conditioning contexts.
Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. $$$$$ We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. $$$$$ Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.
Subsequently, we replicated Gildea's experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. $$$$$ This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.

Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). $$$$$ In addition to the effects of the unpublished details, we also have new evidence to show that the discriminative power of Collins’ model does not lie where once thought: Bilexical dependencies play an extremely small role in Collins’ models (Gildea 2001), and head choice is not nearly as critical as once thought.
Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). $$$$$ Note that if the modifying nonterminals were generated completely independently, the model would be very impoverished, but in actuality, because it includes the distance and subcategorization frame features, the model captures a crucial bit of linguistic reality, namely, that words often have well-defined sets of complements and adjuncts, occurring with some well-defined distribution in the right-hand sides of a (context-free) rewriting system.
Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). $$$$$ We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
Subsequently, we duplicated Gildea's experiment with a complete emulation of Collins' Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). $$$$$ In an additional experiment, we have examined the number of times that the parser is able, while decoding Section 00, to deliver a requested probability for the modifier-word generation model using the increasingly less-specific contexts of the three back-off levels.

The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. $$$$$ Finally, thanks to my Ph.D. advisor Mitch Marcus, who during the course of this work was, as ever, a source of keen insight and unbridled optimism.
The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. $$$$$ Table 5 shows the effect on overall parsing performance of independently removing or changing certain of the more than 30 unpublished details.34 Often, the detrimental effect of a particular change is quite insignificant, even by the standards of the performance-obsessed world of statistical parsing, and occasionally, the effect of a change is not even detrimental at all.
The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. $$$$$ One of the two components of this distance metric is what we will call the “verb intervening” feature, which is a predicate vi that is true if a verb has been generated somewhere in the surface string of the previously generated modifiers on the current side of the head.

Furthermore, the work in this paper relates to Bikel (2004)'s work. $$$$$ One such function of the history is the distance metric.
Furthermore, the work in this paper relates to Bikel (2004)'s work. $$$$$ The values of these estimates for a given sentence would be constant across all parses, meaning that the “superficiency” of the model would be irrelevant when determining arg max P(T  |S).
Furthermore, the work in this paper relates to Bikel (2004)'s work. $$$$$ I would especially like to thank Mike Collins for his invaluable assistance and great generosity while I was replicating his thesis results and for his comments on a prerelease draft of this article.
Furthermore, the work in this paper relates to Bikel (2004)'s work. $$$$$ Crucially, this set omits MD, which is the marker for modal verbs.

For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. $$$$$ Punctuation that occurs at the very beginning or end of a sentence is “raised away,” that is, pruned.
For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. $$$$$ Even though decoding proceeds bottom-up, the model is defined in a top-down manner.
For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al, 2004) and we used the rules in (Bikel, 2004) for conversion. $$$$$ Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.

They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). $$$$$ The modifying nonterminals L; and R; are generated conditioning on P and H, as well as a distance metric (based on what material intervenes between the currently generated modifying nonterminal and H) and an incremental subcategorization frame feature (a multiset containing the arguments of H that have yet to be generated on the side of H in which the currently generated nonterminal falls).
They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). $$$$$ We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.
They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). $$$$$ As with all of the treebank- or model-specific aspects of the Collins parser, our engine uses equation (16) or (18) depending on the value of a particular run-time setting.
They were then parsed using Bikel's parser (Bikel,2004) and corrected by hand using the Penn Tree bank Bracketing Guidelines (Bies et al, 1995). $$$$$ While there is a negligible yet detrimental effect on overall parsing performance when one uses an unknown-word threshold of five instead of six, when this change is combined with the “obvious” method for handling unknown words, there is actually a minuscule improvement in overall parsing performance (see Table 5).

Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing. $$$$$ In the standard Wall Street Journal training corpus, Sections 02–21 of the Penn Treebank, there are 120 such sentences that are skipped.
Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing. $$$$$ We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing. $$$$$ PTOPNT is unsmoothed. n/a: not applicable. not the case here).
Conditioning on crossing punctuation could be of help then, playing a role similar to that of comma-counting (Collins, 1997, §2.1) — and 'verb intervening' (Bikel, 2004, §5.1) - in early head-outward models for supervised parsing. $$$$$ The second motivation is related to the first: science dictates that experiments be replicable, for this is the way we may test and validate them.

 $$$$$ The work described here comes in the wake of several previous efforts to replicate this particular model, but this is the first such effort to provide a faithful and equally well-performing emulation of the original.
 $$$$$ This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.
 $$$$$ Finally, we perform experiments that show that the true discriminative power of lexicalization appears to lie in the fact that unlexicalized syntactic structures are generated conditioning on the headword and its part of speech.
 $$$$$ Collins mentions in chapter 7 of his thesis that “[a]ll words occurring less than 5 times in training data, and words in test data which have never been seen in training, are replaced with the ‘UNKNOWN’ token (page 186).” The frequency below which words are considered unknown is often called the unknownword threshold.

 $$$$$ As mentioned in Section 3, instead of generating each modifier independently, the model conditions the generation of modifiers on certain aspects of the history.
 $$$$$ LR (labeled recall) and LP (labeled precision) are the primary scoring metrics.
 $$$$$ But what of the details that were published?

 $$$$$ Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.
 $$$$$ We not only analyze the effect of the unpublished details, but also reanalyze the effect of certain well-known details, revealing that bilexical dependencies are barely used by the model and that head choice is not nearly as important to overall parsing performance as once thought.
 $$$$$ In general, we will limit our discussion to Collins’ Model 2, but we make occasional reference to Model 3, as well.
 $$$$$ As a shorthand, y(M(t)i) = y(Mi),tMi.

 $$$$$ As a way to guarantee the consistency of the model, the model also generates two hidden +STOP+ nonterminals as the leftmost and rightmost children of every parent (see Figure 7).
 $$$$$ One of the two components of this distance metric is what we will call the “verb intervening” feature, which is a predicate vi that is true if a verb has been generated somewhere in the surface string of the previously generated modifiers on the current side of the head.
 $$$$$ All reported scores are for sentences of length < 40 words.
 $$$$$ The (unlexicalized) nonterminal-mapping functions alpha and gamma are defined in Section 6.1.

We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). $$$$$ We also show a cleaner and equally well-performing method for the handling of punctuation and conjunction and reveal certain other probabilistic oddities about Collins’ parser.
We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). $$$$$ This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.
We used Bikel's reimplementation of Collins' parsing model 2 (Bikel, 2004). $$$$$ As a way to guarantee the consistency of the model, the model also generates two hidden +STOP+ nonterminals as the leftmost and rightmost children of every parent (see Figure 7).

Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. $$$$$ This is especially true in an area like statistical parsing that has seen rapid maturation followed by a soft “plateau” in performance.
Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. $$$$$ There are three primary motivations for this work.
Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. $$$$$ This is the subject of our current research.
Our best performing model incorporates three dimensions of parametrization and our best result (75.25%) is similar to the one obtained by the parser of (Bikel, 2004) for Modern Standard Arabic (75%) using a fully lexicalized model and a training corpus about three times as large as our newest MH tree bank. $$$$$ First, Collins’ parsing model represents a widely used and cited parsing model.

This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. $$$$$ A few of the preprocessing steps rely on the notion of a coordinated phrase.
This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. $$$$$ While there is a negligible yet detrimental effect on overall parsing performance when one uses an unknown-word threshold of five instead of six, when this change is combined with the “obvious” method for handling unknown words, there is actually a minuscule improvement in overall parsing performance (see Table 5).
This was followed by (Bikel, 2004) who showed that bilexical-information is used in only 1.49% of the decisions in Collins' Model-2 parser, and that removing this information results in an exceedingly small drop in performance. $$$$$ As there are some interesting idiosyncrasies with these headword-generation parameters, we describe them in more detail in Section 6.9.

We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. $$$$$ This insertion ensures that NPB nodes are always dominated by NP nodes.
We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. $$$$$ For example, for two chart items to be considered equal, they must have the same label (the label of the root of their respective derivation forests’ subtrees), the same headword and tag, and the same left and right subcat.
We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. $$$$$ As a way to guarantee the consistency of the model, the model also generates two hidden +STOP+ nonterminals as the leftmost and rightmost children of every parent (see Figure 7).
We use 2-best parse trees of Berkeley parser (Petrov and Klein, 2007) and 1-best parse tree of Bikel parser (Bikel, 2004) and Stanford parser (Klein and Manning, 2003) as inputs to the full parsing based system. $$$$$ Careful examination of its intricacies will also allow researchers to deviate from the original model when they think it is warranted and accurately document those deviations, as well as understand the implications of doing so.

This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). $$$$$ Accordingly, if a comma in an input Overall parsing results using only details found in Collins (1997, 1999).
This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). $$$$$ We have documented what we believe is the complete set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, thi s article contains all information necessary to duplicate Collins’ benchmark results.
This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). $$$$$ Accordingly, if a comma in an input Overall parsing results using only details found in Collins (1997, 1999).
This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrov et al (2006), and the effects of different transformations have been studied by Johnson (1998), Klein and Manning (2003), and Bikel (2004). $$$$$ However, Gildea (2001) reimplemented Collins’ Model 1 (essentially Model 2 but without subcats) and altered the PLw and PRw parameters so that they no longer had the top level of context that included the headword (he removed back-off level 0, as depicted in Table 1).

Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. $$$$$ This article documents a large set of heretofore unpublished details Collins used in his parser, such that, along with Collins’ (1999) thesis, this article contains all information necessary to duplicate Collins’ benchmark results.
Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. $$$$$ Collins’ thesis gives a pseuSince the goal of the decoding process is to determine the maximally likely theory, if during decoding a proposed chart item is equal (or, technically, equivalent) to an item that is already in the chart, the one with the greater score survives.
Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. $$$$$ Such dependencies might finally be able to capture the semantic preferences that were thought to be captured by standard bilexical statistics, as well as to alleviate the sparse-data problems associated with standard bilexical statistics.
Collins's statistical parser (CBP; (Collins, 1997)), improved by Bikel (Bikel, 2004), is based on the probabilities between head-words in parse trees. $$$$$ The node labels of sentences with no subjects are transformed from S to SG.

The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). $$$$$ In such cases, the chart item will contain a back pointer to the chart item that represents the base NP.
The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). $$$$$ Every nonterminal label in every tree is lexicalized: the label is augmented to include a unique headword (and that headword’s part of speech) that the node dominates.
The English sentences were parsed using the Bikel parser (Bikel, 2004), and the sentences were aligned with GIZA++ (Och and Ney,2000). $$$$$ For consistency’s sake, when an NP has been relabeled as NPB, a normal NP node is often inserted as a parent nonterminal.

For English we use the Bikel parser default head word rules (Bikel, 2004). $$$$$ Also, thanks to the anonymous reviewers for their helpful and astute observations.
For English we use the Bikel parser default head word rules (Bikel, 2004). $$$$$ Also, thanks to the anonymous reviewers for their helpful and astute observations.
For English we use the Bikel parser default head word rules (Bikel, 2004). $$$$$ Indeed, these as-yet-unpublished details account for an 11% relative increase in error from an implementation including all details to a clean-room implementation of Collins’ model.
For English we use the Bikel parser default head word rules (Bikel, 2004). $$$$$ In particular, for notational brevity we use M(w, t)i to refer to the three items Mi, tMi, and wMi that constitute some fully lexicalized modifying nonterminal and similarly M(t)i to refer to the two items Mi and tMi that constitute some partially lexicalized modifying nonterminal.

 $$$$$ That is, taken together, all the unpublished details have a significant effect on overall parsing performance.
