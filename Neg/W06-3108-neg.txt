Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). $$$$$ One class for the positions to the left and one class for the positions to the right.
Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). $$$$$ We present discriminative reordering models for phrase-based statistical machine translation.
Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). $$$$$ As already mentioned in Section 1, many current phrase-based statistical machine translation systems use a very simple reordering model: the costs for phrase movements are linear in the distance.
Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). $$$$$ HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org).

Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). $$$$$ As already mentioned in Section 4.3, a richer feature set could be helpful.
Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). $$$$$ Here, we let the maximum entropy training decide which features are important and which features can be neglected.
Despite their high perplexities, reordered LMs yield some improvements when integrated to a PSMT baseline that already includes a discriminative phrase orientation model (Zens and Ney, 2006). $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.

Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. $$$$$ In our experiments, we use about 10% of the corpus for testing and the remaining part for training the feature weights of the reordering model with the GIS algorithm using YASMET (Och, 2001).
Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. $$$$$ This is a strong evidence that the maximum entropy framework is suitable for this task.
Unlike previous discriminative local orientation models (Zens and Ney, 2006), our framework permits the definition of global features. $$$$$ So far, the word classes were not used for the translation experiments.

Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. $$$$$ To avoid overfitting, (Chen and Rosenfeld, 1999) have suggested a smoothing method where a Gaussian prior distribution of the parameters is assumed.
Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. $$$$$ Then, the feature functions will be defined in Section 4.3.
Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. $$$$$ For the NIST task, we use the BLEU score as primary criterion which is optimized on the NIST 2002 evaluation set using the Downhill Simplex algorithm (Press et al., 2002).
Adopting the idea of predicting the orientation, (Zens and Ney, 2006) started exploiting the context and grammar which may relate to phrase reorderings. $$$$$ We evaluate the overall performance of the reordering models as well as the contribution of the individual feature types on a word-aligned corpus.

Figure 2 $$$$$ We use a state-of-the-art baseline system which would have obtained a good rank in the last NIST evaluation (NIST, 2005).
Figure 2 $$$$$ As the word classes help for the classification task, we might expect further improvements of the translation results.
Figure 2 $$$$$ For Arabic-English, the baseline is with 6.3% already very low.

Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). $$$$$ The difference is that, here, we do not constrain the phrase reordering.
Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). $$$$$ This very simple reordering model is widely used, for instance in (Och et al., 1999; Koehn, 2004; Zens et al., 2005).
Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). $$$$$ As a decision rule, we obtain: This approach is a generalization of the sourcechannel approach (Brown et al., 1990).
Contrasting the direct use of the reordering probabilities used in (Zens and Ney, 2006), we utilize the probabilities to adjust the word distance-based reordering cost, where the reordering cost of a sentence is computed as P o (f, e). $$$$$ This means that the word order in Arabic is very similar to the word order in English.

In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. $$$$$ The models are trained using the maximum entropy principle.
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. $$$$$ Finally, we will conclude in Section 6.
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. $$$$$ This seems to be reasonable as Japanese has usually a different sentence structure, subject-objectverb compared to subject-verb-object in English.
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. $$$$$ The models are trained using the maximum entropy principle.

Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. $$$$$ . fj ... fJ, which is to be translated into a target language sentence eI1 = e1 ... ei ... eI.
Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. $$$$$ Finally, we will conclude in Section 6.
Zens and Ney (2006) and Xiong et al (2006) utilized contextual information to improve phrase reordering. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.

In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). $$$$$ The statistics of the training and test alignment links is shown in Table 2.
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). $$$$$ We presented improved translation results for three language pairs on the BTEC task and for the large data track of the Chinese-English NIST task.
In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006). $$$$$ In none of the cases additional features have hurt the classification performance on the held-out test corpus.

 $$$$$ To make use of word level information, we need the word alignment within the phrase pairs.
 $$$$$ We present discriminative reordering models for phrase-based statistical machine translation.
 $$$$$ We have presented a novel discriminative reordering model for statistical machine translation.

The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). $$$$$ None of these methods try to generalize from the words or phrases by using word classes or part-ofspeech information.
The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). $$$$$ The corpus statistics of the bilingual training corpus are shown in Table 3.
The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). $$$$$ We also measured the classification performance for the NIST task.
The first contrastive submission is a phrase-based system, enhanced with a triplet lexicon model and a discriminative word lexicon model (Mauser et al, 2009) both trained on in-domain news commentary data only as well as a sentence-level single-word lex icon model and a discriminative reordering model (Zens and Ney, 2006a). $$$$$ HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org).

Following models were applied $$$$$ It has the advantage that additional models h(·) can be easily integrated into the overall system.
Following models were applied $$$$$ The model scaling factors λM1 are trained with respect to the final translation quality measured by an error criterion (Och, 2003).
Following models were applied $$$$$ As baseline we always choose the most frequent orientation class.
Following models were applied $$$$$ None of these methods try to generalize from the words or phrases by using word classes or part-ofspeech information.

Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). $$$$$ Additionally, we show improved translation performance using these reordering models compared to a state-of-the-art baseline system.
Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). $$$$$ None of these methods try to generalize from the words or phrases by using word classes or part-ofspeech information.
Moreover, we tested the impact of the discriminative reordering model (Zens and Ney, 2006a). $$$$$ The feature functions of the reordering model depend on the last alignment link (j, i) of a phrase.

The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). $$$$$ We present discriminative reordering models for phrase-based statistical machine translation.
The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). $$$$$ Now, the model has to predict if the start position of the next phrase j′ is to the left or to the right of the current phrase.
The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). $$$$$ The models are trained using the maximum entropy principle.
The classifier can be trained with maximum likelihood like Moses lexicalized reordering (Koehn et al, 2007) and hierarchical lexical ized reordering model (Galley and Manning, 2008) or be trained under maximum entropy framework (Zens and Ney, 2006). $$$$$ This is a strong evidence that the maximum entropy framework is suitable for this task.

Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase. $$$$$ The models are trained using the maximum entropy principle.
Figure 3 is an illustration of (Zens and Ney, 2006). j is the source word position which is aligned to the last target word of the current phrase. $$$$$ We use a state-of-the-art baseline system which would have obtained a good rank in the last NIST evaluation (NIST, 2005).

j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase. $$$$$ We use several types of features: based on words, based on word classes, based on the local context.
j is the source word position which is aligned to the first target word position of the next phrase. (Zens and Ney, 2006) proposed a maximum entropy classifier to predict the orientation of the next phrase given the current phrase. $$$$$ HR0011-06-C-0023, and was partly funded by the European Union under the integrated project TC-STAR (Technology and Corpora for Speech to Speech Translation, IST2002-FP6-506738, http://www.tc-star.org).

Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. $$$$$ This material is partly based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No.
Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. $$$$$ In (Koehn et al., 2005) several variants of the orientation model have been tried.
Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. $$$$$ The latter two models are used for both directions: p(f|e) and p(e|f).

One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ The features for the maximum-entropy based reordering model are based on the source and target language words within a window of one.
One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ This approach is also used in the publicly available Pharaoh decoder (Koehn, 2004).
One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ This approach is also used in the publicly available Pharaoh decoder (Koehn, 2004).
One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ We use several types of features: based on words, based on word classes, based on the local context.

Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. $$$$$ Our approach circumvents this problem by using a combination of phrase-level and word-level features and by using word-classes or part-of-speech information.
Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. $$$$$ We present discriminative reordering models for phrase-based statistical machine translation.
Using these Chinese grammatical relations, we improve a phrase orientation classifier (introduced by Zens and Ney (2006)) that decides the ordering of two phrases when translated into English by adding path features designed over the Chinese typed dependencies. $$$$$ The models are trained using the maximum entropy principle.

To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. $$$$$ We use a state-of-the-art baseline system which would have obtained a good rank in the last NIST evaluation (NIST, 2005).
To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. $$$$$ Afterwards, we will evaluate the performance of this new model in Section 5.
To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. $$$$$ Another advantage of our approach is the generalization capability via the use of word classes or part-of-speech information.
To achieve this, we train a discriminative phrase orientation classifier following the work by Zens and Ney (2006), and we use the grammatical relations between words as extra features to build the classifier. $$$$$ Still the fluency of the machine translation output leaves much to desire.
