 $$$$$ In principle, the problem of finding one or more answers to a question from a very large set of documents can be addressed by creating a context for the question and a knowledge representation of each document and then match the question context against each document representation.
 $$$$$ Thus, a mixture of natural language processing and information retrieval methods may be the solution for now.
 $$$$$ The answer extraction dominates the processing time while the question processing part is negligible.
 $$$$$ answers in large collections of texts: paragraph + abductive inference.

Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Answer Extraction The parser enables the recognition of the answer candidates in the paragraph.
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Notes of the Fall AAAI Symposium on Question An
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ A focus is a word or a sequence of words which define the question and disambiguate the question by indicating what the question is looking for.
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Table 4 summarizes the scores provided by NIST for our system.

Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ For example, the recognition of keywords or other question words in an apposition determines the Punctuation_sign-score, the Same_parse_subtreescore, the Comma_3_words-score and the Same_sentence-score to go up.
Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ There were several features of the Zprise IR engine which were not conducive to working within the design of LASSO.
Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ answers in large collections of texts: paragraph + abductive inference.

 $$$$$ answers in large collections of texts: paragraph + abductive inference.
 $$$$$ The number of documents that contain the keywords returned by the Search Engine may be large since only weak Boolean operators were used.
 $$$$$ On the other hand, traditional information retrieval and extraction techniques alone can not be used for question answering due to the need to pinpoint exactly an answer in large collections of open domain texts.
 $$$$$ On the average, the processing time per question is 61 sec., and the time ranges from 1 sec. to 540 sec.

Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ The answer extraction dominates the processing time while the question processing part is negligible.
Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ Notes of the Fall AAAI Symposium on Question An
Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ answers in large collections of texts: paragraph + abductive inference.

(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ Obviously, the questions in Class 2 are more difficult as they require more powerful natural language and reasoning techniques.
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ First, we have acquired new tagging rules and secondly, we have unified the dictionaries of the tagger with semantic dictionaries derived from the Gazetteers and from WordNet (Miller 1995).
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ The Answer Processing module identifies and extracts the answer from the paragraphs that contain the question keywords.
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ In order to better understand the nature of the QA task and put this into perspective, we offer in Table 6 a taxonomy of question answering systems.

To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations. $$$$$ Thus, a mixture of natural language processing and information retrieval methods may be the solution for now.
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations. $$$$$ This approach is not practical yet since it involves advanced techniques in knowledge representation of open text, reasoning, natural language processing, and indexing that currently are beyond the technology state of the art.
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)): Heuristic 1: Whenever the first feedback loop requires the addition of the main verb of the question as a query keyword, generate all verb conjugations as well as its nominalizations. $$$$$ Each expression tagged by the parser with the answer type becomes one of the answer candidates for a paragraph.

Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ Similar to the paragraphwindows used in ordering the paragraphs, we establish an answer-window for each answer candidate.
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ First, we perform the processing of the question by combining syntactic information, resulting from a shallow parse, with semantic information that characterizes the question (e.g. question type, question focus).
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ This approach is not practical yet since it involves advanced techniques in knowledge representation of open text, reasoning, natural language processing, and indexing that currently are beyond the technology state of the art.
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ answers in large collections of texts: paragraph + abductive inference.

The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ In TREC-8, the performance focus was on accuracy.
The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ Notes of the Fall AAAI Symposium on Question An
The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ Out of the 153 questions that our system has answered, 136 belong to Class 1, and 17 to Class 2.

Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ For example, if we have a set of keywords fkl, Id, k3, k41 and in a paragraph kl and Id are matched each twice, whereas k3 is matched only once, and k4 is not matched, we are going to have four different windows, defined by the keywords: [kl-matchl, Id-matchl, k3], [k1-match2,0-match1, k3], [kl-matchl, Idmatch2, k3], and [k1-match2, k2-match, k3].
Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ We have implemented eight different heuristics.
Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ Our methodology of finding answers in large collections of documents relies on natural language processing (NLP) techniques in novel ways.

In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ LASSO's requirements are much more rigid.
In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ In addition to the implementation of grammar rules, we have implemented heuristics capable of recognizing names of persons, organizations, locations, dates, currencies and products.
In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ Several criteria and metrics may be used to measure the performance of a QA system.
In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ The metric used by NIST for accuracy is described in (Voorhees and Tice 1999).
