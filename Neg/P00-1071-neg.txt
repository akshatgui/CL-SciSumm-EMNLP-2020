 $$$$$ Notes of the Fall AAAI Symposium on Question An
 $$$$$ Another important performance parameter is the processing time to answer a question.
 $$$$$ The process of extracting keywords is based on a set of ordered heuristics.

Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ In contrast, the answer for question 13 was found when the verb rent was added to the Boolean query.
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ In TREC-8, the performance focus was on accuracy.
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Indexing identifies the text passages where answers may lie, and natural language processing provides a framework for answer extraction.
Template-based questions and summary asking inquiries cover most of the classes of question complexity proposed in (Moldovan et al, 2000). $$$$$ Notes of the Fall AAAI Symposium on Question An

Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ Notes of the Fall AAAI Symposium on Question An
Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ Thus, it became necessary to implement a more precise determinant for extraction.
Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ First, we perform the processing of the question by combining syntactic information, resulting from a shallow parse, with semantic information that characterizes the question (e.g. question type, question focus).
Correcting this would require a model that jointly models content and bi grams (Hardisty et al., 2010), has a co reference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al 2000). $$$$$ This permits documents to be retrieved when only one of the keywords is present.

 $$$$$ answers in large collections of texts: paragraph + abductive inference.
 $$$$$ Similar heuristics recognize named entities successfully in IE systems.
 $$$$$ It is not sufficient to classify only the types of questions alone, since for the same question the answer may be easier or more difficult to extract depending on how the answer is phrased in the text.
 $$$$$ To facilitate the identification of the document sources, the engine was required to put the document id in front of each line in the document.

Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ Another important performance parameter is the processing time to answer a question.
Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ On the average, the processing time per question is 61 sec., and the time ranges from 1 sec. to 540 sec.
Some Q&A systems, like (Moldovan et al, 2000) relied both on NE recognizers and some empirical indicators. $$$$$ Several criteria and metrics may be used to measure the performance of a QA system.

(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ LASSO's requirements are much more rigid.
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ Several criteria and metrics may be used to measure the performance of a QA system.
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ We opted for the Boolean indexing as opposed to vector indexing (Buckley et al.1998) because Boolean indexing increases the recall at the expense of precision.
(Moldovan et al, 2000) details the empirical methods used in our system for transforming a natural language question into an IR query. $$$$$ answers in large collections of texts: paragraph + abductive inference.

To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)) $$$$$ A window comprises all the text between the lowest positioned keyword in the window and the highest position keyword in the window.
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)) $$$$$ The recognition of the answer type, through the semantic tag returned by the parser, creates a candidate answer.
To decide which keywords should be expanded and what form of alternations should be used we rely on a set of heuristics which complement the heuristics that select the question keywords and generate the queries (as described in (Moldovan et al, 2000)) $$$$$ We have implemented eight different heuristics.

Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ We opted for the Boolean indexing as opposed to vector indexing (Buckley et al.1998) because Boolean indexing increases the recall at the expense of precision.
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ Thus, it became necessary to implement a more precise determinant for extraction.
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ The number of documents that contain the keywords returned by the Search Engine may be large since only weak Boolean operators were used.
Here, we can distinguish between approaches that only return one passage per relevant document (see, for example, (Robertson et al., 1992)) and the ones that allow multiple passages per document (see, for example (Moldovan et al, 2000)). $$$$$ answers in large collections of texts: paragraph + abductive inference.

The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ answers in large collections of texts: paragraph + abductive inference.
The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ Each heuristic returns a set of keywords that are added in the same order to the question keywords.
The Falcon system (Moldovan et al, 2000) uses some semantic relations from WordNet when it expands the question. $$$$$ Table 5 summarizes the relative time spent on each processing component.

Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ Table 3 illustrates some of the scores that were attributed to the candidate answers LASSO has extracted successfully.
Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ Answer Extraction The parser enables the recognition of the answer candidates in the paragraph.
Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ Table 4 summarizes the scores provided by NIST for our system.
Moldovan et al (2000), for instance, select as keywords all named entities that were recognized as proper nouns. $$$$$ Thus, it became necessary to implement a more precise determinant for extraction.

In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ Q-26 What is the name of the &quot;female&quot; counterpart to El Nino, which results in cooling temperatures and very dry weather ?
In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ Notes of the Fall AAAI Symposium on Question An
In the work of Moldovan et al (2000), all why-questions share the single answer type reason. $$$$$ Similar to the paragraphwindows used in ordering the paragraphs, we establish an answer-window for each answer candidate.
