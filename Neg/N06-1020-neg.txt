We should keep in mind that (1) a tree bank PCFG is not state-of-the-art $$$$$ These studies suggest that this type of co-training is most effective when small amounts of labelled training data is available.
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art $$$$$ The improvement from self-training is significant in both macro and micro tests (p < 10−5). freranker are the evaluation of the parser and reranking parser on all sentences, respectively.

 $$$$$ We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.
 $$$$$ The goal is to use the various data sets to produce a model that accurately parses the target domain data despite seeing little or no annotated data from that domain.
 $$$$$ The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005).
 $$$$$ The parser was already doing very well on short sentences.

For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ Less unsupervised methods are more likely to be portable to these new domains, since they do not rely as much on existing annotations.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ Semi-supervised and unsupervised methods are important because good labeled data is expensive, whereas there is no shortage of unlabeled data.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ Finally, we provide some analysis to better understand the phenomenon.

Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.

previously used for self-training of parsers (McClosky et al, 2006). $$$$$ We still have them to try: restricting consideration to more accurately parsed sentences as training data (sentence selection), trying to learn grammatical generalizations directly rather than simply including the data for training, etc.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ However, the memory requirements are largish, about half a gigabyte just to store the data.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples.

 $$$$$ Other methods are “semi-supervised” where they use some labelled data to annotate unlabeled data.
 $$$$$ These additional features are largely responsible for improving the reranker’s performance on section 23 to 91.3% f-score (Charniak and Johnson (2005) reported an f-score of 91.0% on section 23).
 $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
 $$$$$ Sentence boundaries in NANC are induced by a simple discriminative model.

We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ In parsing, we attempt to uncover the syntactic structure from a string of words.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ Finally, we provide some analysis to better understand the phenomenon.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ This process can be iterated over different sets of unlabeled data if desired.

In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ Rather, there is a general improvement in intermediate-length sentences (20-50 words), but no improvement at the extremes: a phenomenon we call the Goldilocks effect.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002).

This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al. (2003) report either minor improvements or significant damage from using self-training for parsing.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ Conversely, language modeling has comparatively less reason to help PP attachment.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ When one learner is confident of its predictions about the data, we apply the predicted label of the data to the training set of the other learners.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ We performed several types of analysis to better understand why the new model performs better.

The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ Some notes regarding evaluations: All numbers reported are f-scores2.
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ We have already noted that the first-stage parser’s one-best has significantly improved (see Table 1).

McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ As for the other three of these graphs, their stories are by no means clear.
McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ First, the error analysis needs to be improved.
McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.

Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ It is also the case that we thought PP attachment might be improved because of the increased coverage of prepositionnoun and preposition-verb combinations that work such as (Hindle and Rooth, 1993) show to be so important.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ One may expect to see a monotonic improvement from this technique, but this is not quite the case, as seen when we add 1,000k sentences.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ While some domain-language pairs have quite a bit of labelled data (e.g. news text in English), many other categories are not as fortunate.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ Some notes regarding evaluations: All numbers reported are f-scores2.

We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ While the reranker was used to produce the reranker-best sentences, we performed this evaluation using only the first-stage parser to parse all sentences from section 22.
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004).
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Gildea (2001) and Bacchiani et al. (2006) show that out-of-domain training data can improve parsing accuracy.
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Another possibility is that these sections contains harder sentences which we cannot parse as accurately and thus are not as useful for self-training.

Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ In parser adaptation, one is given annotated training data from a source domain and unannotated data from a target.
Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.

Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ It is fair to say that neither we, nor anyone we talked to, thought conjunction handling would be improved.
Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples.
Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ Second, there are many other ways to use self-trained information in parsing.
Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.

SELF-CRF $$$$$ The second stage of our parser is a Maximum Entropy reranker, as described in (Charniak and Johnson, 2005).
SELF-CRF $$$$$ Clark et al. (2003) applies self-training to POS-tagging and reports the same outcomes.
SELF-CRF $$$$$ Unlike self-training, co-training requires multiple learners, each with a different “view” of the data.
SELF-CRF $$$$$ Finally, there are “unsupervised” strategies where no data is labeled and all annotations (including the grammar itself) must be discovered (Klein and Manning, 2002).

To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ It only sees the 50-best lists produced by the first-stage parser.
To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004).
To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.

A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ The self-trained parser does not improve prepositional-phrase attachment or the handling of unknown words.
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ Parser adaptation can be framed as a semisupervised or unsupervised learning problem.
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ These studies suggest that this type of co-training is most effective when small amounts of labelled training data is available.
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.

Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ Finally, we provide some analysis to better understand the phenomenon.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ First, we use a generative parser to produce a list of the top n parses.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ This is particularly the case with longer conjunctions, those of VPs and Ss.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ However, the n-best parsing algorithm described in that paper has been replaced by the much more efficient algorithm described in (Jimenez and Marzal, 2000; Huang and Chang, 2005).

Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ We did not train a model where we added 2,000k parser-best sentences. output versus reranker-best output.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ While some domain-language pairs have quite a bit of labelled data (e.g. news text in English), many other categories are not as fortunate.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ Additionally, we explore the use of a reranker.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ In parsing, we attempt to uncover the syntactic structure from a string of words.
