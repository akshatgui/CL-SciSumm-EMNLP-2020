We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable. $$$$$ We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable. $$$$$ We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable. $$$$$ As all distributions are conditioned with five or more features, they are all heavily backed off using Chen back-off (the average-count method from Chen and Goodman (1996)).
We should keep in mind that (1) a tree bank PCFG is not state-of-the-art: its performance is mediocre compared to e.g. Bod (2003) or McClosky et al (2006), and (2) that our tree bank PCFG is binarized as in Klein and Manning (2005) to make results comparable. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.

 $$$$$ Semi-supervised and unsupervised methods are important because good labeled data is expensive, whereas there is no shortage of unlabeled data.
 $$$$$ Looking at the performance of various weights across sections 1, 22, and 24, we decided that the best combination of training data is to give WSJ a relative weight of 5 and use the first 1,750k reranker-best sentences from NANC.
 $$$$$ In parsing, we attempt to uncover the syntactic structure from a string of words.
 $$$$$ We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.

For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ Semi-supervised and unsupervised methods are important because good labeled data is expensive, whereas there is no shortage of unlabeled data.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ The reranker is not able to suggest new parses and, moreover, uses the probability of each parse tree according to the parser as a feature to perform the reranking.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ NANC contains no syntactic information.
For instance, McClosky et al (2006) improved a statistical parser by self-training. $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.

Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ The parser-best and reranker-best lists represent the best parse for each sentence according to the parser and reranker, respectively.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ We would like to thank Michael Collins, Brian Roark, James Henderson, Miles Osborne, and the BLLIP team for their comments.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ Next, we produce two sets of one-best lists from these 50best lists.
Its success stories range from parsing (McClosky et al, 2006) to machine translation (Ueffing, 2006). $$$$$ In parsing, we attempt to uncover the syntactic structure from a string of words.

previously used for self-training of parsers (McClosky et al, 2006). $$$$$ Additionally, we explore the use of a reranker.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ Less unsupervised methods are more likely to be portable to these new domains, since they do not rely as much on existing annotations.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ The reranker takes the 50-best parses for each sentence produced by the first-stage 50best parser and selects the best parse from those 50 parses.
previously used for self-training of parsers (McClosky et al, 2006). $$$$$ The goal is to use the various data sets to produce a model that accurately parses the target domain data despite seeing little or no annotated data from that domain.

 $$$$$ Together with a 0.3% improvement due to superior reranking features, this is a 1.1% improvement over the previous best parser results for section 23 of the Penn Treebank (from 91.0% to 92.1%).
 $$$$$ A variation suggested by Dasgupta et al. (2001) is to add data to the training set when multiple learners agree on the label.
 $$$$$ Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003).

We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ In self-training, the existing model first labels unlabeled data.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ Finally, we provide some analysis to better understand the phenomenon.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ While it appears that the performance still levels off after adding about one million sentences from NANC, the curves corresponding to higher WSJ weights achieve a higher asymptote.
We note that the performance is on the same level as the performance of self-trained parsers, except for McClosky et al. (2006), which is based on the combination of reranking and self-training. $$$$$ In Table 2, we see that the new NANC data contains some information orthogonal to the reranker and improves parsing accuracy of the reranking parser.

In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ The features consist of those described in (Charniak and Johnson, 2005), together with an additional 601,577 features.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ This corresponds to a 12% error reduction assuming that a 100% performance is possible, which it is not.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ Adding more weight to the Wall Street Journal data ensures that the counts of our events will be closer to our more accurate data source while still incorporating new data from NANC.
In contrast, McClosky et al (2006a) report improved accuracy through self-training for a two stage parser and re-ranker. $$$$$ In parsing, we attempt to uncover the syntactic structure from a string of words.

This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ We use the reranking parser to produce 50-best parses of unlabeled news articles from NANC.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ Finally, we provide some analysis to better understand the phenomenon.
This method has been used effectively to improve parsing performance on newspaper text (McClosky et al, 2006a), as well as adapting a Penn Treebank parser to a new domain (McClosky et al, 2006b). $$$$$ Finally, we provide some analysis to better understand the phenomenon.

The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ When one learner is confident of its predictions about the data, we apply the predicted label of the data to the training set of the other learners.
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ Given sufficient labelled data, there are several “supervised” techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004).
The NANC corpus contains approximately 2 million WSJ sentences that do not overlap with Penn's WSJ and has been previously used by McClosky et al (2006) in improving a supervised parser by self training. $$$$$ We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.

McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ One hypothesis we consider is that the reranked NANC data incorporated some of the features from the reranker.
McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ Another possibility is that these sections contains harder sentences which we cannot parse as accurately and thus are not as useful for self-training.
McClosky et al (2006) presented a very effective method for self-training a two-stage parsing system consisting of a first-stage generative lexicalized parser and a second-stage discriminative reranker. $$$$$ Finally, we provide some analysis to better understand the phenomenon.

Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ We also perform some basic cleanups on NANC to ease parsing.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ In parsing, we attempt to uncover the syntactic structure from a string of words.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ Semi-supervised and unsupervised methods are important because good labeled data is expensive, whereas there is no shortage of unlabeled data.
Although our models are based on purely generative PCFG grammars, our best product model performs competitively to the self-trained two-step discriminative reranking parser of McClosky et al (2006), which makes use of many non-local reranking features. $$$$$ We do not show it here, but if we self-train using first-stage one-best, there is no change in oracle rate.

We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Examples of this include self-training (Charniak, 1997) and co-training (Blum and Mitchell, 1998; Steedman et al., 2003).
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ The unsupervised adaptation experiment by Bacchiani et al. (2006) is the only successful instance of parsing self-training that we have found.
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Finally, we provide some analysis to better understand the phenomenon.
We expect that replacing the first-step generative parsing model in McClosky et al (2006) with a product of latent variable grammars would give even higher parsing accuracies. $$$$$ Finally, we provide some analysis to better understand the phenomenon.

Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ The second the change in the log-odds resulting from this factor being present (in the case of CCs and INs, multiplied by the number of them) and the last column is the probability that this factor is really non-zero.
Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ One may expect to see a monotonic improvement from this technique, but this is not quite the case, as seen when we add 1,000k sentences.
Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al, 2006). $$$$$ Together with a 0.3% improvement due to superior reranking features, this is a 1.1% improvement over the previous best parser results for section 23 of the Penn Treebank (from 91.0% to 92.1%).

Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ Finally, we provide some analysis to better understand the phenomenon.
Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ Sentence boundaries in NANC are induced by a simple discriminative model.
Our work is related to self-training (McClosky et al, 2006a; Reichart and Rappoport, 2007) as the algorithm used its own tagging of the sentences collected from the web in order to produce a better final tagging. $$$$$ We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.

SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. $$$$$ This may be due to some sections of NANC being less similar to WSJ or containing more noise.
SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. $$$$$ We are making our current best self-trained parser available3 as machines with a gigabyte or more of RAM are becoming commonplace.
SELF-CRF: Following the self-training paradigm (e.g., (McClosky et al, 2006b; McClosky et al, 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. $$$$$ Finally, there is also the nature of the self-trained data themselves.

To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ One may expect to see a monotonic improvement from this technique, but this is not quite the case, as seen when we add 1,000k sentences.
To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ This work was supported by NSF grants LIS9720368, and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
To produce this, we segment sentences with MXTerminator (Reynar and Ratnaparkhi, 1997) and parse the corpus with the self trained Charniak parser (McClosky et al, 2006). $$$$$ Parser adaptation can be framed as a semisupervised or unsupervised learning problem.

A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ These features consist of the partsof-speech, possibly together with the words, that surround (i.e., precede or follow) the left and right edges of each constituent.
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ The only specific syntactic phenomenon that seems to be affected is conjunctions.
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ On this point, we have some pilot experiments that show great promise.
A noteable exception in constituent-based parsing is the work of McClosky et al (2006) who show that self-training is possible if a reranker is used to inform the underlying parser. $$$$$ It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al. (2003) report either minor improvements or significant damage from using self-training for parsing.

Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ While we did not predict this effect, in retrospect it seems reasonable.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ The improvement from self-training is significant in both macro and micro tests (p < 10−5). freranker are the evaluation of the parser and reranking parser on all sentences, respectively.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ Finally, we provide some analysis to better understand the phenomenon.
Of these, McClosky et al (2006) deal specifically with self training for data-driven statistical parsing. $$$$$ In some cases, some annotated data from the target domain is available as well.

Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ Finally, the improvement in Figure 5 is hard to judge.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ Also, the statistics are lightly pruned to remove those that are statistically less reliable/useful.
Self-training can suffer from over-fitting, in which errors in the original model are repeated and amplified in the new model (McClosky et al, 2006). $$$$$ Rather, there is a general improvement in intermediate-length sentences (20-50 words), but no improvement at the extremes: a phenomenon we call the Goldilocks effect.
