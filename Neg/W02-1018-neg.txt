posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. $$$$$ During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.
posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. $$$$$ The model described in this paper cannot learn that the English word “not” corresponds to the French words “ne” and “pas”.
posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. $$$$$ Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.
posed is a joint model as in (Marcu and Wong, 2002), since target and source phrases are generated jointly. $$$$$ Generate a bag of concepts.

Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. $$$$$ Randomly take a concept and generate a pair of phrases , according to the distribution , whereandeach contain at least one word.
Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. $$$$$ Starting with step 3 of the algorithm in Figure 2, for each sentence pair in a corpus, we greedily produce an initial alignment by linking together phrases so as to create concepts that have high t probabilities.
Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. $$$$$ Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat maximize the probability .
Marcu and Wong (2002) use a joint probability model for blocks where the clumps are contiguous phrases as in this paper. $$$$$ The English side had a total of 1,073,480 words (21,484 unique tokens).

In (Marcu and Wong, 2002), a joint probability phrase model is presented. $$$$$ The French side had a total of 1,177,143 words (28,132 unique tokens).
In (Marcu and Wong, 2002), a joint probability phrase model is presented. $$$$$ In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.
In (Marcu and Wong, 2002), a joint probability phrase model is presented. $$$$$ The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.

The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. $$$$$ We have tried many types of distortion models.
The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. $$$$$ Although Model 1 is fairly unsophisticated, we have found that it produces in practice fairly good alignments.
The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. $$$$$ For simplicity, we initially assume that the bag of concepts and the ordering of the generated phrases are modeled by uniform distributions.
The joint model by (Marcu and Wong, 2002) is refined by (Birch et al, 2006) who use high-confidence word alignments to constrain the search space in training. $$$$$ The rest of the iterations estimate the alignment probabilities using Model 2.

The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. $$$$$ Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.
The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. $$$$$ Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.
The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. $$$$$ The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.
The work by (DeNero et al, 2008) describes a method to train the joint model described in (Marcu and Wong, 2002) with a Gibbs sampler. $$$$$ The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.

 $$$$$ The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.
 $$$$$ The language model is estimated at the word (not phrase) level.
 $$$$$ Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.

Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. $$$$$ The sentences in the corpus were at most 20 words long.
Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. $$$$$ For decoding, we have implemented a greedy procedure similar to that proposed by Germann et al. (2001).
Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. $$$$$ This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.
Marcu and Wong (2002) propose a joint probability model which searches the phrase alignment space, simultaneously learning translations lexicons for words and phrases without consideration of potentially suboptimal word alignments and heuristic for phrase extraction. $$$$$ To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.

A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. $$$$$ We denote this property using the predicate .
A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. $$$$$ In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.
A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model to handle multi-word units. $$$$$ There are also ways in which the words of a sentence F can be partitioned into nonempty sets.

A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. $$$$$ 3.3 EM training on Viterbi alignments Given a non-uniform t distribution, phrase-to-phrase alignments have different weights and there are no other tricks one can apply to collect fractional counts over all possible alignments in polynomial time.
A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. $$$$$ In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).
A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrase based one. $$$$$ These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.

Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. $$$$$ In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.
Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. $$$$$ The probability p(E) is computed using a simple trigram language model that was trained using the CMU Language Modeling Toolkit (Clarkson and Rosenfeld, 1997).
Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. $$$$$ For each group of 100 sentences, we manually determined the number of sentences translated perfectly by the IBM model decoder of Germann et al. (2001) and the decoder that uses the joint probje vais me arreter la . je vais me arreter la .
Marcu and Wong (2002) proposed a phrase-based alignment model which suffered from a massive parameter space and intractable inference using expectation maximisation. $$$$$ 9.46e−08 i am going to stop there . ability model.

The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem $$$$$ In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.
The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem $$$$$ In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.
The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem $$$$$ We hillclimb by modifying an existing alignment/translation through a set of operations that modify locally the aligment/translation built until a given time.
The advent of Statistical Machine Translation, and most recently phrase-based approaches (PBMT, see Marcu and Wong (2002), Koehn et al (2003)) into the commercial arena seems to hold the promise of a solution to this problem $$$$$ The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.

For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. $$$$$ We then iteratively hillclimb by modifying E and the alignment between E and F so as to maximize the formula .
For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. $$$$$ The language model is estimated at the word (not phrase) level.
For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. $$$$$ In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).
For example, (Marcu and Wong, 2002) for a joint phrase based model, (Huang et al, 2003) for a translation memory system; and (Watanabe et al., 2003) for a complex model of insertion, deletion and head-word driven chunk reordering. $$$$$ We are currently investigating the applicability of dynamic programming techniques to increase the speed of the training procedure.

Marcu and Wong (2002) propose a joint probability model. $$$$$ The sentences in the corpus were at most 20 words long.
Marcu and Wong (2002) propose a joint probability model. $$$$$ Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.

However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. $$$$$ To evaluate our system, we trained both Giza (IBM Model 4) (Al-Onaizan et al., 1999) and our joint probability model on a French-English parallel corpus of 100,000 sentence pairs from the Hansard corpus.
However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. $$$$$ When a concept generates two phrases of lengthand, respectively, there are only and words left to link.
However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. $$$$$ Under these conditions, the evidence that a sentence pair (E, F) contributes to the fact that are generated by the same concept is given by the number of alignments that can be built between (E, F) that have a concept that is linked to phrasein sentence E and phrase in sentence F divided by the total number of alignments that can be built between the two sentences.
However, for more permissive models such as Marcu and Wong (2002) and DeNero et al (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. $$$$$ Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.

Indeed, Marcu and Wong (2002) conjectures that none exist. $$$$$ Given a Foreign sentence F, we first produce a gloss of it by selecting phrases inthat maximize the probability .
Indeed, Marcu and Wong (2002) conjectures that none exist. $$$$$ Similarly, the pair cannot be generated using concepts (“a c” : “x”) and (“b” : “y”) because the sequence “a b c” cannot be created by catenating the phrases “a c” and “b”.
Indeed, Marcu and Wong (2002) conjectures that none exist. $$$$$ In other words, all alignments that can generate a sentence pair (E, F) can be assumed to have the same probability.

 $$$$$ The results in Table 1 show that the phrased-based translation model proposed in this paper significantly outperforms IBM Model 4 on both the subjective and objective metrics.
 $$$$$ Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.
 $$$$$ In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).
 $$$$$ Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.

For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). $$$$$ However, our model learns to deal with negation by memorizing longer phrase translation equivalents, such as (“ne est pas”, “is not”); (“est inadmissible”, “is not good enough”); and (“ne est pas ici”, “is not here”).
For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). $$$$$ We eventually settled for the model discussed here because it produces better translations during decoding.
For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). $$$$$ At the end of the training procedure, we take marginals on the joint probability distributionsand .
For the space A of bijective alignments, problems E and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). $$$$$ These operations replace the English side of an alignment with phrases of different probabilities, merge and break existing concepts, and swap words across concepts.

Marcu and Wong (2002) describes an approximation to O. $$$$$ Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.
Marcu and Wong (2002) describes an approximation to O. $$$$$ The model chose to give more weight to the second hypothesis, while preserving some probability mass for the first one.
Marcu and Wong (2002) describes an approximation to O. $$$$$ The language model is estimated at the word (not phrase) level.
Marcu and Wong (2002) describes an approximation to O. $$$$$ But “ne” and “pas” almost never occur in adjacent positions in French texts.

Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f $$$$$ In order to be considered in step 2 of the algorithm, a phrase has to occur at least five times in the corpus.
Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f $$$$$ Under these assumptions, it follows that the probability of generating a sentence pair (E, F) using concepts is given by the product of all phrase-tophrase translation probabilities, that yield bags of phrases that can be ordered linearly so as to obtain the sentences E and F. For example, the sentence pair “a b c” — “x y” can be generated using two concepts, (“a b” : “y”) and (“c” : “x”); or one concept, (“a b c” : “x y”), because in both cases the phrases in each language can be arranged in a sequence that would yield the original sentence pair.
Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f $$$$$ Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.
Other phrase-based models model the joint distribution P (e, f) (Marcu and Wong, 2002) or made P (e) and P (f $$$$$ Sentence pair (S2, T2) offers strong evidence that “b c” in language S means the same thing as “x” in language T. On the basis of this evidence, we expect the system to also learn from sentence pair (S1, T1) that “a” in language S means the same thing as “y” in language T. Unfortunately, if one works with translation models that do not allow Target words to be aligned to more than one Source word — as it is the case in the IBM models (Brown et al., 1993) — it is impossible to learn that the phrase “b c” in language S means the same thing as word “x” in language T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure yields unintuitive translation probabilities.

The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). $$$$$ In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). $$$$$ In order to account for this, we modify slightly the generative process in Model 1 so as to account for distortions.
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). $$$$$ The generative model explains how source words are mapped into target words and how target words are re-ordered to yield well-formed target sentences.
The joint probability model proposed by Marcu and Wong (2002) provides a strong probabilistic framework for phrase-based statistical machine translation (SMT). $$$$$ For example, in our previous work (Marcu, 2001), we have used a statistical translation memory of phrases in conjunction with a statistical translation model (Brown et al., 1993).
