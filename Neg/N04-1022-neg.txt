Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ Under each loss function, the MBR decoding was performed using Equation 3.
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ Under the 0/1 loss function, the decoder of Equation 3 reduces to the MAP decoder MAP This illustrates why we are interested in MBR decoders based on other loss functions: the MAP decoder is optimal with respect to a loss function that is very harsh.
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ For example, the BLEU, NIST and the PER metrics, though effective, do not take into account explicit syntactic information when measuring translation quality.
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ MBR decoding under this loss function allows us to integrate syntactic knowledge into a statistical MT system without building detailed models of linguistic features, and retraining the system from scratch.

This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ The MBR decoder can be thought of as selecting a consensus translation: For each sentence, Equation 3 selects the translation that is closest on an average to all the likely translations and alignments.
This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ The performance of the MBR decoders on the NIST 2001+2002 test set is reported in Table 3.
This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ The first class of loss functions uses no information about word alignments or parse-trees, so that can be reduced to .

This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ Foster et al. (2002) developed a text-prediction system for translators that maximizes expected benefit to the translator under a statistical user model.
This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.
This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ These were multi-reference Word Error Rate (mWER) (Och, 2002), multi-reference Position-independent word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate.
This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ Examples of such loss functions are tree-edit distances between parse-trees, string-edit distances between event representation of parse-trees (Tang et al., 2002), and treekernels (Collins and Duffy, 2002).

The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ This is intended to compensate for any mismatch between decoding strategy of MT systems and their evaluation criteria.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ The procedure aims at direct minimization of the expected risk of translation errors under a given loss function.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ The MBR decoding under the Bitree Loss function performs better under the WER relative to the MAP decoder, but perform poorly under the BLEU metric.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.

Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ We also show how MBR decoding can be used to incorporate syntactic structure into a statistical MT system by building specialized loss functions.
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ These loss functions can use information from word strings, word-to-word alignments and parse-trees of the source sentence and its translation.
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ In this paper we have focused on two situations where this framework could be applied.
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ The two hypothesis translations are very similar at the word level and therefore the BLEU score, PER and the WER are identical.

In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ We will now describe one such loss function using the example in Figures 1 and 2.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ Under the 0/1 loss function, the decoder of Equation 3 reduces to the MAP decoder MAP This illustrates why we are interested in MBR decoders based on other loss functions: the MAP decoder is optimal with respect to a loss function that is very harsh.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ We report the performance of the MBR decoders on a Chinese-to-English translation task.

For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ Each Chinese sentence in this set has four reference translations.
For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ These were multi-reference Word Error Rate (mWER) (Och, 2002), multi-reference Position-independent word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate.

Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ While an -best list contains only a limited reordering of hypotheses, a translation lattice will contain hypotheses with a vastly greater number of re-orderings.
Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.
Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ The two hypothesis translations are very similar at the word level and therefore the BLEU score, PER and the WER are identical.
Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ This rapid progress has been greatly facilitated by the development of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002).

We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). $$$$$ This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.
We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). $$$$$ The gains from MBR decoding under matched conditions are statistically significant in most cases.
We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmannand Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). $$$$$ A loss function of this type depends only on information from word strings.

Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ We observe in most cases that the MBR decoder under a loss function performs the best under the corresponding error metric i.e. matched conditions perform the best.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ This is a framework that can be used with statistical models of speech and language to develop decision processes optimized for specific loss functions.

We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ Using an example, we have shown that this loss function can measure qualities of translation that string (and ngram) based metrics cannot capture.
We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.

Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ This is a framework that can be used with statistical models of speech and language to develop decision processes optimized for specific loss functions.
Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ Under the 0/1 loss function, the decoder of Equation 3 reduces to the MAP decoder MAP This illustrates why we are interested in MBR decoders based on other loss functions: the MAP decoder is optimal with respect to a loss function that is very harsh.
Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ The posterior probability of each hypothesis in the -best list is derived from the joint probability assigned by the baseline translation model.
Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ We will now describe a simple procedure that makes use of the word alignment to construct node-to-node alignment between nodes in the source tree and the target tree.

This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ We note that .
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set.
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ We present results under the Bitree loss function as an example of incorporating linguistic information into a loss function; we have not yet measured its correlation with human assessments of translation quality.
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.

For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ The Bitree loss function measures the distance between two trees in terms of distances between their corresponding subtrees.
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ In case of the other metrics, we consider multiple references in the following way.
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ Of course, we do not have access to the true distribution over translations.

To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics.

Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic. $$$$$ Relative to a reference translation with word alignment, the decoder performance is measured as .
Moses Baseline: We trained a Moses system (Koehn et al, 2007) with the following settings: maximum sentence length 80, grow-diag-final and symmetrization of GIZA++ alignments, an interpolated KneserNey smoothed 5-gram language model with KenLM (Heafield, 2011) used at runtime, msd-bidirectional-felexicalized reordering, sparse lexical and domain features (Hasler et al, 2012), distortion limit of 6, 100-best translation options, minimum bayes-risk decoding (Kumar and Byrne, 2004), cube-pruning (Huangand Chiang, 2007) and the no-reordering-over punctuation heuristic. $$$$$ In particular we describe the design of a Bilingual Tree Loss Function that can explicitly use syntactic structure for measuring translation quality.

Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognition (Goel and Byrne, 2000) and bitext word alignment for statistical MT (Kumar and Byrne, 2002), to the problem of building automatic MT systems tuned for specific metrics.
Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ The conventional Maximum A Posteriori (MAP) decoder can be derived as a special case of the MBR decoder by considering a loss function that assigns a equal cost (say 1) to all misclassifications.
Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ The Bitree loss function which depends both on the parse-trees and the word-to-word alignments, is therefore very different for the two translations (Table 2).

With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ In parsing, Goodman (1996) developed parsing algorithms that are appropriate for specific parsing metrics.
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ Given any translation metric, the MBR decoding framework will allow us to optimize existing MT systems for the new criterion.
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ We will show that MBR decoding can be applied to machine translation in two scenarios.

Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ There has also been recent work that combines 1-best hypotheses from multiple translation systems (Bangalore et al., 2002); this approach uses string-edit distance to align the hypotheses and rescores the resulting lattice with a language model.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ We also show how MBR decoding can be used to incorporate syntactic structure into a statistical MT system by building specialized loss functions.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ We expect new automatic MT evaluation metrics to emerge frequently in the future.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ This shows the value of finding decoding procedure matched to the performance criterion of interest.

Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004). $$$$$ Using an example, we have shown that this loss function can measure qualities of translation that string (and ngram) based metrics cannot capture.
Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004). $$$$$ For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function.
Minimum Bayes Risk Rescoring: In this system, we re-ranked the n-best output of our baseline system using Minimum Bayes Risk (Kumarand Byrne, 2004). $$$$$ The conventional Maximum A Posteriori (MAP) decoder can be derived as a special case of the MBR decoder by considering a loss function that assigns a equal cost (say 1) to all misclassifications.
