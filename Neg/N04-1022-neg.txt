Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ These criteria could come from evaluation metrics or from other desiderata (such as syntactic well-formedness) that we wish to see in automatic translations.
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.
Minimum Bayes Risk (MBR) techniques have been successfully applied to a wide range of natural language processing tasks, such as statistical machine translation (Kumar and Byrne, 2004), automatic speech recognition (Goel and Byrne, 2000), parsing (Titov and Henderson, 2006), etc. $$$$$ The MBR decoder under WER and PER perform better than the MAP decoder under all error metrics.

This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ This hierarchy includes the loss functions useful in both situations where we intend to apply MBR decoding.
This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ That machine translation evaluation continues to be an active area of research is evident from recent workshops (AMTA, 2003).
This solution is often referred to as the Minimum Bayes Risk (MBR) solution (Kumar and Byrne,2004). $$$$$ While we have focused on developing MBR procedures for loss functions that measure various aspects of translation quality, this framework can also be used with loss functions which measure application-specific error criteria.

This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ This hierarchy includes the loss functions useful in both situations where we intend to apply MBR decoding.
This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We observe in most cases that the MBR decoder under a loss function performs the best under the corresponding error metric i.e. matched conditions perform the best.
This list is then rescored using Minimum Bayes-Risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We will show that MBR decoding can be applied to machine translation in two scenarios.

The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ We present results under the Bitree loss function as an example of incorporating linguistic information into a loss function; we have not yet measured its correlation with human assessments of translation quality.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system 's translations relative to the model 's distribution over possible translations (Kumar and Byrne, 2004). $$$$$ These loss functions can use information from word strings, word-to-word alignments and parse-trees of the source sentence and its translation.

Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ The posterior probability of each hypothesis in the -best list is derived from the joint probability assigned by the baseline translation model.
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ This hierarchy includes the loss functions useful in both situations where we intend to apply MBR decoding.
Consensus decoding procedures select translations for a single system with a minimum Bayes risk (MBR) (Kumar and Byrne, 2004). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.

In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ In our experiments, a baseline translation model (JHU, 2003), trained on a Chinese-English parallel corpus (NIST, 2003) ( English words and Chinese words), was used to generate 1000-best translation hypotheses for each Chinese sentence in the test set.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ We note that the MAP decoder is not optimal in any of the cases.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ Statistical Machine Translation (Brown et al., 1990) can be formulated as a mapping of a word sequence in a source language to word sequence in the target language that has a word-to-word alignment relative to.
In SMT, MBR decoding allows to minimize the loss of the output for a single translation system. MBR is generally implemented by re-ranking an N best list of translations produced by a first pass decoder (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.

For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ This hierarchy includes the loss functions useful in both situations where we intend to apply MBR decoding.
For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ We first present a hierarchy of loss functions for translation based on different levels of lexical and syntactic information from source and target language sentences.
For each system, we report the performance of max-derivation decoding (MAX) and 1000-best3 MBR decoding (Kumar and Byrne, 2004). $$$$$ For all performance metrics, we show the 70% confidence interval with respect to the MAP baseline computed using bootstrap resampling (Press et al., 2002; Och, 2003).

Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ Using an example, we have shown that this loss function can measure qualities of translation that string (and ngram) based metrics cannot capture.
Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ These loss functions can use information from word strings, word-to-word alignments and parse-trees of the source sentence and its translation.
Finally, we used Minimum Bayes Risk decoding (Kumar and Byrne, 2004) based on the BLEU score (Papineni et al, 2002). $$$$$ The 1000-best lists were then rescored using the different translation loss functions described in Section 2.

We experimented with two decoding settings $$$$$ We did not perform experiments involving this class of loss functions, but mention them for completeness in the hierarchy of loss functions.
We experimented with two decoding settings $$$$$ We present Minimum Bayes-Risk (MBR) decoding for statistical machine translation.
We experimented with two decoding settings $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
We experimented with two decoding settings $$$$$ We also show how MBR decoding can be used to incorporate syntactic structure into a statistical MT system by building specialized loss functions.

Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ Our goal is to find the decoder that has the best performance over all translations.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ Given a loss function and a distribution, it is well known that the decision rule that minimizes the BayesRisk is given by (Bickel and Doksum, 1977; Goel and Byrne, 2000): We shall refer to the decoder given by this equation as the Minimum Bayes-Risk (MBR) decoder.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ For each sentence, we compute the error rate of the hypothesis translation with respect to the most similar reference translation under the corresponding loss function.
Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). $$$$$ Specific Bi-tree loss functions are determined through particular choices of.

We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ The 1000-best lists were then rescored using the different translation loss functions described in Section 2.
We decoded the test set to produce a 300-best list of unique translations, then chose the best candidate for each sentence using Minimum Bayes Risk reranking (Kumar and Byrne, 2004). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.

Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ Each Chinese sentence in this set has four reference translations.
Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
Modifying the multitask objective to incorporate application-specific loss/decoding, such as Minimum Bayes Risk (Kumar and Byrne, 2004). $$$$$ While we have focused on developing MBR procedures for loss functions that measure various aspects of translation quality, this framework can also be used with loss functions which measure application-specific error criteria.

This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ For example, the BLEU, NIST and the PER metrics, though effective, do not take into account explicit syntactic information when measuring translation quality.
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ The English parser was trained on the Penn Treebank and the Chinese parser on the Penn Chinese treebank.
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). $$$$$ We first present a hierarchy of loss functions for translation based on different levels of lexical and syntactic information from source and target language sentences.

For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ That machine translation evaluation continues to be an active area of research is evident from recent workshops (AMTA, 2003).
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ Stringedit distance is measured as the minimum number of edit operations needed to transform a word string to the other word string.
For the system combination task, we first use the minimum Bayes-risk (MBR) (Kumar and Byrne, 2004) decoder to select the best hypothesis as the alignment reference for the Confusion Network (CN) (Mangu et al, 2000). $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.

To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We note that this significance level if otherwise, does meet the customary criteria for minimum significance intervals of 68.3% (Press et al., 2002).
To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ We first present a hierarchy of loss functions for translation based on different levels of lexical and syntactic information from source and target language sentences.
To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). $$$$$ This optimal decoder has the difficulties of search (minimization) and computing the expectation under the true distribution.

Moses Baseline $$$$$ That machine translation evaluation continues to be an active area of research is evident from recent workshops (AMTA, 2003).
Moses Baseline $$$$$ Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.
Moses Baseline $$$$$ The first class of loss functions uses no information about word alignments or parse-trees, so that can be reduced to .
Moses Baseline $$$$$ The performance of the baseline and the MBR decoders under the different loss functions was measured with respect to the four reference translations provided for the test set.

Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ The closeness is measured under the loss function of interest.
Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ We would like to thank all the group members for providing various resources and tools and contributing to useful discussions during the course of the workshop.
Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance.
Our baseline translation system uses Viterbi decoding while our final system uses segment-level Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists using 1 BLEU as the loss function. $$$$$ This work was performed as part of the 2003 Johns Hopkins Summer Workshop research group on Syntax for Statistical Machine Translation.

With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ However we observe that the sentences differ substantially in their syntactic structure (as seen from Parse-Trees in Figure 3), and to a lesser extent in their word-to-word alignments (Figure 1) to the source sentence.
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ We finally report the performance of MBR decoders optimized for each loss function.
With large training data, moving to a 5-gram language model, increasing the cube pruning pop limit to 1000, and using Minimum Bayes-Risk decoding (Kumar and Byrne, 2004) over 500-best lists collectively show a slight improvement. $$$$$ We present results under the Bitree loss function as an example of incorporating linguistic information into a loss function; we have not yet measured its correlation with human assessments of translation quality.

Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ The conventional Maximum A Posteriori (MAP) decoder can be derived as a special case of the MBR decoder by considering a loss function that assigns a equal cost (say 1) to all misclassifications.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ Given an automatic MT metric, we design a loss function based on the metric and use MBR decoding to tune MT performance under the metric.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ The MBR decoder under BLEU loss function obtains a similar (or worse) performance relative to MAP decoder on all metrics other than BLEU.
Kumar and Byrne (2004) first introduced MBR decoding to SMT field and developed it on the N-best list translations. $$$$$ We note that this significance level if otherwise, does meet the customary criteria for minimum significance intervals of 68.3% (Press et al., 2002).

Minimum Bayes Risk Rescoring $$$$$ In contrast, the maximum likelihood techniques that underlie the decision processes of most current MT systems do not take into account these application specific goals.
Minimum Bayes Risk Rescoring $$$$$ We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings, word-to-word alignments from an MT system, and syntactic structure from parse-trees of source and target language sentences.
Minimum Bayes Risk Rescoring $$$$$ Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions.
