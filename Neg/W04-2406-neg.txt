 $$$$$ These feature vectors are in fact the first order context vectors of the feature words (and not target word).
 $$$$$ The set of words forming the rows/columns of the co–occurrence matrix are treated as the feature words.
 $$$$$ We present the discrimination results for six configurations of features, context representations and clustering algorithms.
 $$$$$ To be clear, we do not believe that unsupervised word sense discrimination needs to be carried out relative to a pre-existing set of senses.

The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ We are also planning to automatically attach descriptive labels to the discovered clusters that capture the underlying word sense.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ Thus, the second order context representation relies on the first order context vectors of feature words.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ As can be seen from column S in Table 1, most of the words have 2 to 4 senses on an average.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ By comparing such descriptive features of each cluster with the words that occur in actual dictionary definitions of the target word, we plan to carry out fully automated word sense disambiguation that does not rely on any manually annotated text.

Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ This research is supported by a National Science Foundation Faculty Early CAREER Development Award (#0092784).
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ Both the co–occurrences and the bigrams must occur in at least two instances in the training data, and the two words must have a log– likelihood ratio in excess of 3.841, which has the effect of removing co–occurrences and bigrams that have more than 95% chance of being independent of the target word.
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ Divisive methods start with all instances in the same cluster and split one cluster into two during each iteration until all instances are in their own cluster.
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ This occurs because in small and sparse data, direct first order features are seldom observed in both the training and the test data.

(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ However, solutions to disambiguation usually require the availability of an external knowledge source or manually created sense– tagged training data.
(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ Pedersen and Bruce use a small number of local features that include co–occurrence and part of speech information near the target word.
(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ In creating our evaluation standard, we assume that each instance will be assigned to at most a single cluster.

This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ The most widely known criteria functions used with hierarchical agglomerative algorithms are single link, complete link, and average link, also known as UPGMA.
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ Thus, Sch¨utze represents each feature as a vector of words that occur in its context and then computes the context of the target word by adding the feature vectors of significant content words that occur near the target word in that context.
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ Any score that is significantly greater than the majority sense (according to a paired t–test) is shown in bold face.

A detailed description can be found in Purandare and Pedersen (2004). $$$$$ The particular features we are interested in are bigrams and co–occurrences.
A detailed description can be found in Purandare and Pedersen (2004). $$$$$ We measure the precision and recall based on this maximally accurate assignment of sense tags to clusters.

The method is described in Purandare and Pedersen (2004). $$$$$ When we cluster test instances, we specify an upper limit on the number of clusters that can be discovered.
The method is described in Purandare and Pedersen (2004). $$$$$ Thus, the words representing the rows of the bigram matrix make the feature set while the words representing the columns form the dimensions of the feature space.
The method is described in Purandare and Pedersen (2004). $$$$$ We believe this is the case because in a large sample of data, it is very likely that the features that occur in the training data will also occur in the test data, making it possible to represent test instances with fairly rich feature sets.
The method is described in Purandare and Pedersen (2004). $$$$$ Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space.

Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ Thus, Sch¨utze represents each feature as a vector of words that occur in its context and then computes the context of the target word by adding the feature vectors of significant content words that occur near the target word in that context.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ We present an extensive comparative analysis of word sense discrimination techniques using first order and second order context vectors, where both can be employed in similarity and vector space.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ When agglomerative clustering starts, each node is in its own cluster and is considered to be the centroid of that cluster.

 $$$$$ We will draw our large corpora from a variety of sources, including the British National Corpus, the English GigaWord Corpus, and the Web.
 $$$$$ This information is recorded in a (dis)similarity matrix whose rows/columns represent the instances of the target word that are to be discriminated.
 $$$$$ Each row of the bigram matrix can be seen as a bigram vector that shows the scores of all bigrams in which the word represented by that row occurs as the first word.
 $$$$$ This information is recorded in a (dis)similarity matrix whose rows/columns represent the instances of the target word that are to be discriminated.

Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ We believe this is the case because in a large sample of data, it is very likely that the features that occur in the training data will also occur in the test data, making it possible to represent test instances with fairly rich feature sets.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Thus, we have chosen to use average link/UPGMA as our criteria function for the agglomerative experiments.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ In particular, Zhao and Karypis recommend a hybrid approach known as Repeated Bisections.

Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ This occurs because in small and sparse data, direct first order features are seldom observed in both the training and the test data.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ This occurs because in small and sparse data, direct first order features are seldom observed in both the training and the test data.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ The F–measure associated with the discrimination of each word is shown in Table 1.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ After selecting a set of co-occurrences or bigrams from a corpus of training data, a first order context representation is created for each test instance.

Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ However, the data specific to a given target word will capture the word usages in the immediate context of the target word.
Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ We summarize techniques for clustering in vector versus similarity spaces, and then present our experimental methodology, including a discussion of the data used in our experiments.
Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ Our basic strategy for evaluation is to assign available sense tags to the discovered clusters such that the assignment leads to a maximally accurate mapping of senses to clusters.

First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ The SENSEVAL-2 data is relatively small, in that each word has approximately 50-200 training and test instances.
First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ This research is supported by a National Science Foundation Faculty Early CAREER Development Award (#0092784).
First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ As yet there has not been any systematic study to determine which set of techniques results in better sense discrimination.
First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ We evaluate the discriminated clusters by carrying out experiments ussense–tagged instances of 24 words and the well known corpora.

(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ It then assigns every other vector to one of the K clusters whose centroid is closest to that vector.
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ When the number of clusters is greater than the number of actual senses, then some clusters will be left unassigned.

Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ From that we compute the F–measure, which is two times the precision and recall, divided by the sum of precision and recall.
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ This corresponds to several well known problems, among them the Assignment Problem in Operations Research, or determining the maximal matching of a bipartite graph in Graph Theory.
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ Thereafter we show the number of training (TRN) and test instances (TST) that remain after filtering, and the number of senses found in the test data (S).
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ As can be seen from column S in Table 1, most of the words have 2 to 4 senses on an average.

In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space.
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ Pedersen and Bruce use a small number of local features that include co–occurrence and part of speech information near the target word.
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ Hierarchical algorithms are either agglomerative or divisive.
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ Any score that is significantly greater than the majority sense (according to a paired t–test) is shown in bold face.

Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ The (dis)similarity is computed from the first order context vectors of the instances which show each instance as a vector of features that directly occur near the target word in that instance.
Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ After all vectors are assigned, it recomputes the cluster centroids by averaging all of the vectors assigned to that cluster.
Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ In short, Table 3 compares PB1 against PB2 and SC1 against SC2.

These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ Sch¨utze, by contrast, uses the second order context representation that averages the first order context vectors of individual features that occur near the target word in the instance.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ This is constructed from the co–occurrence pairs, and is a symmetric adjacency matrix whose cell values show the loglikelihood ratio for the pair of words representing the corresponding row and column.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ As can be seen from column S in Table 1, most of the words have 2 to 4 senses on an average.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ We present the discrimination results for six configurations of features, context representations and clustering algorithms.

Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ As such this is a hybrid method that combines a hierarchical divisive approach with partitioning.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ Pedersen and Bruce represent the context of each test instance as a vector of features that directly occur near the target word in that instance.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ First order context vectors directly indicate which features make up a context.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ Experiment PB2 clusters the Pedersen and Bruce style (first order) context vectors using the Sch¨utze like clustering scheme, while SC2 tries to see the effect of using the Pedersen and Bruce style clustering method on Sch¨utze style (second order) context vectors.

Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ We measure the precision and recall based on this maximally accurate assignment of sense tags to clusters.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ The context of each instance is represented as a vector in a high dimensional feature space.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ Table 2 compares PB1 against PB3, and SC1 against SC3, when these methods are used to discriminate the 24 SENSEVAL-2 words.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ Our motivation is that the larger corpora will provide more generic co–occurrence information about words without regard to a particular target word.
