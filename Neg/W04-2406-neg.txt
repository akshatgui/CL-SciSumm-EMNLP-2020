 $$$$$ The data is particularly challenging for unsupervised algorithms due to the large number of fine grained senses, generally 8 to 12 per word.
 $$$$$ We also randomly selected five pairs of words from the SENSEVAL-2 data and mixed their instances together (while retaining the training and test distinction that already existed in the data).
 $$$$$ The actual choice of which clusters to split or merge is decided by a criteria function.
 $$$$$ We believe this is the case because in a large sample of data, it is very likely that the features that occur in the training data will also occur in the test data, making it possible to represent test instances with fairly rich feature sets.

The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ By contrast, word sense discrimination is an unsupervised clustering problem.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ We begin with a summary of previous work, and then a discussion of features and two types of context vectors.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ Manually sense tagged text is not required, nor are specific knowledge rich resources like dictionaries or ontologies.
The first one to propose this idea of context-group discrimination was Schutze (1998), and many researchers followed a similar approach to sense induction (Purandare and Pedersen, 2004). $$$$$ Each type of data brings with it unique characteristics, and sheds light on different aspects of our experiments.

Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ As yet there has not been any systematic study to determine which set of techniques results in better sense discrimination.
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ We believe that second order methods work better on smaller amounts of data, in that the feature spaces are quite small, and are not able to support the degree of exact matching of features between instances that first order vectors require.
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ These labels will be created from the most characteristic features used by the instances belonging to the same cluster.
Common operations include altering feature weights and dimensionality reduce Document-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al, 1975) Co-occurrence Models HAL (Burgess and Lund, 1997) COALS (Rohde et al, 2009) Approximation Models Random Indexing (Sahlgren et al, 2008) Reflective Random Indexing (Cohen et al, 2009) TRI (Jurgens and Stevens, 2009) BEAGLE (Jones et al, 2006) Incremental Semantic Analysis (Baroni et al, 2007) Word Sense Induction Models Purandare and Pedersen (Purandare and Pedersen, 2004) HERMIT (Jurgens and Stevens, 2010). $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.

(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ First order context vectors directly indicate which features make up a context.
(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ The set of words forming the rows/columns of the co–occurrence matrix are treated as the feature words.
(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ In similarity space, each instance can be viewed as a node of a fully connected weighted graph whose edges indicate the similarity between the instances they connect.
(Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. $$$$$ Then we describe our approach to the evaluation of unsupervised word sense discrimination.

This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ By contrast, word sense discrimination is an unsupervised clustering problem.
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ Table 4 summarizes the overall performance of each of these experiments compared with the majority class.
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ This shows how many times each feature occurs in the context of the target word (i.e., within 20 positions from the target word) in that instance.
This finding coincides to that of Purandare and Pedersen (2004) and Pedersen (2010) who found that with large amounts of data, first-order vectors perform better than second-order vectors, but second-order vectors are a good option when large amounts of data are not available. $$$$$ This information is recorded in a (dis)similarity matrix whose rows/columns represent the instances of the target word that are to be discriminated.

A detailed description can be found in Purandare and Pedersen (2004). $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.
A detailed description can be found in Purandare and Pedersen (2004). $$$$$ They both proceed iteratively, and merge or divide clusters at each step.
A detailed description can be found in Purandare and Pedersen (2004). $$$$$ However, the data specific to a given target word will capture the word usages in the immediate context of the target word.
A detailed description can be found in Purandare and Pedersen (2004). $$$$$ We evaluate the discriminated clusters by carrying out experiments ussense–tagged instances of 24 words and the well known corpora.

The method is described in Purandare and Pedersen (2004). $$$$$ We summarize techniques for clustering in vector versus similarity spaces, and then present our experimental methodology, including a discussion of the data used in our experiments.
The method is described in Purandare and Pedersen (2004). $$$$$ Table 1 shows all words that were used in our experiments along with their parts of speech.
The method is described in Purandare and Pedersen (2004). $$$$$ We begin with a summary of previous work, and then a discussion of features and two types of context vectors.

Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ When given smaller amounts of data like SENSEVAL-2, second order context vectors and a hybrid clustering method like Repeated Bisections perform better.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ Nearly all of the experiments carried out with the 6 different methods perform better than the majority sense in the case of the mix-words.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ The context of each instance is represented as a vector in a high dimensional feature space.
Purandare and Pedersen (2004) report that this method generally performed better where there was a reasonably large amount of data available (i.e., several thousand contexts). $$$$$ Semantically related instances use words that are conceptually the same but perhaps not lexically.

 $$$$$ These features are very sparse, and as such most instances do not share many common features with other instances, making first order clustering difficult.
 $$$$$ When the number of clusters is greater than the number of actual senses, then some clusters will be left unassigned.
 $$$$$ When given smaller amounts of data like SENSEVAL-2, second order context vectors and a hybrid clustering method like Repeated Bisections perform better.
 $$$$$ After selecting a set of co-occurrences or bigrams from a corpus of training data, a first order context representation is created for each test instance.

Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Put another way, contexts that are grouped together in the same class represent a particular word sense.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ Bigrams are ordered pairs of words that co–occur within five positions of each other.
Table 4 also shows several unsupervised systems, all of which except Cymfony and (Purandare and Pedersen, 2004) participated in S3LS (check (Mihalcea et al, 2004) for further details on the systems). $$$$$ This is particularly useful, since this is the accuracy that would be attained by a baseline clustering algorithm that puts all test instances into a single cluster.

Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ Thus, we will test the hypothesis that a smaller sample of data where each instance includes the target word is more effective for sense discrimination than a more general corpus of training data.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ We plan to conduct experiments that compare the effect of using very large amounts of training data versus smaller amounts where each instance includes the target word (as is the case in this paper).
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data. $$$$$ The F–measure associated with the discrimination of each word is shown in Table 1.

Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ (Sch¨utze, 1998) points out that single link clustering tends to place all instances into a single elongated cluster, whereas (Pedersen and Bruce, 1997) and (Purandare, 2003) show that hierarchical agglomerative clustering using average link (via McQuitty’s method) fares well.
Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ We believe this is the case because in a large sample of data, it is very likely that the features that occur in the training data will also occur in the test data, making it possible to represent test instances with fairly rich feature sets.
Schutze's (1998) approach has been implemented in the SenseClusters program (Purandare and Pedersen, 2004), which also incorporates some interesting variations on and extensions to the original algorithm. $$$$$ We then eliminate any words that have less than 90 training instances after filtering.

First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ The F–measure associated with the discrimination of each word is shown in Table 1.
First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ As such these are knowledge intensive methods that are difficult to adapt to new domains.
First, Purandare and Pedersen (2004) defend the use of bigram features instead of simple word features. $$$$$ However, bigram features show clear improvement in the results of second order context vectors (SC).

(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ This research is supported by a National Science Foundation Faculty Early CAREER Development Award (#0092784).
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ To be clear, we do not believe that unsupervised word sense discrimination needs to be carried out relative to a pre-existing set of senses.
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ After mixing, the data was filtered such that any sense that made up less than 10% in the training or test data of the new mixed sample was removed; this is why the total number of instances for the mixed pairs is not the same as the sum of those for the individual words.
(Purandare and Pedersen,2004 ,p.2) and will be used throughout this paper. $$$$$ However, here we employ sense–tagged text in order to evaluate the clusters that we discover.

Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ Experiment PB2 clusters the Pedersen and Bruce style (first order) context vectors using the Sch¨utze like clustering scheme, while SC2 tries to see the effect of using the Pedersen and Bruce style clustering method on Sch¨utze style (second order) context vectors.
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ And when the number of senses is greater than the number of clusters, some senses will not be assigned to any cluster.
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ Discrimination is achieved by clustering these context vectors directly in vector space and also by finding pairwise similarities among the vectors and then clustering in similarity space.
Purandare and Pedersen (2004) used a log likelihood test to select their features, probably because of the intuition that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation (Schutze, 1998 ,p.102). $$$$$ From this, we observe that with both first order and second order context vectors Repeated Bisections is more effective than UPGMA.

In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ Then the value of cell (i,j) indicates the log–likelihood ratio of the words in the bigram WORDi<>WORDj.
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ We plan to conduct experiments that compare the effect of using very large amounts of training data versus smaller amounts where each instance includes the target word (as is the case in this paper).
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ All of the experiments in this paper were carried out with version 0.47 of the SenseClusters package, freely available from the URL shown on the title page.
In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. $$$$$ The second order experiments in this paper use two different types of features, co–occurrences and bigrams, defined as they are in the first order experiments.

Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ The SENSEVAL-2 words have a relatively small number of training and test instances (around 50-200).
Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ Our motivation is that the larger corpora will provide more generic co–occurrence information about words without regard to a particular target word.
Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ Each type of data brings with it unique characteristics, and sheds light on different aspects of our experiments.
Pedersen et al.(2006) and Purandare and Pedersen (2004 ) integrate second-order co-occurrence of words into the similarity function. $$$$$ All of the SC experiments use second order context vectors and hence follow the approach suggested by Sch¨utze.

These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ This is motivated by (Miller and Charles, 1991), who hypothesize that words with similar meanings are often used in similar contexts.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ In all of our experiments, the context of the target word is limited to 20 surrounding content words on either side.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ In similarity space, each instance can be viewed as a node of a fully connected weighted graph whose edges indicate the similarity between the instances they connect.
These hubs are used as a representation of the senses induced by the system, the same way that clusters of examples are used to represent senses in clustering approaches to WSD (Purandare and Pedersen, 2004). $$$$$ This paper also proposes and evaluates several extensions to these techniques.

Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ In all of our experiments, the context of the target word is limited to 20 surrounding content words on either side.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ During evaluation we assign one sense to at most one cluster, and vice versa.
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ Table 1 shows that the top 3 experiments for each of the mixed-words are all second order vectors (SC).
Another system similar to ours is (Purandare and Pedersen, 2004), which unfortunately was evaluated on Senseval 2 data and is not included in the table. $$$$$ This research is supported by a National Science Foundation Faculty Early CAREER Development Award (#0092784).

Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ Both the co–occurrences and the bigrams must occur in at least two instances in the training data, and the two words must have a log– likelihood ratio in excess of 3.841, which has the effect of removing co–occurrences and bigrams that have more than 95% chance of being independent of the target word.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ In particular, Zhao and Karypis recommend a hybrid approach known as Repeated Bisections.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ We summarize techniques for clustering in vector versus similarity spaces, and then present our experimental methodology, including a discussion of the data used in our experiments.
Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch ?utze, 1998), and the Information Bottleneck (Niu et al., 2007). $$$$$ However, the data specific to a given target word will capture the word usages in the immediate context of the target word.
