Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it. $$$$$ In this domain companies typically say things, and texts often begin with a statement of what a company or an individual has said (see sentence (1) in Figure 3).
Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it. $$$$$ We performed Post-hoc Tukey tests to further examine whether there are any significant differences among the different features and between our model and the baseline.
Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it. $$$$$ We describe a model that learns constraints on sentence order from a corpus of domainspecific texts and an algorithm that yields the most likely order among several alternatives.
Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's t, which is a measure of how much an ordering differs from the OSO --- the underlying assumption is that most reasonable sentence orderings should be fairly similar to it. $$$$$ The author was supported by EPSRC grant number R40036.

 $$$$$ There are a number of issues that must be addressed in future work.
 $$$$$ An example of the texts in this newswire corpus is shown in Figure 3.
 $$$$$ Examples of applications that require some form of text structuring, are single- and multidocument summarization as well as question answering.
 $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.

 $$$$$ Barzilay et al. (2002) address the problem of information ordering in multidocument summarization and show that naive ordering algorithms such as majority ordering (selects most frequent orders across input documents) and chronological ordering (orders facts according to publication date) do not always yield coherent summaries although the latter produces good results when the information is eventbased.
 $$$$$ Our model operates on the surface level rather than the logical form and is therefore suitable for text-to-text generation systems; it acquires ordering constraints automatically, and can be easily ported to different domains and text genres.
 $$$$$ Some of the dependencies for sentence (2) from Figure 3 are shown in Table 1.

The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). $$$$$ The denominator expresses the number of times ai−1,k is attested in the corpus (preceded by any feature).
The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). $$$$$ Barzilay et al. (2002) address the problem of information ordering in multidocument summarization and show that naive ordering algorithms such as majority ordering (selects most frequent orders across input documents) and chronological ordering (orders facts according to publication date) do not always yield coherent summaries although the latter produces good results when the information is eventbased.
The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.
The same data and similar methods were used by Barzilay and Lee (2004) to compare their probabilistic approach for ordering sentences with that of Lapata (2003). $$$$$ Each theme is next syntactically analysed into predicate argument structures; the structures that are repeated often enough are chosen to be included into the summary.

It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. $$$$$ Recall that the summaries describe events across documents.
It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. $$$$$ Ordering information is a critical task for natural language generation applications.
It is typically applicable in the text generation field, both for concept-to-text generation and text-to text generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. $$$$$ Finding the tree that yields the best possible text is effectively a search problem.

For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. $$$$$ We also assess the appropriateness of such a model for multidocument summarization.
For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. $$$$$ Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.
For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. $$$$$ The greedy algorithm implements a search procedure with a beam of width one.
For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. $$$$$ We also used 20 randomly chosen texts (disjoint from the test data) for development purposes (average length 16.2).

The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. $$$$$ The output of MINIPAR is a dependency graph which represents the dependency relations between words in a sentence (see Table 1 for an example).
The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. $$$$$ We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task.
The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. $$$$$ First we calculate which of the three sentences S1, S2, and S3 is most likely to start the text (during training we record which sentences appear in the beginning of each text).
The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. $$$$$ The author was supported by EPSRC grant number R40036.

In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005). $$$$$ Our results indicate that the model can successfully generate orders for texts taken from the corpus on which it is trained.
In contrast, more recent research has focused on stochastic approaches that model discourse coherence at the local lexical (Lapata, 2003) and global levels (Barzilay and Lee, 2004), while preserving regularities recognized by classic discourse theories (Barzilay and Lapata, 2005). $$$$$ We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task.

 $$$$$ But before we outline the details of our experiments we discuss our choice of metric for comparing different orders.
 $$$$$ An obvious question is whether a trigram model performs better than the model presented here.
 $$$$$ The node which yields the highest conditional probability is selected and ordered ahead.

 $$$$$ Some of the dependencies for sentence (2) from Figure 3 are shown in Table 1.
 $$$$$ Structuring a set of facts into a coherent text is a non-trivial task which has received much attention in the area of concept-to-text generation (see Reiter and Dale 2000 for an overview).
 $$$$$ This is in agreement with Experiment 1 and points to the importance of lexical and structural information for the ordering task.

In contrast, the greedy algorithm of Lapata (2003) makes grave search errors. $$$$$ The probabilities P(ai,j|ai−1,k) will be unreliable when the frequency estimates for f (ai, j,ai−1,k) are small, and undefined in cases where the feature combinations are unattested in the corpus.
In contrast, the greedy algorithm of Lapata (2003) makes grave search errors. $$$$$ Mellish et al. (1998) advocate stochastic search as an alternative to exhaustively examining the search space.
In contrast, the greedy algorithm of Lapata (2003) makes grave search errors. $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.
In contrast, the greedy algorithm of Lapata (2003) makes grave search errors. $$$$$ In this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus.

The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. $$$$$ Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.
The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. $$$$$ Nouns.
The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.
The genetic algorithms of Mellish et al (1998) and Karamanis and Manarung (2002), as well as the greedy algorithm of Lapata (2003), provide no theoretical guarantees on the optimality of the solutions they propose. $$$$$ Since we need to compare sentence pairs with varied numbers of features, we will normalize the conditional probabilities P(Si|Si−1) by the number feature of pairs that form the Cartesian product over Si and Si−1.

Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). $$$$$ According to (1), the task of predicting the next sentence is dependent on its n − i previous sentences.
Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). $$$$$ So, P(h|e) will be 0.16 given that f(h,e) is one and f(e) is six (see the normalization in (5)).
Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). $$$$$ We experimented with different feature encodings and showed that lexical and syntactic information is important for the ordering task.
Adjacency of sentences has been previously used to model local coherence (Lapata, 2003). $$$$$ The metric is sensitive to the fact that some sentences may be always ordered next to each other even though their absolute orders might differ.

Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ The model operates on sentences rather than facts in a knowledge base and is potentially useful for text-to-text generation applications.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ The node which yields the highest conditional probability is selected and ordered ahead.
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ Table 3 gives the average i (T) for all 20 test texts when the following features are used: lemmatized verbs (VL), tensed verbs (VT), lemmatized nouns (NL), lemmatized verbs and nouns (VLNL), tensed verbs and lemmatized nouns (VTNL), verb-related dependencies (VD), noun-related dependencies (ND), verb and noun dependencies (VDND), and all available dependencies (AD).
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. $$$$$ The algorithm starts by assigning each vertex v  V a probability.

In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. $$$$$ In the future we plan to experiment with larger widths (e.g., two or three) and also take into account features that express semantic similarities across documents either by relying on WordNet or on automatic clustering methods.
In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. $$$$$ Such experiments however are crucial for determining how coherent the generated texts are and whether they convey the same semantic content as the originally authored texts.

Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering. $$$$$ We next compare the probabilities P(S1|S2) and P(S3|S2).
Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering. $$$$$ Recall that the summaries describe events across documents.
Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering. $$$$$ The parser is a “maximum-entropy inspired” probabilistic generative model.
Lapata (2003) has suggested a probabilistic model of text structuring and its application to the sentence ordering. $$$$$ So far our evaluation metric measures order similarities or dissimilarities.

Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies). $$$$$ We evaluate the automatically generated orderings against authored texts from our corpus and against human subjects that are asked to mimic the model’s task.
Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies). $$$$$ Some of the dependencies for sentence (2) from Figure 3 are shown in Table 1.
Even though we could not compare our experiment with the probabilistic approach (Lapata, 2003) directly due to the difference of the text corpora, the Kendall coefficient reported higher agreement than Lapata's experiment (Kendall=0.48 with lemmatized nouns and Kendall=0.56 with verb-noun dependencies). $$$$$ Thanks also to Stephen Clark, Nikiforos Karamanis, Frank Keller, Alex Lascarides, Katja Markert, and Miles Osborne for helpful comments and suggestions.

Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.
Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. $$$$$ Lin (1998) evaluated the parser on the SUSANNE corpus (Sampson, 1996), a domain independent corpus of British English, and achieved a recall of 79% and precision of 89% on the dependency relations.
Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. $$$$$ This seems appropriate given that flipping the introduction in a document with the conclusions seriously disrupts coherence.
Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. $$$$$ The model learns which sequences of features are likely to co-occur and makes predictions concerning preferred orderings.

Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. $$$$$ Barzilay et al. further conduct a study where subjects are asked to produce a coherent text from the output of a multidocument summarizer.
Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. $$$$$ Our results indicate that the model can successfully generate orders for texts taken from the corpus on which it is trained.
Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. $$$$$ We experimented with different feature encodings and showed that lexical and syntactic information is important for the ordering task.
Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.

As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences. $$$$$ A language generation system outputs a sentence (per theme) from the selected predicate argument structures.
As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences. $$$$$ So far our evaluation metric measures order similarities or dissimilarities.
As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences. $$$$$ We are grateful to Regina Barzilay and Noemie Elhadad for making available their software and for providing valuable comments on this work.
As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences. $$$$$ The model learns which sequences of features are likely to co-occur and makes predictions concerning preferred orderings.
