In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ In the 2003 Sighan Bakeoff (Sproat & Emer son 2003), no single model performed well on all corpora included in the task.
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ In the 2003 Sighan Bakeoff (Sproat & Emer son 2003), no single model performed well on all corpora included in the task.

Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ Table 1 The number of features in each corpus # of data features # of lambda weights AS 2,558,840 8,076,916 HK 2,308,067 7,481,164 PK 1,659,654 5,377,146 MSR 3,634,585 12,468,890 3.2 Experiments.
Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for pa rameter optimization.
Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). $$$$$ Our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features.
For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). $$$$$ If the new punctuation were not unknown to us, our per formance on HK data would have gone up to 0.952 F and the unknown word features would have not hurt the system too much.

(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.

 $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
 $$$$$ Sometimes, ?8?

They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for.
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.

The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ We used two such fea ture functions, one that fires if the previous and the current character, C-1 and C0, are identical and one that does so if the subsequent and the previous characters, C-1 and C1, are identical.
The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).
The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ 0.960 HK 0.941 0.946 0.943?
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ We concluded that, for future systems, generaliza tion across such different Mandarin varieties is crucial.
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.

 $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
 $$$$$ In the 2003 Sighan Bakeoff (Sproat & Emer son 2003), no single model performed well on all corpora included in the task.

Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ Thanks to Kristina Toutanova for her generous help and to Jenny Rose Finkel who devel oped such a great conditional random field package.
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.

English-Chinese: For training we used the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
English-Chinese: For training we used the LDC Sinorama and FBIS tests (LDC2005T10 and LDC2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
 $$$$$ Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).
 $$$$$ Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for pa rameter optimization.
 $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Thanks to Kristina Toutanova for her generous help and to Jenny Rose Finkel who devel oped such a great conditional random field package.
For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.

 $$$$$ In the 2003 Sighan Bakeoff (Sproat & Emer son 2003), no single model performed well on all corpora included in the task.
 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
 $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ We concluded that, for future systems, generaliza tion across such different Mandarin varieties is crucial.

This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled.
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ Thanks to Kristina Toutanova for her generous help and to Jenny Rose Finkel who devel oped such a great conditional random field package.
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ In the 2003 Sighan Bakeoff (Sproat & Emer son 2003), no single model performed well on all corpora included in the task.
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.

The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ Thanks to Kristina Toutanova for her generous help and to Jenny Rose Finkel who devel oped such a great conditional random field package.
The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.

For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.
For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ We give our results using just character identity based features; character identity features plus unknown words and reduplication features.
The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Our system builds on research into condi tional random field (CRF), a statistical sequence modeling framework first introduced by Lafferty et al (2001).
The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.

Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). $$$$$ Finally, some errors are due to inconsistencies in the gold segmentation of non-hanzi char acter.
Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). $$$$$ Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.
Stanford Chinese word segmenter (STANFORD): The Stanford Chinese word segmenter is another well-known CWS tool (Tseng et al., 2005). $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.
