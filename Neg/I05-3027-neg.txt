In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: ( ) ( )??
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al, 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. $$$$$ We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.

Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: ( ) ( )??
Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
Using the same approach as in (Tseng et al, 2005), we extracted the most frequent words tagged with 'B', indicating a prefix, and the last words tagged with 'I', denoting a suffix. $$$$$ Work by Peng et al (2004) first used this framework for Chinese word segmen tation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one.

For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stanford segmenter (Tseng et al, 2005). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ We also did normalization of punctuations due to the fact that Mandarin has a huge variety of punctuations.
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
(Tseng et al, 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively. $$$$$ Our segmenter was built using a condi tional random field sequence model that provides a framework to use a large number of linguistic features such as character identity, morphological and character reduplication features.

 $$$$$ We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.
 $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
 $$$$$ 3.1 Features.

They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ Our system used a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features.
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ We concluded that, for future systems, generaliza tion across such different Mandarin varieties is crucial.
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.
They are selected from similar sources to the newswire articles, and are normalized (Zhang and Kahn, 2008) and word segmented (Tseng et al, 2005a). $$$$$ This in dicates that our unknown word features were more useful for corpora with segmentation stan dards that tend to result in longer words.

The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.
The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.
The Chinese sentences are segmented using the Stanford Chinese word segmenter (Tseng et al, 2005). $$$$$ Thus, our system does not overfit the variety of Mandarin most familiar to the system's designers.

We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ ?to burn some one?
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
We use the Stanford Chinese word segmenter (Tseng et al, 2005) and POS tagger (Toutanova et al., 2003) for preprocessing and Cilin for synonym definition during matching. $$$$$ Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled.

 $$$$$ The 2005 Sighan Bakeoff included four dif ferent corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking Univer sity (PK), and Microsoft Research Asia (MSR), each of which has its own definition of a word.
 $$$$$ Across cor pora, variation is seen in both the lexicons and also in the word segmentation standards.
 $$$$$ 0.718?
 $$$$$ Y is the label sequence for the sentence, X is the sequence of unsegmented characters, Z(X) is a normalization term, fk is a feature function, and c indexes into characters in the sequence being labeled.

Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ A CRF allows us to utilize a large number of n-gram features and different state sequence 168 based features and also provides an intuitive framework for the use of morphological features.
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: ( ) ( )??
Morphological segmentation for these two languages was carried out using MeCab (MeCab, 2011) and the Stanford Word Segmenter (Tseng et al, 2005), respectively. $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.

English-Chinese $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.
English-Chinese $$$$$ Table 4 present recalls (R), precisions (P), f scores (F) and recalls on both unknown (Roov) and known words (Riv).

 $$$$$ Gaussian priors were used to prevent overfitting and a quasi-Newton method was used for pa rameter optimization.
 $$$$$ 3.1 Features.
 $$$$$ The 2005 Sighan Bakeoff included four dif ferent corpora, Academia Sinica (AS), City University of Hong Kong (HK), Peking Univer sity (PK), and Microsoft Research Asia (MSR), each of which has its own definition of a word.
 $$$$$ ?Cc k c cXYkkXZ XYP f ,,exp)( 1| ??

For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for.
For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.
For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIStests (LCD2005T10 and LCD2003E14), and segmented the Chinese side with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Additionally, we also use reduplication feature functions that are active based on the repetition of a given character.

 $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
 $$$$$ We concluded that, for future systems, generaliza tion across such different Mandarin varieties is crucial.
 $$$$$ Across cor pora, variation is seen in both the lexicons and also in the word segmentation standards.

Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ in PK) and ??
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
Chinese was automatically segmented by the Stanford segmenter (Tseng et al, 2005), and traditional characters were simplified. $$$$$ 4) C0 suffix feature functions are defined over suffix table characters, and fire if the char acter in the current state matches the feature function?s character.

This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ The linguistic features used in our model fall into three categories: character identity n-grams,morphological and character reduplication fea tures.
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ In the 2003 Sighan Bakeoff (Sproat & Emer son 2003), no single model performed well on all corpora included in the task.
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).
This character-by-character method was first proposed by (Xue, 2003), and a number of discriminative sequential learning algorithms have been exploited, including structured perceptron (Jiang et al, 2009), the Passive-Aggressive algorithm (Sun, 2010), conditional random fields (CRFs) (Tseng et al, 2005), and latent variable CRFs (Sun et al, 2009). $$$$$ Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for.

The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ Because our morphological features were extracted from the training cor pora automatically, our system was not biased toward any particular variety of Mandarin.
The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ Rather, systems tended to do well on corpora largely drawn from a set of similar Mandarin varieties to the one they were originally developed for.
The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ To this end, we proposed a new model using character identity, morphological and character reduplication features in a conditional random field modeling framework.
The Chinese data was word segmented using the GALE Y2 retest release of the Stanford CRF segmenter (Tseng et al, 2005). $$$$$ This work was funded by the Ad vanced Research and Development Activity's Advanced Question Answering for Intelligence Program, National Science Foundation award IIS-0325646 and a Stanford Graduate Fellow ship.

For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ Thanks to Kristina Toutanova for her generous help and to Jenny Rose Finkel who devel oped such a great conditional random field package.
For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ In the HK corpus, when we added in un known word features, our performance dropped.
For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al, 2005). $$$$$ We extracted our morphological information automatically to prevent overfitting Mandarin from particular Mandarin-speaking area.

The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ The probability assigned to a label sequence for a particular sequence of characters by a CRF is given by the equation below: ( ) ( )??
The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ 3.2.1 Results on Sighan bakeoff 2003 Experiments done while developing this system showed that its performance was signifi cantly better than that of Peng et al (2004).
The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al, 2005). $$$$$ Our final system achieved a F-score of 0.947 (AS), 0.943 (HK), 0.950 (PK) and 0.964 (MSR).

Stanford Chinese word segmenter (STANFORD) $$$$$ We present a Chinese word seg mentation system submitted to the closed track of Sighan bakeoff 2005.
Stanford Chinese word segmenter (STANFORD) $$$$$ ?Cc k c cXYkkXZ XYP f ,,exp)( 1| ??
Stanford Chinese word segmenter (STANFORD) $$$$$ We also did normalization of punctuations due to the fact that Mandarin has a huge variety of punctuations.
