(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ This penalty may be defined, for example, as the number of labeled spans on which the two trees do not agree.
(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ We assume that the feature vector for a sentence and parse tree (x, y) decomposes into a sum of the feature vectors for its parts: In CFGs, the function O(x, r) can be any function mapping a rule production and its position in the sentence x, to some feature vector representation.
(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ There is a major problem with both the primal and the dual formulations above: since each potential mistake must be ruled out, the number of variables or constraints is proportional to |G(x)|, the number of possible parse trees.

Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ The feasible set for α is Now let Δm be the space of marginal vectors which are feasible: Then our original optimization problem can be reframed as maxµEAm Qm(µ).
Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ We have reduced the problem to a polynomial size QP, which, in principle, can be solved using standard QP toolkits.

We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ For clarity of presentation, we restrict the grammar to be in Chomsky normal form (CNF), where all rules in the grammar are of the form hA → B Ci or hA → ai, where A, B and C are non-terminal symbols, and a is some terminal symbol.
We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ The primary contribution of this paper is the extension of the max-margin approach of Taskar et al. (2003) to context free grammars.

We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ The tuple specifies a particular rule A —* B C, and its position, including split point m, within the sentence i, such as q in figure 1(b), and corresponds to the traditional notion of a traversal in a tabular parser.
We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ This follows from substituting the factored definitions of the feature representation Φ and loss function L together with definition of marginals.
We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ We have presented a maximum-margin approach to parsing, which allows a discriminative SVM-like objective to be applied to the parsing problem.

To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Recently, it has also been extended to graphical models (Taskar et al., 2003; Altun et al., 2003) and shown to outperform the standard maxlikelihood methods.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Even in grammars without unary chains or empty elements, the number of parses is generally exponential in the length of the sentence, so we cannot expect to solve the above problem without any assumptions about the feature-vector representation `b and loss function L. For that matter, for arbitrary representations, to find the best parse given a weight vector, we would have no choice but to enumerate all trees and score them.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ While like most discriminative models it is compute-intensive to train, it allows fast parsing, remaining cubic despite the incorporation of lexical features.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ We omit the details of the structured SMO procedure, but the important fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights.

 $$$$$ This distinction can be very significant, as the set of n-best parses often does not contain the true parse.
 $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).
 $$$$$ 2 (after adding additional variables αi,yz and renormalizing by C) is given by: where Ii,y = I(xi, yi, y) indicates whether y is the true parse yi.
 $$$$$ We used two such auxiliary classifiers, giving a prediction feature for each span (these classifiers predicted only the presence or absence of a bracket over that span, not bracket labels).

 $$$$$ For this kind of feature, one must take care that its reliability on the training not be vastly greater than its reliability on the test set.
 $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).
 $$$$$ We will represent each parse as a set of two types of parts.

It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ We used two such auxiliary classifiers, giving a prediction feature for each span (these classifiers predicted only the presence or absence of a bracket over that span, not bracket labels).
It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ To ensure that such features are as noisy on the training data as the test data, we split the training into two folds.
It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ For example figure 1(a) shows a tree in this form.
It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ For each mistake-exclusion constraint, the dual contains a variable αi,y.

An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).
An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ Even in grammars without unary chains or empty elements, the number of parses is generally exponential in the length of the sentence, so we cannot expect to solve the above problem without any assumptions about the feature-vector representation `b and loss function L. For that matter, for arbitrary representations, to find the best parse given a weight vector, we would have no choice but to enumerate all trees and score them.
An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ The empirical utility of models such as logistic regression and support vector machines (SVMs) in flat classification tasks like text categorization, word-sense disambiguation, and relevance routing has been repeatedly demonstrated.
An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ The idea of this decomposition has previously been used for sequences and other Markov random fields in Taskar et al. (2003), but the present extension to CFGs is novel.

A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ We have reduced the problem to a polynomial size QP, which, in principle, can be solved using standard QP toolkits.
A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ Given dual variables α, we define the marginals µi,r(α) for all i, r, as follows: Since the dual variables αi form probability distributions over parse trees for each sentence i, the marginals µi,r(αi) represent the proportion of parses that would contain part r if they were drawn from a distribution αi.

Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ Note that parts for a basic PCFG model are not just rewrites (which can occur multiple times), but rather anchored items.
Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ In addition, it turns out that the set of dual variables αi = {αi,y : y ∈ G(xi)} for each example i is constrained to be non-negative and sum to 1.
Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ First, it is driven by the repeated extraction, over the training examples, of incorrect parses which the model currently prefers over the true parses.
Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ In addition, it turns out that the set of dual variables αi = {αi,y : y ∈ G(xi)} for each example i is constrained to be non-negative and sum to 1.

For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ While like most discriminative models it is compute-intensive to train, it allows fast parsing, remaining cubic despite the incorporation of lexical features.
For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).

We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ In SVMs, the optimization problem is solved by working with the dual of a quadratic program analogous to Eq.
We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ The constant C dictates the desired trade-off between margin size and outliers.

To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Equivalently, the function R(x, y) maps a derivation y to the set of parts which it includes.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ The traditional method of estimating the parameters of PCFGs assumes a generative grammar that defines P(x, y) and maximizes the joint log-likelihood Ei log P(xi, yi) (with some regularization).
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ We then trained the auxiliary classifiers in jacknife fashion on each fold, and using their predictions as features on the other fold.

 $$$$$ Our framework permits the use of a rich variety of input features, while still decomposing in a way that exploits the shared substructure of parse trees in the standard way.
 $$$$$ While like most discriminative models it is compute-intensive to train, it allows fast parsing, remaining cubic despite the incorporation of lexical features.
 $$$$$ Our formulation uses a factorization analogous to the standard dynamic programs for parsing.
 $$$$$ First, there is no notion of headword here, nor is there any modeling of word-toword attachment.

Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ As a concrete (and particularly clean) example of how these features can sway a decision, consider the sentence The Egyptian president said he would visit Libya today to resume the talks.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ The function L(x, y, ˆy) measures the penalty for proposing the parse yˆ for x when y is the true parse.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ The resulting grammar had 3975 non-terminal symbols and contained two kinds of productions: binary nonterminal rewrites and tag-word rewrites.5 The scores for the binary rewrites were estimated using unsmoothed relative frequency estimators.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ However, although the number of variables and constraints in the factored dual is polynomial in the size of the data, the number of coefficients in the quadratic term in the objective is very large: quadratic in the number of sentences and dependent on the sixth power of sentence length.

Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ However, although the number of variables and constraints in the factored dual is polynomial in the size of the data, the number of coefficients in the quadratic term in the objective is very large: quadratic in the number of sentences and dependent on the sixth power of sentence length.
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ In this representation, indices s and e refer to positions between words, rather than to words themselves.
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ It can be shown that the original objective Q(α) can be expressed in terms of these 4The constituent loss function does not exactly correspond to the standard scoring metrics, such as F1 or crossing brackets, but shares the sensitivity to the number of differences between trees.
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.

In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ For example figure 1(a) shows a tree in this form.
In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).
In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ In this paper, we describe a dynamic programming approach to discriminative parsing that is an alternative to maximum entropy estimation.
In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ Essentially, we need to enforce the condition that the expected proportions of parses having particular parts should be consistent with each other.

We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ We have reduced the problem to a polynomial size QP, which, in principle, can be solved using standard QP toolkits.
We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ For each mistake-exclusion constraint, the dual contains a variable αi,y.

This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ We have reduced the problem to a polynomial size QP, which, in principle, can be solved using standard QP toolkits.
This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).
