(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ We have not thoroughly investigated the exact interplay between the various loss choices and the various parsing metrics.
(Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. $$$$$ We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.

Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ It can be shown that the original objective Q(α) can be expressed in terms of these 4The constituent loss function does not exactly correspond to the standard scoring metrics, such as F1 or crossing brackets, but shares the sensitivity to the number of differences between trees.
Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ We have presented a maximum-margin approach to parsing, which allows a discriminative SVM-like objective to be applied to the parsing problem.
Previous work has also used surface features in their parsers, but the focus has been on machine learning methods (Taskar et al, 2004), latent annotations (Petrov and Klein, 2008a; Petrov and Klein, 2008b), or implementation (Finkel et al, 2008). $$$$$ Our framework permits the use of a rich variety of input features, while still decomposing in a way that exploits the shared substructure of parse trees in the standard way.

We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ A discriminative model is then used to choose between these candidates.
We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ The primary contribution of this paper is the extension of the max-margin approach of Taskar et al. (2003) to context free grammars.
We use the terminology in (Taskar et al, 2004) for a generic structured output prediction, and define a part. $$$$$ As shown in Taskar et al. (2003), the dual in Eq.

We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ Recently, it has also been extended to graphical models (Taskar et al., 2003; Altun et al., 2003) and shown to outperform the standard maxlikelihood methods.
We follow Taskar et al (2004) and Turian and Melamed (2005) in training and testing on? 15word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ The auxiliary classifiers were then retrained on the entire training set, and their predictions used as features on the development and test sets.

To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Recent work has shown that discriminative techniques frequently achieve classification accuracy that is superior to generative techniques, over a wide range of tasks.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ First, there is no notion of headword here, nor is there any modeling of word-toword attachment.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates.

 $$$$$ These features represent the preference of the word today for being the first and and last word in constituent spans of length 1.6 In the LEXICAL model, however, these features have quite large positive weights: 0.62 each.
 $$$$$ Note that parts for a basic PCFG model are not just rewrites (which can occur multiple times), but rather anchored items.
 $$$$$ The idea of this decomposition has previously been used for sequences and other Markov random fields in Taskar et al. (2003), but the present extension to CFGs is novel.
 $$$$$ While like most discriminative models it is compute-intensive to train, it allows fast parsing, remaining cubic despite the incorporation of lexical features.

 $$$$$ Parts of the first type are single constituent tuples (A, s, e, i), consisting of a non-terminal A, start-point s and end-point e, and sentence i, such as r in figure 1(b).
 $$$$$ We omit the details of the structured SMO procedure, but the important fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights.
 $$$$$ We omit the details of the structured SMO procedure, but the important fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights.
 $$$$$ The constant C dictates the desired trade-off between margin size and outliers.

It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ Our framework permits the use of a rich variety of input features, while still decomposing in a way that exploits the shared substructure of parse trees in the standard way.
It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.
It is expensive to train the MF approximation on the whole WSJ corpus, so instead we use only sentences of length at most 15, as in (Taskar et al, 2004) and (Turian and Melamed, 2006). $$$$$ In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates.

An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ Given the dual solution α*, the solution to the primal problem w* is simply a weighted linear combination of the feature vectors of the correct parse and mistaken parses: This is the precise sense in which mistakes with large α contribute more strongly to the model.
An other interesting model for parsing re-ranking based on tree kernel is presented in (Taskar et al, 2004). $$$$$ We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.

A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ Tag-word production weights were fixed to be the log of the generative P(w|t) model.
A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ However, our grammars and representations are generally structured to enable efficient inference.
A refinement of such technique was presented in (Taskar et al, 2004). $$$$$ Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.

Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ The traditional method of estimating the parameters of PCFGs assumes a generative grammar that defines P(x, y) and maximizes the joint log-likelihood Ei log P(xi, yi) (with some regularization).
Taskar et al (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. $$$$$ We have reduced the problem to a polynomial size QP, which, in principle, can be solved using standard QP toolkits.

For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ Another, more strict definition would be to define l(x, y, r) to be 0 if r of the type (A —* B C, s, m, e, i) is in the derivation y and 1 otherwise.
For example, Taskar et al (2004) took several months to train on the 15 word sentences in the English Penn Treebank (Dan Klein, p.c.). $$$$$ Max-margin estimation has been used for parse reranking (Collins, 2000).

We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.
We follow Taskar et al (2004) in training and testing on 15 word sentences in the English Penn Treebank (Taylor et al, 2003). $$$$$ As a concrete (and particularly clean) example of how these features can sway a decision, consider the sentence The Egyptian president said he would visit Libya today to resume the talks.

To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ We have presented a maximum-margin approach to parsing, which allows a discriminative SVM-like objective to be applied to the parsing problem.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ This definition would lead to L(x, y, ˆy) being the number of CFrule-tuples in yˆ which are not seen in y.4 Finally, we define indicator variables I(x, y, r) which are 1 if r ∈ R(x, y), 0 otherwise.
To situate our results in the literature, we compare our results to those reported by Taskar et al (2004) and Turian and Melamed (2005) for their discriminative parsers, which were also trained and tested on 15 word sentences. $$$$$ Unlike reranking methods, which consider only a pre-pruned selection of “good” parses, our method is an end-to-end discriminative model over the full space of parses.

 $$$$$ We omit the details of the structured SMO procedure, but the important fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights.

Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ The features are lexical in the sense than they allow specific words 5Unary rewrites were compiled into a single compound symbol, so for example a subject-gapped sentence would have label like s+vp.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ We omit the details of the structured SMO procedure, but the important fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ A discriminative model is then used to choose between these candidates.
Collins and Roark (2004) and Taskar et al (2004) beat the generative baseline only after using the standard trick of using the output from a generative model as a feature. $$$$$ We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.

Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ We provide an efficient algorithm for learning such models and show experimental evidence of the model’s improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.
Additionally, we exploit the flexibility of the discriminative framework both to improve the treatment of unknown words as well as to include span features (Taskar et al, 2004), giving the benefit of some input features integrally in our dynamic program. $$$$$ Furthermore, the objective can be expressed in terms of expectations with respect to these distributions: We now consider how to efficiently solve the max-margin optimization problem for a factored model.

In re ranking, one can incorporate any such features, of course, but even in our dynamic programming approach it is possible to include features that decompose along the dynamic program structure, as shown by Taskar et al (2004). $$$$$ We have reduced the problem to a polynomial size QP, which, in principle, can be solved using standard QP toolkits.

We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ We also define sets R(xi) = ∪yEG(xi)R(xi, y) for the training examples i = 1... n. Thus, R(xi) is the set of parts that is seen in at least one of the objects {(xi, y) : y ∈ G(xi)}.
We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ For example, (Johnson, 2001; Geman and Johnson, 2002; Miyao and Tsujii, 2002; Clark and Curran, 2004; Kaplan et al., 2004) describe approaches based on conditional log-linear (maximum entropy) models, where variants of the inside-outside algorithm can be used to efficiently calculate gradients of the log-likelihood function, despite the exponential number of trees represented by the parse forest.
We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.
We use non-local span features, which condition on properties of input spans (Taskar et al, 2004). $$$$$ Note that parts for a basic PCFG model are not just rewrites (which can occur multiple times), but rather anchored items.

This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ Hence, in our experiments we use an online coordinate descent method analogous to the sequential minimal optimization (SMO) used for SVMs (Platt, 1999) and adapted to structured max-margin estimation in Taskar et al. (2003).
This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ This trade-off between the complexity, accuracy and efficiency of a parsing model is an important area of future research.
This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ We omit the details of the structured SMO procedure, but the important fact about this kind of training is that, similar to the basic perceptron approach, it only requires picking up sentences one at a time, checking what the best parse is according to the current primal and dual weights, and adjusting the weights.
This is the approach taken by Taskar et al (2004), but their approach assumes that the loss function can be decomposed into local loss functions. $$$$$ Furthermore, the objective can be expressed in terms of expectations with respect to these distributions: We now consider how to efficiently solve the max-margin optimization problem for a factored model.
