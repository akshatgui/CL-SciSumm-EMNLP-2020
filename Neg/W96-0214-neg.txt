In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents. $$$$$ This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data.
In Goodman (1996), an efficient parsing strategy is given that maximizes the expected number of correct constituents. $$$$$ Given the tree of Figure 1, we can use the DOP model to convert it into the STSG of Figure 2.

 $$$$$ However, Bod then neglects analysis of this € -2 term, assuming that it is constant.
 $$$$$ Unfortunately, existing algorithms are both computationally intensive and difficult to implement.

Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively. $$$$$ It takes about 6 seconds per sentence to run our algorithm on an HP 9000/715, versus 3.5 hours to run Bod's algorithm on a Sparc 2 (Bod, 1995b).
Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively. $$$$$ This will be complicated by the fact that sufficient details of Bod's implementation are not available.
Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively. $$$$$ Parsing using the DOP model is especially difficult.
Although Sima'an (1996) and Goodman (1996) also report experiments on unedited ATIS trees, their results do not refer to the most probable parse but to the most probable derivation and the maximum constituents parse respectively. $$$$$ The results are disappointing, as shown in Table 2.

This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). $$$$$ Ten data sets were constructed by randomly splitting minimally edited ATIS (Hemphill et al., 1990) sentences into a 700 sentence training set, and 88 sentence test set, then discarding sentences of length > 30.
This does not scale well to large treebanks, forcing the use of implicit representations (Goodman, 1996) or heuristic subsets (Bod, 2001). $$$$$ In the perl script for analyzing Bod's data is available by anonymous FTP from ftp://ftp.das.harvard.eduipub/goodman/analyze.perl 4Actually, this is a slight overestimate for a few reasons, including the fact that the 75 sentences are drawn without replacement.

Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005). $$$$$ We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length.
Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005). $$$$$ Still, the Monte Carlo algorithm has never been tested on sentences longer than those in the ATIS corpus; there is good reason to believe the algorithm will not work as well on longer sentences.
Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005). $$$$$ This paper contains the first published replication of the full DOP model, i.e. using a parser which sums over derivations.
Indeed, our methods were inspired by past work on variational decoding for DOP (Goodman, 1996) and for latent-variable parsing (Matsuzaki et al, 2005). $$$$$ Were previous results due only to the choice of test data, or are the differences in implementation partly responsible?

Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper. $$$$$ In that case, there is significant future work required to understand which differences account for Bod's exceptional performance.
Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper. $$$$$ Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm.
Although see (Goodman 1996) for an efficient algorithm for the DOP model, which we discuss in section 7 of this paper. $$$$$ We show two of them.

(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set. $$$$$ These results are significant since the DOP model has perhaps the best reported parsing accuracy; previously the full DOP model had not been replicated due to the difficulty and computational complexity of the existing algorithms.
(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set. $$$$$ Thus he concludes that his algorithm runs in polynomial time.
(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set. $$$$$ Thus, every STSG tree would be produced by the PCFG with equal probability.
(Goodman 1996) gives a polynomial time conversion of a DOP model into an equivalent PCFG whose size is linear in the size of the training set. $$$$$ We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct constituents, rather than the probability of a correct parse tree.

 $$$$$ For a node such we will generate the following eight PCFG rules, where the number in parentheses following a rule is its probability.
 $$$$$ Simibk ci aj a, larly, for another case, trees headed by A the probability of the tree is*kk-0 = 1.

The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996). $$$$$ Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm.
The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996). $$$$$ This makes it harder to achieve statistical significance.
The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996). $$$$$ The probability that a potential constituent occurs in the correct parse tree, P(X ws...wtIS wi...tv„), will be called g(s,t, X).
The relation between DOP and enrichment/conditioning models was clarified by Joshua Goodman, who devised an efficient PCFG transform of the DOP1 model (Goodman, 1996). $$$$$ The n-ary productions can be parsed in a straightforward manner, by converting them to binary branching form; however, there are at least three different ways to convert them, as illustrated in Table 3.

Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002). $$$$$ We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length.
Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002). $$$$$ Were previous results due only to the choice of test data, or are the differences in implementation partly responsible?
Although we omit the details, we can prove the NP-hardness by observing that a stochastic tree substitution grammar (STSG) can be represented by a PCFG-LA model in a similar way to one described by Goodman (1996a), and then using the NP-hardness of STSG parsing (Sima'an, 2002). $$$$$ In words, it is the probability that, given the sentence wi...w., a symbol X generates ws...wt• We can compute this probability using elements of the Inside-Outside algorithm.

Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees. $$$$$ We performed a statistical analysis using a t-test on the paired differences between DOP and Pereira and Schabes performance on each run.
Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees. $$$$$ We also ran experiments using Bod's data, 75 sentence test sets, and no limit on sentence length.
Implicit grammars Goodman (1996, 2003 ) defined a transformation for some versions of DOP to an equivalent PCFG-based model, with the number of rules extracted from each parse tree linear in the size of the trees. $$$$$ First, compute the inside probabilities, e(s,t, X) = P(X Ws.--wt).

Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). $$$$$ Then, for trees such as the probability of the tree is.
Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). $$$$$ However, Bod then neglects analysis of this € -2 term, assuming that it is constant.
Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). $$$$$ This research also shows the importance of testing on more than one small test set, as well as the importance of not making cross-corpus comparisons; if a new corpus is required, then previous algorithms should be duplicated for comparison.
Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). $$$$$ We will call a PCFG subderivation isomorphic to a STSG tree if the subderivation begins with an external non-terminal, uses internal nonterminals for intermediate steps, and ends with external non-terminals.

The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting. $$$$$ Bod himself does not know which technique he used for n-ary productions, since the chart parser he used was written by a third party (Bod, personal communication).
The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting. $$$$$ Consider a modified form of the DOP model, in which when subtrees occurred multiple times in the training corpus, their counts were not merged: both identical trees are added to the grammar.
The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting. $$$$$ We have given efficient techniques for parsing the DO? model.
The difference is that Goodman (1996a) collapses our BEGIN and END rules into the binary productions, giving a larger grammar which is less convenient for weighting. $$$$$ In this paper we solve the first problem by a novel reduction of the DOP model to,a small, equivalent probabilistic context-free grammar.

 $$$$$ This paper contains the first published replication of the full DOP model, i.e. using a parser which sums over derivations.
 $$$$$ Since the probability of the most probable parse decreases exponentially in sentence length, the number of random samples needed to find this most probable parse increases exponentially in sentence length.

 $$$$$ This differs significantly from the results reported by Bod, and is comparable to results from a duplication of Pereira and Schabes's (1992) experiment on the same data.
 $$$$$ These results are significant since the DOP model has perhaps the best reported parsing accuracy; previously the full DOP model had not been replicated due to the difficulty and computational complexity of the existing algorithms.

The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Simaa'as; an 1995) or the labelled recall parse of a sentence (Goodman 1996). $$$$$ Were previous results due only to the choice of test data, or are the differences in implementation partly responsible?
The most probable parse can be estimated by iterative Monte Carlo sampling (Bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of a sentence (Bod 1995, Simaa'as; an 1995) or the labelled recall parse of a sentence (Goodman 1996). $$$$$ Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo parsing algorithm.
