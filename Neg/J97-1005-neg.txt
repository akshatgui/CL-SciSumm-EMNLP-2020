Passonneau and Litman (1997) describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie. $$$$$ No boundaries are assigned by NP.
Passonneau and Litman (1997) describe an experiment where seven untrained annotators were asked to find discourse segments in a corpus of transcribed narratives about a movie. $$$$$ First, we must take into account the dimensions along which the three algorithms differ, apart from the different types of linguistic information used.

We turned the segmentation task into a classification task by using boundaries between dialogue acts as one class and non-boundaries a the other (see Passonneau and Litman (1997) for a similar practice). $$$$$ The hypothesis that discourse has a tree struchire has frequently been questioned (Dale 1992; Moore and Pollack 1992; Hearst 1994; Walker 1995), and the magnitude of our segmentation task precludes asking subjects to specify hierarchical relations.
We turned the segmentation task into a classification task by using boundaries between dialogue acts as one class and non-boundaries a the other (see Passonneau and Litman (1997) for a similar practice). $$$$$ In the second part of our study, data abstracted from the subjects' segmentations serve as a target for evaluating two sets of algorithms that use utterance features to perform segmentation.
We turned the segmentation task into a classification task by using boundaries between dialogue acts as one class and non-boundaries a the other (see Passonneau and Litman (1997) for a similar practice). $$$$$ (Recall that significance of a boundary increases exponentially with the number of subjects who agree on a boundary.)

By using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have a higher accuracy than methods using a single marker alone (Passonneau and Litman, 1997). $$$$$ We have been engaged in a two-part study addressing this gap.
By using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have a higher accuracy than methods using a single marker alone (Passonneau and Litman, 1997). $$$$$ In Section 4,2, we test an initial set of algorithms for computing segment boundaries from a particular type of linguistic feature, either referential noun phrases, cue phrases, or pauses.

Even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (PassonneauandLitman, 1997). $$$$$ The second discourse feature of interest is that the usage of a wide range of lexicogrammatical devices seems to constrain or be constrained by this more abstract structure.
Even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (PassonneauandLitman, 1997). $$$$$ The ratios of test to training data measured in narratives, prosodic phrases, and clauses, respectively, are 50.0%, 43.1%, and 44.9%.
Even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (PassonneauandLitman, 1997). $$$$$ We have been engaged in a two-part study addressing this gap.
Even annotating linear segmentation is challenging, particularly in the vicinity of segment boundaries (PassonneauandLitman, 1997). $$$$$ While none of the algorithms approached human performance, the fact that performance improved with the number of features coded, and by combining algorithms in a simple additive way, suggested directions for improvement.

These referential and lexical features build on the work of Passonneau and Litman (1997), who use them in discourse segmentation. $$$$$ We do not, however, ask coders to identify hierarchical relations among segments.
These referential and lexical features build on the work of Passonneau and Litman (1997), who use them in discourse segmentation. $$$$$ The evaluations in this section allow us to compare the utility of two tuning methods: error analysis, and machine learning.
These referential and lexical features build on the work of Passonneau and Litman (1997), who use them in discourse segmentation. $$$$$ In Section 4,2, we test an initial set of algorithms for computing segment boundaries from a particular type of linguistic feature, either referential noun phrases, cue phrases, or pauses.
These referential and lexical features build on the work of Passonneau and Litman (1997), who use them in discourse segmentation. $$$$$ We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion.

First, if non-canonicals are indicators of attentional stack pops, they should be more likely at segment boundaries; hence, we expect an increased presence of cue words (Passonneau and Litman, 1997) on non-canonicals compared to canonicals. $$$$$ It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations.
First, if non-canonicals are indicators of attentional stack pops, they should be more likely at segment boundaries; hence, we expect an increased presence of cue words (Passonneau and Litman, 1997) on non-canonicals compared to canonicals. $$$$$ The hypothetical subjects assign boundaries randomly (but with no repetition).
First, if non-canonicals are indicators of attentional stack pops, they should be more likely at segment boundaries; hence, we expect an increased presence of cue words (Passonneau and Litman, 1997) on non-canonicals compared to canonicals. $$$$$ Table 4 shows the average performance of the cue word algorithm.
First, if non-canonicals are indicators of attentional stack pops, they should be more likely at segment boundaries; hence, we expect an increased presence of cue words (Passonneau and Litman, 1997) on non-canonicals compared to canonicals. $$$$$ The first part of our paper presents a method for empirically validating multiutterance units referred to as discourse segments.

In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). $$$$$ All subjects assigned boundaries relatively infrequently.
In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). $$$$$ The first pertains to an abstract structure consisting of meaningful discourse segments and their interrelations.
In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). $$$$$ In particular, the NP algorithm (which used three features) outperformed both the cue phrase and pause algorithms (each of which used only a single feature).
In this sense, we are performing low-level discourse segmentation, as opposed to segmenting text into chunks or topics (e.g., Passonneau and Litman (1997)). $$$$$ We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues.

Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries. $$$$$ The first pertains to an abstract structure consisting of meaningful discourse segments and their interrelations.
Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries. $$$$$ Both authors' work was partially supported by DARPA and ONR under contract N00014-89+1782; Passonneau was also partly supported by NSF grants IRI-91-13064 and IRI-95-28998.
Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries. $$$$$ Passonneau's work was not conducted under Bellcore auspices.
Passonneau and Litman (1997) found that pause length correlates with discourse segment boundaries. $$$$$ We present quantitative results of a two-part study using a corpus of spontaneous, narrative monologues.

This makes it different from others using surface features like (Passonneau and Litman, 1997). $$$$$ The redefinition of FICU motivated by error analysis led to fewer clauses.
This makes it different from others using surface features like (Passonneau and Litman, 1997). $$$$$ On the other hand, there is wide performance variation around the mean.
This makes it different from others using surface features like (Passonneau and Litman, 1997). $$$$$ We report highly significant results of segmentations performed by naive subjects, where a commonsense notion of speaker intention is the segmentation criterion.

Automatic discourse segmentation, as shallow annotation of discourse structure, also provides a testing grounds for linguistic theories of discourse (Passonneau and Litman, 1997) and provides a natural unit of measure in linguistic corpora (Biber et al, 2004). $$$$$ Both authors' work was partially supported by DARPA and ONR under contract N00014-89+1782; Passonneau was also partly supported by NSF grants IRI-91-13064 and IRI-95-28998.
Automatic discourse segmentation, as shallow annotation of discourse structure, also provides a testing grounds for linguistic theories of discourse (Passonneau and Litman, 1997) and provides a natural unit of measure in linguistic corpora (Biber et al, 2004). $$$$$ Each utterance of a discourse contributes to the communicative import of preceding utterances, or constitutes the onset of a new unit of meaning or action that subsequent utterances may add to.
Automatic discourse segmentation, as shallow annotation of discourse structure, also provides a testing grounds for linguistic theories of discourse (Passonneau and Litman, 1997) and provides a natural unit of measure in linguistic corpora (Biber et al, 2004). $$$$$ We have been engaged in a two-part study addressing this gap.

Human annotators demonstrate frequent disagreement about the number of segments and exactly where the transitions between segments occur, while still demonstrating statistically significant agreement (Passonneau and Litman, 1997). $$$$$ Note that it is conservative also because it is based on the proportion of identical matches between two data sets.
Human annotators demonstrate frequent disagreement about the number of segments and exactly where the transitions between segments occur, while still demonstrating statistically significant agreement (Passonneau and Litman, 1997). $$$$$ We use Krippendorff's a (1980) to evaluate the reliability of the two data sets from partitions A and B.
Human annotators demonstrate frequent disagreement about the number of segments and exactly where the transitions between segments occur, while still demonstrating statistically significant agreement (Passonneau and Litman, 1997). $$$$$ We have shown that an atheoretical notion of speaker intention is understood sufficiently uniformly by naive subjects to yield highly significant agreement across subjects on segment boundaries in a corpus of spoken narratives.
