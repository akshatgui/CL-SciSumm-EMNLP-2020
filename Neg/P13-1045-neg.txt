 $$$$$ The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.
 $$$$$ Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of DARPA, AFRL, or the US government.
 $$$$$ We then continue to train both parsers on two similar sentences and then analyze if the parsers correctly transferred the knowledge.

 $$$$$ This model resulted in 90.44% on the final test set (WSJ section 23).
 $$$$$ Note that any PCFG, including latent annotation PCFGs (Matsuzaki et al., 2005) could be used.
 $$$$$ Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.
 $$$$$ The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.

Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ Richard is supported by a Microsoft Research PhD fellowship.
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ Features in such parsers can be seen as defining effective dimensions of similarity between categories.
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ This model is linguistically more plausible since it chooses different composition functions for a parent node based on the syntactic categories of its children.

To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ However, subdividing a category like NP into 30 or 60 subcategories can only provide a very limited representation of phrase meaning and semantic similarity.
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ The compositional vectors are learned with a new syntactically untied recursive neural network.
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ The CVG is inspired by two lines of research: Enriching PCFG parsers through more diverse sets of discrete states and recursive deep learning models that jointly learn classifiers and continuous feature representations for variable-sized inputs.
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ We thank Percy Liang for chats about the paper.

To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ After training, the CVG parses both correctly, while the factored Stanford parser incorrectly attaches both PPs to spaghetti.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.

 $$$$$ The CVG model merges ideas from both generative models that assume discrete syntactic categories and discriminative models that are trained using continuous vectors.
 $$$$$ The authors gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) prime contract no.
 $$$$$ The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%.
 $$$$$ The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.

This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ The 25-dimensional vectors provided by Turian et al. (2010) provided the best performance and were faster than 50-,100- or 200dimensional ones.
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ Richard is supported by a Microsoft Research PhD fellowship.
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.

Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ We introduced Compositional Vector Grammars (CVGs), a parsing model that combines the speed of small-state PCFGs with the semantic richness of neural word representations and compositional phrase vectors.
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Features in such parsers can be seen as defining effective dimensions of similarity between categories.
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains.

We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.
We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ The CVG computes the first parent vector via the SU-RNN: where W(B,C) ∈ Rn×2n is now a matrix that depends on the categories of the two children.
We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ However, recent work has shown that parsing results can be greatly improved by defining more fine-grained syntactic gory,vector) representations at each node.
We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ For example, much work has shown the usefulness of syntactic representations for subsequent tasks such as relation extraction, semantic role labeling (Gildea and Palmer, 2002) and paraphrase detection (Callison-Burch, 2008).

Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ The main differences are (i) the dual representation of nodes as discrete categories and vectors, (ii) the combination with a PCFG, and (iii) the syntactic untying of weights based on child categories.
Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ We fix the same regularization of λ = 10−4 for all parameters.
Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.

Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ Our compositional distributed representation allows a CVG parser to make accurate parsing decisions and capture similarities between phrases and sentences.
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ Both the original parsing system and its probabilistic interpretation (Titov and Henderson, 2007) learn features that represent the parsing history and do not provide a principled linguistic representation like our phrase representations.
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness.
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ The authors gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) prime contract no.

First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ Derivatives are computed via backpropagation through structure (BTS) (Goller and K¨uchler, 1996).
First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ The matrix is then applied to the sibling node’s vector during the composition.
First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ Analysis of Error Types.
First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ The compositional vectors are learned with a new syntactically untied recursive neural network.

Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ To minimize the objective, we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ However, many parsing decisions show fine-grained semantic factors at work.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ One of the largest sources of improved performance over the original Stanford factored parser is in the correct placement of PP phrases.

 $$$$$ After training, the CVG parses both correctly, while the factored Stanford parser incorrectly attaches both PPs to spaghetti.
 $$$$$ For further details and evaluations of these embeddings, see (Turian et al., 2010; Huang et al., 2012).
 $$$$$ We generalize the fully tied RNN to one with syntactically untied weights.
 $$$$$ FA8750-13-2-0040, and the DARPA Deep Learning program under contract number FA8650-10C-7020.
