 $$$$$ The two diagonals show clearly the two blocks that are multiplied with the left and right children, respectively.
 $$$$$ The authors gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) prime contract no.
 $$$$$ This model is linguistically more plausible since it chooses different composition functions for a parent node based on the syntactic categories of its children.

 $$$$$ The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
 $$$$$ The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.
 $$$$$ This beam search inference procedure is still considerably slower than using only the simplified base PCFG, especially since it has a small state space (see next section for details).
 $$$$$ This can be interpreted as the log probability of a discrete-continuous rule application with the following factorization: Note, however, that due to the continuous nature of the word vectors, the probability of such a CVG rule application is not comparable to probabilities provided by a PCFG since the latter sum to 1 for all children.

Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ CVGs combine the advantages of standard probabilistic context free grammars (PCFG) with those of recursive neural networks (RNNs).
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ We thank Percy Liang for chats about the paper.
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ Table 1 compares our results to the two Stanford parser variants (the unlexicalized PCFG (Klein and Manning, 2003a) and the factored parser (Klein and Manning, 2003b)) and other parsers that use richer state representations: the Berkeley parser (Petrov and Klein, 2007), Collins parser (Collins, 1997), SSN: a statistical neural network parser (Henderson, 2004), Factored PCFGs (Hall and Klein, 2012), CharniakSelfTrain: the self-training approach of McClosky et al. (2006), which bootstraps and parses additional large corpora multiple times, Charniak-RS: the state of the art self-trained and discriminatively re-ranked Charniak-Johnson parser combining (Charniak, 2000; McClosky et al., 2006; Charniak and Johnson, 2005).
Another application of word vectors is compositional vector grammar (Socher et al, 2013). $$$$$ In many natural language systems, single words and n-grams are usefully described by their distributional similarities (Brown et al., 1992), among many others.

To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ We also cross-validated on AdaGrad’s learning rate which was eventually set to α = 0.1 and word vector size.
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ The code of our parser is available at nlp.stanford.edu.
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ We directly compare models with fully tied and untied weights.
To our best knowledge, this is the first work on this issue in SMT community; In current work, RNN has only been verified to be useful on monolingual structure learning (Socher et al, 2011a; Socher et al, 2013). $$$$$ Two strands of work therefore attempt to go further.

To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ But, even with large corpora, many n-grams will never be seen during training, especially when n is large.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ Richard is supported by a Microsoft Research PhD fellowship.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ FA8750-13-2-0040, and the DARPA Deep Learning program under contract number FA8650-10C-7020.
To obtain DCS trees from natural language, we use Stanford CoreNLP 5 for dependency parsing (Socher et al, 2013), and convert Stanforddependencies to DCS trees by pattern matching on POS tags and dependency labels. $$$$$ We decreased the state splitting of the PCFG grammar (which helps both by making it less sparse and by reducing the number of parameters in the SU-RNN) by adding the following options to training: ‘-noRightRec dominatesV 0 -baseNP 0’.

 $$$$$ The main differences are (i) the dual representation of nodes as discrete categories and vectors, (ii) the combination with a PCFG, and (iii) the syntactic untying of weights based on child categories.
 $$$$$ This paper uses several ideas of (Socher et al., 2011b).
 $$$$$ The compositional vectors are learned with a new syntactically untied recursive neural network.

This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ We define the word representations as (vector, POS) pairs: ((a, A), (b, B), (c, C)), where the vectors are defined as in Sec.
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ We use the code provided by Kummerfeld et al. (2012) and compare to the previous version of the Stanford factored parser as well as to the Berkeley and Charniak-reranked-self-trained parsers (defined above).
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ Menchetti et al. (2005) use RNNs to re-rank different parses.
This initialization of the composition matrices has previously been effective for parsing (Socher et al, 2013a). $$$$$ The compositional vectors are learned with a new syntactically untied recursive neural network.

Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness.
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Syntactic descriptions standardly use coarse discrete categories such as NP for noun phrases or PP for prepositional phrases.
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Other related work includes (Henderson, 2004), who discriminatively trains a parser based on synchrony networks and (Titov and Henderson, 2006), who use an SVM to adapt a generative parser to different domains.
Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al, 2013a). $$$$$ Analysis of Composition Matrices.

We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness.
We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ We introduced Compositional Vector Grammars (CVGs), a parsing model that combines the speed of small-state PCFGs with the semantic richness of neural word representations and compositional phrase vectors.
We attempted to apply syntactically-untied RNNs (Socher et al, 2013a) to our data with the idea that associating separate matrices for phrasal categories would improve representations at high-level nodes. $$$$$ Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.

Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ Note that any PCFG, including latent annotation PCFGs (Matsuzaki et al., 2005) could be used.
Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ Like the above work on parsing, the model addresses the problem of representing phrases and categories.
Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ We use this knowledge to speed up inference via two bottom-up passes through the parsing chart.
Socher et al (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. $$$$$ This model is linguistically more plausible since it chooses different composition functions for a parent node based on the syntactic categories of its children.

Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ FA8750-13-2-0040, and the DARPA Deep Learning program under contract number FA8650-10C-7020.
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser.
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ Therefore, we generalize gradient ascent via the subgradient method (Ratliff et al., 2007) which computes a gradient-like direction.
Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al, 2011), syntactic category in parsing (Socher et al, 2013a) and phrase reordering pattern in SMT (Li et al, 2013). $$$$$ FA8750-13-2-0040, and the DARPA Deep Learning program under contract number FA8650-10C-7020.

First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ For each sentence, we use the method described above to efficiently find an approximation for the optimal tree.
First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations.
First, these parsers are among the best in the literature, with a test performance of 90.7 F 1 for the baseline Berkeley parser on the Wall Street Journal corpus (compared to 90.4 for Socher et al (2013) and 90.1 for Henderson (2004)). $$$$$ 1.

Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ We also cross-validated on AdaGrad’s learning rate which was eventually set to α = 0.1 and word vector size.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ We fix the same regularization of λ = 10−4 for all parameters.
Recursive neural networks, which have the ability to generate a tree structured output, are applied to natural language parsing (Socher et al, 2011), and they are extended to recursive neural tensor networks to explore the compositional aspect of semantics (Socher et al, 2013). $$$$$ Table 2 shows a detailed comparison of different errors.

 $$$$$ We begin by computing the activation for p1 using the children’s word vectors.
 $$$$$ The first block is multiplied by the left child and the second by the right child:
 $$$$$ The compositional vectors are learned with a new syntactically untied recursive neural network.
 $$$$$ It has the following form: where the tree is found by the Compositional Vector Grammar (CVG) introduced below and then scored via the function s. The higher the score of a tree the more confident the algorithm is that its structure is correct.
