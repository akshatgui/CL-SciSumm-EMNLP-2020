Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail). $$$$$ Jeffrey Mark Siskind.
Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail). $$$$$ This procedure is repeated until no more words match environment objects.
Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al (2009) (see Section 4 for more detail). $$$$$ First, for each vocabulary word w, we define a feature that is one if w is the last word of a’s consumed words W'.

Previous work (Branavan et al, 2009) is only able to handle low-level instructions. $$$$$ Res.
Previous work (Branavan et al, 2009) is only able to handle low-level instructions. $$$$$ The key idea of our approach is to leverage the validation process as the main source of supervision to guide learning.
Previous work (Branavan et al, 2009) is only able to handle low-level instructions. $$$$$ In total, there are 8,094 features.
Previous work (Branavan et al, 2009) is only able to handle low-level instructions. $$$$$ We assume access to a reward function that defines the quality of the executed actions.

This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments. $$$$$ The learning problem is to find an optimal policy that maps states to actions, through a trial-and-error process of repeated interaction with the user.
This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments. $$$$$ (JAIR), 15:31–90.
This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments. $$$$$ For instance, there are a surprising variety of linguistic constructs — as Figure 4 shows, in the Windows domain even a simple command is expressed in at least six different ways.
This environment reward function is a simplification of the one described in Branavan et al (2009), and it performs comparably in our experiments. $$$$$ Intell.

In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009). $$$$$ The mapping state s is observed after each action.
In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009). $$$$$ The reinforcement learning state space encodes information about the goals of the user and what they say at each time step.
In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009). $$$$$ The initial mapping state s0 for document d is (£d, d, 0, 0); £d is the unique starting environment state for d. Performing action a in state s = (£, d, j, W) leads to a new state s' according to distribution p(s'|s, a), defined as follows: £ transitions according to p(£'|£, c, R), W is updated with a’s selected words, and j is incremented if all words of the sentence have been mapped.
In addition to the look-ahead features described in Section 5.2, the policy also includes the set of features used by Branavan et al (2009). $$$$$ Since the reward correlates with action sequence correctness, the 0 that maximizes expected reward will yield the best actions.

Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09. $$$$$ This approach is able to use environment-based rewards, such as task completion, to learn to analyze text.
Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09. $$$$$ Second, we introduce features for pairs of vocabulary word w and attributes of action a, e.g., the line orientation and grid locations of the squares that a would remove.
Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09. $$$$$ Thus, with this reward policy gradient is equivalent to stochastic gradient ascent with a maximum likelihood objective.
Baselines As a baseline, we compare our method against the results reported by Branavan et al (2009), denoted here as BCZB09. $$$$$ In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions.

Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009). $$$$$ The Environment The environment state £ specifies the set of objects available for interaction, and their properties.
Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009). $$$$$ This distribution is a priori unknown to the learner.
Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009). $$$$$ We showed that having access to a suitable reward function can significantly reduce the need for annotations.
Performing policy-gradient with this function is equivalent to training a fully supervised, stochastic gradient algorithm that optimizes conditional likelihood (Branavan et al, 2009). $$$$$ Our goal is to predict a sequence of actions.

 $$$$$ We use a policy gradient algorithm to estimate the parameters of a log-linear model for action selection.
 $$$$$ This variant solves 34% of the puzzles, suggesting that access to the instructions significantly improves performance.
 $$$$$ Figure 5 shows the overall tradeoff between annotation effort and system performance for the two domains.
 $$$$$ For the applications we consider in this work, environment state transitions, and consequently mapping state transitions, are deterministic.

To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009]. $$$$$ We construct this sequence by repeatedly choosing an action given the current mapping state, and applying that action to advance to a new state.
To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009]. $$$$$ Table 2 presents evaluation results on the test sets.
To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009]. $$$$$ We construct this sequence by repeatedly choosing an action given the current mapping state, and applying that action to advance to a new state.
To learn from an unaligned corpus, we derive a new training algorithm that combines the Generalized Grounding Graph (G3) framework introduced by Tellex et al [2011] with the policy gradient method described by Branavan et al [2009]. $$$$$ Although the fully supervised approach performs the best, adding just a few annotated training examples to the environment-based learner significantly reduces the performance gap.

 $$$$$ We apply our method to interpret instructions in two domains — Windows troubleshooting guides and game tutorials.
 $$$$$ As we will see in Section 5, our approach avoids having to directly estimate this distribution.
 $$$$$ We showed that having access to a suitable reward function can significantly reduce the need for annotations.

We also compare against the policy gradient learning algorithm of Branavan et al (2009). $$$$$ The environment state £ changes in response to the execution of command c with parameters R according to a transition distribution p(£'J£, c, R).
We also compare against the policy gradient learning algorithm of Branavan et al (2009). $$$$$ The reinforcement learning state space encodes information about the goals of the user and what they say at each time step.
We also compare against the policy gradient learning algorithm of Branavan et al (2009). $$$$$ First, for each vocabulary word w, we define a feature that is one if w is the last word of a’s consumed words W'.
We also compare against the policy gradient learning algorithm of Branavan et al (2009). $$$$$ In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions.
