We follow the notation of Di Eugenio and Glass (2004). $$$$$ It is, for example, appropriate in situations in which item i may be tagged by different coders than item j (Fleiss 1971).
We follow the notation of Di Eugenio and Glass (2004). $$$$$ The results for Example 1 should then be reported as κCo = 0.6724, κS&C = 0.6632, 2P(A)−1 = 0.6666; those for Example 6 as κCo = 0.418, κS&C = 0.27, and 2P(A)−1 = 0.3.

To assess this classification task we also used the kappa statistics which yielded KCo=0.922 (following (Eugenio and Glass, 2004) we report Kas KCo, indicating that we calculate K a la Cohen (Cohen, 1960). $$$$$ In the computational linguistics literature, r. has been used mostly to validate coding schemes: Namely, a “good” value of r. means that the coders agree on the categories and therefore that those categories are “real.” We noted previously that assessing what constitutes a “good” value for r. is problematic in itself and that different scales have been proposed.
To assess this classification task we also used the kappa statistics which yielded KCo=0.922 (following (Eugenio and Glass, 2004) we report Kas KCo, indicating that we calculate K a la Cohen (Cohen, 1960). $$$$$ The results for Example 1 should then be reported as κCo = 0.6724, κS&C = 0.6632, 2P(A)−1 = 0.6666; those for Example 6 as κCo = 0.418, κS&C = 0.27, and 2P(A)−1 = 0.3.
To assess this classification task we also used the kappa statistics which yielded KCo=0.922 (following (Eugenio and Glass, 2004) we report Kas KCo, indicating that we calculate K a la Cohen (Cohen, 1960). $$$$$ 1 To be precise, Krippendorff uses a computation very similar to Siegel and Castellan’s to produce a statistic called alpha.

Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. $$$$$ 23(1):13–31.
Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. $$$$$ Krippendorff’s scale has been adopted without question, even though Krippendorff himself considers it only a plausible standard that has emerged from his and his colleagues’ work.
Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. $$$$$ Our initial example in Figure 1 is also affected by bias.
Feinstein and Cicchetti (1990), followed by Di Eugenio and Glass (2004) proved that Kappa is subject to the effect of prevalence and that different marginal distributions can lead to very different Kappa values for the same observed agreement. $$$$$ κ had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students’ diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981).

But Di Eugenio and Glass (2004) have found that this interpretation does not hold true for all tasks. $$$$$ To use κCo but to guard against bias, Cicchetti and Feinstein (1990) suggest that κCo be supplemented, for each coding category, by two measures of agreement, positive and negative, between the coders.
But Di Eugenio and Glass (2004) have found that this interpretation does not hold true for all tasks. $$$$$ Currently, the de facto standard for assessing intercoder agreement is the κ coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980).
But Di Eugenio and Glass (2004) have found that this interpretation does not hold true for all tasks. $$$$$ This work is supported by grant N00014-00-1-0640 from the Office of Naval Research.

Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g. (Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution. $$$$$ Second, we discuss how prevalence and bias affect the κ measure.
Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g. (Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution. $$$$$ The issue that remains open is which computation of κ to choose.
Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g. (Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution. $$$$$ Step 1.
Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g. (Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution. $$$$$ Thanks to Janet Cahn and to the anonymous reviewers for comments on earlier drafts.
