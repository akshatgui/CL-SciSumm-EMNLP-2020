Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ We argue that this kind of error is less detrimental than the errors made by Pk.
Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ Total penalty depends on the distance between the false positive and the relevant correct boundaries; on average, it is k2, assuming a uniform distribution of boundaries across the document.

To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.
To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ The algorithm is penalized if ri =� ai (which is computed as |ri − ai |> 0).
To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ However, it has some problems of its own.

Pevzner and Hearst (2002, pp. 3-4) explain Pk well $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.
Pevzner and Hearst (2002, pp. 3-4) explain Pk well $$$$$ Type B errors have an average penalty of k, as for Pk.
Pevzner and Hearst (2002, pp. 3-4) explain Pk well $$$$$ However, Pk is more lenient for pure false positives that occur close to boundaries.

Pevzner and Hearst (2002, pp. 5-10) identified that Pk $$$$$ metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
Pevzner and Hearst (2002, pp. 5-10) identified that Pk $$$$$ In the analysis below, we assume an application for which it is important not to introduce spurious boundaries.
Pevzner and Hearst (2002, pp. 5-10) identified that Pk $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
Pevzner and Hearst (2002, pp. 5-10) identified that Pk $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.

To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.
To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ It is not clear whether this is a good thing or not, but it seems to be preferable to overpenalizing near misses.

Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ It penalizes false positives significantly less than false negatives, particularly if the false positives are uniformly distributed throughout the document.
Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution.
Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.
Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.

For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ This is not an entirely undesirable side effect.
For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ If it occurs j < k sentences from the beginning or the end of the segment, the segmentation is penalized j times.
For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.

Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ We will call this kind of error a near-miss error, distinct from a false positive or false negative error.
Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ We explored a weighted version of WindowDiff, in which the penalty is weighted by the difference |ri − ai|.
Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.

To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.
To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ This work was completed while the second author was a visiting professor at Harvard University.

When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ Some evaluators use a weighted combination of the two known as the F-measure (Baeza-Yates and Ribeiro-Neto 1999), but this is difficult to interpret (Beeferman, Berger, and Lafferty 1999).
When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ For the simulation runs described below, three metrics were implemented: In these studies, a single trial consists of generating a reference segmentation of 1,000 segments with some distribution, generating different experimental segmentations of a specific type 100 times, computing the metric based on the comparison of the reference and experimental segmentations, and averaging the 100 results.

To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ It penalizes false positives significantly less than false negatives, particularly if the false positives are uniformly distributed throughout the document.
To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ More formally, where b(i, j) represents the number of boundaries between positions i and j in the text and N represents the number of sentences in the text.
To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ The first set of tests was designed to test the metric’s performance on texts with different segment size distributions (Problem 3).

We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ WD is only slightly affected by variation of segment size distribution, gives equal weight to the false positive penalty and the false negative penalty, is able to catch mistakes in small segments just as well as mistakes in large segments, and penalizes near-miss errors less than pure false positives of equal magnitude.
We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ Thus, although it makes the mistake of penalizing Algorithm A-1 as much as Algorithms A-0 and A-2, it correctly recognizes that the error made by Algorithm A-3 is a near miss and assigns it a smaller penalty than Algorithm A-1 or any of the others.
We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ The penalty will then decrease linearly with Size(A)+Size(B) so long as k < Size(A)+ Size(B) < 2k.

In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task $$$$$ metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.
In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task $$$$$ Both Pk and WD appear to solve the problem of underpenalizing false positives, but WD has the added benefit of being more stable across variations in segment size distribution.

As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ The discussion above addresses Problems 1 through 4 but does not address Problem 5: how does one interpret the values produced by the metric?
As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ The amended metric, which we call WindowDiff, works as follows: for each position of the probe, simply compare the number of reference segmentation boundaries that fall in this interval (ri) with the number of boundaries that are assigned by the algorithm (ai).
As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.

We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ This work was completed while the second author was a visiting professor at Harvard University.
We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ The first, which we call Pk, simply doubles the false positive penalty.
We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.

Pevzner and Hearst (2002) highlighted several problems of the Pk metric. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Pevzner and Hearst (2002) highlighted several problems of the Pk metric. $$$$$ 1998; Beeferman, Berger, and Lafferty 1997, 1999).
Pevzner and Hearst (2002) highlighted several problems of the Pk metric. $$$$$ This means that as the variation of segment size increases, the metric becomes more lenient, since it severely underpenalizes errors in smaller segments, while not making up for this by overpenalizing errors in larger segments.

Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ The second, which we call WindowDiff (WD), counts the number of boundaries between the two ends of a fixed-length probe, and compares this number with the number of boundaries found in the same window of text for the reference segmentation.
Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.

However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ This approach clearly eliminates the asymmetry between the false positive and false negative penalties seen in the Pk metric.
However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ Thus, if Pk outputs a penalty of p for 500 false negatives, it would have a penalty of p2 for 250 false negatives.
However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ WindowDiff successfully distinguishes the near-miss error as a separate kind of error and penalizes it a different amount, something that Pk is unable to do.
However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.

Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ Thus, although it makes the mistake of penalizing Algorithm A-1 as much as Algorithms A-0 and A-2, it correctly recognizes that the error made by Algorithm A-3 is a near miss and assigns it a smaller penalty than Algorithm A-1 or any of the others.
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution.

To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ (As we will show, the differences are similar when we use a smaller probability of false negative/positive occurrence.)
To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.
To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ Precision and recall are standard evaluation measures for information retrieval tasks and are often applied to evaluation of text segmentation algorithms as well.
To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ From the tests we have run, it appears that the WD metric grows in a roughly linear fashion with the difference between the reference and the experimental segmentations.
