Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ We proposed two modifications to tackle these problems.
Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.

To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.
To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.
To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
To select an automatic segmenter for a particular task, a variety of segmentation evaluation metrics have been proposed, including Pk (Beeferman and Berger, 1999, pp. 198-200), WindowDiff (WD; Pevzner and Hearst 2002, p. 10), and most recently Segmentation Similarity (S; Fournier and Inkpen 2012, p. 154-156). $$$$$ WD penalizes all pure false positives the same amount regardless of how close they are to an actual boundary.

Pevzner and Hearst (2002, pp. 3-4) explain Pk well: a window of size k — where k is half of the mean manual segmentation length — is slid across both automatic and manual segmentations. $$$$$ This work was completed while the second author was a visiting professor at Harvard University.
Pevzner and Hearst (2002, pp. 3-4) explain Pk well: a window of size k — where k is half of the mean manual segmentation length — is slid across both automatic and manual segmentations. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Pevzner and Hearst (2002, pp. 3-4) explain Pk well: a window of size k — where k is half of the mean manual segmentation length — is slid across both automatic and manual segmentations. $$$$$ Pk simply doubles the false positive penalty, while WD counts and compares the number of boundaries between the two ends of the probe, as described earlier.

Pevzner and Hearst (2002, pp. 5-10) identified that Pk: i) penalizes false negatives (FNs)2 more than false positives (FPs); ii) does not penalize full misses within k units of a reference boundary; iii) penalize near misses too harshly in some situations; and iv) is sensitive to internal segment size variance. $$$$$ We have found that the Pk error metric for text segmentation algorithms is affected by the variation of segment size distribution, becoming slightly more lenient as the variance increases.
Pevzner and Hearst (2002, pp. 5-10) identified that Pk: i) penalizes false negatives (FNs)2 more than false positives (FPs); ii) does not penalize full misses within k units of a reference boundary; iii) penalize near misses too harshly in some situations; and iv) is sensitive to internal segment size variance. $$$$$ To translate this into actual values, we assume the metric is linear with respect to the number of errors (a reasonable assumption, supported by our experiments).
Pevzner and Hearst (2002, pp. 5-10) identified that Pk: i) penalizes false negatives (FNs)2 more than false positives (FPs); ii) does not penalize full misses within k units of a reference boundary; iii) penalize near misses too harshly in some situations; and iv) is sensitive to internal segment size variance. $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.
Pevzner and Hearst (2002, pp. 5-10) identified that Pk: i) penalizes false negatives (FNs)2 more than false positives (FPs); ii) does not penalize full misses within k units of a reference boundary; iii) penalize near misses too harshly in some situations; and iv) is sensitive to internal segment size variance. $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.

To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.
To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ This example shows the consequences of two different locations of false positives: on the left, the penalty is k2; on the right, it is k. Now, consider false positives.
To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ The table also shows the performance of the two other metrics.
To solve Pk's issues, Pevzner and Hearst (2002, pp. 10) proposed a modification referred to as WindowDiff (WD). $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.

Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ The Pk evaluation metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size distribution.
Hence the standard Pk (Beeferman et al, 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.

For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ This modification addresses all of the problems listed above.
For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ WD is only slightly affected by variation of segment size distribution, gives equal weight to the false positive penalty and the false negative penalty, is able to catch mistakes in small segments just as well as mistakes in large segments, and penalizes near-miss errors less than pure false positives of equal magnitude.
For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ The algorithm is penalized if ri =� ai (which is computed as |ri − ai |> 0).
For the TDT data we use the error metric pk (Beeferman et al, 1999) and WindowDiff (Pevzner and Hearst, 2002) which are implemented in the LCseg toolkit. $$$$$ Thus, if Pk outputs a penalty of p for 500 false negatives, it would have a penalty of p2 for 250 false negatives.

Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ This version, referred to as Pk, is calculated by setting k to half of the average true segment size and then computing penalties via a moving window of length k. At each location, the algorithm determines whether the two ends of the probe are in the same or different segments in the reference segmentation and increases a counter if the algorithm’s segmentation disagrees.
Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ We propose a simple modification to the Pk metric that remedies these problems.
Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.

To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ WindowDiff successfully distinguishes the near-miss error as a separate kind of error and penalizes it a different amount, something that Pk is unable to do.
To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ The second, which we call WindowDiff (WD), counts the number of boundaries between the two ends of a fixed-length probe, and compares this number with the number of boundaries found in the same window of text for the reference segmentation.
To attempt to mitigate the shortcomings of Pk, Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were counted, named WindowDiff (WD). $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.

When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ WindowDiff successfully distinguishes the near-miss error as a separate kind of error and penalizes it a different amount, something that Pk is unable to do.
When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ This approach clearly eliminates the asymmetry between the false positive and false negative penalties seen in the Pk metric.
When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ Now, consider how Pk treats the five types of mistakes above.
When Pevzner and Hearst (2002) proposed WD, they demonstrated that it was not as sensitive as Pk to variations in the size of segments inside a segmentation. $$$$$ In order for this to be true, both the segment to the left and the segment to the right of the missed boundary have to be of size greater than k; otherwise, the penalty can only be equal to the size of the smaller segment.

To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ WD is only slightly affected by variation of segment size distribution, gives equal weight to the false positive penalty and the false negative penalty, is able to catch mistakes in small segments just as well as mistakes in large segments, and penalizes near-miss errors less than pure false positives of equal magnitude.
To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ This is clear from the fact that the decrease is greater in the actual data than in the estimate.
To show this, they simulated how WD performs upon a segmentation comprised of 1000 segments with four different uniformly distributed ranges of internal segment sizes (keeping the mean at approximately 25 units) in comparison to a hypothesis segmentation with errors (false positives, false negatives, and both) uniformly distributed within segments (Pevzner and Hearst, 2002, pp. 11-12). $$$$$ The results of these tests are shown in Table 1.

We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ However, this would undermine the probabilistic nature of the metric.
We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ Pk penalized FP2 and FNP2 the least in their respective categories, and FP1 and FNP1 the most, with FP3 and FNP3 falling in between.
We also give the value of WindowDiff (WD), a variant of Pk proposed in (Pevzner and Hearst, 2002) that corrects some of its insufficiencies. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.

In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task: the pk metric (Beeferman et al, 1999), WindowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). $$$$$ metric, initially proposed by Beeferman, Berger, and Lafferty (1997), is becoming the standard measure for assessing text segmentation algorithms.
In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task: the pk metric (Beeferman et al, 1999), WindowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task: the pk metric (Beeferman et al, 1999), WindowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). $$$$$ Let a be the penalty for 500 Type A errors, b the penalty for 500 Type B errors, and c the penalty for 500 Type C errors; then the penalty for the FNP segmentation is p = a2 + b2 + c2.
In these tables, we report the F-measure of identifying the precise location of a story boundary as well as three metrics designed specifically for this type of segmentation task: the pk metric (Beeferman et al, 1999), WindowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998). $$$$$ From the above discussion, we know that WD is more lenient in situations where a false negative and a false positive occur near each other (where “near” means within a distance of k2) than Pk is.

As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ For these trials, we generated the reference segmentation using a uniform distribution of segment sizes in the (15, 35) range.
As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.
As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ Consider, for example, a reference segmentation and the results obtained by two different text segmentation algorithms, as depicted in Figure 1.
As a last measure for segmentation quality we used WindowDiff (Pevzner and Hearst, 2002), which only evaluates segment boundaries not the labels assigned to them. $$$$$ More formally, where b(i, j) represents the number of boundaries between positions i and j in the text and N represents the number of sentences in the text.

We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ Another important problem with the Pk metric is that it allows some errors to go unpenalized.
We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ It turns out that a simple change to the error metric algorithm remedies most of the problems described above, while retaining the desirable characteristic of penalizing near misses less than pure false positives and pure false negatives.
We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ This approach clearly eliminates the asymmetry between the false positive and false negative penalties seen in the Pk metric.
We assessed segmentation performance using the Pk and WindowDiff (WD) error measures proposed by (Beeferman et al, 1999) and (Pevzner and Hearst, 2002) respectively. $$$$$ Let us begin the analysis by trying to explain why Pk scores for the FNP segmentation make sense.


Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ This new metric—called WindowDiff—moves a fixed-sized window across the text and penalizes the algorithm whenever the number of boundaries within the window does not match the true number of boundaries for that window of text.
Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ This work was completed while the second author was a visiting professor at Harvard University.
Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ In addition, doubling the penalty may not always be the correct solution, since segment size will vary from the average, and false positives are not necessarily uniformly distributed throughout the document.
Pevzner and Hearst (2002) propose the alternative metric called WindowDiff. $$$$$ They would also like to thank the anonymous reviewers for their valuable comments.

However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ We proposed two modifications to tackle these problems.
However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ However, a theoretical analysis of the metric finds several problems: the metric penalizes false negatives more heavily than false positives, overpenalizes near misses, and is affected by variation in segment size dis- We propose a simple modification to the that remedies these problems.
However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ More formally, where b(i, j) represents the number of boundaries between positions i and j in the text and N represents the number of sentences in the text.
However, unlike Pk and Pk, WindowDiff takes into account how many boundaries fall within the window and is penalizing in 'how many discrepancies occur between the reference and the system results' rather than 'determining how often two units of text are incorrectly labeled as being in different segments' (Pevzner and Hearst, 2002). $$$$$ Thus, although it makes the mistake of penalizing Algorithm A-1 as much as Algorithms A-0 and A-2, it correctly recognizes that the error made by Algorithm A-3 is a near miss and assigns it a smaller penalty than Algorithm A-1 or any of the others.

Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ It turns out that a simple change to the error metric algorithm remedies most of the problems described above, while retaining the desirable characteristic of penalizing near misses less than pure false positives and pure false negatives.
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ We argue that this kind of error is less detrimental than the errors made by Pk.
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ Both authors thank Barbara Grosz and Stuart Shieber, without whom this work would not have happened, and Freddy Choi for some helpful explanations.
Another issue regarding WindowDiff is that it is not clear 'how does one interpret the values produced by the metric' (Pevzner and Hearst, 2002). $$$$$ Partial support for the research reported in this paper was provided by National Science Foundation Grants IRI-9618848 and CDA-94-01024.

To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ Some evaluations circumvent this difficulty by detecting boundaries in sets of concatenated documents, where there can be no disagreements about the fact of the matter (Reynar 1994; Choi 2000); others have several human judges make ratings to produce a “gold standard.” The second difficulty with evaluating these algorithms is that for different applications of text segmentation, different kinds of errors become important.
To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ Type B errors have an average penalty of k, as for Pk.
To measure the quality of segmentation of the lecture transcript, we use two standard metrics, Pk (Beeferman et al, 1999) and WindowDiff (WD) (Pevzner and Hearst, 2002), but both metrics disregard the alignment links (i.e. the topic labels). $$$$$ Since the probabilities are uniformly distributed across all segments and all boundaries, on average one would expect the following distribution of errors: A Type A error is a standard false positive, so the average penalty is k2.
