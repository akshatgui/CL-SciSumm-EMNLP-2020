Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ Our second evaluation test (using comparison to a gold standard) gave mixed results: the best performance was obtained by the simple additive model, with PLSR coming in second place.
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.

Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. $$$$$ Widdows (2008) Obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model.
Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. $$$$$ We then extracted the 10 nearest neighbours for each of the three modelled predictions, but this time the subspace included all predictions, as well as all the original observations (380 x 4 = 1520 items).

Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ 784â€“787).
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.

The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. $$$$$ Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).
The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. $$$$$ Mitchell & Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).
The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. $$$$$ For the sake of simplifying the implementation of evaluation methods, in this paper we will compare the first two approaches, vector addition and vector pointwise-multiplication, with regression modelling by partial least squares.
The general equation for the two functions is the following, where B is a matrix of weights that is multiplied by the noun vector v to produce the AN vector p. p= Bv (5) In the linear map (lim) approach proposed by Guevara (2010), one single matrix B is learnt that represents all adjectives. $$$$$ In what follows we briefly evaluate the three resulting models of compositionality.

Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. $$$$$ The first method draws a direct comparison of the different predicted vectors for each candidate A-N pair by computing the Euclidean distance between the observed vector and the modelled predictions.
Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. $$$$$ This has been successfully applied to document-based applications such as the computation of document similarity in information retrieval.
Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. $$$$$ This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.
Following Guevara (2010), we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented inthe Rpls package (Mevik and Wehrens, 2007), setting the latent dimension parameter of PLSR to 300. $$$$$ Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.

Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ The corpus was pre-processed to represent frequent Adjective-Noun lemma pairs as a single token (e.g. while in the original corpus the A-N phrase nice house consists in two separate lemmas (nice and house), in the processed corpus it appears as a single entry nice_house).
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: In the DSM literature, the additive model has become a de facto standard approach to approximate the composed meaning of a group of words (or a document) as the sum of their vectors (which results in the centroid of the starting vectors).

We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model.
We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ Another important issue that we still have not touched is the role played by lexical association (collocations) in the prediction models.
We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ Another important issue that we still have not touched is the role played by lexical association (collocations) in the prediction models.

The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. $$$$$ We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.
The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. $$$$$ For the sake of simplifying the implementation of evaluation methods, in this paper we will compare the first two approaches, vector addition and vector pointwise-multiplication, with regression modelling by partial least squares.
The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the co-occurrence vectors of corpus-observed ANs to train a supervised composition model. $$$$$ In this paper we explore the computational modelling of compositionality in distributional models of semantics.

In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. $$$$$ Its main advantage is the fact that it is designed to approximate functions in problems of multivariate multiple regression where the number of observations is relatively small if compared to the number of variables (dimensions).
In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. $$$$$ Sahlgren, 2006).
In the linear map (lm) approach proposed by Guevara (2010), a composite AN vector is obtained by multiplying a weight matrix by the concatenation of the adjective and noun vectors, so that each dimension of the generated AN vector is a linear combination of dimensions of the corresponding adjective and noun vectors. $$$$$ This poor results can be attributed to the very restrictive nature of our gold standard and, also, to the asymmetrical composition of the compared data (gold standard: 3,800 neighbours from a pool of just 380 different items; prediction space: 11,400 neighbours from a pool of 1,520 items).

(2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. $$$$$ Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.
(2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. $$$$$ PLSR is widely used in in unrelated fields such as spectroscopy, medical chemistry, brain-imaging and marketing (Mevik & Wehrens, 2007).
(2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. $$$$$ This is, in principle, a tractable problem that can be solved by standard machine learning techniques such as multilayer perceptrons or support vector machines.
(2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al (2012) and Collobert et al (2011) have been proposed. $$$$$ The original dimensions were the 3,000 most frequent content words in the BNC.

Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. $$$$$ This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.
Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. $$$$$ In order to evaluate the three models of compositionality that were built, we devised two different procedures based on the Euclidean measure of geometric distance.
Guevara (2010), Mitchell and Lapata (2010), Socher et al (2011) and Zanzotto et al (2010) generalize the simple additive model by applying structure-encoding operators to the vectors of two sister nodes before addition, thus breaking the inherent symmetry of the simple additive model. $$$$$ For the sake of simplifying the implementation of evaluation methods, in this paper we will compare the first two approaches, vector addition and vector pointwise-multiplication, with regression modelling by partial least squares.

This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. $$$$$ Its main advantage is the fact that it is designed to approximate functions in problems of multivariate multiple regression where the number of observations is relatively small if compared to the number of variables (dimensions).
This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. $$$$$ We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.
This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. $$$$$ While the vector-based representation of word meaning has been used for a long time in computational linguistics, the techniques that are currently used have not seen much development with regards to one of the main aspects of semantics in natural language: compositionality.
This work extends the basic methodology presented in Guevara (2010) with new data collection techniques, improved evaluation metrics and new case studies. $$$$$ In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.

Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. $$$$$ Our second evaluation test (using comparison to a gold standard) gave mixed results: the best performance was obtained by the simple additive model, with PLSR coming in second place.
Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. $$$$$ The data were divided into a training set (1,000 A-N pairs) and a testing set (the remaining 380 A-N pairs).
Guevara (2010) and Baroni and Zamparelli (2010) introduce a different approach to model semantic compositionality in distributional spaces by extracting context vectors from the corpus also for the composed vector. $$$$$ However, given that sequences of words tend to be of very low frequency (and thus difficult to represent in a DSM), suitable data sets will inevitably suffer the curse of dimensionality: we will often have many more variables (dimensions) than observations.

The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. $$$$$ We filtered the candidate list by frequency (> 400) obtaining 1,380 different A-N pairs.
The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. $$$$$ In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens.
The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression. $$$$$ We use a general-purpose vector space extracted from the British National Corpus.

Let us start by setting the syntactic relation that we want to focus on for the purposes of this study $$$$$ 784â€“787).
Let us start by setting the syntactic relation that we want to focus on for the purposes of this study $$$$$ We then extracted the 10 nearest neighbours for each of the three modelled predictions, but this time the subspace included all predictions, as well as all the original observations (380 x 4 = 1520 items).
Let us start by setting the syntactic relation that we want to focus on for the purposes of this study $$$$$ In this paper we explore the computational modelling of compositionality in distributional models of semantics.

Guevara (2010) and Mitchell and Lapata (2010). $$$$$ We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.
Guevara (2010) and Mitchell and Lapata (2010). $$$$$ This is, in principle, a tractable problem that can be solved by standard machine learning techniques such as multilayer perceptrons or support vector machines.

A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). $$$$$ In this paper we explore the computational modelling of compositionality in distributional models of semantics.
A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). $$$$$ The method, Partial Least Squares Regression, is well known in other dataintensive fields of research, but to our knowledge had never been put to work in computational distributional semantics.
A more general form of the additive model (full add) has been proposed by Guevara (2010) (see also Zanzotto et al (2010)). $$$$$ In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: In the DSM literature, the additive model has become a de facto standard approach to approximate the composed meaning of a group of words (or a document) as the sum of their vectors (which results in the centroid of the starting vectors).

Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices $$$$$ Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.
Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices $$$$$ By simply combining the vector representations of the independent Adjectives and Nouns in our data-set (v1 and v2) we built an additive prediction model (v1 + v2) and a simplified pointwise multiplicative prediction model (v1 x v2) for each candidate pair.
Guevara (2010) and Zanzotto et al (2010) propose the full additive model (full add), where the two vectors to be added are pre-multiplied by weight matrices $$$$$ Widdows (2008) Obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model.

Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ In particular, we propose that the usual procedures from machine learning tasks must be implemented also in the search for semantic compositionality in DSM.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ However, given that sequences of words tend to be of very low frequency (and thus difficult to represent in a DSM), suitable data sets will inevitably suffer the curse of dimensionality: we will often have many more variables (dimensions) than observations.

Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzottoet al (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors $$$$$ This has been successfully applied to document-based applications such as the computation of document similarity in information retrieval.
