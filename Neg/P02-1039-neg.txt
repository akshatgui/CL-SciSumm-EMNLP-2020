In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102. $$$$$ In addition, only limited part-of-speech labels are considered to reduce the number of possible decoded-tree structures.
In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102. $$$$$ Theoretically we need an LM which gives the prior probability of an English parse tree.
In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102. $$$$$ This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).
In comparison, in (Yamada and Knight, 2002), which was a phrasal structure based statistical MT system for Chinese to English translation, the Bleu score reported for short sentences (less than 14 words) is 0.099 to 0.102. $$$$$ Because the input of the channel model is an English parse tree, the decoding algorithm is based on conventional syntactic parsing, and the grammar is expanded by the channel operations of the TM.

Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. $$$$$ An example is shown in Figure 2.
Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. $$$$$ The translation model was extended to incorporate phrasal translations.
Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. $$$$$ The use of an LM needs consideration.
Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. $$$$$ The n-table specifies the probability of having the third tree (Inserted) given the second tree.

The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). $$$$$ In practice, the phrase lengths (, ) are limited to reduce the model size.
The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). $$$$$ To obtain a tree in the English order, we apply the reverse of the reorder operation (back-reordering) using the information associated to the rule expanded by the r-table.
The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). $$$$$ In Figure 1, the r-table specifies the probability of having the second tree (Reordered) given the first tree.
The model is further extended to incorporate phrasal translations performed at each node of the input parse tree (Yamada and Knight, 2002). $$$$$ Using the selected sentence pairs, we retrained only the r-table and the PCFG.

While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. $$$$$ At this point, we gave up using the PCFG as a component of the LM.
While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. $$$$$ For the IBM models defined by a pioneering paper (Brown et al., 1993), a decoding algorithm based on a left-to-right search was described in (Berger et al., 1996).
While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. $$$$$ However, we can approximate it with an n-gram LM, which is wellstudied and widely implemented.
While (Yamada and Knight, 2002) represent syntactical information in the decoding process through a series of transformation operations, we operate directly at the phrase level. $$$$$ Section 4 presents the basic idea for decoding.

The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al (1993) for translation by Yamada and Knight (2002). $$$$$ This paper describes a decoding algorithm for a syntax-based translation model (Yamada and Knight, 2001).
The syntactically supervised model has been found to outperform the IBM word-level alignment models of Brown et al (1993) for translation by Yamada and Knight (2002). $$$$$ We will discuss this point later in Section 7.

(Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. $$$$$ See (Yamada, 2002) for details.
(Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. $$$$$ An example is shown in Figure 2.
(Yamada and Knight, 2002) propose a syntax-based decoder that restrict word reordering based on reordering operations on syntactic parse-trees of the input sentence. $$$$$ For the IBM models defined by a pioneering paper (Brown et al., 1993), a decoding algorithm based on a left-to-right search was described in (Berger et al., 1996).

Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. $$$$$ Their models are based on mechanisms that generate two languages at the same time, so an English tree is obtained as a subproduct of parsing.
Yamada and Knight (2002) presents a decoder for syntax-based MT that uses so-called phrasal translation units that correspond to blocks. $$$$$ As in other statistical machine translation systems, the decoder has to cope with a huge search sentence that maximizes P , which also maximizes P according to Bayes’ rule.

As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. $$$$$ As in other statistical machine translation systems, the decoder has to cope with a huge search sentence that maximizes P , which also maximizes P according to Bayes’ rule.
As an alternative option to our verb-modifier experiments, structured language models (Chelba and Jelinek, 1998) might be considered to improve clause coherence, until full-featured syntax-based MT models (Yamada and Knight (2002), Eisner (2003), Chiang (2005) among many others) are tested when translating to morphologically rich languages. $$$$$ Formally, the channel probability P is where , , and is a sequence of leaf words of a tree transformed byfrom .

It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). $$$$$ We applied several pruning techniques and obtained good decoding quality and coverage.
It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). $$$$$ Because the input of the channel model is an English parse tree, the decoding algorithm is based on conventional syntactic parsing, and the grammar is expanded by the channel operations of the TM.
It follows the decoding-as-parsing idea exemplified by Wu (1996) and Yamada and Knight (2002). $$$$$ These are statistics which are not modeled in the TM.

This model is then decoded as described in (Yamada and Knight, 2002). $$$$$ For each non-lexical rule in the original English grammar (such as “VP VB NP PP”), we supplement it with reordered rules (e.g.
This model is then decoded as described in (Yamada and Knight, 2002). $$$$$ We have presented a decoding algorithm for a syntax-based statistical machine translation.
This model is then decoded as described in (Yamada and Knight, 2002). $$$$$ In addition, only limited part-of-speech labels are considered to reduce the number of possible decoded-tree structures.

For reasons of speed, Yamada and Knight (2002) limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus. $$$$$ In addition, since the TM assumes an English parse tree as input, a trigram LM might not be appropriate.
For reasons of speed, Yamada and Knight (2002) limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus. $$$$$ If we use a trigram model for the LM, a convenient implementation is to first build a decodedtree forest and then to pick out the best tree using a trigram-based forest-ranking algorithm as described in (Langkilde, 2000).
For reasons of speed, Yamada and Knight (2002) limited training to sentences of length 30, and were able to use only one fifth of the available Chinese-English parallel corpus. $$$$$ To see if it was really a corpus problem, we selected a good portion of the corpus and re-trained the r-table.

One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al, 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). $$$$$ We applied several pruning techniques and obtained good decoding quality and coverage.
