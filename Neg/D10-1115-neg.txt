Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter.
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ More in detail, we evaluate here the composition methods (and the adjective and noun baselines) by computing, for each of them, the cosine of the test set AN vectors they generate (the “predicted” ANs) with the 41K vectors representing our extended vocabulary in semantic space, and looking at the position of the corresponding observed ANs (that were not used for training, in the supervised approaches) in the cosine-ranked lists.

We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). $$$$$ We tried to alleviate this problem by assigning a 0 to composite dimensions where the two input vectors had different signs.

The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ The training data are pairs of adjective-noun vector concatenations, as input, and corpus-derived AN vectors, as output.
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ Given two vectors u and v, they identify two general classes of composition models, (linear) additive models: where A and B are weight matrices, and multiplicative models: where C is a weight tensor projecting the uv tensor product onto the space of p. Mitchell and Lapata derive two simplified models from these general forms.
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ One of the most important avenues for further work will be to come to a better characterization of the behaviour of corpus-observed ANs, where they work and where the don’t.

Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ Full co-occurrence matrix The 10K lemmas (nouns, adjectives or verbs) that co-occur with the largest number of items in the core vocabulary constitute the dimensions (columns) of our cooccurrence matrix.
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ We consider the following issues to be the most pressing ones.
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ The training data are pairs of adjective-noun vector concatenations, as input, and corpus-derived AN vectors, as output.

Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.
Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ In Section 3, we introduce our proposal.
Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ The work we reported constitutes an encouraging start for our approach to modeling (AN) composition.

Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ Our two “indirect” representations achieve similar performance, and they are (slightly) better than the traditional method based on adjective co-occurrence vectors.
Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ Second, at least subjectively, we find that in many cases the nearest neighbor of predicted AN is actually more sensible than that of observed AN: current element (vs. left) for current dimension, historical reality (vs. different today) for historical thing, special thing (vs. little animal) for special something, young image (vs. important song) for young photo.

It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ Third, we have some reasonable hope that our functions can capture to a certain extent the polysemous nature of adjectives: we could learn, for example, a green matrix with large positive weights mapping from noun features that pertain to concrete objects to color dimensions of the output vector (green chair), as well as large positive weights from features characterizing certain classes of abstract concepts to political/social dimensions in the output (green initiative).
It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ Mitchell and Lapata also consider a constrained version of the multiplicative approach that reduces to componentwise multiplication, where the i-th component of the composed vector is given by: pi = uivi.
It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ By crossing the selected adjectives and nouns, we constructed a test set containing 26,440 ANs, all attested in the sample corpus (734 ANs per adjective on average, ranging from 1,337 for new to 202 for mental).

Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ There is a high inverse correlation between the frequency of occurrence of an AN and the rank of the observed AN with respect to the predicted one (p =−0.48), suggesting that our model is worse at approximating the observed vectors of rare forms, that might, in turn, be those for which the corpusbased representation is less reliable.
Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ It remains to be seen if the approach we proposed will scale up to such challenges.
Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ In our case, the independent variables for the regression equations are the dimensions of the corpusbased vectors of the component nouns, whereas the AN vectors provide the dependent variables.

Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ We estimate the coefficients using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007).
Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ As a matter of fact, the current model is too syntax-sensitive and does not capture similarities across different constructions.
Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ To start simple, we assume here that adjectives in the attributive position (AN) are linear functions from n-dimensional (noun) vectors onto n-dimensional vectors, an operation that can be expressed as multiplication of the input noun column vector by a n x n matrix, that is our representation for the adjective (in the language of linear algebra, an adjective is an endomorphic linear map in noun space).

In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ Evaluation-wise, the differences between observed and predicted ANs must be analyzed more extensively, to support the claim that, when their vectors differ, model-based prediction improves on the observed vector.
In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ If adjectives are functions, and not corpus-derived vectors, is it still possible to compare them meaningfully?

Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ We consider the following issues to be the most pressing ones.
Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ Since we do not collect vectors for the “functor” component of a composition process (for AN constructions, the adjective), our approach naturally extends to processes that involve bound morphemes, such as affixation, where we would not need to collect independent co-occurrence information for the affixes.
Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ Table 5 shows that all methods are significantly better than chance.
Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ Since the observed vectors look like plausible representations of composite meaning, we expect that the closer the modelgenerated vectors are to the observed ones, the better they should also perform in any task that requires access to the composite meaning, and thus that the results of the current evaluation should correlate with applied performance.

In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ We cluster with the CLUTO toolkit (Karypis, 2003), using the repeated bisections with global optimization method, accepting all of CLUTO’s default values for this choice.
In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ We tried to alleviate this problem by assigning a 0 to composite dimensions where the two input vectors had different signs.

Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ If adjectives are functions, and not corpus-derived vectors, is it still possible to compare them meaningfully?
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ Evaluation-wise, the differences between observed and predicted ANs must be analyzed more extensively, to support the claim that, when their vectors differ, model-based prediction improves on the observed vector.
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.

Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Mitchell and Lapata also consider a constrained version of the multiplicative approach that reduces to componentwise multiplication, where the i-th component of the composed vector is given by: pi = uivi.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Having tentatively established that the sort of vectors we can harvest for ANs by directly collecting their corpus co-occurrences are reasonable representations of their composite meaning, we move on to the core question of whether it is possible to reconstruct the vector for an unobserved AN from information about its components.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Our proposed method, alm, emerges as the best approach.

Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ This mapping could now be sensitive to the particular noun the adjective receives, and it does not need to return a subset of the original noun denotation (as in the case of fake N).
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ The left side of Table 4 compares the nearest neighbors (excluding each other) of the observed and alm-predicted vectors in 10 ranSIMILAR DISSIMILAR adj N obs. neighbor pred. neighbor adj N obs. neighbor pred. neighbor common understanding common approach common vision American affair Am. development Am. policy different authority diff. objective diff. description current dimension left (a) current element different partner diff. organisation diff. department good complaint current complaint good beginning general question general issue same great field excellent field gr. distribution historical introduction hist. background same historical thing different today hist. reality necessary qualification nec. experience same important summer summer big holiday new actor new cast same large pass historical region large dimension recent request recent enquiry same special something little animal special thing small drop droplet drop white profile chrome (n) white show young engineer young designer y. engineering young photo important song young image where rank of observed w.r.t. predicted is 1.
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ In the proposed adjective-specific linear map (alm) method, an AN is generated by multiplying an adjective weight matrix with a noun (column) vector.
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ Evaluation-wise, the differences between observed and predicted ANs must be analyzed more extensively, to support the claim that, when their vectors differ, model-based prediction improves on the observed vector.

Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ 5 Study 1: ANs in semantic space The actual distribution of ANs in the corpus, as recorded by their co-occurrence vectors, is fundamental to what we are doing.
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ In our approach, the weight matrix B is specific to a single adjective – as we will see in Section 7 below, it is our representation of the meaning of the adjective.
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ These problems have prompted a more flexible FS representation for attributive adjectives — functions from the meaning of a noun onto the meaning of a modified noun (Montague, 1970a).
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ Additive AN vectors (add method) are obtained by summing the corresponding adjective and noun vectors after normalizing them (non-normalized addition was also tried, but it did not work nearly as well as the normalized variant).

Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ We conclude that, although our approach does not provide a direct encoding of adjective meaning in terms of such independently collected vectors, it does have meaningful ways to represent their semantic properties.
Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.
Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ Section 5 provides some empirical justification for using corpusharvested AN vectors as the target of our function learning and evaluation benchmark.

The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ Fourth, the approach is naturally syntax-sensitive, since we train it on observed data for a specific syntactic position: we would train separate linear models for, say, the same adjective in attributive (AN) and predicative (N is A) position.
The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ Moreover, coherently with this view, our evaluation below will be based on how closely the models approximate the observed vectors of unseen ANs.

Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Mitchell and Lapata also consider a constrained version of the multiplicative approach that reduces to componentwise multiplication, where the i-th component of the composed vector is given by: pi = uivi.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Closer to our current purposes is the general framework for vector composition proposed by Mitchell and Lapata (2008), subsuming various earlier proposals.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Their simplified additive model p = αu + Qv was a common approach to composition in the earlier literature, typically with the scalar weights set to 1 or to normalizing constants (Foltz et al., 1998; Kintsch, 2001; Landauer and Dumais, 1997).

In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ Section 5 provides some empirical justification for using corpusharvested AN vectors as the target of our function learning and evaluation benchmark.
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ Following the insight of FS, we treat attributive adjectives as functions over noun meanings; however, noun meanings are vectors, not sets, and the functions are learnt from corpus-based noun-AN vector pairs.
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ Formal semantics (FS), the research program stemming from Montague (1970b; 1973), has opposite strengths and weaknesses.
