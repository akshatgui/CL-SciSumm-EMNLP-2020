Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ The lower the rank, the better the approximation.
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ The random baseline distribution is obtained by 10K random assignments of adjectives to the clusters, under the constraint that no cluster is empty.
Compositionality of adjective-noun phrases and how it can be adequately modeled in VSMs is the main concern in Baroni and Zamparelli (2010) and Guevara (2010), who are in search of the best composition operator for combining adjective with noun meanings. $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.

We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ The experimental setting is described in Section 4.
We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ This is at odds with Guevara’s experiment in which slm outperformed mult and add on the task of ranking predicted ANs with respect to a target observed AN.
We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs $$$$$ Evaluation-wise, the differences between observed and predicted ANs must be analyzed more extensively, to support the claim that, when their vectors differ, model-based prediction improves on the observed vector.

The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ First, although we use a supervised learning method (least squares regression), we do not need hand-annotated data, since the target AN vectors are automatically collected from the corpus just like vectors for single words are.
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ We use a subset of the extended vocabulary containing only nouns and adjectives (the core vocabulary) for feature selection and dimensionality reduction, so that we do not implicitly bias the structure of the semantic space by our choice of ANs.
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ Our two “indirect” representations achieve similar performance, and they are (slightly) better than the traditional method based on adjective co-occurrence vectors.
The final approach we re-implement is the one proposed by Baroni and Zamparelli (2010), who treat attributive adjectives as functions from noun meanings to noun meanings. $$$$$ Guevara adopts the full additive composition form from Equation (1) and he estimates the A and B weights using partial least squares regression.

Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ First, we notice some ANs that are difficult to interpret out-of-context (important summer, white profile, young photo, large pass, ... ).
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ Importantly, the neighbors pick up the composite meaning rather than that of the adjective or noun alone.
Baroni and Zamparelli (2010) show that their model significantly outperforms other vector composition methods, including addition, multiplication and Guevara's approach, in the task of approximating the correct vectors for previously unseen (but corpus-attested) ANs. $$$$$ We estimate the coefficients using (multivariate) partial least squares regression (PLSR) as implemented in the R pls package (Mevik and Wehrens, 2007).

Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ Still, we need to design a good testing scenario to evaluate the quality of such model-generated constructions.
Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ As discussed in the introduction, we will take adjectives in attributive position to be functions from one noun meaning to another.
Note that they follow very closely the procedure of Baroni and Zamparelli (2010), including choices of source corpus and parameter values, so that we expect their results on the quality of the various models in predicting ANs to also hold for our re-implementations. $$$$$ The experimental setting is described in Section 4.

Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ The test set built according to these criteria contains 4 classes: color (white, black, red, green), positive evaluation (nice, excellent, important, major, appropriate), time (recent, new, current, old, young), and size (big, huge, little, small, large).
Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ We evaluate performance on the task of clustering those 19 adjectives in our set that can be relatively straightforwardly categorized into general classes comprising a minimum of 4 items.
Confirming the results of Baroni and Zamparelli (2010), non-normalized versions of add and mult were also tested, but did not produce significant results (in the case of multiplication, normalization amounts to multiplying the composite vector by a scalar, so it only affects the length-dependent vector length measure). $$$$$ The best results on the task of paraphrasing noun-verb combinations with ambiguous verbs (sales slump is more like declining than slouching) are obtained using the multiplicative approach, and by weighted combination of addition and multiplication (we do not test model combinations in our current experiments).

It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ In our case, the independent variables for the regression equations are the dimensions of the corpusbased vectors of the component nouns, whereas the AN vectors provide the dependent variables.
It is important to note that, as reported in Baroni and Zamparelli (2010), the mult method can be expected to perform better in the original, non reduced semantic space because the SVD dimensions can have negative values, leading to counter intuitive results with component-wise multiplication (multiplying large opposite-sign values results in large negative values instead of being cancelled out). $$$$$ Formal semantics (FS), the research program stemming from Montague (1970b; 1973), has opposite strengths and weaknesses.

Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ In these cases, dissimilarities between observed and expected vectors, rather than signaling problems with the model, might indicate that the predicted vector, based on a composition function learned from many examples, is better than the one directly extracted from the corpus.
Following Guevara, we estimate the coefficients of the equation using (multivariate) partial least squares regression (PLSR) as implemented in the Rpls package (MevikandWehrens, 2007), with the latent dimension parameter of PLSR set to 50, the same value used by Baroni and Zamparelli (2010). $$$$$ As a matter of fact, the current model is too syntax-sensitive and does not capture similarities across different constructions.

Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ The above-mentioned researchers do not exploit corpus evidence about the p vectors that result from composition, despite the fact that it is straightforward (at least for short constructions) to extract direct distributional evidence about the composite items from the corpus (just collect co-occurrence information for the composite item from windows around the contexts in which it occurs).
Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ Again, the multiplicative model works best in Erk and Pad´o’s experiments.
Finally, in the adjective-specific linear map (alm) method of Baroni and Zamparelli (2010), an AN is generated by multiplying an adjective weight matrix with a noun vector. $$$$$ Closer to our current purposes is the general framework for vector composition proposed by Mitchell and Lapata (2008), subsuming various earlier proposals.

In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ We currently train each adjective-specific model separately: We should explore hierarchical modeling approaches that exploit similarities across adjectives (and possibly syntactic constructions) to estimate better models.
In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ The single linear mapping model (slm) proposed by Guevara (2010) is doing even worse than the multiplicative method, suggesting that a single set of weights does not provide enough flexibility to model a variety of adjective transformations successfully.
In Baroni and Zamparelli (2010), the alm model performed far better than add and mult in approximating the correct vectors for unseen ANs, while on this (in a sense, more meta linguistic) task add and mult work better, while alm is successful only in the more sophisticated measure of neighbor density. $$$$$ We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors.

Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ Adjectives are also often assumed to denote properties: in this view redadj would be the set of ‘entities which are red’, plasticadj, the set of ‘objects made of plastic’, and so forth.
Although, somewhat disappointingly, the model that has been shown in a previous study (Baroni and Zamparelli, 2010) to be the best at capturing the semantics of well-formed ANs turns out to be worse than simple addition and multiplication. $$$$$ For the add, mult, adj and noun methods, we ran the tests of Section 6 not only in the SVD-reduced space, but also in the original 10K-dimensional cooccurrence space.

In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ Unlike Guevara, (i) we train separate models for each adjective (we learn adjective-specific functions, whereas Guevara learns a generic “AN-slot” function) and, consequently, (ii) corpus-harvested adjective vectors play no role for us (their values would be constant across the training input vectors).
In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ In our case, the independent variables for the regression equations are the dimensions of the corpusbased vectors of the component nouns, whereas the AN vectors provide the dependent variables.
In particular, Baroni and Zamparelli (2010) tackle adjective-noun compositions using a vector representation for nouns and learning a matrix representation for each adjective. $$$$$ These measures (unlike mean and variance) are not affected by the cut-off after 1K neighbors.

Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ To construct the AN test set, we first selected 36 adjectives across various classes: size (big, great, huge, large, major, small, little), denominal (American, European, national, mental, historical, electronic), colors (white, black, red, green) positive evaluation (nice, excellent, important, appropriate), temporal (old, recent, new, young, current), modal (necessary, possible), plus some common abstract antonymous pairs (difficult, easy, good, bad, special, general, different, common).
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ There is a high inverse correlation between median rank and adjective frequency (Spearman’s p = −0.56).
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ Since we are working in the 300-dimensional right singular vector space, for each adjective we have 300 regression problems with 300 independent variables, and the training data (the noun-AN pairs available for each test set adjective) range from about 200 to more than 1K items.
Work by Baroni and Zamparelli (2010) models nouns as vectors in some semantic space and adjectives as matrices. $$$$$ If nir is the number of items from the i-th true (gold standard) class assigned to the r-th cluster, n is the total number of items and k the number of clusters, then: Purity = n �r—1rmax(nir).

Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ Finally, although adjective representations are not directly harvested from corpora, we can still meaningfully compare adjectives to each other or other words by using their estimated matrix, or an average vector for the ANs that contain them: both options are tested in Section 7 below.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ In our empirical tests, we compare our approach to the simplified additive and multiplicative models of Mitchell and Lapata (the former with normalization constants as scalar weights) as well as to Guevara’s approach.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ The linear map for a specific adjective is learnt, using linear regression, from pairs of noun and AN vectors extracted from a corpus.
Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. $$$$$ We looked at the nearest neighbors of these centroids in semantic space among the 41K items (adjectives, nouns and ANs) in our extended vocabulary (here and in all experiments below, similarity is quantified by the cosine of the angle between two vectors).

Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ Evaluation in a more applied task should also be pursued – in particular, we will design a paraphrasing task similar to the one proposed by Mitchell and Lapata to evaluate noun-verb constructions.
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ Fourth, the approach is naturally syntax-sensitive, since we train it on observed data for a specific syntactic position: we would train separate linear models for, say, the same adjective in attributive (AN) and predicative (N is A) position.
Baroni and Zamparelli (2010) and Guevara (2010) focus on how best to represent compositionality in adjective-noun phrases considering different types of composition operators. $$$$$ The main innovation of Guevara (2010), who focuses on adjective-noun combinations (AN), is to use the cooccurrence vectors of observed ANs to train a supervised composition model (we became aware of Guevara’s approach after we had developed our own model, that also exploits observed ANs for training).

Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ Right: nearest neighbors of predicted and observed ANs for random set where rank of observed w.r.t. predicted is > 1K. domly selected cases where the observed AN is the nearest neighbor of the predicted one.
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ As a matter of fact, the current model is too syntax-sensitive and does not capture similarities across different constructions.
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ We leave it to further work to assess the quality of the generated ANs in an applied setting, for example adapting Mitchell and Lapata’s paraphrasing task to ANs.
Baroni and Zamparelli (2010) computed the parent vector of adjective-noun pairs by p= Ab, where A is an adjective matrix and b is a vector for a noun. $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.

Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training.
Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ We consider the following issues to be the most pressing ones.
Relevant papers include O Seaghdha (2010), who evaluates several topic models adapted to learning selectional preference using co-occurence and Baroni and Zamparelli (2010), who represent nouns as vectors and adjectives as matrices, thus treating them as functions over noun meaning. $$$$$ The random baseline distribution is obtained by 10K random assignments of adjectives to the clusters, under the constraint that no cluster is empty.

The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ Somewhat optimistically, we hope that chair will have near-0 values on the relevant abstract dimensions, like initiative on the concrete features, and thus the weights will not interfere.
The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ The work we reported constitutes an encouraging start for our approach to modeling (AN) composition.
The model by Baroni and Zamparelli (2010) emerges as a suitable model of adjectival composition, while multiplication and addition shed mixed results. $$$$$ However, FS has nothing to say on how these functions should be constructed.

Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ We cluster with the CLUTO toolkit (Karypis, 2003), using the repeated bisections with global optimization method, accepting all of CLUTO’s default values for this choice.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Ultimately, we want to compose larger and larger constituents, up to full sentences.
Interestingly, recent approaches to the semantic composition of adjectives with nouns such as Baroni and Zamparelli (2010) and Guevara (2010) draw on the classical analysis of adjectives within the Montagovian tradition of formal semantic theory (Montague, 1974), on which they are treated as higher order predicates, and model adjectives as matrices of weights that are applied to noun vectors. $$$$$ Second, our approach rests on the assumption that the corpus-derived AN vectors are interesting objects that should constitute the target of what a composition process tries to approximate.

In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ We realize that the evidence presented here is of a very preliminary and intuitive nature.
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ Second, at least subjectively, we find that in many cases the nearest neighbor of predicted AN is actually more sensible than that of observed AN: current element (vs. left) for current dimension, historical reality (vs. different today) for historical thing, special thing (vs. little animal) for special something, young image (vs. important song) for young photo.
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ Their simplified additive model p = αu + Qv was a common approach to composition in the earlier literature, typically with the scalar weights set to 1 or to normalizing constants (Foltz et al., 1998; Kintsch, 2001; Landauer and Dumais, 1997).
In the adjective-specific linear map (alm) model, proposed by Baroni and Zamparelli (2010), a different matrix B is learnt for each adjective. $$$$$ The simplified additive model produces a sort of (statistical) union of features, whereas component-wise multiplication has an intersective effect.
