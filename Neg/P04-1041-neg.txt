 $$$$$ In LFG, LDDs are resolved at the f-structure level, obviating the need for empty productions and tracesin trees (Dalrymple, 2001), using functional uncer tainty (FU) equations.
 $$$$$ F-structure anno tations allow us to distinguish passive and activeframes.
 $$$$$ : GFn to sub-f-structure h; retrieve local PRED:l; add GF:g to h iff ? GF is not present at h wh-less TOPIC-REL # wh-less TOPIC-REL # subj 5692 adjunct 1314 xcomp:adjunct 610 obj 364 xcomp:obj 291 xcomp:xcomp:adjunct 96 comp:subj 76 xcomp:subj 67 Table 5: Most frequent wh-less TOPIC-REL paths 02?21 23 23 /(02?21) TOPIC 26 7 2 FOCUS 13 4 0 TOPIC-REL 60 22 1 Table 6: Number of path types extracted?
 $$$$$ S S NP U.N. VP V signs NP treaty NP Det the N headline VP V said ? ?

For this experiment, we choose the C&C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ V ? signs ?PRED=sign??
For this experiment, we choose the C&C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ Unlike many other approaches, our ex traction process does not predefine frames, fully reflects LDDs in the source data-structures (cf.Figure 3), discriminates between active and passive frames, computes GF, GF:CFG category pair as well as CFG category-based subcategorisation frames and associates conditional probabilities with frames.
For this experiment, we choose the C&C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ erated by the f-structure annotation algorithm for the original Penn-II trees, in a CCG-style (Hockenmaier, 2003) evaluation experiment Pipeline Integrated PCFG P-PCFG A-PCFG PA-PCFG 2416 Section 23 trees.
For this experiment, we choose the C&C parser (Clark and Curran, 2003) for CCG, Enju parser (Miyao and Tsujii, 2008) for HPSG and pipeline automatic annotator (Cahill et al, 2004) with Charniak parser for LFG. $$$$$ V ? signs ?PRED=sign??

 $$$$$ Model 3, (Johnson, 2002), (Hocken maier, 2003) ), treebank-based probabilistic parsersreturn fairly simple ?surfacey?
 $$$$$ In the pipeline architecture a standard PCFG is extracted from the ?raw?
 $$$$$ In the pipeline architecture a standard PCFG is extracted from the ?raw?

The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual Lexical Functional Grammar (LFG) (Bresnan, 2001) resources from tree banks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al, 2004). $$$$$ We report on the evalu ation of GF-based frames for the full frames with complete prepositional and particle infomation.
The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual Lexical Functional Grammar (LFG) (Bresnan, 2001) resources from tree banks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al, 2004). $$$$$ Johnson?s (2002) work is closest toours in spirit.
The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual Lexical Functional Grammar (LFG) (Bresnan, 2001) resources from tree banks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al, 2004). $$$$$ This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
The research presented in this paper forms part of an ongoing effort to develop methods to induce wide-coverage multilingual Lexical Functional Grammar (LFG) (Bresnan, 2001) resources from tree banks by means of automatically associating LFG f-structure information with constituency trees produced by probabilistic parsers (Cahill et al, 2004). $$$$$ In our approach, LDDs are resolved in f-structure, not trees.

The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al, 2004) uses configurational, categorial, function tag and trace information. $$$$$ Few hand-crafted, deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank: (Riezler et al., 2002) show how a deep, carefully hand-craftedLFG is successfully scaled to parse the Penn-II treebank (Marcus et al, 1994) with discriminative (log linear) parameter estimation techniques.The last 20 years have seen continuously increas ing efforts in the construction of parse-annotated corpora.
The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al, 2004) uses configurational, categorial, function tag and trace information. $$$$$ h together with GF is locally complete and co herent with respect to a semantic form s for l rank resolution by P(s|l) ? P(p|t) The algorithm supports multiple, interacting TOPIC, TOPIC-REL and FOCUS LDDs.
The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al, 2004) uses configurational, categorial, function tag and trace information. $$$$$ grammars.
The f-structure annotation algorithm used for inducing LFG resources from the Penn-II treebank for English (Cahill et al, 2004) uses configurational, categorial, function tag and trace information. $$$$$ Using (Johnson, 2002)?s method of adding empty nodes to the parse-trees results in an f-score of 79.75%.(Hockenmaier, 2003) provides CCG-based models of LDDs.

A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II tree bank has been described by Cahill et al (2004). $$$$$ The Penn-II treebank employs CFG trees with addi tional ?functional?
A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II tree bank has been described by Cahill et al (2004). $$$$$ Figure 4: Shallow-Parser Output with UnresolvedLDD and Incomplete Argument Structure (cf.
A methodology for automatically obtaining LFG f-structures from trees output by probabilistic parsers trained on the Penn-II tree bank has been described by Cahill et al (2004). $$$$$ Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).

Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al (2004), which assigns grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. $$$$$ Including prepositions associated with the subcategorised OBLs and particles, this number goes up to 14348.
Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al (2004), which assigns grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. $$$$$ Unlike many other approaches, our ex traction process does not predefine frames, fully reflects LDDs in the source data-structures (cf.Figure 3), discriminates between active and passive frames, computes GF, GF:CFG category pair as well as CFG category-based subcategorisation frames and associates conditional probabilities with frames.
Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al (2004), which assigns grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. $$$$$ NP ? U.N V ? signs ?PRED=U.N. ?PRED=sign Figure 1: Simple LFG C- and F-Structure Uparrows point to the f-structure associated with the mother node, downarrows to that of the local node.The equations are collected with arrows instanti ated to unique tree node identifiers, and a constraint solver generates an f-structure.
Some properties of Spanish and the encoding of syntactic information in the Cast3LB treebank make it non-trivial to apply the method of automatically mapping c-structures to f-structures used by Cahill et al (2004), which assigns grammatical functions to tree nodes based on their phrasal category, the category of the mother node and their position relative to the local head. $$$$$ However, with few notable exceptions(e.g. Collins?

Cahill et al (2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups $$$$$ Threshold 1% Threshold 5% P R F-Score P R F-Score Exp. 73.7% 22.1% 34.0% 78.0% 18.3% 29.6% Table 4: COMLEX ComparisonWe further acquire finite approximations of FU equations.
Cahill et al (2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups $$$$$ Research on adequate probability models for unification grammars is important.
Cahill et al (2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups $$$$$ Given a set of semantic forms s with probabilities P(s|l) (where l is a lemma), a set of paths p withP(p|t) (where t is either TOPIC, TOPIC-REL or FO CUS) and an f-structure f , the core of the algorithm to resolve LDDs recursively traverses f to: find TOPIC|TOPIC-REL|FOCUS:g pair; retrieve TOPIC|TOPIC-REL|FOCUS paths; for each path p with GF1 : . . .
Cahill et al (2004), in their presentation of LFG parsing resources, distinguish 32 types of dependencies, divided into two major groups $$$$$ We achieve between77.68% and 80.24% against the PARC 700 follow ing the experiments in (Kaplan et al, 2004).

The translation and reference files are analyzed by a tree bank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al, 2004), which produces a set of dependency triples for each input. $$$$$ h together with GF is locally complete and co herent with respect to a semantic form s for l rank resolution by P(s|l) ? P(p|t) The algorithm supports multiple, interacting TOPIC, TOPIC-REL and FOCUS LDDs.
The translation and reference files are analyzed by a tree bank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al, 2004), which produces a set of dependency triples for each input. $$$$$ Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).
The translation and reference files are analyzed by a tree bank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al, 2004), which produces a set of dependency triples for each input. $$$$$ Nodes followed by annotations are treated as a monadic category for grammar extraction and parsing.
The translation and reference files are analyzed by a tree bank-based, probabilistic Lexical-Functional Grammar (LFG) parser (Cahill et al, 2004), which produces a set of dependency triples for each input. $$$$$ However, with few notable exceptions(e.g. Collins?

Cahill et al (2004) presents Penn-II Treebank based LFG parsing resources. $$$$$ VP ? V NP ?=?
Cahill et al (2004) presents Penn-II Treebank based LFG parsing resources. $$$$$ 2.
Cahill et al (2004) presents Penn-II Treebank based LFG parsing resources. $$$$$ Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).
Cahill et al (2004) presents Penn-II Treebank based LFG parsing resources. $$$$$ : GFn : GF, traverse f along GF1 : . . .

In this paper, we use the parser developed by Cahill et al (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates. $$$$$ Forms 10969 14348 Frame Types 38 577 Active Frame Types 38 548 Passive Frame Types 21 177 Table 2: Verb Results Semantic Form Occurrences Prob.
In this paper, we use the parser developed by Cahill et al (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates. $$$$$ This partitions local subtrees of depth one (corresponding to CFG rules) into left and rightcontexts (relative to head).
In this paper, we use the parser developed by Cahill et al (2004), which automatically annotates input text with c-structure trees and f-structure dependencies, reaching high precision and recall rates. $$$$$ This case, however, is surprisingly rare for our grammars: only 0.0018% (85 out of 48424) of theoriginal Penn-II trees (without FRAGs) fail to pro duce an f-structure due to inconsistent annotations(Table 1), and for parsing section 23 with the in tegrated model (A-PCFG), only 9 sentences do notreceive a parse because no f-structure can be gen erated for the highest ranked tree (0.4%).

We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al, 2004) automatically extracted from tree banks, maximising the probability of a tree given an f-structure. $$$$$ For ourexample in Figures 3 and 4, the relevant lexical en tries are: V ? said ?PRED=say??
We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al, 2004) automatically extracted from tree banks, maximising the probability of a tree given an f-structure. $$$$$ or ?deep?
We present a novel PCFG-based architecture for robust probabilistic generation based on wide-coverage LFG approximations (Cahill et al, 2004) automatically extracted from tree banks, maximising the probability of a tree given an f-structure. $$$$$ To account for the fronted sentential con stituents in Figures 3 and 4, an FU equation of the form ? TOPIC = ? COMP* COMP would be required.The equation states that the value of the TOPIC at tribute is token identical with the value of the finalCOMP argument along a path through the immedi ately enclosing f-structure along zero or more COMPattributes.

In this paper we present a novel PCFG-based architecture for probabilistic generation based onwide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from tree banks (Cahill et al, 2004). $$$$$ TOPIC [ SUBJ [ PRED U.N. ] PRED sign OBJ [ PRED treaty ] ] 1 SUBJ [ SPEC the PRED headline ] PRED say COMP 1 ? ?
In this paper we present a novel PCFG-based architecture for probabilistic generation based onwide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from tree banks (Cahill et al, 2004). $$$$$ ?OBJ=?
In this paper we present a novel PCFG-based architecture for probabilistic generation based onwide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from tree banks (Cahill et al, 2004). $$$$$ Many second gen eration treebanks provide a certain amount of deep syntactic or dependency information (e.g. in the form of Penn-II functional tags and traces) supporting the computation of representations ofdeep linguistic information.
In this paper we present a novel PCFG-based architecture for probabilistic generation based onwide-coverage, robust Lexical Functional Grammar (LFG) approximations automatically extracted from tree banks (Cahill et al, 2004). $$$$$ TOPIC [ SUBJ [ PRED U.N. ] PRED sign OBJ [ PRED treaty ] ] 1 SUBJ [ SPEC the PRED headline ] PRED say COMP 1 ? ?

Cahill et al (2004) present two parsing architectures $$$$$ This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
Cahill et al (2004) present two parsing architectures $$$$$ (or ?shallow?)

The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al (2004). $$$$$ C(onstituent)-structure represents the grouping of words and phrases into largerconstituents and is realised in terms of a CF PSG grammar.
The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al (2004). $$$$$ h together with GF is locally complete and co herent with respect to a semantic form s for l rank resolution by P(s|l) ? P(p|t) The algorithm supports multiple, interacting TOPIC, TOPIC-REL and FOCUS LDDs.
The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al (2004). $$$$$ grammars (Johnson, 2002), i.e. they do not resolve LDDs but interpret linguistic material purely locally where it oc curs in the tree.
The generation architecture presented here builds on the integrated parsing architecture resources of Cahill et al (2004). $$$$$ fstructures with LDDs unresolved resulting in in complete argument structures as in Figure 4.

 $$$$$ Fig ure 3)
 $$$$$ : GFn : GF, traverse f along GF1 : . . .
 $$$$$ For a substantial number of linguistic phenomena such as topicalisation, wh-movement in relative clausesand interrogative sentences, however, there is an important difference between the location of the (surface) realisation of linguistic material and the location where this material should be interpreted semantically.

 $$$$$ Research on adequate probability models for unification grammars is important.
 $$$$$ This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
 $$$$$ Currently our best automaticallyinduced grammars achieve 80.97% f-score for f structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU1051 and 80.24% against the PARC 700 Depen dency Bank (King et al, 2003), performing at the same or a slightly better level than state-of-the-art hand-crafted grammars (Kaplan et al, 2004).
 $$$$$ The resulting parse-trees are then annotated by the automatic f-structure annotation algorithm and resolved into f-structures.

This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al (2004) into probabilistic generation grammars. $$$$$ For the proper f-structure in Figure 3 we obtain sign([subj,obj]) and say([subj,comp]).
This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al (2004) into probabilistic generation grammars. $$$$$ The paper is structured as follows: we give a brief introduction to LFG.
This conditioning effectively turns the f-structure annotated PCFGs of Cahill et al (2004) into probabilistic generation grammars. $$$$$ The paper is structured as follows: we give a brief introduction to LFG.

Our back off uses the built-in lexical macros of the automatic f-structure annotation algorithm of Cahill et al (2004) to identify potential part-of-speech categories corresponding to a particular set of features. $$$$$ If the automatic f-structureannotation algorithm outlined in Section 3 generates high quality f-structures, reliable seman tic forms can be extracted (reverse-engineered): for each f-structure generated, for each level of embedding we determine the local PRED valueand collect the governable, i.e. subcategoris able grammatical functions present at that level of embedding.
Our back off uses the built-in lexical macros of the automatic f-structure annotation algorithm of Cahill et al (2004) to identify potential part-of-speech categories corresponding to a particular set of features. $$$$$ From the f-structure annotated treebank they automatically extract wide-coverage, robust, PCFG-based LFG approximations that parse new text into trees and f-structure representations.
Our back off uses the built-in lexical macros of the automatic f-structure annotation algorithm of Cahill et al (2004) to identify potential part-of-speech categories corresponding to a particular set of features. $$$$$ We use P(s|l) ? P(p|t) to rank a solution, depending on how likely the PRED takes semantic frame s, and how likely the TOPIC, FOCUS or TOPIC-REL is resolved using path p. The algorithm also supports resolution of LDDs where no overt linguistic material introducesa source TOPIC-REL function (e.g. in reduced rela tive clause constructions).
Our back off uses the built-in lexical macros of the automatic f-structure annotation algorithm of Cahill et al (2004) to identify potential part-of-speech categories corresponding to a particular set of features. $$$$$ Lexical information is provided via macros for POS tag classes.

The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004). $$$$$ This paper shows how finite approximations of long distance dependency (LDD) resolution can be obtained automatically for wide-coverage, robust, probabilistic Lexical-Functional Grammar (LFG) resources acquired from treebanks.
The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004). $$$$$ We extract LFG subcategorisation frames and paths linking LDD reentrancies fromf-structures generated automatically for the PennII treebank trees and use them in an LDD resolu tion algorithm to parse new text.
The feasibility of such post-parse deepening (for a statistical parser) is demonstrated by Cahill et al (2004). $$$$$ To account for the fronted sentential con stituents in Figures 3 and 4, an FU equation of the form ? TOPIC = ? COMP* COMP would be required.The equation states that the value of the TOPIC at tribute is token identical with the value of the finalCOMP argument along a path through the immedi ately enclosing f-structure along zero or more COMPattributes.
