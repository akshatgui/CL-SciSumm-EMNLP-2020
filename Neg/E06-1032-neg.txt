Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ Orejuela appeared calm as he was being led to the American plane that was to carry him to Miami in Florida.
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ Table 4 gives a comparison between the output of the system that was ranked 2nd by Bleu3 (top) and of the entry that was ranked 6th in Bleu but 1st in the human evaluation (bottom).
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements.
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.

Callison-Burch et al (2006) point out three prominent factors $$$$$ The brevity penalty is calculated as: where c is the length of the corpus of hypothesis translations, and r is the effective reference corpus length.1 Thus, the Bleu score is calculated as A Bleu score can range from 0 to 1, where higher scores indicate closer matches to the reference translations, and where a score of 1 is assigned to a hypothesis translation which exactly 1The effective reference corpus length is calculated as the sum of the single reference translation from each set which is closest to the hypothesis translation.
Callison-Burch et al (2006) point out three prominent factors $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.
Callison-Burch et al (2006) point out three prominent factors $$$$$ We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality.
Callison-Burch et al (2006) point out three prominent factors $$$$$ We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality.

This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match.
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ Reference: Iran had already announced Kharazi would boycott the conference after Jordan’s King Abdullah II accused Iran of meddling in Iraq’s affairs. necessarily be indicative of a genuine improvement in translation quality.

Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ We argue that Bleu is insufficient by showing that Bleu admits a huge amount of variation for identically scored hypotheses.
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another.
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.

Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is.
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003).
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.

Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ The problem is further exacerbated by Bleu not having any facilities for matching synonyms or lexical variants.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ Finally, we discuss appropriate uses for Bleu and suggest that for some research projects it may be preferable to use a focused, manual evaluation instead.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ However, there is a question as to whether minimizing the error rate with respect to Bleu does indeed guarantee genuine translation improvements.

This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). $$$$$ We have shown that Bleu’s rather coarse model of allowable variation in translation can mean that an improved Bleu score is not sufficient to reflect a genuine improvement in translation quality.
This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). $$$$$ The manual evaluation conducted for the NIST MT Eval is done by English speakers without reference to the original Arabic or Chinese documents.

This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ Inappropriate uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus.
This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ Last year’s evaluation exercise (Lee and Przybocki, 2005) was startling in that Bleu’s rankings of the ArabicEnglish translation systems failed to fully correspond to the manual evaluation.

It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.
It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ Should we give up on using Bleu entirely?
It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems.

The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ If the complete corpus consisted of this single sentence peared, as, being, calm, carry, escorted, he, him, in, led, plane, quite, seemed, take, that, the, to, to, to, was , was, which, while, will, would,,,.
The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.
The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ Table 3 gives the interpretations of the scores.
The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ We have further shown that it is not necessary to receive a higher Bleu score in order to be judged to have better translation quality by human subjects, as illustrated in the 2005 NIST Machine Translation Evaluation and our experiment manually evaluating Systran and SMT translations.

Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004).

We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006). $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.

 $$$$$ This is a theoretical point, and while the variants are artificially constructed, it does highlight the fact that Bleu is quite a crude measurement of translation quality.
 $$$$$ Should we give up on using Bleu entirely?
 $$$$$ Her future work section suggests that she has preliminary evidence that statistical machine translation systems receive a higher Bleu score than their non-n-gram-based counterparts.
 $$$$$ Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match.

 $$$$$ Banerjee and Lavie (2005) introduce the Meteor metric, which also incorporates recall on the unigram level and further provides facilities incorporating stemming, and WordNet synonyms as a more flexible match.
 $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
 $$$$$ We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004).

Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ We show that under some circumstances an improvement in Bleu is not sufficient to reflect a genuine improvement in translation quality, and in other circumstances that it is not necessary to improve Bleu in order to achieve a noticeable improvement in translation quality.
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ Should we give up on using Bleu entirely?
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ Two judges assigned each sentence in Table 4: Two hypothesis translations with similar Bleu scores but different human scores, and one of four reference translations the hypothesis translations a subjective 1–5 score along two axes: adequacy and fluency (LDC, 2005).
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.

For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ As such, the NIST MT Eval provides an excellent source of data that allows Bleu’s correlation with human judgments to be verified.
For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ Should we give up on using Bleu entirely?
For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ Inappropriate uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus.
For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ A number of projects in the past have looked into ways of extending and improving the Bleu metric.

Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future $$$$$ Her future work section suggests that she has preliminary evidence that statistical machine translation systems receive a higher Bleu score than their non-n-gram-based counterparts.
Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future $$$$$ Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does.
Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future $$$$$ Last year’s evaluation exercise (Lee and Przybocki, 2005) was startling in that Bleu’s rankings of the ArabicEnglish translation systems failed to fully correspond to the manual evaluation.

In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ Finally, we discuss appropriate uses for Bleu and suggest that for some research projects it may be preferable to use a focused, manual evaluation instead.
In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ Bleu attempts to capture allowable variation in word choice through the use of multiple reference translations (as proposed in Thompson (1991)).
In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003).
In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.

Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.

For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ For example, even though the bigram “to Miami” is repeated across all four reference translations in Table 1, it is counted only once in a hypothesis translation.
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.
