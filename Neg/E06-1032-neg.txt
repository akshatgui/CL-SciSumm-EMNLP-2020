Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ We show that for an average hypothesis translation there are millions of possible variants that would each receive a similar Bleu score.
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another.
Although both methods have gained mainstream acceptance and have shown good correlations with human judgments, their deficiencies have become more evident and serious as research in MT and summarization progresses (Callison-Burch et al, 2006). $$$$$ We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examples of Bleu vastly underestimating the translation quality of systems.

Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. $$$$$ We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. $$$$$ Finally, that the fact that Bleu’s correlation with human judgments has been drawn into question may warrant a re-examination of past work which failed to show improvements in Bleu.
Callison-Burch et al (2006) point out three prominent factors: Synonyms and paraphrases are only handled if they are in the set of multiple reference translations [available]; The scores for words are equally weighted so missing out on content-bearing material brings no additional penalty; The brevity penalty is a stop-gap measure to compensate for the fairly serious problem of not being able to calculate recall. $$$$$ If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is.

This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ We think that the advantages of Bleu are still very strong; automatic evaluation metrics are inexpensive, and do allow many tasks to be performed that would otherwise be impossible.
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ Workshops commonly compare systems using Bleu scores, often without confirming these rankings through manual evaluation.
This substitution technique has shown some improvement in translation quality (Callison-Burch et al, 2006). $$$$$ Conference papers routinely claim improvements in translation quality by reporting improved Bleu scores, while neglecting to show any actual example translations.

Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004).
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ This problem is made worse by the fact that Bleu equally weights all items in the reference sentences (Babych and Hartley, 2004).
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.
Nevertheless, it was later found that its correlation factor with subjective evaluations (the original reason for its success) is actually not so high as first thought (Callison-Burch et al, 2006). $$$$$ It is notable that one entry received a much higher human score than would be anticipated from its low Bleu score.

Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ In this paper we have shown theoretical and practical evidence that Bleu may not correlate with human judgment to the degree that it is currently believed to do.
Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. $$$$$ For example, work which failed to detect improvements in translation quality with the integration of word sense disambiguation (Carpuat and Wu, 2005), or work which attempted to integrate syntactic information but which failed to improve Bleu (Charniak et al., 2003; Och et al., 2004) may deserve a second look with a more targeted manual evaluation.

Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ The example is interesting because the number of matching n-grams for the two hypothesis translations is roughly similar but the human scores are quite different.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ This means that it is possible to have items which receive identical Bleu scores but are judged by humans to be worse.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ Finally, we discuss appropriate uses for Bleu and suggest that for some research projects it may be preferable to use a focused, manual evaluation instead.
Although Callison-Burch et al (2006) have recently called into question the utility of BLEU. $$$$$ Reference: Iran had already announced Kharazi would boycott the conference after Jordan’s King Abdullah II accused Iran of meddling in Iraq’s affairs. necessarily be indicative of a genuine improvement in translation quality.

This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). $$$$$ Here we denote bigram mismatches for the hypothesis translation given in Table 1 with vertical bars: Appeared calm  |when  |he was  |taken | to the American plane  |,  |which will | to Miami, Florida.
This is a phenomenon seen in MT, where BLEU seems to favour text that has been produced using a similar statistical n-gram language model over other symbolic models (Callison-Burch et al, 2006). $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.

This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ Machine translation evaluation metrics differ from other metrics that use a reference, like the word error rate metric that is used in speech recognition, because translations have a degree of variation in terms of word choice and in terms of variant ordering of some phrases.
This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ In order to overcome the problem of variation in phrase order, Bleu uses modified n-gram precision instead of WER’s more strict string edit distance.
This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.
This is a concern shared by all automatic evaluation metrics, and potential problems in stand-alone metrics have been analyzed (Callison-Burch et al, 2006). $$$$$ Inappropriate uses for Bleu include comparing systems which employ radically different strategies (especially comparing phrase-based statistical machine translation systems against systems that do not employ similar n-gram-based approaches), trying to detect improvements for aspects of translation that are not modeled well by Bleu, and monitoring improvements that occur infrequently within a test corpus.

It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ The manual evaluation conducted for the NIST MT Eval is done by English speakers without reference to the original Arabic or Chinese documents.
It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ The important thing therefore is to recognize which uses of Bleu are appropriate and which uses are not.
It has certain shortcomings for comparing different machine translation systems, especially if comparing conceptually different systems, e.g. phrase-based versus rule-based systems, as shown in (Callison-Burch et al, 2006). $$$$$ Finally, that the fact that Bleu’s correlation with human judgments has been drawn into question may warrant a re-examination of past work which failed to show improvements in Bleu.

The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu’s correlation with human judgments of quality.
The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al (2006), but the criticism is widespread. $$$$$ Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph.

Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ As the number of identically scored variants goes up, the likelihood that they would all be judged equally plausible goes down.
Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph.
Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
Callison-Burch et al (2006) report that BLEU and NIST favour n-gram-based MT models such as Pharaoh (Koehn, 2004), so the translations produced by rule-based systems score lower on the automatic evaluation, even though human judges consistently rate their output higher than Pharaoh's translation. $$$$$ We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems.

We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006). $$$$$ Table 2 shows the n-gram sets created from the reference translations.
We believe this is a satisfactory result, given the fairly good starting performance, and given that the BLEU metric is known not to be very sensitive to word order variations (Callison-Burchetal., 2006). $$$$$ The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003).

 $$$$$ Table 4 gives a comparison between the output of the system that was ranked 2nd by Bleu3 (top) and of the entry that was ranked 6th in Bleu but 1st in the human evaluation (bottom).
 $$$$$ In practice each pn is generally assigned an equal weight.
 $$$$$ If Bleu’s correlation with human judgments has been overestimated, then the field needs to ask itself whether it should continue to be driven by Bleu to the extent that it currently is.
 $$$$$ Coughlin (2003) performs a large-scale investigation of Bleu’s correlation with human judgments, and finds one example that fails to correlate.

 $$$$$ In this paper we give a number of counterexamples for Bleu’s correlation with human judgments.
 $$$$$ A number of prominent factors contribute to Bleu’s crudeness: Each of these failures contributes to an increased amount of inappropriately indistinguishable translations in the analysis presented above.
 $$$$$ We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
 $$$$$ The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005).

Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ This is a theoretical point, and while the variants are artificially constructed, it does highlight the fact that Bleu is quite a crude measurement of translation quality.
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ We then performed a manual evaluation where we had three judges assign fluency and adequacy ratings for the English translations of 300 French sentences for each of the three systems.
Callison-Burch et al (2006b) show that in general a higher BLEU score is not necessarily indicative of better translation quality. $$$$$ Reference: Iran had already announced Kharazi would boycott the conference after Jordan’s King Abdullah II accused Iran of meddling in Iraq’s affairs. necessarily be indicative of a genuine improvement in translation quality.

For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003).
For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ This begs the question as to whether this is only a theoretical concern or whether Bleu’s inadequacies can come into play in practice.
For example, Callison-Burch et al (2006) presented a number of counter-examples to the claim that BLEU agrees with human judgements. $$$$$ The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 2003).

Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). $$$$$ The NIST Machine Translation Evaluation exercise has run annually for the past five years as part of DARPA’s TIDES program.
Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). $$$$$ Bleu’s inability to distinguish between randomly generated variations in translation hints that it may not correlate with human judgments of translation quality in some cases.
Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). $$$$$ These comments do not apply solely to Bleu.
Of course, deciding between MT outputs in the general case is a problem that currently has no good solution, and is unlikely to in the near future: Bleu (and similar metrics) require one or more reference texts to distinguish between candidate outputs with the level of accuracy that they achieve, and even then they are open to substantial criticism (Callison-Burch et al, 2006). $$$$$ These comments do not apply solely to Bleu.

In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ We further illustrate that in practice a higher Bleu score is not necessarily indicative of better translation quality by giving two substantial examples of Bleu vastly underestimating the translation quality of systems.
In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ Lin and Hovy (2003) as well as Soricut and Brill (2004) present ways of extending the notion of ngram co-occurrence statistics over multiple references, such as those used in Bleu, to other natural language generation tasks such as summarization.
In spite of its shortcomings (Callison-Burch et al, 2006), it has been considered the standard automatic measure in the development of SMT systems (with new measures being added to it, but not substituting it, see for e.g. $$$$$ 2-grams: American plane, Florida., Miami,, Miami in, Orejuela appeared, Orejuela seemed, appeared calm, as he, being escorted, being led, calm as, calm while, carry him, escorted to, he was, him to, in Florida, led to, plane that, plane which, quite calm, seemed quite, take him, that was, that would, the American, the plane, to Miami, to carry, to the, was being, was led, was to, which will, while being, will take, would take, , Florida 3-grams: American plane that, American plane which, Miami , Florida, Miami in Florida, Orejuela appeared calm, Orejuela seemed quite, appeared calm as, appeared calm while, as he was, being escorted to, being led to, calm as he, calm while being, carry him to, escorted to the, he was being, he was led, him to Miami, in Florida., led to the, plane that was, plane that would, plane which will, quite calm as, seemed quite calm, take him to, that was to, that would take, the American plane, the plane that, to Miami,, to Miami in, to carry him, to the American, to the plane, was being led, was led to, was to carry, which will take, while being escorted, will take him, would take him, , Florida . then the modified precisions would be p1 = .83, p2 = .59, p3 = .31, and p4 = .2.

Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ Her future work section suggests that she has preliminary evidence that statistical machine translation systems receive a higher Bleu score than their non-n-gram-based counterparts.
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MT quality indicator. $$$$$ Meteor (Banerjee and Lavie, 2005), Precision and Recall (Melamed et al., 2003), and other such automatic metrics may also be affected to a greater or lesser degree because they are all quite rough measures of translation similarity, and have inexact models of allowable variation in translation.

For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ In the next section we give two significant examples that show that Bleu can indeed fail to correlate with human judgments in practice.
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742.
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ Reference: Iran had already announced Kharazi would boycott the conference after Jordan’s King Abdullah II accused Iran of meddling in Iraq’s affairs. necessarily be indicative of a genuine improvement in translation quality.
For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001). $$$$$ This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another.
