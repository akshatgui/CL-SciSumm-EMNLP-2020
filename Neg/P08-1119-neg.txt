(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. $$$$$ Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.
(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. $$$$$ We conducted experiments on four semantic classes and consistently achieved high accuracies.
(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. $$$$$ This approach produces an efficient bootstrapping process that performs reasonably well, but it cannot take advantage of productivity information because of the dynamic nature of the process.

We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to $$$$$ We will refer to the ability of a candidate to generate new instances as its productivity.
We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to $$$$$ Our best-performing algorithm, however, was the 2-step process that begins with an exhaustive search (reckless bootstrapping) and then ranks the candidates using the Outdegree scoring function, which represents productivity.

With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. $$$$$ We wanted the system to be as language-independent as possible, so we refrained from using any taggers or parsing tools.
With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. $$$$$ To address concerns about coverage, we embedded the search in a bootstrapping process.
With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. $$$$$ We conducted experiments on four semantic classes and consistently achieved high accuracies.

Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. $$$$$ Our initial hypothesis was that such a specific pattern would be able to maintain high precision because non-class members would be unlikely to cooccur with the pattern.
Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. $$$$$ A candidate is if it frequently leads to the discovery of other instances.

In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). $$$$$ The first step is expensive, however, because it exhaustively applies the pattern to the web until no more extractions are found.
In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). $$$$$ Second, we assign a score to each node in the graph using a scoring function that takes into account both the in-degree (popularity) and out-degree (productivity) of each node.
In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). $$$$$ We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern.

In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). $$$$$ Relying on surface-level patterns, however, is risky because incorrect items are frequently extracted due to polysemy, idiomatic expressions, parsing errors, etc.
In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). $$$$$ The main contribution of this work is a novel method for combining hyponym patterns with graph structures that capture two properties associated with pattern extraction: popularity and productivity.

Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ A candidate word is productive if it frequently leads to the discovery of other words.
Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.
Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ If more hyponym patterns are used, then this could get considerably more expensive, but the process could be easily parallelized to perform queries across a cluster of machines.
Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper.

One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances.
One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.
One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ To address concerns about coverage, we embedded the search in a bootstrapping process.
One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ For example, the word FORD could refer to an automobile or a person, but in the pattern “CARS such as FORD and *” it will almost certainly refer to an automobile.

(Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. $$$$$ We wanted the system to be as language-independent as possible, so we refrained from using any taggers or parsing tools.
(Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. $$$$$ We conducted experiments on four semantic classes and consistently achieved high accuracies.
(Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. $$$$$ Type 2 errors were caused by instances that formerly belonged to the semantic class (e.g., SerbiaMontenegro and Czechoslovakia are no longer countries).

To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). $$$$$ We run this bootstrapping process for a fixed number of iterations (search ply), or until no new class members are produced.
To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). $$$$$ Michi -gan).
To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). $$$$$ Other researchers have successfully used sets of hyponym patterns (e.g., (Hearst, 1992; Etzioni et al., 2005; Pas¸ca, 2004)), and multiple patterns could be used with our algorithms as well.

Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. $$$$$ Our work began with the simple idea of using an extremely specific pattern to extract semantic class members with high accuracy.
Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. $$$$$ The doubly-anchored nature of the pattern serves two purposes.
Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. $$$$$ Type 4 errors were caused by sentences that were just flat out wrong in their factual assertions.
Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. $$$$$ Our results consistently produced high accuracy and for the states and countries categories produced very high recall.

Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ The bootstrapping process begins by instantiating the doubly-anchored pattern with the seed class member, issuing a web query to generate new candidate instances, and adding these new instances to the graph.
Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user.
Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern.

To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. $$$$$ A candidate is if it frequently leads to the discovery of other instances.
To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. $$$$$ Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.
To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. $$$$$ Name variants and former class members may in fact be useful to have.

Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ If more hyponym patterns are used, then this could get considerably more expensive, but the process could be easily parallelized to perform queries across a cluster of machines.
Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ Incorporating a noun phrase chunker would eliminate some of these cases, but far from all of them.
Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ An NP chunker might prevent some of these cases, but we suspect that many of them would have been misparsed regardless.
Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ Incorporating additional hyponym patterns will almost certainly improve coverage, and could potentially improve the quality of the graphs as well.

The approach we describe here is most similar to that of Kozareva et al (2008). $$$$$ A candidate is if it frequently leads to the discovery of other instances.
The approach we describe here is most similar to that of Kozareva et al (2008). $$$$$ Knowing the semantic classes of words (e.g., “trout” is a kind of FISH) can be extremely valuable for many natural language processing tasks.

Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). $$$$$ For the U.S. states category, our system achieved 100% recall and 100% precision for the first 50 items generated, and KnowItAll performed similarly achieving 98% recall with 100% precision.
Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). $$$$$ Many of these approaches employ surface-level patterns to identify words and their associated semantic classes.
Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). $$$$$ For common noun classes, we extract just one word, if it is not capitalized.

Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. $$$$$ We conducted experiments on four semantic classes and consistently achieved high accuracies.
Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. $$$$$ The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes.

Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. $$$$$ The singers and fish categories are much larger, open classes.
Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. $$$$$ The candidates are then ranked based on properties of the HPLG.

Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ A candidate is if it frequently leads to the discovery of other instances.
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ However, all of our experiments were conducted using just a single hyponym pattern.
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ The KnowItAll system (Etzioni et al., 2005) achieved 97% precision with 58% recall, and 79% precision with 87% recall.5 To the best of our knowledge, other researchers have not reported results for the singer and fish categories.
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ Intuitively, we expect true class members to occur frequently in pattern contexts with other class members.

We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ Because the doubly-anchored pattern also requires a class member (e.g., “PRESIDENTS such as BILL GATES and *”), it is likely to generate only the desired types of instances.
We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes.
We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ We conducted experiments on four semantic classes and consistently achieved high accuracies.
We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ Pasca did not evaluate his system on states.
