(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. $$$$$ We will refer to these as hyponym patterns.
(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. $$$$$ Together, these two measures capture not only frequency of occurrence, but also cross-checking that the candidate occurs both near the class name and near other class members.
(Kozareva et al, 2008) proposed the use of a doubly-anchored hyponym pattern and a graph to represent the links between hyponym occurrences in these patterns. $$$$$ We display results for the top-ranked N candidates, for all instances that have a productivity value > zero.4 The Popularity columns show results for the bootstrapping algorithm described in Section 3.3, using three different scoring functions.

We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision. $$$$$ The three scoring metrics that use both popularity and productivity also performed well, but productivity information by itself seems to perform better in some cases.
We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision. $$$$$ We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances.
We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision. $$$$$ It can be difficult to compare the results of different semantic class learners because there is no standard set of benchmark categories, so researchers report results for different classes.
We are particularly interested in the usage of recursive patterns for the learning of semantic relations not only because it is a novel method, but also because recursive patterns of the DAP fashion are known to: (1) learn concepts with high precision compared to singly-anchored pat terns (Kozareva et al, 2008), (2) use only one seed instance for the discovery of new previously unknown terms, and (3) harvest knowledge with minimal supervision. $$$$$ It is worth noting that incorrect instances of Types 2 and 3 may not be problematic to encounter in a dictionary or ontology.

With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. $$$$$ We developed two algorithms that begin with just a class name and one seed instance and then automatically generate a ranked list of new class instances.
With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. $$$$$ A candidate is if it frequently leads to the discovery of other instances.
With the same goal, Kozareva et al (2008) apply similar textual patterns to the web. $$$$$ We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern.

Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. $$$$$ An NP chunker might prevent some of these cases, but we suspect that many of them would have been misparsed regardless.
Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. $$$$$ As a result, instances are extracted using only word boundaries and orthographic information.
Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. $$$$$ We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern.
Similarly, (Kozareva et al,2008) evaluated only a small number (a few hundreds) of harvested instances. $$$$$ Automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized categories or domains.

In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). $$$$$ We hypothesized that a doubly-anchored pattern, which includes both the class name and a class member, would achieve high accuracy because of its specificity.
In our experiments, we use the doubly-anchored lexico-syntactic patterns and bootstrapping algorithm introduced by (Kozareva et al., 2008) and (Hovy et al, 2009). $$$$$ For the states, countries, and singers categories, we ran experiments with 5 different initial seeds, which were randomly selected.

In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). $$$$$ A candidate is if it frequently leads to the discovery of other instances.
In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). $$$$$ Several techniques for semantic class induction have also been developed specifically for learning from the web.
In response, many automatic and semi-automatic methods of creating sets of named entities have been proposed, some are supervised (Zhou and Su, 2001), unsupervised (Pantel and Lin 2002, Nadeau et al 2006), and others semi-supervised (Kozareva et al 2008). $$$$$ A singly-anchored pattern (e.g., “PRESIDENTS such as *”) might generate lists of other types of presidents (e.g., country presidents, university presidents, etc.).

Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ With access to a cluster of ordinary PCs, this technique could be used to automatically create extremely large, high-quality semantic lexicons, for virtually any categories, without external training resources.
Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances.
Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ We present a novel approach to weakly supervised semantic class learning from the web, using a single powerful hyponym pattern combined with graph structures, which capture two properties associated with pattern-based Ina candidate is it was discovered many times by other instances in the pattern.
Approaches in the first category use lexical-syntactic formulation to define patterns, either manually (Kozareva et al, 2008) or automatically (Girju et al, 2006), and apply those patterns to mine instances of the patterns. $$$$$ We present two algorithms that use hyponym pattern linkage graphs (HPLGs) to represent popularity and productivity information.

One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ We run this bootstrapping process for a fixed number of iterations (search ply), or until no new class members are produced.
One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ In retrospect, we realized that we should have searched for “U.S. states” instead ofjust “states”.
One approach for taxonomy deduction is to use explicit expressions (Iwaska et al, 2000) or lexical and semantic patterns such as is a (Snow et al, 2004), similar usage (Kozareva et al, 2008), synonyms and antonyms (Lin et al, 2003), purpose (Cimiano and Wenderoth, 2007), and employed by (Bunescu and Mooney, 2007) to extract and organize terms. $$$$$ To operationalize this intuition, we create a hyponym pattern linkage graph, which represents the frequencies with which candidate instances generate each other in the pattern contexts.

(Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. $$$$$ Our popularity-based algorithm was very effective and is practical to use.
(Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. $$$$$ Our results consistently produced high accuracy and for the states and countries categories produced very high recall.
(Kozareva et al., 2008) introduced a bootstrapping scheme using the doubly-anchored pattern (DAP) that is guided through graph ranking. $$$$$ Pasca’s previously mentioned system (Pas¸ca, 2004) applies hyponym patterns to the web and acquires contexts around them.

To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). $$$$$ The second method is a two-step procedure that begins with an exhaustive pattern search that acquires popularity and productivity information about candidate instances.
To assess how well our algorithm compares with previous semantic class learning methods, we compared our results to those of (Kozareva et al, 2008). $$$$$ We will refer to these as hyponym patterns.

Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. $$$$$ The asterisk (*) indicates the location of the extracted words.
Consequently, we can compare the results produced by the first iteration of our algorithm (before intermediate concepts are learned) to those of (Kozareva et al, 2008) for the Animal and People categories, and then compare again after 10 bootstrapping iterations of intermediate concept learning. $$$$$ Our results consistently produced high accuracy and for the states and countries categories produced very high recall.

Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ Figure 2 shows the recall and precision curves for countries and states.
Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ Our expectation was that a very specific pattern would virtually eliminate the most common types of false hits that are caused by phenomena such as polysemy and idiomatic expressions.
Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ Figure 3 also includes a vertical line indicating where the candidate list was cut (at 180 instances) based on the zero productivity cutoff.
Bootstrapping with intermediate concepts produces nearly 5 times as many basic-level concepts and instances than (Kozareva et al, 2008) obtain, while maintaining similar levels of precision. $$$$$ For the fish category we ran each algorithm using just the seed “salmon”.

To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. $$$$$ This approach produces an efficient bootstrapping process that performs reasonably well, but it cannot take advantage of productivity information because of the dynamic nature of the process.
To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. $$$$$ A candidate is if it frequently leads to the discovery of other instances.
To group adjectives, we use a bootstrapping technique (Kozareva et al 2008) that learns which adjectives tend to co-occur, and groups these together to form an at tribute class. $$$$$ Incorporating additional hyponym patterns will almost certainly improve coverage, and could potentially improve the quality of the graphs as well.

Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003).
Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ We will refer to these as hyponym patterns.
Kozareva et al (2008) use a boot strapping approach that extends the fixed-pattern approach of Hearst (1992) in two intriguing ways. $$$$$ A candidate is if it frequently leads to the discovery of other instances.

The approach we describe here is most similar to that of Kozareva et al (2008). $$$$$ Type 5 errors were caused by broken expressions found in the retrieved snippets (e.g.
The approach we describe here is most similar to that of Kozareva et al (2008). $$$$$ For the U.S. states category, our system achieved 100% recall and 100% precision for the first 50 items generated, and KnowItAll performed similarly achieving 98% recall with 100% precision.
The approach we describe here is most similar to that of Kozareva et al (2008). $$$$$ However, all of our experiments were conducted using just a single hyponym pattern.

Kozareva et al (2008) test their approach on relatively simple and objective categories like states, countries (both closed sets), singers and fish (both open, the former more so than the latter), but not on complex categories in which members are tied both to a general category, like food, and to a stereotypical property, like sweet (Veale and Hao, 2007). $$$$$ However, such patterns tend to overgenerate (i.e., deliver incorrect results) and hence require additional filtering mechanisms.

Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. $$$$$ The Out-degree scoring function ranked the candidates well.
Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. $$$$$ A candidate is if it frequently leads to the discovery of other instances.
Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. $$$$$ The first step is expensive, however, because it exhaustively applies the pattern to the web until no more extractions are found.
Following Kozareva et al (2008), we can either indulge in reckless bootstrapping, which ignores the question of noise until all bootstrapping is finished, or we can apply a noise filter after each incremental step. $$$$$ The second method is a two-step procedure that begins with an exhaustive pattern search that acquires popularity and productivity information about candidate instances.

Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. $$$$$ For proper name classes, we extract all capitalized words that immediately follow the pattern.
Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. $$$$$ However, their graph is based entirely on syntactic relations between words, while our graph captures the ability of instances to find each other in a hyponym pattern based on web querying, without any part-ofspeech tagging or parsing.
Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. $$$$$ The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the web and then evaluates them further by computing mutual information scores based on web queries.
Kozareva et al (2008) and Navigli et al (2011) both develop systems that create taxonomies end to-end, i.e., discover the terms, their relations, and how these are hierarchically organized. $$$$$ For the states, countries, and singers categories, we ran experiments with 5 different initial seeds, which were randomly selected.

Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ Similarly, the class “PRESIDENT” could refer to country presidents or corporate presidents, and “BUSH” could refer to a plant or a person.
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ Name variants and former class members may in fact be useful to have.
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ Table 1 shows the results for 4 iterations of reckless bootstrapping for four semantic categories: U.S. states, countries, singers, and fish.
Similarly to Kozareva et al (2008) and Navigli et al (2011), our model operates over a graph whose nodes represent terms and edges their relationships. $$$$$ Our system does not use part-of-speech tagging or parsing, so the pattern itself is the only guide for finding an appropriate linguistic context.

We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ An NP chunker might prevent some of these cases, but we suspect that many of them would have been misparsed regardless.
We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ The Out-degree (outD) score for vertex v is the weighted sum of v’s outgoing edges, normalized by the number of other nodes in the graph.
We apply boot strapping (Kozareva et al 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). $$$$$ The singers and fish categories, which are much larger open classes, also achieved high accuracy and generated many instances, but the resulting lists are far from complete.
