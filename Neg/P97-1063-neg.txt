All translation models were induced using the method of Melamed (1997). $$$$$ Then, two word tokens (u, v) are said to co-occur in the
All translation models were induced using the method of Melamed (1997). $$$$$ Thanks also to Mike Collins, George Foster, Mitch Marcus, Lyle Ungar, and three anonymous reviewers for helpful comments.
All translation models were induced using the method of Melamed (1997). $$$$$ Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.
All translation models were induced using the method of Melamed (1997). $$$$$ The model uses two hidden parameters to estimate the confidence of its own predictions.

SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). $$$$$ Each application of the word-to-word model can choose its own balance between link token precision and recall.
SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). $$$$$ Let n(,,v) be the co-occurrence frequency of u and v and k(u,v) be the number of links between tokens of u and v3.
SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). $$$$$ Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.
SWAT considers only those translations c that has been linked with w based the Competitive Linking Algorithm (Melamed 1997) and logarithmic likelihood ratio (Dunning 1993). $$$$$ Thanks to Alexis Nasr for hand-evaluating the weather translation lexicon.

In its basic form the Competitive Linking algorithm (Melamed, 1997) allows for only up to one link per word. $$$$$ With the exception of (Fung, 1995b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1995a; Melamed, 1995).
In its basic form the Competitive Linking algorithm (Melamed, 1997) allows for only up to one link per word. $$$$$ A bitext comprises a pair of texts in two languages, where each text is a translation of the other.
In its basic form the Competitive Linking algorithm (Melamed, 1997) allows for only up to one link per word. $$$$$ The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories.

The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). $$$$$ Word co-occurrence can be defined in various ways.
The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). $$$$$ Word co-occurrence can be defined in various ways.
The selection order is similar to that in the competitive linking algorithm (Melamed, 1997). $$$$$ This imbalance foils thresholding strategies, clever as they might be (Gale & Church, 1991; Wu & Xia, 1994; Chen, 1996).

The first algorithm is similar to Competitive Linking (Melamed, 1997). $$$$$ If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.
The first algorithm is similar to Competitive Linking (Melamed, 1997). $$$$$ For these applications, we have designed a fast algorithm for estimating word-to-word models of translational equivalence.
The first algorithm is similar to Competitive Linking (Melamed, 1997). $$$$$ The purpose of the competitive linking algorithm is to help us re-estimate the model parameters.
The first algorithm is similar to Competitive Linking (Melamed, 1997). $$$$$ Unlike other translation models, the word-to-word model can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.

In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. $$$$$ The weather bitext was prepared at the University of Montreal, under the direction of Richard Kittredge.
In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. $$$$$ This research was supported by an equipment grant from Sun MicroSystems and by ARPA Contract #N66001-94C-6043.
In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. $$$$$ The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.
In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. $$$$$ In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel) in a non-monotonic fashion.

The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. $$$$$ One class represented content-word links and the other represented function-word links4.
The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. $$$$$ This research was supported by an equipment grant from Sun MicroSystems and by ARPA Contract #N66001-94C-6043.
The competitive linking algorithm (CLA) (Melamed, 1997) is a greedy word alignment algorithm. $$$$$ The most detailed evaluation of link tokens to date was performed by (Macklovitch & Hannan, 1996), who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards.

(Melamed,1997) has proposed the Competitive Linking Algorithm for linking the word pairs and a method which calculates the optimized correspondence level of the word pairs by hill climbing. $$$$$ The hidden parameters can be conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge.
(Melamed,1997) has proposed the Competitive Linking Algorithm for linking the word pairs and a method which calculates the optimized correspondence level of the word pairs by hill climbing. $$$$$ Similarly, the hidden parameters can be conditioned on the linked parts of speech.
(Melamed,1997) has proposed the Competitive Linking Algorithm for linking the word pairs and a method which calculates the optimized correspondence level of the word pairs by hill climbing. $$$$$ The confidence estimates enable direct control of the balance between the model's precision and recall via a simple threshold.
(Melamed,1997) has proposed the Competitive Linking Algorithm for linking the word pairs and a method which calculates the optimized correspondence level of the word pairs by hill climbing. $$$$$ With the exception of (Fung, 1995b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1995a; Melamed, 1995).

Every English word has either 0 or 1 alignments (Melamed, 1997). $$$$$ Similarly, the hidden parameters can be conditioned on the linked parts of speech.
Every English word has either 0 or 1 alignments (Melamed, 1997). $$$$$ By using the EM algorithm (Dempster et al., 1977), they can guarantee convergence towards the globally optimum parameter set.
Every English word has either 0 or 1 alignments (Melamed, 1997). $$$$$ The test sample contained only about 400 content words5, and the links for both models were evaluated post-hoc by only one evaluator.
Every English word has either 0 or 1 alignments (Melamed, 1997). $$$$$ Such a compact model requires relatively little computational effort to induce and to apply.

The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation. $$$$$ Word order can be taken into account by conditioning the hidden parameters on the relative positions of linked word tokens in their respective sentences.
The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation. $$$$$ With the exception of (Fung, 1995b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1995a; Melamed, 1995).
The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation. $$$$$ The model's precision/recall trade-off can be directly controlled via one threshold parameter.
The problem, which was described in (Melamed, 1997) in a word-to-word alignment context, is as follows: if e1 is the translation of f1 and f2 has a strong monolingual association with f1, e1 and f2 will also have a strong correlation. $$$$$ The confidence estimates enable direct control of the balance between the model's precision and recall via a simple threshold.

EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). $$$$$ Our translation model consists of the hidden parameters A+ and )c, and likelihood ratios L(u, v).
EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). $$$$$ Thanks also to Mike Collins, George Foster, Mitch Marcus, Lyle Ungar, and three anonymous reviewers for helpful comments.
EM is then performed by first discovering an initial phrasal alignments using a greedy algorithm similar to the competitive linking algorithm (Melamed, 1997). $$$$$ This feature makes the model more suitable for applications that are not fully statistical.

 $$$$$ Then, two word tokens (u, v) are said to co-occur in the
 $$$$$ For example, frequent words are translated less consistently than rare words (Melamed, 1997).
 $$$$$ Just as easily, we can model links that coincide with entries in a pre-existing translation lexicon separately from those that do not.

This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. $$$$$ Unlike other translation models, it can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. $$$$$ This feature makes the model more suitable for applications that are not fully statistical.
This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. $$$$$ The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest.
This can be seen as a generalization of the one-to-one assumption for word-to-word translation used by Melamed (1997a) and is exploited for the same purpose, i.e. to exclude large numbers of candidate alignments, when good initial alignments have been found. $$$$$ The weather bitext was prepared at the University of Montreal, under the direction of Richard Kittredge.

The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). $$$$$ The majority of indirect associations can be filtered out by a simple competition heuristic: Whenever several word tokens ui in one half of the bitext co-occur with a particular word token v in the other half of the bitext, the word that is most likely to be v's translation is the one for which the likelihood L(u, v) of translational equivalence is highest.
The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). $$$$$ If such units can be identified a priori, their translations can be estimated without modifying the word-to-word model.
The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). $$$$$ When the hidden parameters are conditioned on different link classes, the estimation method does not change; it is just repeated for each link class.
The basic algorithm combines the K-vec approach, described by Fung and Church (1993), with the greedy word-to-word algorithm of Melamed (1997a). $$$$$ Link types with negative log-likelihood were discarded after each iteration.

Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates. $$$$$ With the exception of (Fung, 1995b), previous methods for automatically constructing statistical translation models begin by looking at word cooccurrence frequencies in bitexts (Gale & Church, 1991; Kumano & Hirakawa, 1994; Fung, 1995a; Melamed, 1995).
Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates. $$$$$ The French/English software manuals were provided by Gary Adams of Sun MicroSystems Laboratories.
Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates. $$$$$ Another interesting extension is to broaden the definition of a &quot;word&quot; to include multi-word lexical units (Smadja, 1992).
Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus, we continue to integrate the statistical word-alignment techniques (Melamed, 1997) and dictionaries to find the translation candidates for each of the two collocates. $$$$$ Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of modeling the full range of translation phenomena.
