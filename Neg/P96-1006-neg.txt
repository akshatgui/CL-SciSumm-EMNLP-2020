We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. $$$$$ In addition, sense definitions are only available for root words in a dictionary.
We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. $$$$$ This approach integrates a diverse set of knowledge sources to disambiguate word sense.

Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%. $$$$$ Subsequently, in the test phase, LEXAS is given new, previously unseen sentences.
Likewise, (Ng and Lee, 1996) report overall accuracy for the noun interest of 87%, and find that that when their feature set only consists of co-occurrence features the accuracy only drops to 80%. $$$$$ This approach integrates a diverse set of knowledge sources to disambiguate word sense.

 $$$$$ In contrast, LEXAS learns from tagged sentences, without human engineering of complex rules.
 $$$$$ We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.
 $$$$$ LEXAS follows this convention by first converting each word in an input sentence into its morphological root using the morphological analyzer of WORD NET, before assigning the appropriate word sense to the root form.
 $$$$$ This approach integrates a diverse set of knowledge sources to disambiguate word sense.

 $$$$$ These sense tagged word occurrences consist of 191 most frequently occurring and most ambiguous nouns and verbs.
 $$$$$ When tested on this large data set, LEXAS performs better than the default strategy of picking the most frequent sense.
 $$$$$ Using similar conditional probability criteria for the selection of keywords, collocations that are predictive of a certain sense are selected to form the possible values for a collocation feature.

Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. $$$$$ In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994).
Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. $$$$$ This data set is almost two orders of magnitude larger in size than the above &quot;interest&quot; data set.
Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. $$$$$ To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving System), we tested it on a common data set involving the noun &quot;interest&quot; used by Bruce and Wiebe (Bruce and Wiebe, 1994).
Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. $$$$$ Making such an assumption is reasonable since POS taggers that can achieve accuracy of 96% are readily available to assign POS to unrestricted English sentences (Brill, 1992; Cutting et al., 1992).

The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. $$$$$ In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation.
The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. $$$$$ LEXAS performs WSD by first learning from a training corpus of sentences in which words have been pre-tagged with their correct senses.

Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). $$$$$ Unfortunately, in the data set made available in the public domain, there is no indication of which sentences are used as test sentences.
Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). $$$$$ In each run, we utilized only one knowledge source and compute the average classification accuracy and the standard deviation.
Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). $$$$$ For instance, given the sentence &quot;A reduction of principal and interest is one way the problem may be solved.&quot;, since the word &quot;interest&quot; appears as a noun in this sentence, LEXAS will only consider the noun senses of &quot;interest&quot; but not its verb senses.
Exemplar-based method makes use of typical contexts (exemplars) of a word sense, e.g., verb noun collocations or adjective-noun collocations, and identifies the correct sense of a word in a particular context by comparing the context with the exemplars (Ng and Lee, 1996). $$$$$ In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.

Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). $$$$$ These tokens are converted to lower case form before being considered as candidates for keywords.
Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). $$$$$ In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
Moreover, the effectiveness of this method on disambiguating words in large-scale corpora into fine-grained sense distinctions needs to be further investigated (Ng and Lee, 1996). $$$$$ In the training phase, LEXAS is given a set S of sentences in the training corpus in which sense-tagged occurrences of w appear.

Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). $$$$$ To our knowledge, very few of the existing work on WSD has been tested and compared on a common data set.
Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). $$$$$ The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET.
Hence, besides gathering examples from the widely usedSEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). $$$$$ Since WORDNET orders its senses such that sense 1 is the most frequent sense, one possibility is to always pick sense 1 as the best sense assignment.

Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. $$$$$ Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc.
Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. $$$$$ To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving System), we tested it on a common data set involving the noun &quot;interest&quot; used by Bruce and Wiebe (Bruce and Wiebe, 1994).
Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. $$$$$ Note that in Bruce and Wiebe's test run, the proportion of sentences in each sense in the test set is approximately equal to their proportion in the whole data set.

Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1. $$$$$ For example, in machine translation, knowing the correct word sense helps to select the appropriate target words to use in order to translate into a target language.
Our approach to memory-based all-words WSD follows the memory based approach of (Ng and Lee, 1996), and the work by (Veenstra et al, 2000) on a memory based approach to the English lexical sample task of SENSEVAL-1. $$$$$ This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.

The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. $$$$$ For our work, we use the sense definitions as given in WORDNET, which is comparable to a good desktop printed dictionary in its coverage and sense distinction.
The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. $$$$$ Table 2 shows the distribution of sense tags from the data set that we obtained.
The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. $$$$$ When tested on a large, separately collected data set, our program performs better than the default strategy of picking the most frequent sense.
The keywords were selected through a selection method suggested by (Ng and Lee, 1996) within three sentences around the ambiguous word; only content words were used as candidates. $$$$$ The processing speed of LEXAS is satisfactory.

In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). $$$$$ In this paper, we present a new approach for WSD using an exemplar-based learning algorithm.
In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). $$$$$ LEXAS assumes that each word in an input sentence has been pre-tagged with its correct POS, so that the possible senses to consider for a content word w are only those associated with the particular POS of w in the sentence.

This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. $$$$$ Improvement in the accuracy of identifying the correct word sense will result in better machine translation systems, information retrieval systems, etc.
This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. $$$$$ We use the following definition of distance between two symbolic values Vi and v2 of a feature f: feature f. 7/ is the total number of senses for a word w. This metric for measuring distance is adopted from (Cost and Salzberg, 1993), which in turn is adapted from the value difference metric of the earlier work of (Stanfill and Waltz, 1986).
This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of the-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3evaluations. $$$$$ In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm.

The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. $$$$$ To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined senses of WORDNET.
The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. $$$$$ Although the work of (Yarowsky, 1994) can be applied to WSD, the results reported in (Yarowsky, 1994) only dealt with accent restoration, which is a much simpler problem.
The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. $$$$$ The work of (McRoy, 1992) pointed out that a diverse set of knowledge sources are important to achieve WSD, but no quantitative evaluation was given on the relative importance of each knowledge source.
The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. $$$$$ The sense of a morphologically inflected content word is the sense of its uninflected form.

The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. $$$$$ POS of words are given in the data set, as well as the bracketings of noun groups.
The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. $$$$$ However, almost all existing work in WSD deals only with disambiguating content words too.
The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. $$$$$ Condition 1 ensures that a selected keyword is indicative of some sense i of w since cp(ijk) is at least some minimum probability M1.
The disambiguation is then done using k-NN (Ng and Lee, 1996) where the k nearest neighbors of the test sentence are identified using this scoring function. $$$$$ Local collocation knowledge yields the highest accuracy, followed by POS and morphological form.

The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). $$$$$ When tested on a common data set, our WSD program gives higher classification accuracy than previous work on WSD.
The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). $$$$$ The work of (Miller et al., 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET.
The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996). $$$$$ In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm.

We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ LEXAS assumes that each word in an input sentence has been pre-tagged with its correct POS, so that the possible senses to consider for a content word w are only those associated with the particular POS of w in the sentence.
We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ In this paper, we have presented a new approach for WSD using an exemplar based learning algorithm.
We report results of comparing our lexicon with theWordNet cousins as well as the inter-annotator disagreement observed between two semantically an notated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech (POS) of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.

To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ When tested on a large, separately collected data set, our program performs better than the default strategy of picking the most frequent sense.
To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ There is now a large body of past work on WSD.
To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ We tested our WSD program, named LEXAS, on both a common data set used in previous work, as well as on a large sense-tagged corpus that we separately constructed.
To test if the sense partitions in our lexicon constitute an appropriate (or useful) level of granularity, we applied it to the inter-annotator disagreement observed in two semantically annotated corpora: WordNet Semcor (Landes et al, 1998) and DSO (Ng and Lee, 1996). $$$$$ To our knowledge, this is the first time that a WSD program has been tested on such a large scale, and yielding results better than the most frequent heuristic on highly ambiguous words with the refined sense distinctions of WORDNET.

The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000). $$$$$ This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech (POS) of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation.
The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000). $$$$$ For our purpose, the term collocation does not imply idiomatic usage, just words that are frequently adjacent to the word to be disambiguated.
The set of features needed for the training of the system is described in figure 1, and is based on the feature selection made by Ng and Lee (1996) and Escudero et al (2000). $$$$$ In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm.
