In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. $$$$$ The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.
In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. $$$$$ In fact, on average, around 9.5% of the generated items were discarded without expertsâ€™ intervention13.
In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. $$$$$ To address these issues, in this paper we devise a cost-effective methodology to create cross-lingual textual entailment corpora.
In our first experiment we used the English-German portion of the CLTE corpus described in (Negri et al., 2011), consisting of 500 multi-directional entailment pairs which we equally divided into training and test sets. $$$$$ These include the decomposition of a complex content generation task in a pipeline of simpler subtasks accessible to a large crowd of non-experts, and the integration of quality control mechanisms at each stage of the process.

The corpora used in the experiments comes from a cross-lingual Textual Entailment dataset presented in (Negri et al, 2011), and provided by the task organizers. $$$$$ As a consequence, 43 pairs featuring wrong entailment annotations were encountered.
The corpora used in the experiments comes from a cross-lingual Textual Entailment dataset presented in (Negri et al, 2011), and provided by the task organizers. $$$$$ Despite the differences in the design of the tasks, all the released datasets were collected through similar procedures, always involving expensive manual work done by expert annotators.
The corpora used in the experiments comes from a cross-lingual Textual Entailment dataset presented in (Negri et al, 2011), and provided by the task organizers. $$$$$ This work has been partially supported by the ECfunded project CoSyne (FP7-ICT-4-24853).
The corpora used in the experiments comes from a cross-lingual Textual Entailment dataset presented in (Negri et al, 2011), and provided by the task organizers. $$$$$ cide which of two English sentences (the original ENG, and a modified ENG1) provides more information.

Defining "entailment" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011). $$$$$ Until now, however, the scarcity of such data on the one hand, and the costs of creating new datasets of reasonable size on the other, have represented a bottleneck for a steady advancement of the state of the art.
Defining "entailment" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011). $$$$$ There is an increasing need of annotated data to develop new solutions to the Textual Entailment problem, explore new entailment-related tasks, and set up experimental frameworks targeting real-world applications.
Defining "entailment" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011). $$$$$ Following the recent trends promoting annotation efforts that go beyond the established RTE Challenge framework (unidirectional entailment between monolingual T-H pairs), in this paper we addressed the multilingual dimension of the problem.
Defining "entailment" is quite difficult when dealing with expert annotators and still more with non-experts, as was noted by Negri et al. (2011). $$$$$ Considering that the most expensive phase in the creation of a TE dataset is the generation of the pairs, this is a significant achievement.

Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011). $$$$$ Most of the approaches to content generation proposed so far rely on post hoc verification to filter out undesired low-quality data (Mrozinski et al., 2008; Mihalcea and Strapparava, 2009; Wang and Callison-Burch, 2010).
Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011). $$$$$ Through manual verification of more than 50% of the corpus (900 pairs), a total number of 53 pairs (5.9%) were found incorrect.
Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011). $$$$$ However, adhering to the TE definition, co-referring expressions are equivalent, and their realization does not play any role in the entailment decision.
Afterwards, the creation of CLTE corpus by using Mechanical Turk is described on (Negri et al, 2011) and a corpus freely available for CLTE is published (Castillo, 2011). $$$$$ The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE,

The dataset provided by the organizers consists of 500 CLTE pairs translated to four languages following the crowdsourcing-based methodology proposed in (Negri et al., 2011). $$$$$ ENG1: New theories were rising, which announced a kind of veiled racism.
The dataset provided by the organizers consists of 500 CLTE pairs translated to four languages following the crowdsourcing-based methodology proposed in (Negri et al., 2011). $$$$$ However, at least two major differences with our work have to be remarked.
The dataset provided by the organizers consists of 500 CLTE pairs translated to four languages following the crowdsourcing-based methodology proposed in (Negri et al., 2011). $$$$$ Among the advantages of our method it is worth mentioning: i) the full alignment between the created corpora, ii) the possibility to easily extend the dataset to new languages, and iii) the feasibility of creating general-purpose corpora, featuring multi-directional entailment relations, that subsume the traditional RTE-like annotation.

The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps. $$$$$ Even considering the time needed for an expert to manage the pipeline (i.e. one week to prepare gold units, and to handle the I/O of each HIT), these figures show that our methodology provides a cheaper and faster way to collect entailment data in comparison with the RTE average costs reported in Section 1.
The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps. $$$$$ Differently, the entailment assessment phase appears to be more problematic, accounting for the majority of errors.
The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps. $$$$$ The retained paraphrases were paired with their corresponding original sentences, and sent to HIT-3 (Bidirectional Entailment) to be judged for semantic equivalence.
The dataset was created following the crowdsourcing methodology proposed in (Negri et al, 2011), which consists of the following steps. $$$$$ As described in Section 4.1, the resulting monolingual English TE corpus (ENG/ENG1) is used to create the following mono/cross-lingual TE corpora:

Two datasets were provided by the organization of SemEval 2012 (Negri et al, 2011): a training set and a test set, both composed by a set of 500 pairs of sentences. $$$$$ As a contribution beyond the few works on TE/CLTE data acquisition, we define an effective methodology that: i) does not involve experts in the most complex (and costly) stages of the process, ii) does not require preprocessing tools, and iii) does not rely on the availability of already annotated RTE corpora. to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services.
Two datasets were provided by the organization of SemEval 2012 (Negri et al, 2011): a training set and a test set, both composed by a set of 500 pairs of sentences. $$$$$ As a contribution beyond the few works on TE/CLTE data acquisition, we define an effective methodology that: i) does not involve experts in the most complex (and costly) stages of the process, ii) does not require preprocessing tools, and iii) does not rely on the availability of already annotated RTE corpora. to non-experts, difficult to accomplish, and not suitable for the application of the quality-check mechanisms provided by current crowdsourcing services.
Two datasets were provided by the organization of SemEval 2012 (Negri et al, 2011): a training set and a test set, both composed by a set of 500 pairs of sentences. $$$$$ Following the recent trends promoting annotation efforts that go beyond the established RTE Challenge framework (unidirectional entailment between monolingual T-H pairs), in this paper we addressed the multilingual dimension of the problem.

There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011). $$$$$ To this aim, we do not resort to already annotated data, nor languagespecific preprocessing tools.
There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011). $$$$$ The resulting dataset, obtained from a pipeline of different jobs routed to Amazon Mechanical Turk, contains more than 1,600 aligned pairs for each combination of texts-hypotheses in English, Italian and German.
There was one training set for each French-English, German-English, Italian-English, Spanish-English language combination (Negri et al, 2011). $$$$$ In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing the intervention of expert annotators?

The values of the parameters were chosen based on the CLTE development dataset (Negri et al, 2011) and were as follows. $$$$$ Second, their approach involves qualitative analysis of the collected data only a posteriori, after manual removal of invalid and trivial generated hypotheses.
The values of the parameters were chosen based on the CLTE development dataset (Negri et al, 2011) and were as follows. $$$$$ The task consists of deciding, given a text (T) and an hypothesis (H) in different languages, if the meaning of H can be inferred from the meaning of T. As in other NLP applications, both for monolingual and cross-lingual TE,
The values of the parameters were chosen based on the CLTE development dataset (Negri et al, 2011) and were as follows. $$$$$ Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and counter facts related to texts extracted from an existing RTE corpus annotated with named entities.
The values of the parameters were chosen based on the CLTE development dataset (Negri et al, 2011) and were as follows. $$$$$ In particular, we focus on the following problems: (1) Is it possible to collect T-H pairs minimizing the intervention of expert annotators?
