We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ Our hypothesis is that, when there is a paucity of initial seed data, coverage is a major obstacle that co-training can address.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ The likely reason for this dip is noise in the parse trees added by cotraining.

Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Finally, future work will also track comparative performance between the LTAG and Collins-CFG models.
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Here we report on system performance on unseen material (namely section 23 of the WSJ).
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.

 $$$$$ Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
 $$$$$ The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser.
 $$$$$ Self-training was used by Charniak (1997), where a modest gain was reported after re-training his parser on 30 million words.
 $$$$$ This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000).

 $$$$$ This experiment also gives us some insight into the differences between the two parsing models.
 $$$$$ We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.
 $$$$$ We therefore chose the following parsers:

This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ Thus, a bootstrapping method might improve performance of the LTAG statistical parser beyond the current state-of-the-art performance on the Treebank.
This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ L'A and LiB are the labelled training examples for A and B at step i. and assign scores to them according to their scoring functions JA and fB.
This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ The parser performance is boosted from 75% to 77.3%.
This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ Given 40,000 sentences of labelled data, we can obtain a projected value of how much performance can be improved with additional reliably labelled data.

(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000).
(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers' views do not fully satisfy the independence assumption.

The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.
The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption.
The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ This suggests that there is very little room for performance improvement for the Collins-CFG parser by simply adding more labelled data (using co-training or other bootstrapping methods or even manually).
The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ In this paper, we presented an experimental study in which a pair of statistical parsers were trained on labelled and unlabelled data using co-training Our results showed that simple heuristic methods for choosing which newly parsed sentences to add to the training data can be beneficial.

The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ This is because a parse tree is really a large collection of individual decisions, and retraining upon an entire tree means committing to all such decisions.
The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ We therefore chose the following parsers:
The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.

 $$$$$ That is, a subset of those sentences labelled by the LTAG parser is added to the training set of the Collins PCFG parser, and vice versa.
 $$$$$ In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
 $$$$$ Both scoring and selection phases are controlled by a simple incremental algorithm, which is detailed in section 3.2.

Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000).
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ Thus, a bootstrapping method might improve performance of the LTAG statistical parser beyond the current state-of-the-art performance on the Treebank.

These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ The upper curve in Figure 8 shows the outcome of this experiment.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ Co-training can be informally described in the following manner (Blum and Mitchell, 1998): Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ LTAG can provide a larger domain over which hi-lexical information is defined due to the arbitrary depth of the elementary trees it uses, and hence can provide novel lexical relationships for the Collins-CFG model, while the Collins-CFG model can paste together novel elementary trees for the LTAG model.

Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ A summary of the differences between the two models is given in Figure 2, which provides an informal argument for why the two parsers provide contrastive views for the co-training experiments.
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ Section 2 reviews co-training theory.
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ MA and ivriB are models of A and B at step i. U is a large pool of unlabelled sentences.
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.

Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ We therefore chose the following parsers:
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ Other possible selection methods include selecting examples that one parser scored highly and another parser scored lowly, and methods based on disagreements on the label between the two parsers.
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can improve an initial weak learner using unlabelled data.

In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ This is because a parse tree is really a large collection of individual decisions, and retraining upon an entire tree means committing to all such decisions.
In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.

It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing. $$$$$ We therefore chose the following parsers:
It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing. $$$$$ Blum and Mitchell prove that, when the two views are conditionally independent given the label, and each view is sufficient for learning the task, co-training can improve an initial weak learner using unlabelled data.
It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing. $$$$$ As is customary in the statistical parsing literature, we view all our previous experiments using section 0 of the Penn Treebank WSJ as contributing towards development.
It is not surprising that self-training is not normally effective: Charniak (1997) and Steedman et al (2003) report either minor improvements or significant damage from using self-training for parsing. $$$$$ Ui is a small cache holding subset of U at step i. L is the manually labelled seed data.

 $$$$$ In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.
 $$$$$ Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell).
 $$$$$ We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.

Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers' views do not fully satisfy the independence assumption.
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space.

Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ When training data is projected to a size of 400K manually created Treebank sentences, the performance of the Collins-CFG parser is projected to be 89.2% with an absolute upper bound of 89.3%.
Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ For reference, Figure 4 shows a similar curve for the LTAG parser.
Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ L'A and LiB are the labelled training examples for A and B at step i. and assign scores to them according to their scoring functions JA and fB.

 $$$$$ This final result is significant as it bears upon the general problem of having to build models when little or no labelled training material is available for some new domain.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.
 $$$$$ Section 2 reviews co-training theory.
 $$$$$ During the selection phase, we pick a subset of the newly labelled sentences to add to the training sets of both parsers.

Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ Abney also presents a greedy algorithm that maximises agreement on unlabelled data.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell).
