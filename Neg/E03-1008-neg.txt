We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ The difference in corpus domain does not hinder cotraining.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ Other possible scoring functions include a normalized version of fprob which does not penalize longer sentences, and a scoring function based on the entropy of the probability distribution over all parses returned by the parser.
We have explored using different settings for the seed set size (Steedman et al, 2003). $$$$$ Given 40,000 sentences of labelled data, we can obtain a projected value of how much performance can be improved with additional reliably labelled data.

Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Self-training experiments were conducted in which each parser was retrained on its own output.
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption.
Specifically for parsing and POS tagging, self training (Reichart and Rappoport, 2007), co-training (Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly supervised setup. $$$$$ Co-training performance may improve if we consider co-training using sub-parses.

 $$$$$ Next, a subset of the sentences newly labelled by one parser is added to the training data of the other parser, and vice versa.
 $$$$$ This projected value was obtained by fitting a curve to the observed convergence results using a least-squares method from MAT LAB.
 $$$$$ Abney also presents a greedy algorithm that maximises agreement on unlabelled data.
 $$$$$ Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers' views do not fully satisfy the independence assumption.

 $$$$$ In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data.
 $$$$$ We therefore chose the following parsers:
 $$$$$ In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.

This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ At each co-training iteration, a small set of sentences is drawn from a large pool of unlabelled sentences and stored in a cache.
This protocol and that of Steedman et al (2003a) were applied to the problem, with the same seed, self-training and test sets. $$$$$ We therefore chose the following parsers:

(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.
(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.
(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.
(Steedman et al, 2003a) used the first 500 sentences of WSJ training section as seed data. $$$$$ As is customary in the statistical parsing literature, we view all our previous experiments using section 0 of the Penn Treebank WSJ as contributing towards development.

The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ In contrast, this paper considers co-training two diverse statistical parsers: the Collins lexicalized PCFG parser and a Lexicalized Tree Adjoining Grammar (LTAG) parser.
The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ However, the results are not as dramatic as those reported in other co-training papers, such as Blum and Mitchell (1998) for web-page classification and Collins and Singer (1999) for namedentity recognition.
The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ As is customary in the statistical parsing literature, we view all our previous experiments using section 0 of the Penn Treebank WSJ as contributing towards development.
The self-training protocol of (Steedman et al, 2003a) does not actually improve over the baseline of using only the seed data. $$$$$ Section 3 considers how co-training applied to training statistical parsers can be made computationally viable.

The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ Section 3 considers how co-training applied to training statistical parsers can be made computationally viable.
The only previous work that adapts a parser trained on a small dataset between domains is that of (Steedman et al, 2003a), which used co-training (no self-training results were reported there or elsewhere). $$$$$ We therefore chose the following parsers:

 $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.
 $$$$$ Abney also presents a greedy algorithm that maximises agreement on unlabelled data.

Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space.
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ This is because a parse tree is really a large collection of individual decisions, and retraining upon an entire tree means committing to all such decisions.
Steedman et al (2003) apply co-training to parser adaptation and find that co-training can work across domains. $$$$$ We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.

These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ However, the results are not as dramatic as those reported in other co-training papers, such as Blum and Mitchell (1998) for web-page classification and Collins and Singer (1999) for namedentity recognition.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ As is customary in the statistical parsing literature, we view all our previous experiments using section 0 of the Penn Treebank WSJ as contributing towards development.
These parser output trees can by produced by a second parser in a co-training scenario (Steedman et al, 2003), or by the same parser with a re ranking component in a type of self training scenario (McClosky et al, 2006). $$$$$ Our ongoing work is addressing this point, largely in terms of re-ranked parsers.

Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ The various experiments are summarised in Table 1.
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ We therefore chose the following parsers:
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ This final result is significant as it bears upon the general problem of having to build models when little or no labelled training material is available for some new domain.
Self training is the process of training a parser on its own output, and earlier self-training experiments using generative statistical parsers did not yield encouraging results (Steedman et al, 2003). $$$$$ The domains over which the two models operate are quite distinct.

Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers' views do not fully satisfy the independence assumption.
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ We therefore chose the following parsers:
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ Other possible selection methods include selecting examples that one parser scored highly and another parser scored lowly, and methods based on disagreements on the label between the two parsers.
Steedman et al (2003) as well as Reichart and Rappoport (2007) examine self-training for PCFG parsing in the small seed case (< 1k labeled data), with different results. $$$$$ These methods build on the idea that the newly labelled data should not only be reliably labelled by the teacher, but also be as useful as possible for the student.

In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ Finally, future work will also track comparative performance between the LTAG and Collins-CFG models.
In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ This final result is significant as it bears upon the general problem of having to build models when little or no labelled training material is available for some new domain.
In the iterative setting, we follow Steedman et al (2003) and parse 30 sentences from which 20 are selected in every iteration. $$$$$ Of course there is still the question of whether the two parsers really are independent enough for effective co-training to be possible; in the results section we show that the Collins-CFG parser is able to learn useful information from the output of the LTAG parser.

It is not surprising that self-training is not normally effective $$$$$ Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell).
It is not surprising that self-training is not normally effective $$$$$ We therefore chose the following parsers:
It is not surprising that self-training is not normally effective $$$$$ The training data of the LTAG parser was augmented in the same way, using the 20 highest scoring parses from the set of 30, but using the Collins-CFG parser to label the sentences and provide the joint probability for scoring.

 $$$$$ Self-training provides a useful comparison with co-training because any difference in the results indicates how much the parsers are benefiting from being trained on the output of another parser.
 $$$$$ Co-training can be informally described in the following manner (Blum and Mitchell, 1998): Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other.
 $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.

Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ The graph shows a rapid growth in accuracy which tails off as increasing amounts of training data are added.
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ Our two different sources were the parsed section of the Brown corpus and the Penn Treebank WSJ.
Bootstrapping was applied to syntax learning by Steedman et al (2003). $$$$$ This work has been supported, in part, by the NSF/DARPA funded 2002 Language Engineering Workshop at Johns Hopkins University.

Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ In order to conduct co-training experiments between statistical parsers, it was necessary to choose two parsers that generate comparable output but use different statistical models.
Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ We therefore chose the following parsers:
Steedman et al (2003) reported some degradation using a lexicalized tree adjoining grammar parser and minor improvement using Collins lexicalized PCFG parser; however, this gain was obtained only when the parser was trained on a small labeled set. $$$$$ Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.

 $$$$$ We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
 $$$$$ Co-training performance may improve if we consider co-training using sub-parses.
 $$$$$ Both parsers then attempt to parse every sentence in the cache.

Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ We would like to thank Michael Collins, Andrew McCallum, and Fernando Pereira for helpful discussions, and the reviewers for their comments on this paper.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ The results show a modest improvement under each co-training scenario, indicating that, for the Collins-CFG parser, there is useful information to be had from the output of the LTAG parser.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.
Steedman et al (2003b) bootstrap two parsers that use different statistical models via co-training. $$$$$ However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space.
