For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. $$$$$ Indeed, showing all the nouns in the numbered categories would take up too much space: they average about 70 nouns apiece.
For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. $$$$$ This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.
For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. $$$$$ A note worth adding: it is not clear that the &quot;exact match&quot; criterion — that is, evaluating algorithms by the percentage of exact matches of sense selection against a human-judged baseline — is the right task.
For instance, work on word sense disambiguation i corpora (e.g. Resnik 1995), could lead to an estimate of frequencies for word senses in general, with rule-derived senses simply being a special case. $$$$$ The previous section provided illustrative examples, demonstrating the performance of the algorithm on some interesting cases.

An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and ad verbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. $$$$$ As an upper bound, Judge 1 was correct on 68.6% of those test instances.
An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and ad verbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. $$$$$ Indeed, showing all the nouns in the numbered categories would take up too much space: they average about 70 nouns apiece.
An adaptation of Lesk dictionary-based WSD algorithm has been used to disambiguate adjectives and ad verbs (Banerjee and Pedersen, 2002), an adaptation of the Resnik algorithm has been used to disambiguate nouns (Resnik, 1995), while the algorithm we developed for disambiguating verbs exploits the nouns in the context of the verb as well as the nouns both in the glosses and in the phrases that WordNet utilizes to describe the usage of a verb. $$$$$ Notice that by equation (1), support [i, k] is a sum of log probabilities, and therefore preferring senses with high support is equivalent to optimizing a product of probabilities.

The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. $$$$$ The disambiguation algorithm for noun groups is inspired by the observation that when two polysemous words are similar, their most informative subsumer provides information about which sense of each word is the relevant one.
The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. $$$$$ The concept c that maximizes the expression in (1) will be referred to as the most informative subsumer of wi and w2.
The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. $$$$$ For example, Hearst and Schtitze (1993) take steps toward a distributional treatment of WordNet-based classes, using Schtitze's (1993) approach to constructing vector representations from a large co-occurrence matrix.
The procedure is obtained by making some variations to the algorithm designed by Resnik (1995) for disambiguating noun groups. $$$$$ The word stray probably should be excluded also, since it most likely appears on this list as an adjective (as in &quot;stray bullet&quot;).

JIGSAW nouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. $$$$$ As in the above-cited work, there is no presupposition that sense-annotated text is available.
JIGSAW nouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. $$$$$ Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional word clustering techniques improve.
JIGSAW nouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. $$$$$ In this section, I present experimental results using a more rigorous evaluation methodology.
JIGSAW nouns differs from the original algorithm by Resnik (1995) in the similarity measure used to compute relatedness of two senses. $$$$$ The difficulty with the latter kind of knowledge is that, until now, the widespread success in characterizing lexical behavior in terms of distributional relationships has applied at the level of words — indeed, word forms — as opposed to senses.

Although the assessment of semantic similarity using a dictionary database as knowledge source has been recognized as providing significant cues for word clustering (Resnik 1995b) and the determination of lexical cohesion (Morris& amp; Hirst, 1991), its relevance for word disambiguation in running text remains relatively unexplored. $$$$$ As an upper bound, Judge 1 was correct on 68.6% of those test instances.
Although the assessment of semantic similarity using a dictionary database as knowledge source has been recognized as providing significant cues for word clustering (Resnik 1995b) and the determination of lexical cohesion (Morris& amp; Hirst, 1991), its relevance for word disambiguation in running text remains relatively unexplored. $$$$$ The word stray probably should be excluded also, since it most likely appears on this list as an adjective (as in &quot;stray bullet&quot;).
Although the assessment of semantic similarity using a dictionary database as knowledge source has been recognized as providing significant cues for word clustering (Resnik 1995b) and the determination of lexical cohesion (Morris& amp; Hirst, 1991), its relevance for word disambiguation in running text remains relatively unexplored. $$$$$ In this case, words rob and robbing were excluded because they were not nouns in WordNet.

Resnik (1995a) defines the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. $$$$$ Machine-generated thesaurus entry (Grefenstette, 1994): method, test, mean, procedure, technique I chose this grouping at random from a thesaurus created automatically by Grefenstette's syntacticodistributional methods, using the MED corpus of medical abstracts as its source.
Resnik (1995a) defines the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. $$$$$ Two human judges were independently given the test cases to disambiguate.
Resnik (1995a) defines the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. $$$$$ There is a tradition in sense disambiguation of taking particularly ambiguous words and evaluating a system's performance on those words.
Resnik (1995a) defines the semantic similarity between two words as the entropy value of the most informative concept subsuming the two words in a hierarchically structured thesaurus. $$$$$ This would be the case in query expansion for information retrieval, for example, where indiscriminately adding inappropriate words to a query can degrade performance (Voorhees, 1994).

At present, we are trying to integrate the word sense disambiguation method proposed in (Resnik, 1995) into our system. $$$$$ In this case, words rob and robbing were excluded because they were not nouns in WordNet.
At present, we are trying to integrate the word sense disambiguation method proposed in (Resnik, 1995) into our system. $$$$$ For Judge 1, there were 99 test instances with sufficiently high confidence to be considered.
At present, we are trying to integrate the word sense disambiguation method proposed in (Resnik, 1995) into our system. $$$$$ Machine-generated thesaurus entry (Grefenstette, 1994): method, test, mean, procedure, technique I chose this grouping at random from a thesaurus created automatically by Grefenstette's syntacticodistributional methods, using the MED corpus of medical abstracts as its source.

Semantic tags are assigned from on-line thesaura like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992). $$$$$ As a baseline, ten runs were done selecting senses by random choice, with the average percent correct being 33.3%, standard deviation 3.83.
Semantic tags are assigned from on-line thesaura like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992). $$$$$ Machine-generated thesaurus entry (Grefenstette, 1994): method, test, mean, procedure, technique I chose this grouping at random from a thesaurus created automatically by Grefenstette's syntacticodistributional methods, using the MED corpus of medical abstracts as its source.
Semantic tags are assigned from on-line thesaura like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992). $$$$$ As an upper bound, Judge 1 was correct on 68.6% of those test instances.
Semantic tags are assigned from on-line thesaura like WordNet (Basili et al 1996) (Resnik, 1995), Roget's categories (Yarowsky 1992) (Chen and Chen, 1996), the Japanese BGH (Utsuro et al 1993), or assigned manually (Basili et al 1992). $$$$$ This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.

The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng& amp; Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et ai. $$$$$ This paper begins from a rather different starting point.
The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng& amp; Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et ai. $$$$$ Immediate plans include a larger scale version of the experiment presented here, involving thesaurus classes, as well as a similarly designed evaluation of how the algorithm fares when presented with noun groups produced by distributional clustering.
The verbs are tagged with respect to senses in WordNet (Miller 1990), which has become widely used, for example in corpus-annotation projects (Miller et al 1994, Ng& amp; Hian 1996, and Grishman et al 1994) and for performing disambiguation (Resnik 1995 and Leacock et ai. $$$$$ Distributional cluster (Brown et al., 1992): head, body, hands, eye, voice, arm, seat, hair, mouth As noted in Section 2.1, this group represents a set of words similar to burglar, according to Schtitze's method for deriving vector representation from corpus behavior.

Finally, disambiguating the direct object according to WordNet categories, e.g., Resnik (1995), would improve the accuracy of using these categories to disambiguate verbs. $$$$$ One obvious solution to this problem would be to extend distributional grouping methods to word senses.
Finally, disambiguating the direct object according to WordNet categories, e.g., Resnik (1995), would improve the accuracy of using these categories to disambiguate verbs. $$$$$ Results are presented here individually by judge.
Finally, disambiguating the direct object according to WordNet categories, e.g., Resnik (1995), would improve the accuracy of using these categories to disambiguate verbs. $$$$$ As an upper bound, Judge 1 was correct on 68.6% of those test instances.
Finally, disambiguating the direct object according to WordNet categories, e.g., Resnik (1995), would improve the accuracy of using these categories to disambiguate verbs. $$$$$ This paper presents a method for automatic sense disambiguation of nouns appearing within sets of related nouns — the kind of data one finds in on-line thesauri, or as the output of distributional clustering algorithms.

There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). $$$$$ Note that by equations (1) through (3), if two senses have the virtual root node as their only upper bound then their similarity value is 0.
There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). $$$$$ Qualitatively, the algorithm does a good job in most of the categories.
There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). $$$$$ Example.

Eventually, the sense supported by those patterns which are semantically closer to the context in question is selected as the most likely one (see, among others, [Dolan, 1994], [Resnik, 1995a, 1995b], [Agirre and Rigau, 1996], [Sanfilippo, 1997]). $$$$$ The working hypothesis in this paper is that this holds true in general.
Eventually, the sense supported by those patterns which are semantically closer to the context in question is selected as the most likely one (see, among others, [Dolan, 1994], [Resnik, 1995a, 1995b], [Agirre and Rigau, 1996], [Sanfilippo, 1997]). $$$$$ Distributional cluster (Brown et al., 1992): head, body, hands, eye, voice, arm, seat, hair, mouth As noted in Section 2.1, this group represents a set of words similar to burglar, according to Schtitze's method for deriving vector representation from corpus behavior.
Eventually, the sense supported by those patterns which are semantically closer to the context in question is selected as the most likely one (see, among others, [Dolan, 1994], [Resnik, 1995a, 1995b], [Agirre and Rigau, 1996], [Sanfilippo, 1997]). $$$$$ Note that by equations (1) through (3), if two senses have the virtual root node as their only upper bound then their similarity value is 0.

Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)). $$$$$ This paper begins from a rather different starting point.
Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)). $$$$$ Regardless of the criterion for success, the algorithm does need further evaluation.
Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)). $$$$$ I assume throughout this paper that finer-grained distinctions than that are necessary.
Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al, 1994), (Resnik, 1995)). $$$$$ , /D}, with each word wz having an associated set Si = {si,i, , of possible senses.
