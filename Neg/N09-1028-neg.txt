Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ Other than the reordering phenomena covered by our rules in Table 1, there could be still some local or long distance reordering.
Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ First of all, we study a wide range of SOV languages using manually extracted precedence rules, not just for one language like in these studies.
Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.

It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). $$$$$ The results for both hierarchical systems and those combined with the precedence reordering are shown in Table 4, together with the best normal phrase-based systems we copy from Table 3.
It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). $$$$$ Then, we show experimental results of applying this approach to phrasebased SMT systems for translating from English to five SOV languages (Korean, Japanese, Hindi, Urdu and Turkish).

In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ Most recently, a hierarchical phrase reordering model (Galley and Manning, 2008) was proposed to dynamically determine phrase boundaries using efficient shift-reduce parsing.
In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ We use English as a representative of SVO languages and Korean as a representative for SOV languages in our discussion about the word orders.
In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ In Korean, when a noun is modified by a prepositional phrase, such as in “the way to happiness”, the prepositional phrase is usually moved in front of the noun, resulting in “q (happiness) ° 71at= 7J (to the way)” .

Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ The maximum entropy models are trained on all events extracted from training data word alignments using the LBFGS algorithm (Malouf, 2002).
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ After that, we see that the child node rooted at “know” matches a verb rule, with five children nodes labeled as (mark, nsubj, aux, neg, ccomp), with weights (0, 0, -2, -2, 0).
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ This motivates us to use a dependency parser for English to perform the reordering.
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ We also point out that hierarchical phrase-based systems require a chart parsing algorithm during decoding.

Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ Some preliminary error analysis already show that indeed some sentences suffer from parser errors.
Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ The resulting sentence “John the ball can hit .” will only need monotonic translation.
Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ In the future, we plan to investigate along this direction and extend the rules to languages other than SOV.
Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ There has been some work on syntactic word order model for English to Japanese machine translation (Chang and Toutanova, 2007).

Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ For preposition head node with an object modifier, € %1901 9 ZL tig N 4 °1844 . the order is the object first and the preposition last.
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ The decoder needs to make a jump of 13 words in order to put the translation of “is used” at the end of the translation.
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ Except for Hindi, applying the precedence reordering rules in a hierarchical system can achieve statistically significant improvements over a normal hierarchical system.
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ Note that all of the children are optional.

Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. $$$$$ They build a log-linear model using these features and apply the model to re-rank N-best lists from a baseline decoder.
Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. $$$$$ Several approaches use syntactical analysis to provide multiple source sentence reordering options through word lattices (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).
Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. $$$$$ We handle these types of reordering by the noun and preposition precedence rules in the third and fourth panel of Table 1.

Reversing the children order (Xu et al, 2009) reconnects is and popular. $$$$$ For all 5 languages, we achieve statistically significant improvements in BLEU scores over a state-of-the-art phrasebased baseline system.
Reversing the children order (Xu et al, 2009) reconnects is and popular. $$$$$ Syntactical analysis of source language can be used to deterministically reorder input sentences (Xia and McCord, 2004; Collins et.al., 2005; Wang et.al., 2007; Habash, 2007), or to provide multiple orderings as weighted options (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).
Reversing the children order (Xu et al, 2009) reconnects is and popular. $$$$$ First of all, we study a wide range of SOV languages using manually extracted precedence rules, not just for one language like in these studies.

We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ Therefore, it is very difficult in general to translate English into Korean with proper word order.
We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ This motivates us to use a dependency parser for English to perform the reordering.
We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ Therefore, in our adjective precedence rule in the second panel of Table 1, we group the auxiliary verb, the passive auxiliary verb and the negation and move them together after reversing their order.

(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ We successfully applied this approach to systems translating English to 5 SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.
(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ Since we only analyzed English and Korean sentences, it is possible that our rules are more geared toward Korean.
(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ Compared to these approaches, our work has a few differences.

Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ With the rules defined in Table 1, we now show a more complex example in Figure 4.
Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ In the recent years, several studies have tried to address this issue by using a word lattice instead of one reordering as input (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).
Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ Therefore, in our adjective precedence rule in the second panel of Table 1, we group the auxiliary verb, the passive auxiliary verb and the negation and move them together after reversing their order.

Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ Fourth, we use dependency parse trees rather than constituency trees.
Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ Finally, researchers have also tried to put source language syntax into reordering in machine translation.
Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ Note that all of the children are optional.

Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ This verb group is moved to the end of the sentence.
Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ However, unlike Habash, 2007, our manually written rules handle unseen children and their order naturally because we have a default precedence weight and order type, and we do not need to match an often too specific condition, but rather just treat all children independently.
Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ Therefore, using a hierarchical phrase-based system can improve those cases.

The work by Xu et al (2009) is the closest to our approach. $$$$$ Therefore, translating between SVO and SOV languages is a very important area to study.
The work by Xu et al (2009) is the closest to our approach. $$$$$ Distance based distortion model (Och, 2002; Koehn et.al., 2003) is a simple way of modeling phrase level reordering.
The work by Xu et al (2009) is the closest to our approach. $$$$$ The preprocessing reordering like ours is known to be sensitive to parser errors.
The work by Xu et al (2009) is the closest to our approach. $$$$$ In the recent years, several studies have tried to address this issue by using a word lattice instead of one reordering as input (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).

Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ In this study, we use manually created rules only.
Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ After reordering, the sentence becomes: “because we what the future has know n’t do Living exciting is .”.
Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ They also focus on either Chinese to English (Zhang et.al., 2007; Li et.al., 2007) or English to Danish (Elming, 2008), which arguably have less long distance reordering than between English and SOV languages.
Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ Some preliminary error analysis already show that indeed some sentences suffer from parser errors.

We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ The preprocessing reordering like ours is known to be sensitive to parser errors.
We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ When training a system for English to any of the 5 SOV languages, the word alignment step includes 3 iterations of IBM Model-1 training and 2 iterations of HMM training.
We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ Similar to the distance based reordering models, the syntactical or hierarchical approaches also rely on other models to get word alignments.

On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish. $$$$$ In linguistics, it is possible to define a basic word order in terms of the verb (V) and its arguments, subject (S) and object (O).
On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish. $$$$$ Along this line of research, we think some kind of tree-to-string model (Liu et.al., 2006) could be interesting directions to pursue.
On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish. $$$$$ Finally, researchers have also tried to put source language syntax into reordering in machine translation.
On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages: Korean, Japanese, Hindi, Urdu and Turkish. $$$$$ Second, as we will show in the next section, we compare our approach to a very strong baseline with more advanced distance based reordering model, not just the simplest distortion model.

Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ This indicates that the order difference between English and Korean is very significant.
Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ In the recent years, several studies have tried to address this issue by using a word lattice instead of one reordering as input (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).
Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ Several approaches use syntactical analysis to provide multiple source sentence reordering options through word lattices (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).

During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ Over the past ten years, statistical machine translation has seen many exciting developments.
During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.
During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.

R5 Superset of rules from (Xu et al., 2009). $$$$$ We do not use Model-4 because it is slow and it does not add much value to our systems in a pilot study.
R5 Superset of rules from (Xu et al., 2009). $$$$$ Distance based distortion model (Och, 2002; Koehn et.al., 2003) is a simple way of modeling phrase level reordering.
R5 Superset of rules from (Xu et al., 2009). $$$$$ In linguistics, it is possible to define a basic word order in terms of the verb (V) and its arguments, subject (S) and object (O).
R5 Superset of rules from (Xu et al., 2009). $$$$$ It would be very interesting to investigate ways to have efficient procedure for training EM models and getting word alignments using word lattices on the source side of the parallel data.
