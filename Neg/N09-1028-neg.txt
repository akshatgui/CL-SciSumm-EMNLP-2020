Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ The maximum entropy models are trained on all events extracted from training data word alignments using the LBFGS algorithm (Malouf, 2002).
Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ In order to do this, we disallow any movement across punctuation and conjunctions.
Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ We successfully applied this approach to systems translating English to 5 SOV languages: Korean, Japanese, Hindi, Urdu and Turkish.
Another similar work is that of (Xu et al, 2009). They created a pre-ordering rule set for dependency parsers from English to several SOV languages. $$$$$ Finally, researchers have also tried to put source language syntax into reordering in machine translation.

It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). $$$$$ Therefore, for sentences like “John hit the ball but Sam threw the ball”, the reordering result would be “John the ball hit but Sam the ball threw”, instead of “John the ball but Sam the ball threw hit”.
It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). $$$$$ For source words, we use the current aligned word, the word before the current aligned word and the next aligned word; for target words, we use the previous two words in the immediate history.
It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). $$$$$ We use the standard phrase extraction algorithm (Koehn et.al., 2003) to get all phrases up to length 5.
It is true that tree-based reordering cannot cover all word movement operations in language translation, previous work showed that this method is still very effective in practice (Xu et al., 2009, Visweswariah et al, 2010). $$$$$ Imagine when we translate the sentence “English is used as the first or second language in many countries around the world .”.

In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ In this work, a global word order model is proposed based on features including word bigram of the target sentence, displacements and POS tags on both source and target sides.
In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ Normally in a phrase-based decoder, very long distance reordering is not allowed because of efficiency considerations.
In addition, a pre-reorder system using manual rules as (Xu et al, 2009) is included for the English to-Japanese experiment (ManR-PR). $$$$$ In order to implement the precedence rules, we need a dependency parser.

Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ There has been some work on syntactic word order model for English to Japanese machine translation (Chang and Toutanova, 2007).
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ We do not use Model-4 because it is slow and it does not add much value to our systems in a pilot study.
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.
Xu et al (2009) designed a clever precedence reordering rule set for translation from English to several SOV languages. $$$$$ Some preliminary error analysis already show that indeed some sentences suffer from parser errors.

Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ When the sentence gets longer with more complex structure, the number of words to move over during decoding can be quite high.
Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ We believe our rules are flexible and can cover many linguistic reordering phenomena.
Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ Similar to the verbs, adjectives can also take an auxiliary verb, a passive auxiliary verb and a negation as modifiers.
Recently, Xu et al (2009) and Hong et al (2009) proposed rule-based preprocessing methods for SOV languages. $$$$$ Third, our precedence reordering rules, like those in Habash, 2007, are more flexible than those other rules.

Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ The dev set is used to perform MERT training, while the test set is used to select trained weights due to some nondeterminism of MERT training.
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.
Hong et al (2009) used Stanford parser (de Marneffe et al, 2006), which outputs semantic head based dependencies; Xu et al (2009) also used the same representation. $$$$$ In the simple example in Figure 1, if we analyze the English sentence and know that “John” is the subject, “can hit” is the verb and “the ball” is the object, we can reorder the English into SOV order.

Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. $$$$$ We do not use Model-4 because it is slow and it does not add much value to our systems in a pilot study.
Xu et al (2009) used a semantic head-based dependency parser for a similar purpose. $$$$$ The amount of training data for the 5 languages varies from around 17M to more than 350M words, including some noisy data from the web.

Reversing the children order (Xu et al, 2009) reconnects is and popular. $$$$$ The order type NORMAL means we preserve the original order of the children, while REVERSE means we flip the order.
Reversing the children order (Xu et al, 2009) reconnects is and popular. $$$$$ In the recent years, several studies have tried to address this issue by using a word lattice instead of one reordering as input (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).

We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ They build a log-linear model using these features and apply the model to re-rank N-best lists from a baseline decoder.
We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ For a set of five subject-object-verb (SOV) order languages, we show significant improvements in BLEU scores when translating from English, compared to other reordering approaches, in state-of-the-art phrase-based SMT systems.
We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ The format of our rules also makes it possible to automatically extract rules from word aligned corpora.
We obtained four more sets of alignments from the Berkeley aligner (BA) (Liang et al, 2006), the HMM aligner (HA) (Vogel et al, 1996), the alignment based on partial words (PA), and alignment based on dependency based reordering (DA) (Xu et al,2009). $$$$$ In the example of Figure 2, the correct Korean word order is “�� (hit) T L}(can) .

(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ For all 5 SOV languages, we use the target side of the parallel data and some more monolingual text from crawling the web to build 4gram language models.
(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ We use English as a representative of SVO languages and Korean as a representative for SOV languages in our discussion about the word orders.
(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ Along this line of research, discriminative reordering models based on a maximum entropy classifier (Zens and Ney, 2006; Xiong, et.al., 2006) also showed improvements over the distance based distortion model.
(Xu et al, 2009) showed that translation between subject-verb-object (English) and subject-object-verb (Pashto) languages can be improved by reordering the source side of the parallel data. $$$$$ In the rest of the paper, we first introduce our dependency parser based reordering approach based on the analysis of the key issues when translating SVO languages to SOV languages.

Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ Therefore, using a hierarchical phrase-based system can improve those cases.
Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ The decoder needs to make a jump of 13 words in order to put the translation of “is used” at the end of the translation.
Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ Our precedence reordering approach based on a dependency parser is motivated by those previous works, but we also distinguish from their studies in various ways.
Using the rules and algorithm described in (Xu et al, 2009) we reordered all of the source side and used GIZA++ to align the sentences. $$$$$ We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.

Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ The decoder needs to make a jump of 13 words in order to put the translation of “is used” at the end of the translation.
Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ In the recent years, several studies have tried to address this issue by using a word lattice instead of one reordering as input (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).
Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ Furthermore, when the same preprocessing reordering is performed for the training data, we can still apply other reordering approaches, such as distance based reordering and hierarchical phrase reordering, to capture additional local reordering phenomena that are not captured by the preprocessing reordering.
Compared with flat phrases, syntactic rules are good at capturing global reordering, which has been reported to be essential for translating between languages with substantial structural differences, such as English and Japanese, which is a subject-object verb language (Xu et al, 2009). $$$$$ Studies most similar to ours are those preprocessing reordering approaches (Xia and McCord, 2004; Collins et.al., 2005; Wang et.al., 2007; Habash, 2007).

Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ Similar to the distance based reordering models, the syntactical or hierarchical approaches also rely on other models to get word alignments.
Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ Compared to these approaches, our work has a few differences.
Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
Xu et al (2009) utilized a dependency parser with several hand-labeled precedence rules for reordering English to subject-object-verb order like Korean and Japanese. $$$$$ Our precedence reordering approach based on a dependency parser is motivated by those previous works, but we also distinguish from their studies in various ways.

The work by Xu et al (2009) is the closest to our approach. $$$$$ Syntactical analysis of source language can be used to deterministically reorder input sentences (Xia and McCord, 2004; Collins et.al., 2005; Wang et.al., 2007; Habash, 2007), or to provide multiple orderings as weighted options (Zhang et.al., 2007; Li et.al., 2007; Elming, 2008).
The work by Xu et al (2009) is the closest to our approach. $$$$$ Furthermore, when the same preprocessing reordering is performed for the training data, we can still apply other reordering approaches, such as distance based reordering and hierarchical phrase reordering, to capture additional local reordering phenomena that are not captured by the preprocessing reordering.
The work by Xu et al (2009) is the closest to our approach. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
The work by Xu et al (2009) is the closest to our approach. $$$$$ The reordering rules can be either manually written or automatically extracted from data.

Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ Similar to other preprocessing reordering approaches, our method can efficiently incorporate linguistic knowledge into SMT systems without increasing the complexity of decoding.
Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ Another possible reason is that after the reordering rules apply in preprocessing, English sentences in the training data are very close to the SOV order.
Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ The resulting sentence “John the ball can hit .” will only need monotonic translation.
Our approach is a true tree-to-string model and differs from (Xu et al, 2009), which uses trees only as an intermediate representation to rearrange the original sentences. $$$$$ Later on, this model was extended to lexicalized phrase reordering (Tillmann, 2004; Koehn, et.al., 2005; Al-Onaizan and Papineni, 2006) by applying different weights to different phrases.

We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ The first class of approaches tries to explicitly model phrase reordering distances.
We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ Normally in a phrase-based decoder, very long distance reordering is not allowed because of efficiency considerations.
We use a superset of the reordering rules proposed by Xu et al (2009), which flatten a dependency tree into SOV word order that is suitable for all three languages. $$$$$ Therefore, using a hierarchical phrase-based system can improve those cases.

On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages $$$$$ These models typically combine machine translation decoding with chart parsing, therefore significantly increase the decoding complexity.
On Web text, Xu et al (2009) report significant improvements applying one set of hand-crafted rules to translation from English to each of five SOV languages $$$$$ The order, however, is reversed after the movement.

Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ Compared to these approaches, our work has a few differences.
Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ A precedence reordering rule is a mapping from T to a set of tuples {(L, W, O)}, where T is the part-of-speech (POS) tag of the head in a dependency parse tree node, L is a dependency label for a child node, W is a weight indicating the order of that child node and O is the type of order (either NORMAL or REVERSE).
Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ We also collected about 10K English sentences from the web randomly.
Following Collins et al (2005a) and Wang (2007), Xu et al (2009) showed that when translating from English to Japanese (and to other SOV languages such as Korean and Turkish) applying reordering as a preprocessing step that manipulates a source sentence parse tree can significantly outperform state-of-the-art phrase-based and hierarchical machine translation systems. $$$$$ Finally, we have another node rooted at “has” that matches the verb rule again.

During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ As our results in Table 3 show, for all 5 languages, by using the precedence reordering rules as described in Table 1, we achieve significantly better BLEU scores compared to the baseline system.
During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ For all 5 languages, we achieve statistically significant improvements in BLEU scores over a state-of-the-art phrasebased baseline system.
During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ We introduce a novel precedence reordering approach based on a dependency parser to statistical machine translation systems.
During both training and testing, the system reorders source-language sentences in a preprocessing step using a set of rules written in the framework proposed by (Xu et al, 2009) that reorder an English dependency tree into target word order. $$$$$ For example, auxiliary and passive auxiliary verbs are often grouped together with the main verb and moved together with it.

R5 Superset of rules from (Xu et al., 2009). $$$$$ In the table, We use two stars (**) to mean that the statistical significance test using the bootstrap method (Koehn, 2004) gives an above 95% significance level when compared to the baselie.
R5 Superset of rules from (Xu et al., 2009). $$$$$ Although there is clearly room for improvements, we also feel that using one reordering during training may not be good enough either.
R5 Superset of rules from (Xu et al., 2009). $$$$$ Second, as we will show in the next section, we compare our approach to a very strong baseline with more advanced distance based reordering model, not just the simplest distortion model.
R5 Superset of rules from (Xu et al., 2009). $$$$$ In the rest of the paper, we first introduce our dependency parser based reordering approach based on the analysis of the key issues when translating SVO languages to SOV languages.
