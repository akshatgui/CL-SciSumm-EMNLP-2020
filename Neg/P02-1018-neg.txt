Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. $$$$$ Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.
Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. $$$$$ If c is the count value for a pattern and m is its match value, then the algorithm discards that pattern when the lower bound of a 67% confidence interval for its success probability (given c successes out of m trials) is less than 1/2.
Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. $$$$$ It seems that in the cases where the parser does not construct a phrase in the appropriate location to serve as the antecedent for an empty node, the syntactic structure is typically so distorted that either the pattern-matcher fails or the head-finding algorithm does not return the “correct” head either.
Johnson (2002) proposes an algorithm that is able to find long-distance dependencies, as a post processing step, after parsing. $$$$$ This section describes two evaluation procedures for such algorithms.

As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002). $$$$$ In an attempt to devise an evaluation measure for empty node co-indexation that depends less on syntactic structure we experimented with a modified augmented empty node representation in which each antecedent is represented by its head’s category and location.
As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002). $$$$$ Let G be the set of such empty node representations derived from the “gold standard” evaluation corpus and T the set of empty node representations column is the number of times the pattern was found, and the Match column is an estimate of the number of times that this pattern matches some subtree in the training corpus during empty node recovery, as explained in the text. derived from the corpus to be evaluated.
As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002). $$$$$ The patternmatching algorithm combines both simplicity and reasonable performance over the frequently occuring types of empty nodes.
As our interest lies in trace detection and antecedent recovery, we adopt the evaluation measures introduced by Johnson (2002). $$$$$ It seems that in the cases where the parser does not construct a phrase in the appropriate location to serve as the antecedent for an empty node, the syntactic structure is typically so distorted that either the pattern-matcher fails or the head-finding algorithm does not return the “correct” head either.

 $$$$$ This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.
 $$$$$ For patterns of the kind described here, patterns can be indexed on their topmost local tree (i.e., the pattern’s root node label and the sequence of node labels of its children).
 $$$$$ If the parser makes a single parsing error anywhere in the tree fragment matched by a pattern, the pattern will no longer match.

In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002). $$$$$ (Also, while we did not systematically investigate this, there seems to be a number of errors in the annotation of free vs. co-indexed NP * in the treebank).
In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002). $$$$$ Alternatively, one might try to design a “sloppy” pattern matching algorithm which in effect recognizes and corrects common parser errors in these constructions.
In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002). $$$$$ This section describes two evaluation procedures for such algorithms.
In this section, we validate the two-step approach, by applying the parser to the output of the trace tagger, and comparing the antecedent recovery accuracy to Johnson (2002). $$$$$ This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.

 $$$$$ This section describes two evaluation procedures for such algorithms.
 $$$$$ It can also be regarded as a kind of tree transformation, so the overall system architecture (including the parser) is an instance of the “transform-detransform” approach advocated by Johnson (1998).
 $$$$$ This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.
 $$$$$ This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.

Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). $$$$$ In fact this head-based antecedent representation yields scores very similiar to those obtained using the phrase-based representation.
Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). $$$$$ Table 4 provides these measures for the same two corpora described earlier.
Comparing our results to Johnson (2002), we find that the NOINSERT model outperforms that of Johnson by 4.6% (see Table 7). $$$$$ The accuracy of transitivity labelling was not systematically evaluated here.

Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ (Usually this set of antecedents is either empty or contains a single node).
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
Excluding Johnson (2002)'s pattern-matching algorithm, most recent work on finding head - dependencies with statistical parser has used statistical versions of deep grammar formalisms, such as CCG (Clark et al, 2002) or LFG (Riezler et al, 2002). $$$$$ In fact this head-based antecedent representation yields scores very similiar to those obtained using the phrase-based representation.

Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. $$$$$ We experimented with lexicalizing patterns, but the simple method we tried did not improve results.
Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. $$$$$ This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.
Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. $$$$$ (Also, while we did not systematically investigate this, there seems to be a number of errors in the annotation of free vs. co-indexed NP * in the treebank).
Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP*) s: among other differences, unlike Jijkoun and de Rijke's metric (taken from (Johnson, 2002)), Dienes and Dubey's is sensitive to the string extent of the antecedent node, penalizing them if the parser makes attachment errors involving the antecedent even if the system recovered the long-distance dependency itself correctly. $$$$$ Evaluating the algorithm on the output of Charniak’s parser (Charniak, 2000) and the Penn treebank (Marcus et al., 1993) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity.

Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. $$$$$ The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. $$$$$ This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.
Johnson (2002) used corpus-induced patterns to insert gaps into both gold standard trees and parser output. $$$$$ In an attempt to devise an evaluation measure for empty node co-indexation that depends less on syntactic structure we experimented with a modified augmented empty node representation in which each antecedent is represented by its head’s category and location.

We compare our algorithm under a variety of conditions to the work of (Johnson, 2002) and (Gabbard et al, 2006). $$$$$ The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
We compare our algorithm under a variety of conditions to the work of (Johnson, 2002) and (Gabbard et al, 2006). $$$$$ Table 4 provides these measures for the same two corpora described earlier.

The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. $$$$$ The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. $$$$$ Note that this is a particularly stringent evaluation measure for a system including a parser, since it is necessary for the parser to produce a non-empty node of the correct category in the correct location to serve as an antecedent for the empty node.
The first metric, which was introduced by Johnson (2002), has been widely reported by researchers investigating gap insertion. $$$$$ To evaluate co-indexation of empty nodes and their antecedents, we augment the representation of empty nodes as follows.

Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). $$$$$ For example, in Figure 1 the verb likes is relabelled VBZ t in this step.
Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). $$$$$ The match value is obtained by making a second pre-order traversal through a version of the training data from which empty nodes are removed.
Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003). $$$$$ It suggests that one might improve performance by integrating parsing, empty node recovery and antecedent finding in a single system, in which case the current algorithm might serve as a useful baseline.

While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. $$$$$ This seems to be a hard problem, and lexical information (especially the class of the governing verb) seems relevant.
While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. $$$$$ As a comparison of tables 3 and 4 shows, the pattern-matching algorithm’s biggest weakness is its inability to correctly distinguish co-indexed NP * (i.e., NP PRO) from free (i.e., unindexed) NP *.
While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. $$$$$ In this evaluation, each node is represented by a triple consisting of its category and its left and right string positions.
While Charniak's parser does not generate empty category information, Johnson (2002) has developed an algorithm that extracts patterns from the Treebank which can be used to insert empty categories into the parser's output. $$$$$ To evaluate co-indexation of empty nodes and their antecedents, we augment the representation of empty nodes as follows.

Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. $$$$$ (The intuition behind this is that we do not want to penalize the empty node antecedentfinding algorithm if the parser misattaches modifiers to the antecedent).
Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. $$$$$ We experimented with lexicalizing patterns, but the simple method we tried did not improve results.
Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. $$$$$ The next step of the algorithm determines approximately how many times each pattern can match some subtree of a version of the training corpus from which all empty nodes have been removed (regardless of whether or not the corresponding substitutions would insert empty nodes correctly).
Johnson (2002) was the first post-processing approach to non-local dependency recovery, using a simple pattern-matching algorithm on context-free trees. $$$$$ This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.

This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. $$$$$ It suggests that one might improve performance by integrating parsing, empty node recovery and antecedent finding in a single system, in which case the current algorithm might serve as a useful baseline.
This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. $$$$$ It goes to some lengths to handle complex cases such as adjunction and where two or more empty nodes’ paths cross (in these cases the pattern extracted consists of the union of the local trees that constitute the patterns for each of the empty nodes).
This approach contrasts with Johnson (2002), who treats empty/antecedent identification as a joint task, and with Dienes and Dubey (2003a, b), who always identify empties first and determine antecedents later. $$$$$ (Note that because empty nodes dominate the empty string, their left and right string positions of empty nodes are always identical).

in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. $$$$$ This is not unlikely since the statistical model used by the parser does not model these larger tree fragments.
in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. $$$$$ This seems to be a hard problem, and lexical information (especially the class of the governing verb) seems relevant.
in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. $$$$$ For patterns of the kind described here, patterns can be indexed on their topmost local tree (i.e., the pattern’s root node label and the sequence of node labels of its children).
in an abstract sense it mediates the gap-threading information incorporated into GPSG-style (Gazdar et al., 1985) parsers, and in concrete terms it closely matches the information derived from Johnson (2002)'s connected local tree set patterns. $$$$$ This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.

Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. $$$$$ This paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure, which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus.
Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. $$$$$ The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. $$$$$ The patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it.
Our algorithm's performance can be compared with the work of Johnson (2002) and Dienes and Dubey (2003a) on WSJ. $$$$$ Let G be the set of such empty node representations derived from the “gold standard” evaluation corpus and T the set of empty node representations column is the number of times the pattern was found, and the Match column is an estimate of the number of times that this pattern matches some subtree in the training corpus during empty node recovery, as explained in the text. derived from the corpus to be evaluated.

For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. $$$$$ For example, in Figure 1 the verb likes is relabelled VBZ t in this step.
For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. $$$$$ After relabelling preterminals as described above, patterns are extracted during a traversal of each of the trees in the training corpus.
For purposes of comparability with Johnson (2002) we used Charniak's 2000 parser as P. $$$$$ For example, we constructed a Boosting classifier which does recover *U* and empty complementizers 0 more accurately than the pattern-matcher described here (although the pattern-matching algorithm does quite well on these constructions), but this classifier’s performance averaged over all empty node types was approximately the same as the pattern-matching algorithm.

P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. $$$$$ If the parser makes a single parsing error anywhere in the tree fragment matched by a pattern, the pattern will no longer match.
P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. $$$$$ The patternmatching algorithm combines both simplicity and reasonable performance over the frequently occuring types of empty nodes.
P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. $$$$$ Then as is standard, the precision P, recall R and f-score f are calculated as follows: Table 3 provides these measures for two different test corpora: (i) a version of section 23 of the Penn Treebank from which empty nodes, indices and unary branching chains consisting of nodes of the same category were removed, and (ii) the trees produced by Charniak’s parser on the strings of section 23 (Charniak, 2000).
P is parser, G is string-to-context-free-gold-tree mapping, A is present remapping algorithm, J is Johnson 2002, D is the COMBINED model of Dienes 2003. $$$$$ Table 1 contains summary statistics on the distribution of empty nodes in the Penn Treebank.

To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. $$$$$ The rest of this section provides a brief introduction to empty nodes, especially as they are used in the Penn Treebank.
To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. $$$$$ This paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information.
To further compare the results of our algorithm with previous work, we obtained the output trees produced by Johnson (2002) and Dienes (2003) and evaluated them on typed dependency performance. $$$$$ The preprocessing step relabels auxiliary verbs and transitive verbs in all trees seen by the algorithm.
