NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010). $$$$$ Moreover, at the web-scale, we are no longer limited to a snapshot in time, which allows broader knowledge to be learned and processed.
NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010). $$$$$ We reduce the running time from quadratic to practically linear in the number of elements to be computed.
NNS is essential in dealing with many search related tasks, and also fundamental to a broad range of Natural Language Processing (NLP) downstream problems including person name spelling correction (Udupa and Kumar, 2010), document translation pair acquisition (Krstovski and Smith, 2011), large-scale similar noun list generation (Ravichandran et al, 2005), lexical variants mining (Gouws et al, 2011), and large-scale first story detection (Petrovic et al, 2010). $$$$$ The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text.

 $$$$$ We can represent the signature of vector u as: u� = {hr1(u), hr2(u), , hrd(u)}.
 $$$$$ But as we generate more random vectors the time taken by the algorithm also increases.
 $$$$$ In many applications, researchers have shown that more data equals better performance (Banko and Brill, 2001; Curran and Moens, 2002).

Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. $$$$$ Table 1 gives the characteristics of both corpora.
Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. $$$$$ Moreover, at the web-scale, we are no longer limited to a snapshot in time, which allows broader knowledge to be learned and processed.
Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. $$$$$ We then apply the fast hamming distance search algorithm as described in Section 3.
Ravichandran et al (2005) have shown that by using the LSH nearest neighbors calculation can be done in O (nd) time. $$$$$ In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.

III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005). $$$$$ In this paper, we perform high speed similarity list creation for nouns collected from a huge web corpus.
III and Marcu (2005), who use word class features derived from a Web-scale corpus via a process described in Ravichandran et al (2005). $$$$$ A k dimensional unit random vector, in general, is generated by independently sampling a Gaussian function with mean 0 and variance 1, k number of times.

Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005). $$$$$ These algorithms are thus not readily scalable, and limit the size of corpus manageable in practice to a few gigabytes.
Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005). $$$$$ 2The jumbling is performed by a mapping of the bit index as directed by the random permutation function.
Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005). $$$$$ We apply these algorithms to generate noun similarity lists from 70 million pages.
Our classifier for (m, n)-cousins is derived from the algorithm and corpus given in (Ravichandran et al, 2005). $$$$$ We apply these algorithms to generate noun similarity lists from 70 million pages.

 $$$$$ These techniques are used today, for example, to eliminate duplicate web pages.
 $$$$$ Each document is language identified by the software TextCat4 which implements the paper by Cavnar and Trenkle (1994).
 $$$$$ This process of duplicate elimination is carried out in linear time and involves the creation of signatures for each document.
 $$$$$ We apply these algorithms to generate noun similarity lists from 70 million pages.

Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). $$$$$ Randomized algorithms provide the necessary speed and memory requirements to tap into terascale text sources.
Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). $$$$$ Hence instead of using the usual naive cosine distance calculation between every pair of words we can use the algorithm described in Section 3 to make noun clustering web scalable.
Ravichandran et al (2005) used this cosine variant and showed it to produce over 70% accuracy in extracting synonyms when compared against Pantel and Lin (2002). $$$$$ This kind of feature set does not seem to affect our results.

It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. $$$$$ Definition: Let u and v be two vectors in a k dimensional hyperplane.
It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. $$$$$ Randomized algorithms provide the necessary speed and memory requirements to tap into terascale text sources.
It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. $$$$$ For each noun we take the grammatical context of the noun as identified by Minipar5.
It was used by Ravichandran et al (2005) to improve the efficiency of distributional similarity calculations. $$$$$ Three possible areas for future research investigation to overcoming this plateau include: The above listing may not be exhaustive, but it is probably not a bad bet to work in one of the above directions.

The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). $$$$$ Table 1 gives the characteristics of both corpora.
The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). $$$$$ Also the randomization process makes the the algorithm easily parallelizable since each processor can independently contribute a few bits for every vector.
The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). $$$$$ We reduce the running time from quadratic to practically linear in the number of elements to be computed.
The frequency statistics were weighted using mutual information, as in Ravichandran et al (2005). $$$$$ We rewrite the proof here for clarity.

When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. $$$$$ We reduce the running time from quadratic to practically linear in the number of elements to be computed.
When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. $$$$$ These algorithms are of complexity O(n2k), where n is the number of unique nouns and k is the feature set length.
When the cut-off was increased to 100, as used by Ravichandran et al (2005), the results improved significantly. $$$$$ A case in point is generating noun similarity lists using co-occurrence statistics, which has quadratic running time on the input size.

We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. $$$$$ This implies logn << k and nlogn << nk.
We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. $$$$$ A case in point is generating noun similarity lists using co-occurrence statistics, which has quadratic running time on the input size.
We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. $$$$$ Web Corpus (70 million web pages, 138GB), 2.
We used randomized algorithms (Ravichandran et al, 2005) to build the semantic space efficiently. $$$$$ We reduce the running time from quadratic to practically linear in the number of elements to be computed.

This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). $$$$$ This process creates similarity lists for each of the 100 vectors.
This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). $$$$$ Hence the time complexity of our algorithm is O(nk + nlogn) Pz� O(nk).
This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). $$$$$ In this paper, we investigate the first two avenues.
This scheme was used, e.g., for creating similarity lists of nouns collected from a web corpus in Ravichandran et al (2005). $$$$$ In the next section, we proceed to apply this technique for generating noun similarity lists.

This baseline system follows the design of previous work (Ravichandran et al, 2005). $$$$$ LSH functions are generally based on randomized algorithms and are probabilistic.
This baseline system follows the design of previous work (Ravichandran et al, 2005). $$$$$ Note that the above equation is probabilistic in nature.
This baseline system follows the design of previous work (Ravichandran et al, 2005). $$$$$ We first construct a frequency count vector C(e) = (ce1, ce2, ..., cek), where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in context f. We then construct a mutual information vector MI(e) = (mie1, mie2, ..., miek) for each word e, where mief is the pointwise mutual information between word e and feature f, which is defined as: where n is the number of words and N = En Em j=1 cij is the total frequency count of all i=1 features of all words.
This baseline system follows the design of previous work (Ravichandran et al, 2005). $$$$$ Interestingly, cosine similarity is widely used in NLP for various applications such as clustering.

We followed the notation of the original paper (Ravichandran et al, 2005) here. $$$$$ Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989).
We followed the notation of the original paper (Ravichandran et al, 2005) here. $$$$$ NLP researchers have just begun leveraging the vast amount of knowledge available on the web.
We followed the notation of the original paper (Ravichandran et al, 2005) here. $$$$$ However, to unleash the real power of clustering one has to work with large amounts of text.

Ravichandran et al (2005) applied LSH to the task of noun clustering. $$$$$ A case in point is generating noun similarity lists using co-occurrence statistics, which has quadratic running time on the input size.
Ravichandran et al (2005) applied LSH to the task of noun clustering. $$$$$ This is a huge saving from the original O(n2k) algorithm.
Ravichandran et al (2005) applied LSH to the task of noun clustering. $$$$$ In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.
Ravichandran et al (2005) applied LSH to the task of noun clustering. $$$$$ In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.

However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. $$$$$ We generate a random number from a Gaussian distribution by using Box-Muller transformation (Box and Muller, 1958).
However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. $$$$$ Hence the time complexity of our algorithm is O(nk + nlogn) Pz� O(nk).
However, Ravichandran et al (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. $$$$$ Charikar (2002) proposed the use of random hyperplanes to generate an LSH function that preserves the cosine similarity between every pair of vectors.

In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. $$$$$ LSH functions involve the creation of short signatures (fingerprints) for each vector in space such that those vectors that are closer to each other are more likely to have similar fingerprints.
In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. $$$$$ Proceeding we have Pr[hr(u) =� hr(v)] = 0(u, v)/7r and Pr[hr(u) = hr(v)] = 1 − 0(u, v)/7r.
In practice p is generally large, Ravichandran et al (2005) used p= 1000 in their work. $$$$$ Note that the above equation is probabilistic in nature.

Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). $$$$$ However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.
Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). $$$$$ Assume that we are given n points to cluster with a maximum of k features.
Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). $$$$$ This reduction in complexity of similarity computation makes it possible to address vastly larger datasets, at the cost, as shown in Section 5, of only little reduction in accuracy.
Data sets: We use two data sets: Gigaword (Graff, 2003) and a copy of news web (Ravichandran et al., 2005). $$$$$ By searching IR engines for simple surface patterns, many applications ranging from word sense disambiguation, question answering, and mining semantic resources have already benefited.

We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). $$$$$ This algorithm involves random permutations of the bit streams and their sorting to find the vector with the closest hamming distance.
We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). $$$$$ However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k.
We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). $$$$$ For every noun in the top N list generated by our system we calculate the percentage overlap with the gold standard list.
We set the number of projections k= 3000 for all three methods and for PLEB and FAST-PLEB, we set number of permutations p= 1000 as used in large-scale noun clustering work (Ravichandran et al 2005). $$$$$ Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989).

Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ In this paper, we explore the power of randomized algorithm to address the challenge of working with very large amounts of data.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ Also the randomization process makes the the algorithm easily parallelizable since each processor can independently contribute a few bits for every vector.
Language modeling (Chen and Goodman, 1996), noun-clustering (Ravichandran et al, 2005), constructing syntactic rules for SMT (Galley et al, 2004), and finding analogies (Turney, 2008) are examples of some of the problems where we need to compute relative frequencies. $$$$$ We perform 3 kinds of evaluation: 1.
