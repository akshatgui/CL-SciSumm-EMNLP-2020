For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). $$$$$ The improved version, shown in Figure 2, restricts C[dh] to be the label of a previously derived adjacent constituent.
For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). $$$$$ Other lexicalized formalisms include (Schabes et al., 1988; Mel'euk, 1988; Pollard and Sag, 1994).
For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). $$$$$ In this paper we adopt the following conventions: a, b, c, d denote symbols in VT, w, X, y denote strings in Vat, and a, 0, denote strings in (VN U VT)*.
For certain lexicalized context-free models we even obtain higher time complexities when the size of the grammar is not to be considered as a parameter (Eisner and Satta, 1999). $$$$$ This formalism closely resembles stochastic grammatical formalisms that are used in several existing natural language processing systems (see §1).

We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). $$$$$ We now give a variant of the algorithm of §4; the variant has the same asymptotic complexity but will often be faster in practice.
We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). $$$$$ We use dynamic programming to assemble such subderivations into a full parse.
We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). $$$$$ We have formally described, and given faster parsing algorithms for, three practical grammatical rewriting systems that capture dependencies between pairs of words.
We can reduce the generative power of context free transduction grammars by a syntactic restriction that corresponds to the bi lexical context-free grammars (Eisner and Satta, 1999). $$$$$ A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles.

While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). $$$$$ A head automaton grammar (HAG) is a function H : a 1-4 Ha that defines a head automaton (HA) for each element of its (finite) domain.
While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). $$$$$ A bilexical grammar makes many stipulations about the compatibility of particular pairs of words in particular roles.
While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). $$$$$ Notice that the start symbol is necessarily a lexicalized nonterminal, T[$].
While this criterion is in general NP-hard (Desper and Gascuel, 2005), for projective trees we find that a bi lexical parsing algorithm can be used to find an exact solution efficiently (Eisner and Satta, 1999). $$$$$ Lexicalized grammar formalisms are of both theoretical and practical interest to the computational linguistics community.

However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). $$$$$ Set P is a finite set of productions having the form A —* a, where A E VN , a E (VN U VT)*.
However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). $$$$$ Here p is the maximum number of productions sharing the same pair of terminal symbols (e.g., the pair (b, a) in production (1)).
However, if we restrict u to be in U, as we do in the above, then maximizing c(u) over U can be solved using the bi lexical parsing algorithm from Eisner and Satta (1999). $$$$$ stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.

The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). $$$$$ stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.
The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). $$$$$ All three systems admit naive 0(n5) algorithms.
The algorithmic complexity of (Wu, 1996) is O (n3+4 (m1)), though Huang et al (2005) present a more efficient factorization inspired by (Eisner and Satta, 1999) that yields an overall complexity of O (n3+3 (m1)), i.e., O(n3m). $$$$$ (See §4 for brief discussion.)

While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. $$$$$ For the usual case, split head automaton grammars or equivalent bilexical CFGs, we replace the 0(n3) algorithm of (Eisner, 1997) by one with a smaller grammar constant.
While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. $$$$$ If every production in P has the form A —4 BC or A a, for A, B,C E VN , a E VT, then the grammar is said to be in Chomsky Normal Form (CNF).2 Every language that can be generated by a CFG can also be generated by a CFG in CNF.
While it seems difficult to improve the asymptotic running time of the Eisner algorithm beyond what is presented in (Eisner and Satta, 1999), McDonald et al. $$$$$ We use the general term bilexical for a grammar that records such facts.

Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. $$$$$ The construction therefore implies that we can parse a length-n sentence under H in time 0(n4t3).
Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. $$$$$ We do this by providing a translation from head automaton grammars to bilexical CFGs.4 This result improves on the head-automaton parsing algorithm given by Alshawi, which is analogous to the CKY algorithm on bilexical CFGs and is likewise 0(n5) in practice (see §3).
Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. $$$$$ It is necessary to use an agenda data structure (Kay, 1986) when implementing the declarative algorithm of Figure 2.
Since we are interested in projecting the fractional parse onto the space of projective spanning trees, we can simply employ a dynamic programming parsing algorithm (Eisner and Satta, 1999) where the weight of each edge is given as the fraction of the edge variable. $$$$$ A constituent of nonterminal type A[a] is said to have terminal symbol a as its lexical head, &quot;inherited&quot; from the constituent's head child in the parse tree (e.g., C[a]).

The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. $$$$$ stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.
The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. $$$$$ Note that, e.g., all senses would restore the g 2 factor.
The new model, which bilexicalizes within languages, allows us to use the "hook trick" (Eisner and Satta, 1999) and therefore reduces complexity. $$$$$ We also use the reflexive and transitive closure of written and define L(G) accordingly.

In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. $$$$$ The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice).
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. $$$$$ Collins's 0(n5)).
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. $$$$$ Such cycles would be necessary for Ha itself to accept a formal language such as {(bn, cn) : n > 0}, where word a takes 2n dependents, but we know of no natural-language motivation for ever using them in a HAG.
In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the non projective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. $$$$$ Besides the possible arguments of a word, a natural-language grammar does well to specify possible head words for those arguments.

Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). $$$$$ The new algorithm saves a factor of n by combining those two constituents in two steps, one of which is insensitive to k and abstracts over its possible values, the other of which is insensitive to h' and abstracts over its possible values.
Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). $$$$$ More formally, given H, we simultaneously define La for all a E VT to be minimal such that if (x, y) E L(H a), x' e Lx, y' E Ly, then x'ayi E La, where stands for the concatenation language Lai • • Lai,.
Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). $$$$$ The rationale is that soft selectional restrictions play a crucial role in disambiguation.1 The chart parsing algorithms used by most of the above authors run in time 0(n5), because bilexical grammars are enormous (the part of the grammar relevant to a length-n input has size 0(n2) in practice).
Throughout this paper we will use split bi lexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). $$$$$ Early mechanisms of this sort included categorial grammar (Bar-Hillel, 1953) and subcategorization frames (Chomsky, 1965).

On this point, see (Eisner and Satta, 1999, and footnote 6). $$$$$ We use the general term bilexical for a grammar that records such facts.
On this point, see (Eisner and Satta, 1999, and footnote 6). $$$$$ All three systems admit naive 0(n5) algorithms.
On this point, see (Eisner and Satta, 1999, and footnote 6). $$$$$ Each item now needs to track its head's sense along with its head's position in ID.
On this point, see (Eisner and Satta, 1999, and footnote 6). $$$$$ But in this paper we show that the complexity is not so bad after all: grammars where an 0(n3) algorithm was previously known (Eisner, 1997), the grammar constant can be reduced without harming the 0(n3) property.

First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. $$$$$ The highestprobability derivation for any item can be reconstructed recursively at the end of the parse, provided that each item maintains not only a bit indicating whether it can be derived, but also the probability and instantiated root rule of its highest-probability derivation tree.
First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. $$$$$ Here p is the maximum number of productions sharing the same pair of terminal symbols (e.g., the pair (b, a) in production (1)).
First, we observe without details that we can easily achieve this by starting instead with the algorithm of Eisner (2000), rather than Eisner and Satta (1999), and again refusing to add long tree dependencies. $$$$$ Suppose that the input is iD= d1 • • • dn, and each d2 has up to g possible senses.

The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). $$$$$ We use dynamic programming to assemble such subderivations into a full parse.
The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). $$$$$ We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT.
The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). $$$$$ stochastic parsers use grammars, where each word type idiosyncratically prefers particular complements with parhead words.
The obvious reduction for unsplit head automaton grammars, say, is only O(n4) O (n3k), following (Eisner and Satta, 1999). $$$$$ This improves speed if there are not many such constituents and we can enumerate them in 0(1) time apiece (using a sparse parse table to store the derived items).

In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). $$$$$ A &quot;derives&quot; relation, written is associated with a CFG as usual.
In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). $$$$$ A &quot;derives&quot; relation, written is associated with a CFG as usual.
In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). $$$$$ This improves speed if there are not many such constituents and we can enumerate them in 0(1) time apiece (using a sparse parse table to store the derived items).
In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). $$$$$ We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.

It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. $$$$$ &quot;Convene&quot; requires an NP object, but some NPs are more semantically or lexically appropriate here than others, and the appropriateness depends largely on the NP's head (e.g., &quot;meeting&quot;).
It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. $$$$$ We are concerned here with head automaton grammars H such that every Ha is split.
It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. $$$$$ We introduce next a grammar formalism that captures lexical dependencies among pairs of words in VT.
It achieves its speed in part because it uses the Eisner and Satta (1999) algorithm for n3 bi lexical parsing, but also because dependency parsing has a much lower grammar constant than does standard PCFG parsing after all, there are no phrasal constituents to consider. $$$$$ Collins's 0(n5)).

Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. $$$$$ The pass considers the items in increasing order of width, where the width of an item in (a) is defined as max{h, j} — min{h, j}.
Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. $$$$$ We present parsing algorithms for two bilexical formalisms, improvthe prior upper bounds of For a comspecial case that was known to allow (Eisner, 1997), we present an algorithm with an improved grammar constant.
Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. $$$$$ A parse is just a derivation (proof tree) of lhn and its probability—like that of any derivation we find—is defined as the product of the probabilities of all productions used to condition inference rules in the proof tree.
Folding introduces new intermediate items, perhaps exploiting the distributive law; applications include parsing speedups such as (Eisner and Satta, 1999), as well as well-known techniques for speeding up multi-way database joins, constraint programming, or marginalization of graphical models. $$$$$ The results mentioned in §6 are related to the closure property of CFGs under generalized sequential machine mapping (Hoperoft and Ullman, 1979).

In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). $$$$$ Notice that the ATTACH-LEFT rule of Figure 1(b) tries to combine the nonterminal label B[dhd of a previously derived constituent with every possible nonterminal label of the form C[dh].
In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). $$$$$ We use the general term bilexical for a grammar that records such facts.
In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). $$$$$ The results mentioned in §6 are related to the closure property of CFGs under generalized sequential machine mapping (Hoperoft and Ullman, 1979).
In the sections that follow, we frame various dependency models as a particular variety of CFGs known as split-head bi lexical CFGs (Eisner and Satta, 1999). $$$$$ (A stochastic version of the grammar could implement &quot;soft preferences&quot; by allowing the rules in the second group but assigning them various low probabilities.)

The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). $$$$$ We note that this construction can be straightforwardly extended to convert stochastic HAGs as in (Alshawi, 1996) into stochastic CFGs.
The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). $$$$$ The input to the parser will be a CFG G together with a string of terminal symbols to be parsed, w = d1d2 • • • dn.
The scoring follows the parametrization of a weighted split head-automaton grammar (Eisner and Satta, 1999). $$$$$ We now give a variant of the algorithm of §4; the variant has the same asymptotic complexity but will often be faster in practice.

As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. $$$$$ We also use the reflexive and transitive closure of written and define L(G) accordingly.
As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. $$$$$ Indeed, this approach gives added flexibility: a word's sense, unlike its choice of flip state, is visible to the HA that reads it. three models in (Collins, 1997) are susceptible to the 0(n3) method (cf.
As shown by Eisner (Eisner and Satta,1999) the dynamic programming algorithms for bi lexicalized PCFGs require O (m3) states, so a n-best parser would require O (nm3) states. $$$$$ Notice that the start symbol is necessarily a lexicalized nonterminal, T[$].

The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999). $$$$$ 8 Split head automaton grammars in time 0 (n3 ) For many bilexical CFGs or HAGs of practical significance, just as for the bilexical version of link grammars (Lafferty et al., 1992), it is possible to parse length-n inputs even faster, in time 0(n3) (Eisner, 1997).
The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999). $$$$$ A &quot;derives&quot; relation, written is associated with a CFG as usual.
The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999). $$$$$ A constituent of nonterminal type A[a] is said to have terminal symbol a as its lexical head, &quot;inherited&quot; from the constituent's head child in the parse tree (e.g., C[a]).
The best known parsing algorithm for such a model is O (n3) (Eisner and Satta, 1999). $$$$$ The improved version, shown in Figure 2, restricts C[dh] to be the label of a previously derived adjacent constituent.
