They measure compositionality as a combination of two similarity values $$$$$ Recent work which attempts to discriminate between compositional and non-compositional MWEs include Lin (1999), who used mutualinformation measures identify such phrases, Baldwin et al. (2003), who compare the distribution of the head of the MWE with the distribution of the entire MWE, and Vallada Moir´on & Tiedemann (2006), who use a word-alignment strategy to identify non-compositional MWEs making use of parallel texts.
They measure compositionality as a combination of two similarity values $$$$$ His method was to compare the mutual information measure of the constituents parts of an MWE with the mutual information of similar expressions obtained by substituting one of the constituents with a related word obtained by thesaurus lookup.
They measure compositionality as a combination of two similarity values $$$$$ We computed on the basis of the distribution of the components of the MWE an estimate for the compositional meaning vector for the MWE.
They measure compositionality as a combination of two similarity values $$$$$ Since this technique is often used to model meaning, we will speak in terms of “meaning” similiarity.

Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainning data. $$$$$ One of the obvious improvements to the algorithm could come from better models for simulating compositional meaning.
Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainning data. $$$$$ His method was to compare the mutual information measure of the constituents parts of an MWE with the mutual information of similar expressions obtained by substituting one of the constituents with a related word obtained by thesaurus lookup.
Katz and Giesbrecht (2006) devise a supervised method in which they compute the meaning vectors for the literal and non literal usages of a given expression in the trainning data. $$$$$ We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.

The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal usages of a given expression in the training data. $$$$$ Their hypothesis is that high LSA-based similarity between the MWE and each of its constituent parts is indicative of compositionality.
The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal usages of a given expression in the training data. $$$$$ These are, however, more general issues associated with MWE processing.

The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal us ages of a given expression in the training data. $$$$$ There is some evidence (Baldwin et al., 2003) that part of speech tagging might improve results in this kind of task.
The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal us ages of a given expression in the training data. $$$$$ Since LSA has been used in a number of meaning-related language tasks to good effect (Landauer and Dumais, 1997; Landauer and Psotka, 2000; Cederberg and Widdows, 2003), they had hoped to improve their results by identify non-compositional expressions using a method similar to that which we are exploring here.
The few token-based approaches include a study by Katz and Giesbrecht (2006), who devise a supervised method in which they compute the meaning vectors for the literal and non-literal us ages of a given expression in the training data. $$$$$ Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings.

Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006). $$$$$ Lin, Baldwin et al., and Schone & Jurafsky, all use as their gold standard either idiom dictionaries or WordNet (Fellbaum, 1998).
Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006). $$$$$ It is clear that while Baldwin et al.’s expectation is borne out in the case of the constituent noun (the non-head), it is not in the case of the constituent verb (the head).
Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006). $$$$$ We worked only on raw text data.
Supervised classifiers have been used be fore for this task, notably by Katz and Giesbrecht (2006). $$$$$ These are, however, more general issues associated with MWE processing.

Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers. $$$$$ We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.
Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers. $$$$$ While expressions which may potentially have idiomatic meanings can be identified using various lexical association measures (Evert and Krenn, 2001; Evert and Kermes, 2003), other techniques must be used to determining whether or not a particular MWE does, in fact, have an idiomatic use.
Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers. $$$$$ Indeed den L¨offel abgeben, like to kick the bucket, is a noncompositional idiom meaning ‘to die’.
Note that our results are noticeably higher than those reported by Cook et al (2007), Fazly et al (To appear) and Katz and Giesbrecht (2006) for similar supervised classifiers. $$$$$ We worked only on raw text data.

Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors. $$$$$ We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.
Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors. $$$$$ Their approach is methodologically similar to ours, in that they compute similarity on the basis of contexts of occurrance, making use of LSA.
Katz and Giesbrecht (2006) used a supervised learning method to distinguish between compositional and non-compositional uses of an expression (in German text) by using contextual information in the form of Latent Semantic Analy sis (LSA) vectors. $$$$$ His method was to compare the mutual information measure of the constituents parts of an MWE with the mutual information of similar expressions obtained by substituting one of the constituents with a related word obtained by thesaurus lookup.

There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)). $$$$$ The resulting cosine similarity values range from 0.01 to 0.80.
There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)). $$$$$ We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.
There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)). $$$$$ Zhai (1997), in an early attempt to apply statistical methods to the extraction of noncompositional MWEs, made use of what we take to be a more appropriate evaluation metric.
There are several studies relevant to detecting compositionality of noun-noun, verb-particle and light verb constructions and verb noun pairs (e.g. Katz and Giesbrecht (2006)). $$$$$ In this framework, “meaning” is modeled as an n-dimensional vector, derived via singular value decomposition (Deerwester et al., 1990) from word co-occurrence counts for the expression in question, a technique frequently referred to as Latent Semantic Analysis (LSA).

Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. $$$$$ Making use of latent semantic analysis, we explore the hypothesis that local linguistic context can serve to identify multi-word expressions that have noncompositional meanings.
Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. $$$$$ Our hope is simply that this rough model is sufficient to the task of identifying non-compositional MWEs.
Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. $$$$$ (the words Autobahn ‘highway’ and eigenst¨andig ‘independent’ are given for comparison).
Katz and Giesbrecht (2006) compared the word vector of an idiom in context and that of the constituent words of the idiom using LSA in order to determine if the expression is idiomatic. $$$$$ We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.

The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006). $$$$$ As noted by Sag et al. (2002) many MWEs are simply “institutionalized phrases” whose meanings are perfectly compositional, but whose frequency of use (or other non-linguistic factors) make them highly salient.
The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006). $$$$$ Although intuitively appealing, Lin’s algorithm only achieves precision and recall of 15.7% and 13.7%, respectively (as compared to a gold standard generate from an idiom dictionary—but see below for discussion).
The performance of the supervised one is obtained by the method of Katz and Giesbrecht (2006). $$$$$ 5).

Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. $$$$$ For us the discrimination task involves determining for a given expression whether it has a non-compositional interpretation in addition to its compositional interpretation, and the selection task involves determining in a given context, whether a given expression is being used compositionally or non-compostionally.
Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. $$$$$ The hope was that a significant difference between these measures, as in the case of red tape (mutual information: 5.87) compared to yellow tape (3.75) or orange tape (2.64), would be characteristic of non-compositional MWEs.
Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. $$$$$ Lin, Baldwin et al., and Schone & Jurafsky, all use as their gold standard either idiom dictionaries or WordNet (Fellbaum, 1998).
Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. $$$$$ Our task was to classify these 81 potential MWEs according whether or not thay have an idiomatic meaning.

For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. $$$$$ We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.
For example, the method used in (Katz and Giesbrecht, 2006) relies primarily on local co-occurrence lexicon to construct feature vectors for each target token. $$$$$ In the second experiment we attempt to determine whether the difference between the contexts in which an MWE appears and the contexts in which its component words appear can indeed serve to tell us whether the MWE has an idiomatic use.

Among the earliest studies on token-based classification were the ones by Hashimoto et al (2006) on Japanese and Katz and Giesbrecht (2006) on German. $$$$$ Their approach is methodologically similar to ours, in that they compute similarity on the basis of contexts of occurrance, making use of LSA.
Among the earliest studies on token-based classification were the ones by Hashimoto et al (2006) on Japanese and Katz and Giesbrecht (2006) on German. $$$$$ Baldwin et al., (2003) focus more narrowly on distinguishing English noun-noun compounds and verb-particle constructions which are compositional from those which are not compositional.
Among the earliest studies on token-based classification were the ones by Hashimoto et al (2006) on Japanese and Katz and Giesbrecht (2006) on German. $$$$$ Our study shows that the F-score measure is maximized by taking as threshold for distinguishing non-compositional phrases from compositional ones a cosine similarity value somewhere between 0.1-0.2.

Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. $$$$$ While corpus-based techniques for identifying collocational multi-word expressions by exploiting statistical properties of the co-occurrence of the component words have become increasingly sophisticated (Evert and Krenn, 2001; Evert, 2004), it is well known that mere co-occurrence does not well distinguish compositional from non-compositional expressions (Manning and Sch¨utze, 1999, Ch.
Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. $$$$$ To evaluate the method, we used the careful manual annotation of the PNV database described by Krenn (2000) as our gold standard.
Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. $$$$$ Zhai (1997), in an early attempt to apply statistical methods to the extraction of noncompositional MWEs, made use of what we take to be a more appropriate evaluation metric.
Katz and Giesbrecht (2006) compute meaning vectors for literal and non-literal examples in the training set and then classify test instances based on the closeness of their meaning vectors to those of the training examples. $$$$$ Nevertheless the distribution of fire breathing is quite unrelated to that of its constituents fire and breathing ( the former appears frequently with dragon and circus while the later appear frequently with blaze and lungs, respectively).

Such techniques either do not use any information regarding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non compositionality (Katz and Giesbrecht, 2006). $$$$$ Extending to the general case, our task was to compare the composed vector to the actual vector for all the MWEs in our test set.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non compositionality (Katz and Giesbrecht, 2006). $$$$$ We propose that vector-similarity between distribution vectors associated with an MWE as a whole and those associated with its constitutent parts can serve as a good measure of the degree to which the MWE is compositional.
Such techniques either do not use any information regarding the linguistic properties of MWEs (Birkeand Sarkar, 2006), or mainly focus on their non compositionality (Katz and Giesbrecht, 2006). $$$$$ The hope was that a significant difference between these measures, as in the case of red tape (mutual information: 5.87) compared to yellow tape (3.75) or orange tape (2.64), would be characteristic of non-compositional MWEs.

In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data. $$$$$ Their approach is methodologically similar to ours, in that they compute similarity on the basis of contexts of occurrance, making use of LSA.
In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data. $$$$$ Although intuitively appealing, Lin’s algorithm only achieves precision and recall of 15.7% and 13.7%, respectively (as compared to a gold standard generate from an idiom dictionary—but see below for discussion).
In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data. $$$$$ Occurrences on which the annotators disagreed were thrown out.
In supervised approaches, such as that of Katz and Giesbrecht (2006), co-occurrence vectors for literal and idiomatic meanings are formed from manually annotated training data. $$$$$ As a check we chose, at random, a number of simple clearly-compositional word combinations (not from the candidate MWE list).

However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional. $$$$$ We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn’t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).
However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional. $$$$$ The vectors turned out, as expected, to be almost orthogonal, with a cosine of the angle between them of 0.02.
However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional. $$$$$ Our hope is simply that this rough model is sufficient to the task of identifying non-compositional MWEs.
However, this approach follows that of Katz and Giesbrecht (2006) in assuming that literal meanings are compositional. $$$$$ We rely on there being enough non-compositional uses of an idiomatic MWE in the corpus that the overall meaning vector for the MWE reflects this usage.

We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006). $$$$$ We also compared the literal and non-literal vectors for ins Wasser fallen from the first experiment with the composed vector, computed out of the meaning vectors for Wasser and for fallen.9 The difference isn’t large, but nevertheless the composed vector is more similar to the literal vector (cosine of 0.2937) than to the non-literal vector (cosine of 0.1733).
We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006). $$$$$ The German expression ins Wasser fallen, for example, has a noncompositional interpretation on which it means ‘to fail to happen’ (as in (1)) and a compositional interpretation on which it means ‘to fall into water (as in (2)).1 The discrimination task, then, is to identify ins Wasser fallen as an MWE that has an idiomatic meaning and the selection task is to determine that in (1) it is the compositional meaning that is intended, while in (2) it is the non-compositional meaning.
We also compare our unsupervised methods against the supervised method proposed by Katz and Giesbrecht (2006). $$$$$ We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.

Our results using 1NN, 72 $$$$$ In the first experiment we seek to confirm that the local context of a known idiom can reliably distinguish idiomatic uses from non-idiomatic uses.
Our results using 1NN, 72 $$$$$ These are, however, more general issues associated with MWE processing.
Our results using 1NN, 72 $$$$$ Our hope is simply that this rough model is sufficient to the task of identifying non-compositional MWEs.

L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment. $$$$$ We present experiments that show that low (cosine) similarity does, in fact, correlate with non-compositionality.
L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment. $$$$$ In our experiments we make use of lexical semantic analysis (LSA) as a model of contextsimilarity (Deerwester et al., 1990).
L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment. $$$$$ They evaluate their technique by assessing the correlation between high semantic similarity of the constituents of an MWE to the MWE as a whole with the likelihood that the MWE appears in WordNet as a hyponym of one of the constituents.
L-NCF depends highly on the accuracy of the automatically acquired canonical forms, it is not surprising that these two methods perform 5This was also noted by Katz and Giesbrecht (2006) in their second experiment. $$$$$ Lin’s goal, like ours, was to discriminate noncompositional MWEs from compositional MWEs.
