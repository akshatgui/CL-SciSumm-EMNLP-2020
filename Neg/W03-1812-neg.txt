Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposability) of a given MWE. $$$$$ As the first step down the path toward an empirical model of decomposability, we focus on demarcating simple decomposable MWEs from idiosyncratically decomposable and non-decomposable MWEs.
Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposability) of a given MWE. $$$$$ This produces the unconvincing scores of 15.7% for precision and 13.7% for recall.
Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposability) of a given MWE. $$$$$ This material is partly based upon work supported by the National Science Foundation under Grant No.
Baldwin et al (2003) use LSA as a technique foranalysing the compositionality (or decomposability) of a given MWE. $$$$$ Given that these senses for spill and beans are not readily available at the simplex level other than in the context of this particular MWE, it seems fallacious to talk about them composing together to form the semantics of the idiom.

Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003). $$$$$ This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis.
Some of these are mutual information (Church and Hanks, 1989), distributed frequency (Tapanainen et al, 1998) and Latent Semantic Analysis (LSA) model (Baldwin et al, 2003). $$$$$ In the case that a noun phrase followed the particle candidate, we performed attachment disambiguation to determine the transitivity of the particle candidate.

Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). $$$$$ The verbs are arranged according to troponym or “manner-of” relations, where murder is a manner of killing, so kill immediately dominates murder in the hierarchy.
Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). $$$$$ We used WordNet for evaluation by way of looking at: (a) hyponymy, and (b) semantic distance.
Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). $$$$$ Ideally, we would like to be able to differentiate between three classes of MWEs: nondecomposable, idiosyncratically decomposable and simple decomposable (derived from Nunberg et al.’s sub-classification of idioms (1994)).
Some of them are Frequency, Point-wise mutual information (Church and Hanks, 1989), Distributed frequency of object (Tapanainen et al, 1998), Distributed frequency of object using verb information (Venkatapathyand Joshi, 2005), Similarity of object in verb object pair using the LSA model (Baldwin et al,2003), (Venkatapathy and Joshi, 2005) and Lexical and Syntactic fixedness (Fazly and Stevenson, 2006). $$$$$ One interesting exception is Lin (1999), whose approach is explained as follows: The intuitive idea behind the method is that the metaphorical usage of a noncompositional expression causes it to have a different distributional characteristic than expressions that are similar to its literal meaning.

For example, Baldwin et al (2003) studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. $$$$$ We used these models to compare different words, and to find their neighbours.
For example, Baldwin et al (2003) studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. $$$$$ For both tests, values closer to 0 indicate random distribution of the data, whereas values closer to 1 indicate a strong correlation.
For example, Baldwin et al (2003) studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. $$$$$ BCS-0094638 and also the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University.
For example, Baldwin et al (2003) studied vector extraction for phrases because they were interested in the decomposability of multi word expressions. $$$$$ We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.

According to Baldwin et al (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. $$$$$ We evaluated the method over English NN compounds and verbparticles, and showed it to correlate moderately with WordNet-based hyponymy values.
According to Baldwin et al (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. $$$$$ Significance here is defined as the absence of overlap between the 95% confidence interval of the mutual information scores.
According to Baldwin et al (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. $$$$$ No hyponymy relation holds with non-decomposable or idiosyncratically decomposable MWEs (i.e., they are exocentric), as even if the semantics of the head noun can be determined through decomposition, by definition this will not correspond to a simplex sense of the word.
According to Baldwin et al (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. $$$$$ The correlation results for NN compounds and verb-particles are presented in Table 3, where R2 refers to the output of the linear regression test and HSO refers to Hirst and St-Onge similarity measure.

Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSAto distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. $$$$$ While evaluation pointed to a moderate correlation between LSA similarities and occurrences of hyponymy, we have yet to answer the question of exactly where the cutoffs between simple decomposable, idiosyncratically decomposable and nondecomposable MWEs lie.
Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSAto distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. $$$$$ It gave empirical backing to this assumption by showing that annotator judgements for verbparticle decomposability correlate significantly with non-expert human judgements on the similarity between a verb-particle construction and its head verb.
Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSAto distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. $$$$$ One area in which we plan to extend this research is the analysis of MWEs in languages other than English.
Prior work in discovering non-compositional phrases has been carried out by Lin (1999) and Baldwin et al (2003), who also used LSAto distinguish between compositional and non compositional verb-particle constructions and noun noun compounds. $$$$$ In the case of the verb-particle data, WordNet has no classification of prepositions or particles, so we can only calculate the similarity between the head verb and verbparticle (VPC(head)).

Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). $$$$$ We test the model over English noun-noun compounds and verb-particles, and evaluate its correlation with similarities and hyponymy values in WordNet.
Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). $$$$$ We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.
Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). $$$$$ We evaluated the method over English NN compounds and verbparticles, and showed it to correlate moderately with WordNet-based hyponymy values.
Some of these are Frequency, Mutual Information (Church and Hanks, 1989), distributed frequency of object (Tapanainen et al, 1998) and LSA model (Baldwin et al, 2003) (Schutze, 1998). $$$$$ To summarise, we have proposed a constructioninspecific empirical model of MWE decomposability, based on latent semantic analysis.

An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows (Baldwin et al, 2003). $$$$$ Note that it is arguable whether simple decomposable MWEs belong in the grammar proper, or should be described instead as lexical affinities between particular word combinations.
An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows (Baldwin et al, 2003). $$$$$ Note that the first two similarity measures operate over nouns only, while the last can be applied to any word class.
An interesting way of quantifying the relative compositionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows (Baldwin et al, 2003). $$$$$ Effectively, we could use LSA to measure the extent to which two words or MWEs x and y usually occur in similar contexts.

They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Word net based decomposability theory (Baldwin et al, 2003). $$$$$ We attempt to achieve this by looking at the semantic similarity between an MWE and its constituent words, and hypothesising that where the similarity between the constituents of an MWE and the whole is sufficiently high, the MWE must be of simple decomposable type.
They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Word net based decomposability theory (Baldwin et al, 2003). $$$$$ This paper presents a constructioninspecific model of multiword expression decomposability based on latent semantic analysis.
They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Word net based decomposability theory (Baldwin et al, 2003). $$$$$ The particular reference lexicon we use to evaluate our technique is WordNet 1.7 (Miller et al., 1990), due to its public availability, hierarchical structure and wide coverage.
They evaluate their model on English NN compounds and verb-particles, and showed that the model correlated moderately well with the Word net based decomposability theory (Baldwin et al, 2003). $$$$$ Based on mean hyponymy over partitions of data ranked on similarity, we furnish evidence for the calculated similarities being correlated with the semantic relational content of WordNet.

The LSA model we built is similar to that described in (Schutze, 1998) and (Baldwin et al, 2003). $$$$$ Idiosyncratically decomposable MWEs (e.g. spill the beans, let the cat out of the bag, radar footprint) are decomposable but coerce their parts into taking semantics unavailable outside the MWE.
The LSA model we built is similar to that described in (Schutze, 1998) and (Baldwin et al, 2003). $$$$$ After removing stopwords, the 50,000 most frequent terms were indexed in each model.
The LSA model we built is similar to that described in (Schutze, 1998) and (Baldwin et al, 2003). $$$$$ First, 1000 frequent content words (i.e. not on the stoplist)l were chosen as “content-bearing words”.
The LSA model we built is similar to that described in (Schutze, 1998) and (Baldwin et al, 2003). $$$$$ We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.

 $$$$$ This is calculated over a corpus of text. where C0 is the lowest class in the hierarchy that subsumes both classes. lations” of different strength to determine the similarity of word senses, conditioned on the type, direction and relative distance of edges separating them.
 $$$$$ Our hypothesis of lesser instances of hyponymy for lower similarities is thus supported for low-frequency items but not for high-frequency items, suggesting that LSA similarities are more brittle over high-frequency items for this particular task.
 $$$$$ This material is partly based upon work supported by the National Science Foundation under Grant No.

Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. $$$$$ One area in which we plan to extend this research is the analysis of MWEs in languages other than English.
Katz and Giesbrecht (2006) and Baldwin et al (2003) use Latent Semantic Analysis for this purpose. $$$$$ He offers an evaluation where an item is said to be noncompositional if it occurs in a dictionary of idioms.

(Baldwin et al, 2003) use WordNet: :Similarity to provide an evaluation tool for multi word expressions that are identified via Latent Semantic Analysis. $$$$$ WordNet groups words into sets with similar meaning (known as “synsets”), e.g.
(Baldwin et al, 2003) use WordNet: :Similarity to provide an evaluation tool for multi word expressions that are identified via Latent Semantic Analysis. $$$$$ This is calculated over a corpus of text. where C0 is the lowest class in the hierarchy that subsumes both classes. lations” of different strength to determine the similarity of word senses, conditioned on the type, direction and relative distance of edges separating them.
(Baldwin et al, 2003) use WordNet: :Similarity to provide an evaluation tool for multi word expressions that are identified via Latent Semantic Analysis. $$$$$ Melamed (1997)), there has been little work on detecting “non-compositional” (i.e. non-decomposable and idiosyncratically decomposable) items of variable syntactic type in monolingual corpora.
(Baldwin et al, 2003) use WordNet: :Similarity to provide an evaluation tool for multi word expressions that are identified via Latent Semantic Analysis. $$$$$ The function hyponym(wordi, mwe) thus returns a value of 1 if some sense of wordz subsumes a sense of mwe, and a value of 0 otherwise.

Baldwin et al (2003) proposed a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability. $$$$$ This material is partly based upon work supported by the National Science Foundation under Grant No.
Baldwin et al (2003) proposed a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability. $$$$$ According to LSA, however, sim(chairman, vice chairman) = .508 and sim(president, vice president) = .551.
Baldwin et al (2003) proposed a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability. $$$$$ If our hypothesis is correct, the earlier partitions (with higher LSA similarities) will have higher occurrences of hyponyms than the latter partitions.
Baldwin et al (2003) proposed a LSA-based model for measuring the decomposability of MWEs by examining the similarity between them and their constituent words, with higher similarity indicating the greater decomposability. $$$$$ First, 1000 frequent content words (i.e. not on the stoplist)l were chosen as “content-bearing words”.

Baldwin et al (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. $$$$$ The hierarchy is structured according to different principles for each of nouns, verbs, adjectives and adverbs.
Baldwin et al (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. $$$$$ While recognising the dangers associated with dictionarybased evaluation, we commit ourselves to this paradigm and focus on searching for appropriate means of demonstrating the correlation between dictionary- and corpus-based similarities.
Baldwin et al (2003) investigate semantic decomposability of noun-noun compounds and verb constructions. $$$$$ WordNet groups words into sets with similar meaning (known as “synsets”), e.g.

In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al (2003). $$$$$ This material is partly based upon work supported by the National Science Foundation under Grant No.
In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al (2003). $$$$$ Using these contentbearing words as column labels, the 50,000 most frequent terms in the corpus were assigned row vectors by counting the number of times they oc'A “stoplist” is a list of frequent words which have little independent semantic content, such as prepositions and determiners (Baeza-Yates and Ribiero-Neto, 1999, p167). curred within the same sentence as a content-bearing word.
In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al (2003). $$$$$ One area in which we plan to extend this research is the analysis of MWEs in languages other than English.
In the above model, if a=0 and b=1, the resulting model is similar to that of Baldwin et al (2003). $$$$$ We use latent semantic analysis to determine the similarity between a multiword expression and its constituent words, and claim that higher similarities indicate greater decomposability.

Baldwin et al, (2003) focus more narrowly on distinguishing English noun-noun compound sand verb-particle constructions which are compositional from those which are not compositional. $$$$$ For the NN compounds, we derive two separate rankings, based on the similarity between the head noun and NN compound (NN(head)) and the modifier noun and the NN compound (NN(mod)).
Baldwin et al, (2003) focus more narrowly on distinguishing English noun-noun compound sand verb-particle constructions which are compositional from those which are not compositional. $$$$$ This material is partly based upon work supported by the National Science Foundation under Grant No.
Baldwin et al, (2003) focus more narrowly on distinguishing English noun-noun compound sand verb-particle constructions which are compositional from those which are not compositional. $$$$$ In an ideal world, we would hope that the values for mean hyponymy were nearly 1 for the first partition and nearly 0 for the last.
Baldwin et al, (2003) focus more narrowly on distinguishing English noun-noun compound sand verb-particle constructions which are compositional from those which are not compositional. $$$$$ In the case that a noun phrase followed the particle candidate, we performed attachment disambiguation to determine the transitivity of the particle candidate.

To compare our method with that proposed by Baldwin et al (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to theMWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE. $$$$$ BCS-0094638 and also the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University.
To compare our method with that proposed by Baldwin et al (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to theMWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE. $$$$$ He offers an evaluation where an item is said to be noncompositional if it occurs in a dictionary of idioms.
To compare our method with that proposed by Baldwin et al (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to theMWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE. $$$$$ What makes simple decomposable expressions true MWEs rather than productive word combinations is that they tend to block compositional alternates with the expected semantics (termed anticollocations by Pearce (2001b)).
To compare our method with that proposed by Baldwin et al (2003), we applied their method to our materials, generating LSA vectors for the component content words in our candidate MWEs and comparing their semantic similarity to theMWEs LSA vector as a whole, with the expectation being that low similarity between the MWE as a whole and its component words is indication of the non-compositionality of the MWE. $$$$$ Since the corpora had been tagged with parts-ofspeech, we could build syntactic distinctions into the LSA models — instead of just giving a vector for the string test we were able to build separate vectors for the nouns, verbs and adjectives test.

There is some evidence (Baldwin et al, 2003) that part of speech tagging might improve results in this kind of task. $$$$$ LSA is a method for representing words as points in a vector space, whereby words which are related in meaning should be represented by points which are near to one another, first developed as a method for improving the vector model for information retrieval (Deerwester et al., 1990).
There is some evidence (Baldwin et al, 2003) that part of speech tagging might improve results in this kind of task. $$$$$ Lin provides some examples that suggest he has identified a successful measure of “compositionality”.
There is some evidence (Baldwin et al, 2003) that part of speech tagging might improve results in this kind of task. $$$$$ We would like to thank the anonymous reviewers for their valuable input on this research.

Other approaches use Latent Semantic Analysis (LSA) to determine the similarity between a potential idiom and its components (Baldwin et al, 2003). $$$$$ Essentially, we used a POS tagger and chunker (both built using fnTBL 1.0 (Ngai and Florian, 2001)) to first (re)tag the BNC.
Other approaches use Latent Semantic Analysis (LSA) to determine the similarity between a potential idiom and its components (Baldwin et al, 2003). $$$$$ Effectively, we could use LSA to measure the extent to which two words or MWEs x and y usually occur in similar contexts.
Other approaches use Latent Semantic Analysis (LSA) to determine the similarity between a potential idiom and its components (Baldwin et al, 2003). $$$$$ Note that it is arguable whether simple decomposable MWEs belong in the grammar proper, or should be described instead as lexical affinities between particular word combinations.
