Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ However, the scheme heavily favors precision over recall.
Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.
Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ The weights in option 2 are determined by computing the accuracy of each parser on the held-out set (WSJ section 00).

In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ We present a framework for combining the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers.
In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.
In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ The deterministic parser of Yamada and Matsumoto (2003) uses an algorithm similar to Nivre and Scholz’s, but it makes several successive leftto-right passes over the input instead of keeping a stack.

To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ Zeman & Žabokrtský (2005) apply this dependency voting scheme to Czech with very strong results.
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ This way, we perform an exhaustive search for the tree that represents the heaviest combination of constituents that spans the entire sentence as a well-formed tree.
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ Dependencies extracted from section 00 were used as held-out data, and section 22 was used as additional development data.

Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn Treebank using the same head-table as Yamada and Matsumoto (2003), using sections 02-21 as training data and section 23 as test data, following (McDonald et al., 2005; Nivre & Scholz, 2004; Yamada & Matsumoto, 2003).
Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results.

A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ Zeman & Žabokrtský (2005) apply this dependency voting scheme to Czech with very strong results.
A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.

Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ We refer to this parser as LRRL.
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage.

Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ Over the past decade, remarkable progress has been made in data-driven parsing.
Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ The latter scheme performed better, producing remarkable results despite its simplicity.

An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ The deterministic parser of Yamada and Matsumoto (2003) uses an algorithm similar to Nivre and Scholz’s, but it makes several successive leftto-right passes over the input instead of keeping a stack.
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ The weights are simply the corresponding parser’s accuracy (number of correct dependencies divided by the total number of dependencies).
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ If projectivity (no crossing branches) is desired, Eisner’s (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead.
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score).

The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree.
The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ Option 2 takes into consideration that parsers may have different levels of accuracy, and dependencies proposed by more accurate parsers should be counted more heavily.

Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does not guarantee that the final voted set of dependencies will be a well-formed dependency tree.
Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does not guarantee that the final voted set of dependencies will be a well-formed dependency tree.
Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.

This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ In fact, the resulting graph may not even be connected.
This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results.
This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ This is done in a two stage process of reparsing.

Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.

(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage.
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ The large-margin parser described in (McDonald et al., 2005) was used with no alterations.

Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ If projectivity (no crossing branches) is desired, Eisner’s (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead.
Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ Once this graph is created, we reparse the sentence using a dependency parsing algorithm such as, for example, one of the algorithms described by McDonald et al. (2005).
Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ In dependency reparsing we focus on unlabeled dependencies, as described by Eisner (1996).
Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ We record the values of t for which precision and recall were closest, and for which f-score was highest.

Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage.
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.

Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ First, m parsers each produce one parse tree for an input sentence.
Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.
Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ This is done in a two stage process of reparsing.

However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ In the second stage, a parsing algorithm is applied to the original sentence, taking into account the analyses produced by each parser in the first stage.
However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.
However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ In fact, the resulting graph may not even be connected.
However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.

To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ Table 2 shows the accuracy of each individual parser and for three reparsing settings.
To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ Finding the optimal dependency structure given the set of weighted dependencies is simply a matter of finding the maximum spanning tree (MST) for the directed weighted graph, which can be done using the Chu-Liu/Edmonds directed MST algorithm (Chu & Liu, 1965; Edmonds, 1967).

 $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.
 $$$$$ In this scheme, the syntactic structure for a sentence with n words is a dependency tree representing head-dependent relations between pairs of words.
 $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.

We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ Instead, if we reparse the sentence based on the output of the m parsers, we can maximize the number of votes for a well-formed dependency structure.
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ Henderson and Brill’s voting scheme mentioned in section 3 can be emulated by our reparsing approach by setting all weights to 1.0 and t to (m + 1)/2, but better results can be obtained by setting appropriate weights and adjusting the precision/recall tradeoff.
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ The weights in option 2 are determined by computing the accuracy of each parser on the held-out set (WSJ section 00).
