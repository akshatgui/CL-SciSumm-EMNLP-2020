Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ Zeman & Žabokrtský (2005) apply this dependency voting scheme to Czech with very strong results.
Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ When m parsers each output a set of dependencies (forming m dependency structures) for a given sentence containing n words, the dependencies can be combined in a simple wordby-word voting scheme, where each parser votes for the head of each of the n words in the sentence, and the head with most votes is assigned to each word.
Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). $$$$$ However, when the constraint that structures must be well-formed is enforced, the accuracy of their results drops sharply.

In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993).
In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ This is done in a two stage process of reparsing.
In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ To increase parser diversity, we used a version of Yamada and Matsumoto’s algorithm where the direction of each of the consecutive passes over the input string alternates from left-to-right and right-to-left.
In the second approach, combination of the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. $$$$$ We refer to the two parsers as LR and RL.

To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure.
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ Option 2 takes into consideration that parsers may have different levels of accuracy, and dependencies proposed by more accurate parsers should be counted more heavily.
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.

Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ We then create weighted directed edges between the nodes corresponding to words for which dependencies are obtained from each of the initial structures.1 In cases where more than one dependency structure indicates that an edge should be created, the corresponding weights are simply added.
Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ The latter scheme performed better, producing remarkable results despite its simplicity.
Constant et al (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. $$$$$ Instead, if we reparse the sentence based on the output of the m parsers, we can maximize the number of votes for a well-formed dependency structure.

A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.
A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
A key difference with previous work on shift reduce dependency (Nivre et al, 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions - a shift action for each word-lexical category pair. $$$$$ Intuitively, this should increase precision, since we expect that a constituent that appears in the output of more parsers to be more likely to be correct.

Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree.
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993).
Sagae and Lavie (2006a) describes a shift-reduce parser for the Penn Treebank parsing task which uses best-first search to allow some ambiguity into the parsing process. $$$$$ We present a framework for combining the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers.

Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ This very simple scheme guarantees that the final set of dependencies will have as many votes as possible, but it does not guarantee that the final voted set of dependencies will be a well-formed dependency tree.
Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ The parameter t is set using held-out data (from WSJ section 22) and a simple hill-climbing procedure.
Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. $$$$$ This way, we perform an exhaustive search for the tree that represents the heaviest combination of constituents that spans the entire sentence as a well-formed tree.

An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ The weights in option 3 are determined in a similar manner, but different accuracy figures are computed for each part-of-speech.
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ The parameter t is set using held-out data (from WSJ section 22) and a simple hill-climbing procedure.
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ However, instead of measuring accuracy for each part-of-speech tag of dependents, we measure precision for each non-terminal label.
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). $$$$$ The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree.

The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.
The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al (2006). $$$$$ Over the past decade, remarkable progress has been made in data-driven parsing.

Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ Our approach produces results with accuracy above those of the best individual parsers on both dependency and constituent parsing of the standard WSJ test set.
Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ Setting 1 is the emulation of Henderson and Brill’s voting.
Moreover, beam search strategies can be used to expand the search space of a shift-reduce-based heterogeneous parser (Sagae and Lavie, 2006a). $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.

This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ In this scheme, the syntactic structure for a sentence with n words is a dependency tree representing head-dependent relations between pairs of words.
This group of features are completely identical to those used in Sagae and Lavie (2006a). $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.

Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ Their results on WSJ section 23 were 92.1 precision and 89.2 recall (90.61 f-score), well above the most accurate parser in their experiments (88.6 f-score).
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ The parsers that were used in the constituent reparsing experiments are: (1) Charniak and Johnson’s (2005) reranking parser; (2) Henderson’s (2004) synchronous neural network parser; (3) Bikel’s (2002) implementation of the Collins (1999) model 2 parser; and (4) two versions of Sagae and Lavie’s (2005) shift-reduce parser, one using a maximum entropy classifier, and one using support vector machines.
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. $$$$$ By combining several parsers with f-scores ranging from 91.0% to 86.7%, we obtain reparsed results with a 92.1% f-score.

(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ We present a framework for combining the output of several different accurate parsers to produce results that are superior to those of each of the individual parsers.
(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents. $$$$$ If projectivity (no crossing branches) is desired, Eisner’s (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead.

Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ The weights in option 2 are determined by computing the accuracy of each parser on the held-out set (WSJ section 00).
Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993).
Sagae and Lavie (2006) improve this second scheme by introducing a threshold for the constituent count, and search for the tree with the largest number of count from all the possible constituent combination. $$$$$ We have shown that in the case of dependencies, the reparsing approach successfully addresses the issue of constructing high-accuracy well-formed structures from the output of several parsers.

Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ This way, we perform an exhaustive search for the tree that represents the heaviest combination of constituents that spans the entire sentence as a well-formed tree.
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al., 1993).
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
Sagae and Lavie (2006) combine 5 parsers to obtain a score of 92.1, while they report a score of 91.0 for the best single parser in their paper. $$$$$ This simple vote resulted in trees with f-score significantly higher than the one of the best parser in the combination.

Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ In fact, years of extensive research on training and testing parsers on the Wall Street Journal (WSJ) corpus of the Penn Treebank have resulted in the availability of several high-accuracy parsers.
Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ The maximum spanning tree maximizes the votes for dependencies given the constraint that the resulting structure must be a tree.
Besides the two model scores, we also adopt constituent count as an additional feature inspired by (Henderson and Brill 1999) and (Sagae and Lavie 2006). $$$$$ The parameter t is set using held-out data (from WSJ section 22) and a simple hill-climbing procedure.

However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ Once no more constituent mergers are possible, the resulting constituents are placed on a standard parse chart, but where the constituents in the chart do not contain back-pointers indicating what smaller constituents they contain.
However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ Over the past decade, remarkable progress has been made in data-driven parsing.
However, as suggested in (Sagae and Lavie 2006), this feature favours precision over recall. $$$$$ Over the past decade, remarkable progress has been made in data-driven parsing.

To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ Over the past decade, remarkable progress has been made in data-driven parsing.
To solve this issue, Sagae and Lavie (2006) use a threshold to balance them. $$$$$ We start by decomposing each tree into its constituents, with each constituent being a 4-tuple [label, begin, end, weight], where label is the phrase structure type, such as NP or VP, begin is the index of the word where the constituent starts, end is the index of the word where the constituent ends plus one, and weight is the weight of the constituent.

 $$$$$ We then repeatedly evaluate the combination of parsers, each time decreasing the value of t (by 0.01, say).
 $$$$$ Instead of building a graph out of words (nodes) and dependencies (edges), in constituent reparsing we use the m initial trees to build a weighted parse chart.
 $$$$$ We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers.
 $$$$$ In constituent reparsing, held-out data can be used for setting a parameter that allows for balancing precision and recall, or increasing f-score.

We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ We have presented a reparsing scheme that produces results with accuracy higher than the best individual parsers available by combining their results.
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ First, m parsers each produce one parse tree for an input sentence.
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ In the first stage, m different parsers analyze an input sentence, each producing a syntactic structure.
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here). $$$$$ We apply this idea to dependency and constituent parsing, generating results that surpass state-of-theart accuracy levels for individual parsers.
