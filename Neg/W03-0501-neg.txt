A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al, 2003) and annotated with additional information (Zajic et al, 2004). $$$$$ relative clauses = 3/957 (.3%) determiners = 31/957 (3%); of these, only 16 were “a” or “the” (1.6% overall) S-LEVEL PERCENTAGES2 time expressions = 5/315 (1.5%) trailing PPs = 165/315 (52%) trailing SBARs = 24/315 (8%) 1 No response was given for one of the 73 stories.
A more common, extractive approach operates top-down, by starting from an extracted sentence that is compressed (Dorr et al, 2003) and annotated with additional information (Zajic et al, 2004). $$$$$ Finally, even when both systems produce acceptable output, Hedge Trimmer usually produces headlines which are more fluent or include more useful information. demanding that Chinese authorities respect culture.

HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al, 2003), and Topiary is our implementation of the Topiary system (Zajicet al, 2004). $$$$$ We parsed 218 human-produced headlines using the BBN parser and analyzed the results.
HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al, 2003), and Topiary is our implementation of the Topiary system (Zajicet al, 2004). $$$$$ We parsed 218 human-produced headlines using the BBN parser and analyzed the results.
HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al, 2003), and Topiary is our implementation of the Topiary system (Zajicet al, 2004). $$$$$ Headline generation can be viewed as analogous to statistical machine translation, where a concise document is generated from a verbose one using a Noisy Channel Model and the Viterbi search to select the most likely summarization.
HedgeTrimmer is our implementation of the Hedge Trimer system (Dorr et al, 2003), and Topiary is our implementation of the Topiary system (Zajicet al, 2004). $$$$$ We would like to thank Naomi Chang and Jon Teske for generating reference headlines.

For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ relative clauses = 3/957 (.3%) determiners = 31/957 (3%); of these, only 16 were “a” or “the” (1.6% overall) S-LEVEL PERCENTAGES2 time expressions = 5/315 (1.5%) trailing PPs = 165/315 (52%) trailing SBARs = 24/315 (8%) 1 No response was given for one of the 73 stories.
For this reason, most early headline generation work focused on either extracting and reordering n-grams from the document to be summarized (Banko et al., 2000), or extracting one or two informative sentences from the document and performing linguistically-motivated transformations to them in order to reduce the summary length (Dorr et al., 2003). $$$$$ Step 2 of our algorithm eliminates low-content units.

Hedge Trimmer (Dorr et al, 2003) is a system that creates a headline for an English newspaper story using linguistically-motivated heuristics to choose a potential headline. $$$$$ HMM Hedge was trained on 700,000 news articles and headlines from the TIPSTER corpus.
Hedge Trimmer (Dorr et al, 2003) is a system that creates a headline for an English newspaper story using linguistically-motivated heuristics to choose a potential headline. $$$$$ In contrast to original newspaper headlines, which are often intended only to catch the eye, our approach produces informative abstracts describing the main theme or event of the newspaper article.
Hedge Trimmer (Dorr et al, 2003) is a system that creates a headline for an English newspaper story using linguistically-motivated heuristics to choose a potential headline. $$$$$ As described in Miller et al. (1998), the BBN parser builds augmented parse trees according to a process similar to that described in Collins (1997).

The algorithm used by Dorr et al (2003) removes subordinate clauses, to name one example. $$$$$ Each headline was given a subjective score from 1 to 5, with 1 being the worst and 5 being the best.
The algorithm used by Dorr et al (2003) removes subordinate clauses, to name one example. $$$$$ This suggests that we should proceed with caution with respect to the deletion of trailing PPs; thus we consider this to be an option only if no other is available.
The algorithm used by Dorr et al (2003) removes subordinate clauses, to name one example. $$$$$ In our initial human inspection, we considered each of these categories to be reasonable candidates for deletion in our parse tree and this automatic analysis indicates that we have made reasonable choices for deletion, with the possible exception of trailing PPs, which show up in over half of the human-generated headlines.

 $$$$$ We compared our current system to a statistical headline generation system we presented at the 2001 DUC Summarization Workshop (Zajic et al., 2002), which we will refer to as HMM Hedge.
 $$$$$ We have shown the effectiveness of constructing headlines by selecting words in order from a newspaper story.

 $$$$$ Next, we describe the application of the parse-and-trim approach to the problem of headline generation.
 $$$$$ The University of Maryland authors are supported, in part, by BBNT Contract 020124-7157, DARPA/ITO Contract N66001-97-C-8540, and NSF CISE Research Infrastructure Award EIA0130422.
 $$$$$ We should be able to quickly produce a comparable system for other languages, especially in light of current multi-lingual initiatives that include automatic parser induction for new languages, e.g. the TIDES initiative.
 $$$$$ Edmundson, H. (1969).
