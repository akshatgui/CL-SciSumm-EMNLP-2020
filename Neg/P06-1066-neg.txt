We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ Lexicalized reordering model learns reorderings from training data, but it binds reorderings to individual concrete phrases, which restricts the model to reorderings of phrases seen in training data.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ We dropped unknown words (Koehn et al., 2005) of translations for both tasks before evaluating their BLEU scores.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.

Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ These features are very common in state-of-the-art systems (Koehn et al., 2005; Chiang, 2005) and As are weights of features.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ Different from Och, we just extend one word which is aligned to null on the boundary of target side.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ We define each vertex of block as corner.

In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ Lexicalized reordering model learns reorderings from training data, but it binds reorderings to individual concrete phrases, which restricts the model to reorderings of phrases seen in training data.

As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ We also use some rules to translate numbers, time expressions and Chinese person names.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ It finds the second-best of the final derivation, which makes its children to find their secondbest, and children’s children’s second-best, until the leaf node’s second-best.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ This model is obviously different from the monotone search, which does not use the inverted rule at all.

The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ Phrase reordering is of great importance for phrase-based SMT systems and becoming an active area of research recently.
The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ The reordering example extraction algorithm is shown in Figure 2.

Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ In view of content-independency of the distortion and flat reordering models, several researchers (Och et al., 2004; Tillmann, 2004; Kumar et al., 2005; Koehn et al., 2005) proposed a more powerful model called lexicalized reordering model that is phrase dependent.
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ In all our experiments, we set k = 200.
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ For the efficiency of minimum error rate training, we built our development set using sentences of length at most 50 characters from the NIST MT-02 evaluation test data.
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ Many systems use very simple models to reorder phrases 1.

Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ We also present experiments that indicate that the MaxEnt-based reordering model improves translation significantly compared with other reordering approaches and a state-of-the-art distortion-based system (Koehn, 2004).
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ Therefore the way to generate derivations in cell (i, j) is to merge derivations from any two neighbor sub-cells.
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ Both the bilingual training data and the trigram language model training data are restricted to the supplied corpus, which contains 20k sentences, 179k Chinese words and 157k English words.

For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ A good way to this problem is to use features of blocks as reordering evidences.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ Similarly, we call the topleft and bottomright INVERTED links since they link blocks with the inverted order.

Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ However, these operations are to be considered in our future version of model. used to translate source phrase y into target phrase x and generate a block A.
Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ In this paper we describe our system and the MaxEnt-based reordering model with the associated algorithm.
Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ Using all features for the MaxEnt model (lex + col) is marginally better than using only lex features (lex).
Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.

The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ Tillmann et. al (2005) also use a MaxEnt model to integrate various features.
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ Many systems use very simple models to reorder phrases 1.
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.

Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.
Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).

In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence $$$$$ It is very straight to use maximum entropy model to integrate features to predicate reorderings of blocks.
In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

 $$$$$ For example, if N words are skipped, a penalty of N will be paid regardless of which words are reordered.
 $$$$$ From this point, our decoder is similar to the work by Chiang (2005).
 $$$$$ This template-based scheme not only captures the reorderings of phrases, but also integrates some phrasal generalizations into the global model.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

 $$$$$ This means we can not use blocks as direct reordering evidences.
 $$$$$ The ITG not only decreases reorderings greatly but also makes reordering hierarchical.
 $$$$$ We carried out experiments to compare against various reordering models and systems to demonstrate the competitiveness of MaxEnt-based reordering: Our experiments were made on two Chinese-toEnglish translation tasks: NIST MT-05 (news domain) and IWSLT-04 (travel dialogue domain).
 $$$$$ It makes our model reorder any blocks, observed in training or not.

 $$$$$ For the lexical rule, applying it is assigned a where p(·) are the phrase translation probabilities in both directions, plex(·) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively.
 $$$$$ In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.
 $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.

For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ This straight way makes it clear how rules are used and what they depend on.
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ It can be said that their decoder did not violate the ITG constraints but not that it observed the ITG.
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ During decoding, we pruned the phrase table with b = 100 (default 20), pruned the chart with n = 100, a = 10−5 (default setting), and limited distortions to 4 (default 0).

 $$$$$ (2004).
 $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
 $$$$$ In their approach, translation is generated linearly, word by word and phrase by phrase in a traditional way with respect to the incorporation of the language model.
 $$$$$ You can attach the current block to each of these links.

 $$$$$ These words are nicely at the boundary of blocks.
 $$$$$ Then it finds the thirdbest, forth-best, and so on.
 $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
 $$$$$ However, our MaxEnt model is just a module of the whole log-linear model of translation which uses its score as a real-valued feature.

Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ In our Experiments, we just attach the smallest blocks to the STRAIGHT links, and the largest blocks to the INVERTED links.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ Line 12 and 14 extract reordering examples.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ Collocation features are defined on the combination s1 or t1 between two blocks b1 and b2.

The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ It can be easily generalized to reorder unseen phrases provided that some features are fired on these phrases.
The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ Lexicalized reordering model learns reorderings from training data, but it binds reorderings to individual concrete phrases, which restricts the model to reorderings of phrases seen in training data.
The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
