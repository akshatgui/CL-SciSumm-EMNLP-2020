We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ In his approach, phrases are reorganized into hierarchical ones by reducing subphrases to variables.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ In this paper, we propose a novel solution for phrasal reordering.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al, 2006) on the same corpus. $$$$$ Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.

Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
Some methods have been proposed to improve the reordering model for SMT based on the collocated words crossing the neighboring components (Xiong et al, 2006). $$$$$ 2000) in both directions, we apply the “growdiag-final” refinement rule on the intersection alignments for each sentence pair.

In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ However, our MaxEnt model is just a module of the whole log-linear model of translation which uses its score as a real-valued feature.
In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
In addition, a few models employed the collocation information to improve the performance of the ITG constraints (Xiong et al, 2006). $$$$$ The modularity afforded by this design does not incur any computation problems, and make it easier to update one sub-model with other modules unchanged.

As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ It represents how precisely the feature predicate the class.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
As the first step in this line of research, we explore the usage of FDT-based model training method in a phrase-based SMT system (Xiong et al 2006), which employs Bracketing Transduction Grammar (BTG) (Wu, 1997) to parse parallel sentences. $$$$$ For all experiments, we ran this trainer with the decoder iteratively to tune the weights As to maximize the BLEU score on the development set.

The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ For feature f and class c, the IGR(f, c) where En(·) is the entropy and En(·|·) is the conditional entropy.
The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ The trigram language model training data consists of English texts mostly derived from the English side of the UN corpus (catalog number LDC2004E12), which totally contains 81M English words.
The phrase-based SMT system proposed by Xiong et al 2006) is used as the baseline system, with a MaxEnt principle-based lexicalized reordering model integrated, which is used to handle reorderings in decoding. $$$$$ In this paper, we propose a novel solution for phrasal reordering.

Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ Another advantage of the MaxEnt-based reordering model is that it can take more features into reordering, even though they are nonindependent.
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ However, at the phrase level, reordering is still a computationally expensive problem just like reordering at the word level (Knight, 1999).
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ To do that, it is necessary for the MaxEnt model to incorporate real-valued features such as the block translation probability and the language model probability.
Xiong et al (2006) proposed a constituent reordering model for a bracketing transduction grammar (BTG) (Wu, 1995), which predicts the probability that a pair of subconstituents will reorder when combined to form a new constituent. $$$$$ Each corner has four links in four directions: topright, topleft, bottomright, bottomleft, and each link links a set of blocks which have the corner as their vertex.

Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ To do this, we keep an array to record link information of corners when extracting blocks.
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ It can be said that their decoder did not violate the ITG constraints but not that it observed the ITG.
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ From this point, our decoder is similar to the work by Chiang (2005).
Another extension would try to reorder not words but phrases, following (Xiong et al, 2006), or segment choice models (Kuhn et al, 2006), which assume a single segmentation of the words into phrases. $$$$$ On both tasks, we observe that various reordering approaches show similar and stable performance ranks in different domains and the MaxEnt-based reordering models achieve the best performance among them.

For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ However, since reorderings are related to concrete phrases, researchers have to design their systems carefully in order not to cause other problems, e.g. the data sparseness problem.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ When the whole input sentence is covered, the decoding is over.
For distortion modeling, Li et al (2013) use recursive auto encoders to make full use of the entire merging phrase pairs, going beyond the boundary words with a maximum entropy classifier (Xiong et al, 2006). $$$$$ Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant. bor blocks.

Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ The last one is the histogram pruning which only keeps the top n best derivations for each cell.
Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ For the reordering model Q, we define it on the two consecutive blocks A1 and A2 and their order Under this framework, different reordering models can be designed.
Our baseline decoder is an in-house implementation of Bracketing Transduction Grammar (BTG) (Wu, 1997) in CKY-style decoding with a lexical reordering model trained with maximum entropy (Xiong et al, 2006). $$$$$ In this section, we discuss how to create a maximum entropy based reordering model.

The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ As described above, we defined the reordering model Q on the three factors: order o, block A1 and block A2.
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ Good features can not only capture reorderings, avoid sparseness, but also integrate generalizations.
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ So the phrasal reordering is totally dependent on the language model.
The second one (SYS2) is a phrase-based system (Xiong et al, 2006) based on Bracketing Transduction Grammar (Wu, 1997) with a lex icalized reordering model (Zhang et al, 2007) under maximum entropy framework, where the phrasal translation rules are exactly the same with that of SYS1. $$$$$ Another simple model is flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) which is not content dependent either.

Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ The model provides content-dependent, hierarchical phrasal reordering with generalization based on features automatically learned from a real-world bitext.
Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
Xiong et al (2006) is an enhanced bracket transduction grammar with a maximum entropy-based reordering model (MEBTG). $$$$$ It can be easily generalized to reorder unseen phrases provided that some features are fired on these phrases.

In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule. $$$$$ Therefore the way to generate derivations in cell (i, j) is to merge derivations from any two neighbor sub-cells.
In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule. $$$$$ We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.
In MEBTG (Xiong et al, 2006), three rules are used to derive the translation of each sub sentence: lexical rule, straight rule and inverted rule. $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.

 $$$$$ Performance gains have been reported for systems with lexicalized reordering model.
 $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
 $$$$$ Secondly, features will be extracted from reordering examples according to feature templates.

 $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
 $$$$$ These features are very common in state-of-the-art systems (Koehn et al., 2005; Chiang, 2005) and As are weights of features.
 $$$$$ 2000) in both directions, we apply the “growdiag-final” refinement rule on the intersection alignments for each sentence pair.
 $$$$$ However, in our model this way can’t work because blocks become larger and larger due to using the merging rules, and finally unseen in the training data.

 $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
 $$$$$ In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
 $$$$$ This work was supported in part by National High Technology Research and Development Program under grant #2005AA114140 and National Natural Science Foundation of China under grant #60573188.

For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ We propose a novel reordering model for phrase-based statistical machine translation (SMT) that uses a maximum entropy (MaxEnt) model to predicate reorderings of neighbor blocks (phrase pairs).
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ Performance gains have been reported for systems with lexicalized reordering model.
For example, the MaxEnt reordering model described in (Xiong et al, 2006) provides a hierarchical phrasal reordering system integrated within a CKY-style decoder. $$$$$ In our experiments on Chineseto-English translation, this MaxEnt-based reordering model obtains significant improvements in BLEU score on the NIST MT-05 and IWSLT-04 tasks.

 $$$$$ To be consistent with the official evaluation criterions of both tasks, casesensitive BLEU-4 scores were computed For the NIST MT-05 task and case-insensitive BLEU-4 scores were computed for the IWSLT-04 task 5.
 $$$$$ The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation.
 $$$$$ We present an algorithm to extract all reordering events of neighbor blocks from bilingual data.

 $$$$$ Another advantage of the MaxEnt-based reordering model is that it can take more features into reordering, even though they are nonindependent.
 $$$$$ The central problem is, given two neighbor blocks A1 and A2, how to predicate their order o ∈ {straight, inverted}.
 $$$$$ In the task of NIST MT-05, we obtained about 2.7M reordering examples with the straight order, and 367K with the inverted order, from which 112K lexical features and 1.7M collocation features after deleting those with one occurrence were extracted.
 $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.

Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ The topright and bottomleft link blocks with the straight order, so we call them STRAIGHT links.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ For the reordering model Q, we define it on the two consecutive blocks A1 and A2 and their order Under this framework, different reordering models can be designed.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ The last one is the maximum entropy based reordering model proposed by us, which will be described in the next section.
Without loss of generality, we evaluate our models in a phrase-based SMT system which adapts bracketing transduction grammars to phrasal translation (Xiong et al, 2006). $$$$$ For this task, our experiments were carried out on the small data track.

The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ Beyond the MaxEnt-based reordering model, another feature deserving attention in our system is the CKY style decoder which observes the ITG.
The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ Special thanks to Yajuan L¨u for discussions of the manuscript of this paper and three anonymous reviewers who provided valuable comments.
The reordering model MR predicts the merging order (straight or inverted) by using discriminative contextual features (Xiong et al, 2006). $$$$$ Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possible to reorder phrases hierarchically.
