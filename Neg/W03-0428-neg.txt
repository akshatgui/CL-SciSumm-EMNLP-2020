Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers. $$$$$ Table 3 shows an example of a local decision for Grace in the context at Grace Road, using all of the features defined to date.
Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers. $$$$$ However, because of data sparsity, sophisticated unknown word models are generally required for good performance.
Klein et al (2003) also applied the related Conditional Markov Models for combining classifiers. $$$$$ One then treats the unknown word as a collection of such features.

Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models. $$$$$ Adding even just the previous and next words and tags as (atomic) features raised performance to 82.39%.
Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models. $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.
Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models. $$$$$ The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.
Klein et al (2003) employed a stacked learning system which contains Hidden Markov Models, Maximum Entropy Models and Conditional Markov Models. $$$$$ Therefore, a state like (PERSON, 2) indicates the second letter inside a person phrase.

Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003). $$$$$ Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data).
Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003). $$$$$ The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.
Here there is no significant difference between them and the systems of Klein et al (2003) and Zhang and Johnson (2003). $$$$$ For emissions, we must estimate a quantity of the form , for example, .1 We use an -gram model of order .2 The -gram estimates are smoothed via deleted interpolation.

The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German. $$$$$ In an HMM with an admittedly very local sequence model, switching from a word model to a character model gave an error reduction of about 30%.
The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German. $$$$$ Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data).
The performance of the system of Chieu et al (2003) was not significantly different from the best performance for English and the method of Klein et al (2003) and the approach of Zhang and Johnson (2003) were not significantly worse than the best result for German. $$$$$ Note that the evidence against Grace as a name completely overwhelms the -gram and word preference for PERSON.

 $$$$$ Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character grams is new.
 $$$$$ Given the amount of improvement from using a model backed by character -grams instead of word -grams, the immediate question is whether this benefit is complementary to the benefit from features which have traditionally been of use in word level systems, such as syntactic context features, topic features, and so on.
 $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.
 $$$$$ In the final, much richer chained maxent setting, the reduction from the best model minus -gram features to the reported best model was about 25% – smaller, but still substantial.

 $$$$$ In an HMM with an admittedly very local sequence model, switching from a word model to a character model gave an error reduction of about 30%.
 $$$$$ We found knowing that the previous word was an other wasn’t particularly useful without also knowing its part-of-speech (e.g., a preceding preposition might indicate a location).
 $$$$$ The final system also contained some simple error-driven postprocessing.

Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. $$$$$ Table 3 shows an example of a local decision for Grace in the context at Grace Road, using all of the features defined to date.
Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. $$$$$ In an HMM with an admittedly very local sequence model, switching from a word model to a character model gave an error reduction of about 30%.
Character n-gram based approach (Klein et al, 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. $$$$$ In essence, encoding into the state lets us distinguish the beginnings of phrases, which lets us model trends like named entities (all the classes besides other) generally starting with capital letters in English.

We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information. $$$$$ Adding features representing the previous classification decision ( ) raised the score 2.35% to 85.44%.
We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information. $$$$$ Our best model achieves an overall F of 86.07% on the English test data (92.31% on the development data).
We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information. $$$$$ One then treats the unknown word as a collection of such features.
We plan to experiment with the character n-gram approach (Klein et al, 2003) and include gazetteer information. $$$$$ Table 4 gives a more detailed breakdown of this score, and also gives the results of this system on the English test set, and both German data sets.

We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005). $$$$$ The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.
We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005). $$$$$ In the final, much richer chained maxent setting, the reduction from the best model minus -gram features to the reported best model was about 25% – smaller, but still substantial.
We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005). $$$$$ In total, this final system had an F-score of 92.31% on the English development set.
We experimented with a conditional Markov model tagger that performed well on language-independent NER (Klein et al, 2003) and the identification of gene and protein names (Finkel et al, 2005). $$$$$ The remaining improvements involved a number of other features which directly targetted observed error types.

Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003). $$$$$ The primary argument of this paper is that character substrings are a valuable, and, we believe, underexploited source of model features.
Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003). $$$$$ At this point, the bulk of outstanding errors were plausibly attributable to insufficient context information.
Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003). $$$$$ This paper also again demonstrates how the ease of incorporating features into a discriminative maxent model allows for productive feature engineering.
Where the best results for English with Stanford NER CRF gave a precision of 86.1 percent, a recall of 86.5 percent and F-score of 86.3 percent, for German the best results had a precision of 80.4 percent, a recall of 65.0 percent and an F-score of 71.9 percent, (Klein et al, 2003). $$$$$ The word-level model and the (context disabled) character-level model are intended as a rough minimal pair, in that the only information crossing phrase boundaries was the entity type, isolating the effects of character- vs word-level modeling (a more precise minimal pair is examined in section 3).

Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003). $$$$$ Adding a current tag feature gave a score of 74.17%.
Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003). $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.
Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003). $$$$$ The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.
Another approach we will also focus is dividing words into characters and applying character-level models (Klein et al, 2003). $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.

Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ Adding a current tag feature gave a score of 74.17%.
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge.
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ We present two models in which the basic units are characters and character -grams, instead of words and word phrases.
Since the introduction of this task in MUC-6 (Grishman and Sundheim, 1996), numerous systems using various ways of exploiting entity-specific and local context features were proposed, from relatively simple character based models such as Cucerzan and Yarowsky (2002) and Klein et al (2003) to complex models making use of various lexical, syntactic, morphological, and orthographical information, such as Wacholder et al (1997), Fleischman and Hovy (2002), and Florian et al (2003). $$$$$ These features included letter type pattern features (for example 20-month would become d-x for digitlowercase and Italy would become Xx for mixed case).

Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information. $$$$$ More complex, joint context features which paired the current word and tag with the previous and next words and tags raised the score further to 83.09%, nearly to the level of the HMM, still without actually having any model of previous classification decisions.
Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information. $$$$$ Here, we examine the utility of taking character sequences as a primary representation.
Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information. $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.
Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al, 2003) and the 2004 BioCreative critical assessment of information. $$$$$ Each state’s identity depends only on the previous state.

Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003). $$$$$ Using the substring features alone scored 73.10%, already breaking the the phrase-based CoNLL baseline, though lower than the no-context HMM, which better models the context inside phrases.
Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003). $$$$$ We give F both per-category and overall.
Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003). $$$$$ The first model is a character-level HMM with minimal context information, and the second model is a maximum-entropy conditional markov model with substantially richer context features.
Sometimes, these types of features are referred to as word-external and word-internal (Klein et al, 2003). $$$$$ In an HMM with an admittedly very local sequence model, switching from a word model to a character model gave an error reduction of about 30%.

In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003). $$$$$ In total, this final system had an F-score of 92.31% on the English development set.
In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003). $$$$$ Adding a current tag feature gave a score of 74.17%.
In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al 2003). $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.

The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%. $$$$$ We found knowing that the previous word was an other wasn’t particularly useful without also knowing its part-of-speech (e.g., a preceding preposition might indicate a location).
The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%. $$$$$ This number represents a 25% error reduction over the same model without word-internal (substring) features.
The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%. $$$$$ Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge.
