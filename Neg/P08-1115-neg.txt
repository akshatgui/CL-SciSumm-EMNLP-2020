The concept of compatible coverage vectors for the locations of translated words becomes the notion of reachability between frontier nodes in the lattice (Dyer et al, 2008). $$$$$ Applications of source lattices outside of the domain of spoken language translation have been far more limited.
The concept of compatible coverage vectors for the locations of translated words becomes the notion of reachability between frontier nodes in the lattice (Dyer et al, 2008). $$$$$ Even if the results are not statistically significant for MT06, there is a high decrease in OOV items when using word-lattices.
The concept of compatible coverage vectors for the locations of translated words becomes the notion of reachability between frontier nodes in the lattice (Dyer et al, 2008). $$$$$ HR0011-06-2-0001.

We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct lattice based decoding (Dyer et al, 2008). $$$$$ To generalize this model to word lattices, it is necessary to choose both a path through the lattice and a partitioning of the sentence this induces into a sequence of phrases f1.
We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct lattice based decoding (Dyer et al, 2008). $$$$$ This research was supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.
We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct lattice based decoding (Dyer et al, 2008). $$$$$ Lattice Translation.
We also plan to jointly optimize MT and name tagging by propagating multiple word segmentation and name annotation hypotheses in lattice structure to statistical MT and conduct lattice based decoding (Dyer et al, 2008). $$$$$ We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations.

In applications like the one described by Dyer et al (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. $$$$$ The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences).
In applications like the one described by Dyer et al (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. $$$$$ The authors wish to thank Niyu Ge for the Chinese named-entity analysis, Pi-Chuan Chang for her assistance with the Stanford Chinese segmenter, and Tie-Jun Zhao and Congui Zhu for making the Harbin Chinese segmenter available to us.
In applications like the one described by Dyer et al (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. $$$$$ (2007)) comment directly on the impracticality of using n-best lists to translate speech.
In applications like the one described by Dyer et al (2008), where several different segmenters for Chinese are combined to create the lattice, this is not possible. $$$$$ Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06).

Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses. $$$$$ Additionally, Grune and Jacobs (2008) refines an algorithm originally due to Bar-Hillel for intersecting an arbitrary FSA (of which word lattices are a subset) with a CFG.
Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses. $$$$$ Matusov et al. (2005) decodes monotonically and then uses a finite state reordering model on the single-best translation, along the lines of Bangalore and Riccardi (2000).
Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses. $$$$$ Phrase-based models translate a foreign sentence f into the target language e by breaking up f into a sequence of phrases f1, where each phrase fz can contain one or more contiguous words and is translated into a target phrase ez of one or more contiguous words.
Dyer et al (2008) use it to encode different Chinese word segmentations or Arabic morphological analyses. $$$$$ For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005).

Our work differs from (Dyer et al, 2008) in that we explicitly distinguish the various preprocessing types in the lattice so that we can define specific path features and lexicalize the lattice path probabilities within the phrase model. $$$$$ HR0011-06-2-0001.
Our work differs from (Dyer et al, 2008) in that we explicitly distinguish the various preprocessing types in the lattice so that we can define specific path features and lexicalize the lattice path probabilities within the phrase model. $$$$$ We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance.
Our work differs from (Dyer et al, 2008) in that we explicitly distinguish the various preprocessing types in the lattice so that we can define specific path features and lexicalize the lattice path probabilities within the phrase model. $$$$$ Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f' E F(o) and we seek An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f. There, Bertoldi and others have recently found that, rather than translating a single-best transcription f, it is advantageous to allow the MT decoder to = arg max max Pr(e)Pr(f'|e)Pr(o|f')�(4) e f�EF(o) Following Och and Ney (2002), we use the maximum entropy framework (Berger et al., 1996) to directly model the posterior Pr(e, f'|o) with parameters tuned to minimize a loss function representing the quality only of the resulting translations.

It was noted by Dyer et al (2008) that the standard distance-based reordering model needs to be redefined for lattice input. $$$$$ In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input.
It was noted by Dyer et al (2008) that the standard distance-based reordering model needs to be redefined for lattice input. $$$$$ On both test sets, the shortest path metric improved the BLEU scores.
It was noted by Dyer et al (2008) that the standard distance-based reordering model needs to be redefined for lattice input. $$$$$ This research was supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.

Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints. $$$$$ Table 1 shows the word lattice from Figure 1 represented in matrix form as hF, p, Ri.
Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints. $$$$$ We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations.
Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints. $$$$$ Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06).
Using the shortest path within the lattice is reported to have better performance in (Dyer et al, 2008), however we did not implement it due to time constraints. $$$$$ The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant.

Our word lattices are similar to those used by Dyer et al (2008) for handling word segmentation in Chinese and Arabic. $$$$$ If [0,1] is translated, the decoder must not consider translating [2,3] as a possible extension of this hypothesis since there is no path from node 1 to node 2 and therefore the span [1,2] would never be covered.
Our word lattices are similar to those used by Dyer et al (2008) for handling word segmentation in Chinese and Arabic. $$$$$ Although translation is fundamentally a nonmonotonic relationship between most language pairs, reordering has tended to be a secondary concern to the researchers who have worked on lattice translation.
Our word lattices are similar to those used by Dyer et al (2008) for handling word segmentation in Chinese and Arabic. $$$$$ We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate.

Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ The authors wish to thank Niyu Ge for the Chinese named-entity analysis, Pi-Chuan Chang for her assistance with the Stanford Chinese segmenter, and Tie-Jun Zhao and Congui Zhu for making the Harbin Chinese segmenter available to us.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ Several authors (e.g.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ In large lattices, where a single arc may span many nodes, the possible distances may vary quite substantially depending on what path is ultimately taken, and handling this properly therefore crucial.
Recent studies have shown that SMT systems can benefit from widening the annotation pipeline $$$$$ This research was supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.

Lattice represent the system implemented as Dyer et al, (2008). $$$$$ Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well.
Lattice represent the system implemented as Dyer et al, (2008). $$$$$ Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model.
Lattice represent the system implemented as Dyer et al, (2008). $$$$$ The intuition behind this model is that since most translation is monotonic, the cost of skipping ahead or back in the source should be proportional to the number of words that are skipped.

Same as Dyer et al, (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters. $$$$$ In the parser that forms the basis of the hierarchical decoder described in Section 2.3, no such restriction is necessary since grammar rules are processed in a strictly left-to-right fashion without any skips.
Same as Dyer et al, (2008), we also extracted rules from a combined bilingual corpus which contains three copies from different segmenters. $$$$$ Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model.

Du et al (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al, 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. $$$$$ Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models.
Du et al (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al, 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. $$$$$ We observed that the coverage of named entities (NEs) in our baseline systems was rather poor.
Du et al (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al, 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. $$$$$ Table 2 summarizes the results of the phrasebased systems.

Finally, some researchers have advocated recently the use of shared structures such as parse forests (Mi and Huang, 2008) or word lattices (Dyer et al, 2008) in order to allow a compact representation of alternative inputs to an SMT system. $$$$$ Bertoldi et al. (2007) solve the problem by requiring that their input be in the format of a confusion network, which enables the standard distortion penalty to be used.
Finally, some researchers have advocated recently the use of shared structures such as parse forests (Mi and Huang, 2008) or word lattices (Dyer et al, 2008) in order to allow a compact representation of alternative inputs to an SMT system. $$$$$ Word-lattice translation offers two possible improvements over the conventional approach.
Finally, some researchers have advocated recently the use of shared structures such as parse forests (Mi and Huang, 2008) or word lattices (Dyer et al, 2008) in order to allow a compact representation of alternative inputs to an SMT system. $$$$$ Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model.

All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them. $$$$$ This research was supported by the GALE program of the Defense Advanced Research Projects Agency, Contract No.
All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them. $$$$$ The authors wish to thank Niyu Ge for the Chinese named-entity analysis, Pi-Chuan Chang for her assistance with the Stanford Chinese segmenter, and Tie-Jun Zhao and Congui Zhu for making the Harbin Chinese segmenter available to us.
All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them. $$$$$ We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate.
All of the systems we present use the lattice input format to Moses (Dyer et al, 2008), including the baselines which do not need them. $$$$$ Evaluation was carried out on the NIST 2005 and 2006 evaluation sets (MT05, MT06).

Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmentations with word lattices (Dyer et al, 2008). $$$$$ Our results generalize previous gains for lattice translation of spoken language input, and we have further generalized the approach by introducing an algorithm for lattice decoding using a hierarchical phrase-based model.
Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmentations with word lattices (Dyer et al, 2008). $$$$$ In large lattices, where a single arc may span many nodes, the possible distances may vary quite substantially depending on what path is ultimately taken, and handling this properly therefore crucial.
Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmentations with word lattices (Dyer et al, 2008). $$$$$ The testing was done on the NIST 2005 and 2006 evaluation sets (MT05, MT06).
Recently, several studies have shown that offering more alternatives of annotations to SMT systems will result in significant improvements, such as replacing 1-best trees with packed forests (Miet al, 2008) and replacing 1-best word segmentations with word lattices (Dyer et al, 2008). $$$$$ Note that Ri,j > i for all i, j.

Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider $$$$$ Arabic orthography is problematic for lexical and phrase-based MT approaches since a large class of functional elements (prepositions, pronouns, tense markers, conjunctions, definiteness markers) are attached to their host stems.
Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider $$$$$ The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an E-edge without advancing the dot in an incomplete rule 3) advance the dot across a nonterminal symbol given appropriate antecedents.
Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider $$$$$ The authors wish to thank Niyu Ge for the Chinese named-entity analysis, Pi-Chuan Chang for her assistance with the Stanford Chinese segmenter, and Tie-Jun Zhao and Congui Zhu for making the Harbin Chinese segmenter available to us.
Recent studies has shown that SMT systems can benefit from making the annotation pipeline wider $$$$$ Why, however, should this advantage be limited to translation from spoken input?

As lattice is a more general form of confusion network (Dyer et al, 2008), we expect that replacing confusion networks with lattices will further improve system combination. $$$$$ We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance.
As lattice is a more general form of confusion network (Dyer et al, 2008), we expect that replacing confusion networks with lattices will further improve system combination. $$$$$ We have achieved substantial gains in translation performance by decoding compact representations of alternative source language analyses, rather than single-best representations.
As lattice is a more general form of confusion network (Dyer et al, 2008), we expect that replacing confusion networks with lattices will further improve system combination. $$$$$ Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases.
As lattice is a more general form of confusion network (Dyer et al, 2008), we expect that replacing confusion networks with lattices will further improve system combination. $$$$$ Word lattice decoding has proven useful in spoken language translation; we argue that it provides a compelling model for translation of text genres, as well.

Our implementation's runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al, 2008). $$$$$ Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007).
Our implementation's runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al, 2008). $$$$$ FSAs where every path does not pass through every node.
Our implementation's runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al, 2008). $$$$$ We assume that for many interesting cases of F(o), there will be identical substrings that express the same content, and therefore a lattice representation is appropriate.
Our implementation's runtime and memory overhead is proportional to the size of the lattice, rather than the number of paths in the lattice (Dyer et al, 2008). $$$$$ The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically significant.

Lattice parsing is not new to translation (Dyer et al, 2008), but to our knowledge it has not been used in this way. $$$$$ We show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models.
Lattice parsing is not new to translation (Dyer et al, 2008), but to our knowledge it has not been used in this way. $$$$$ We used both a phrase-based translation model, decoded using our modified version of Moses (Koehn et al., 2007), and a hierarchical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007).
Lattice parsing is not new to translation (Dyer et al, 2008), but to our knowledge it has not been used in this way. $$$$$ Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models.
Lattice parsing is not new to translation (Dyer et al, 2008), but to our knowledge it has not been used in this way. $$$$$ Arabic morphological variation.

Dyer et al (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. $$$$$ In the parser that forms the basis of the hierarchical decoder described in Section 2.3, no such restriction is necessary since grammar rules are processed in a strictly left-to-right fashion without any skips.
Dyer et al (2008) report improvements from multiple Arabic segmentations in translation to English translation, but their goal was to demonstrate the value of lattice-based translation. $$$$$ That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.3 A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments.
