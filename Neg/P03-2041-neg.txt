(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ However, they have a tendency to translate long sentences into word salad.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ A probabilistic TSG also includes a function p(t  |q), which, for each state q, gives a conditional probability distribution over the elementary trees t with root state q.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ The function ` : (V i U E) → L labels each internal node or edge; q E Q is the root state, and s : V f → Q assigns a frontier state to each frontier node (perhaps including r).

(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ One alternates O� converges to a local maximum.
(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ However, we would like to estimate a model from unaligned data.

Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ The generation process uses this distribution to randomly choose which tree t to substitute at a frontier node of T having state q.
Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ It is even helpful to learn mismatches that merely tend to arise during free translation.
Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ Note that least one English node must remain unmatched; it still generates a full subtree, aligned with null.
Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.

For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ Thus, donnent un baiser a` (“give a kiss to”) corresponds to kiss, with the French subject matched to the English subject, and the French indirect object matched to the English direct object.
For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ Note that the depicted trees are not isomorphic. enfants Our main concern is to develop models that can align and learn from these tree pairs despite the “mismatches” in tree structure.
For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ (If dj is null, then t.q must guarantee that tj is the special null tree.)

In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ Let Q be a set of states.
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ This short paper outlines “natural” formalisms and algorithms for training on pairs of trees.
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ An elementary tree t with root state q and frontier states ql ... qk (for k > 0) is analogous to a CFG rule q → t ql ... qk.

 $$$$$ (By including t as a terminal symbol in this rule, we ensure that distinct elementary trees t with the same states correspond to distinct rules.)
 $$$$$ Third, our version of the STSG formalism is more flexible than previous versions.

 $$$$$ As long as T. m¯ =� 0, the process expands some node pair (d1, d2) E T. ¯m.
 $$$$$ The generation process uses this distribution to randomly choose which tree t to substitute at a frontier node of T having state q.
 $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.

A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous, potentially idiomatic units (Och et al., 1999).
A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ This rigidity does not fully describe real data.
A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ One alternates O� converges to a local maximum.

To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ For expository reasons (and to fill a gap in the literature), first we formally present non-synchronous TSG.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ We carefully address the case of empty trees, which are needed to handle freetranslation “mismatches.” In the example, an STSG cannot replace beaucoup d’ (“lots of”) in the NP by quite often in the VP; instead it must delete the former and insert the latter.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous, potentially idiomatic units (Och et al., 1999).
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ The latter are small and simple (Alshawi et al., 2000): tree nodes are words, and there need be no other structure to recover or align.

If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ Unfortunately, that clever method does not permit arbitrary models of elementary tree probabilities, nor does it appear to generalize to our synchronous case.
If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ Any STSG has a weakly equivalent SCFG that generates the same string pairs.
If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ Indeed, an equivalent definition of the generation process first generates a derivation tree from this derivation CFG, and then combines its terminal nodes t (which are elementary trees) into the derived tree T.

Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ Node labels might be words or nonterminals.
Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ The outer loop iterates bottom-up over nodes c1 of T1; an inner loop iterates bottom-up over c2 of T2.
Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ Our methods work on either dependency trees (as shown) or phrase-structure trees.

We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). $$$$$ Then for each j = 1, 2, it substitutes tj at dj if non-null.
We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). $$$$$ Several natural algorithms are now available to us: • Training.

A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ The runtime is quite low when the training trees are fully specified and elementary trees are bounded in size.1 Second, it is not a priori obvious that one can reasonably use STSG instead of the slower but more powerful STAG.
A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ Finally, choose t.q freely from Q, and choose s : t.Vf → Q to associate states with the frontier nodes of t; the free choice is because the nodes of the derived tree T do not specify the states used during the derivation.

More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ For example, (beaucoup d’enfants donnent un baiser a` Sam, kids kiss Sam quite often).
More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ Their alignment and translation accuracy improves when they are forced to translate shallow phrases as contiguous, potentially idiomatic units (Och et al., 1999).
More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ Given a a grammar G and a derived tree T, we may be interested in constructing the forest of T’s possible derivation trees (as defined above).

Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ Finally, choose t.q freely from Q, and choose s : t.Vf → Q to associate states with the frontier nodes of t; the free choice is because the nodes of the derived tree T do not specify the states used during the derivation.
Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ These will constitute the internal nodes of t, and their remaining children will be t’s frontier nodes.
Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ This rigidity does not fully describe real data.

 $$$$$ Given an unaligned tree pair (T1, T2), we can again find the forest of all possible derivations, with expected inside-outside counts of the elementary tree pairs.
 $$$$$ The one exception is the synchronous DOP approach of (Poutsma, 2000), which obtains an STSG by decomposing aligned training trees in all possible ways (and using “naive” count-based probability estimates).
 $$$$$ Unlike previous statistical formalisms (limited to isomorphic TSG local distortion of the tree topology.
 $$$$$ In the figure of section 2, donnent un baiser a` has 2 frontier nodes and kiss has 3, yielding 13 possible matchings.

We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ As an example, the tree pair shown in the introduction might have been derived by “vertically” assembling the 6 elementary tree pairs below.
We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ Many “mismatches” are characteristic of a language pair: e.g., preposition insertion (of → c), multiword locutions (kiss H give a kiss to; misinform H wrongly inform), and head-swapping (float down H descend byfloating).
We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ This short paper outlines “natural” formalisms and algorithms for training on pairs of trees.

Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ Systems for “deep” analysis and generation might wish to learn mappings between deep and surface trees (B¨ohmov´a et al., 2001) or between syntax and semantics (Shieber and Schabes, 1990).
Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.

Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ In the probabilistic case, we have a distribution p(t  |q) just as before, but this time t is an elementary tree pair.
Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ This is just like training, except that we use the Viterbi algorithm to find the single best derivation of the input tree pair.
Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ We call this tree parsing, as it finds ways of decomposing T into elementary trees.

As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003) $$$$$ As long as T. m¯ =� 0, the process expands some node pair (d1, d2) E T. ¯m.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003) $$$$$ This derivation can be regarded as the optimal syntactic alignment.7 We then extract the max-probability synchronous derivation and return the T2 that it derives.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003) $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003) $$$$$ The tree-internal deletion of beaucoup d’ is handled by an empty elementary tree in which the root is itself a frontier node.
