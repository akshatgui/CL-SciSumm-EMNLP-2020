(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ Selectional preferences and other interactions can be accommodated by enriching the states.
(Eisner, 2003) outlines a computationally expensive structural manipulation tool which he has used for intra-lingual translation but has yet to apply to interlingual translation. $$$$$ The generation process uses this distribution to randomly choose which tree t to substitute at a frontier node of T having state q.

(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ (By including t as a terminal symbol in this rule, we ensure that distinct elementary trees t with the same states correspond to distinct rules.)
(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ One alternates O� converges to a local maximum.
(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
(Eisner, 2003) presents a tree-mapping method for use on dependency trees which he claims can be adapted for use with PS trees. $$$$$ (By including t as a terminal symbol in this rule, we ensure that distinct elementary trees t with the same states correspond to distinct rules.)

Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ Edge labels might include grammatical roles such as Subject.
Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ This translation is somewhat free, as is common in naturally occurring data.
Those systems use synchronous context-free grammars (Chiang, 2007), synchronous tree substitution grammars (Eisner, 2003) or even more powerful formalisms like synchronous tree-sequence substitution grammars (Sun et al, 2009). $$$$$ 4The joint probability model can be formulated, if desired, as a language model times a channel model.

For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ The tree parsing algorithm resembles bottom-up chart parsing under the derivation CFG.
For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ Then in an m-ary tree T, the above procedure considers at most mk�1 m�1 connected subgraphs U of order < k rooted at c. For dependency grammars, limiting to m < 6 and k = 3 is quite reasonable, leaving at most 43 subgraphs U rooted at each node c, of which the biggest contain only c, a child c' of c, and a child or sibling of c'.
For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. $$$$$ Statistical machine translation systems are trained on pairs of sentences that are mutual translations.

In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ Edge labels might include grammatical roles such as Subject.
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ One alternates O� converges to a local maximum.
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ Inside probabilities (for example) now have the form βc1,c2(q).
In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). $$$$$ Obviously, in MT, when one has parsers for both the source and target language.

 $$$$$ Such systematic mismatches should be learned by the model, and used during translation.
 $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
 $$$$$ The correct tree is presumed to be an element of the forest.

 $$$$$ This translation is somewhat free, as is common in naturally occurring data.
 $$$$$ T.E' is a version of T.E in which d has been been replaced by t.r.
 $$$$$ This algorithm is essentially alignment to an unknown tree T2; we do not loop over its nodes c2, but choose t2 freely.
 $$$$$ Hence they are able to align any sentence pair, however mismatched.

A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ The grammar constant is the number of possible fits to a node c of a fixed tree.
A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ This observation can be used to prune the space of matchings greatly.
A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). $$$$$ Unlike previous statistical formalisms (limited to isomorphic TSG local distortion of the tree topology.

To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ This translation is somewhat free, as is common in naturally occurring data.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ This makes it possible to train even when the correct parse is not fully known, or not known at all.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ These will constitute the internal nodes of t, and their remaining children will be t’s frontier nodes.
To capture both side syntax contexts, Eisner (2003) studies the bilingual dependency tree-to-tree mapping in conceptual level. $$$$$ The one exception is the synchronous DOP approach of (Poutsma, 2000), which obtains an STSG by decomposing aligned training trees in all possible ways (and using “naive” count-based probability estimates).

If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ One can mechanically transform this algorithm to compute outside probabilities, the Viterbi parse, the parse forest, and other quantities (Goodman, 1999).
If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ In the probabilistic case, we have a distribution p(t  |q) just as before, but this time t is an elementary tree pair.
If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ This makes it possible to train even when the correct parse is not fully known, or not known at all.
If the parse tree of source sentence is provided, decoding (for tree-to-string and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). $$$$$ Then in an m-ary tree T, the above procedure considers at most mk�1 m�1 connected subgraphs U of order < k rooted at c. For dependency grammars, limiting to m < 6 and k = 3 is quite reasonable, leaving at most 43 subgraphs U rooted at each node c, of which the biggest contain only c, a child c' of c, and a child or sibling of c'.

Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ Unfortunately, that clever method does not permit arbitrary models of elementary tree probabilities, nor does it appear to generalize to our synchronous case.
Our approach is based on synchronous tree substitution grammar (STSG, Eisner (2003)), a formalism that can account for structural mismatches, and is trained discriminatively. $$$$$ The outer loop iterates bottom-up over nodes c1 of T1; an inner loop iterates bottom-up over c2 of T2.

We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). $$$$$ Previous work in statistical synchronous grammars has been limited to forms of synchronous context-free grammar (Wu, 1997; Alshawi et al., 2000; Yamada and Knight, 2001).
We hope that some of the work described here might be of relevance to other generation tasks such as machine translation (Eisner, 2003), multi-document summarisation (Barzilay, 2003), and text simplification (Carroll et al, 1999). $$$$$ (It would need exponentially many nonterminals to keep track of an matching of unboundedly many frontier nodes.)

A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ An STSG is a collection of (ordered) pairs of aligned elementary trees.
A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ Many “mismatches” are characteristic of a language pair: e.g., preposition insertion (of → c), multiword locutions (kiss H give a kiss to; misinform H wrongly inform), and head-swapping (float down H descend byfloating).
A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
A synchronous tree substitution grammar (STSG, Eisner (2003)) licenses the space of all possible rewrites. $$$$$ We are now prepared to discuss the synchronous case.

More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ Although this brings the complexity up to O(n2), the real complication is that there can be many fits to (c1, c2).
More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or tree adjoining grammars may better capture the pairings. $$$$$ 4The joint probability model can be formulated, if desired, as a language model times a channel model.

Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ Most statistical MT derives from IBM-style models (Brown et al., 1993), which ignore syntax and allow arbitrary word-to-word translation.
Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ We reformulate it to permit dependency trees, and sketch EM/Viterbi algorithms for alignment, training, and decoding.
Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). $$$$$ An elementary tree pair A = (elle est finalement partie, finally she left) cannot be further decomposed into B = (elle est partie, she left) and C = (finalement, finally).

 $$$$$ First, we know of no previous attempt to learn the “chunk-to-chunk” mappings.
 $$$$$ • 1-best Alignment (if desired).
 $$$$$ Several natural algorithms are now available to us: • Training.
 $$$$$ The runtime is quite low when the training trees are fully specified and elementary trees are bounded in size.1 Second, it is not a priori obvious that one can reasonably use STSG instead of the slower but more powerful STAG.

We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ Often one may wish to learn a tree-to-tree mapping, training it on unaligned pairs of trees, or on a mixture of trees and strings.
We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ A probabilistic TSG also includes a function p(t  |q), which, for each state q, gives a conditional probability distribution over the elementary trees t with root state q.
We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ Unlike previous statistical formalisms (limited to isomorphic TSG local distortion of the tree topology.
We use dynamic programming for parsing under this finite model (Eisner, 2003). $$$$$ Given a node c E T.v, we would like to find all the potential elementary subtrees t of T whose root t.r could have contributed c during the derivation of T. Such an elementary tree is said to fit c, in the sense that it is isomorphic to some subgraph of T rooted at c. The following procedure finds an elementary tree t that fits c. Freely choose a connected subgraph U of T such that U is rooted at c (or is empty).

Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ For example, (beaucoup d’enfants donnent un baiser a` Sam, kids kiss Sam quite often).
Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ The graph (V, E) must be connected and acyclic, and there must be exactly one node r E V (the root) that has no incoming edges.
Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ Most statistical MT derives from IBM-style models (Brown et al., 1993), which ignore syntax and allow arbitrary word-to-word translation.
Eisner (2003) studies how to learn non-isomorphic tree-to-tree/string mappings using a STSG. $$$$$ This rigidity does not fully describe real data.

Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ For example, (beaucoup d’enfants donnent un baiser a` Sam, kids kiss Sam quite often).
Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ Many “mismatches” are characteristic of a language pair: e.g., preposition insertion (of → c), multiword locutions (kiss H give a kiss to; misinform H wrongly inform), and head-swapping (float down H descend byfloating).
Our forest-based tree-to-tree model is based on a probabilistic STSG (Eisner, 2003). $$$$$ When would learned tree-to-tree mappings be useful?

As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. $$$$$ A probabilistic TSG also includes a function p(t  |q), which, for each state q, gives a conditional probability distribution over the elementary trees t with root state q.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. $$$$$ This rigidity does not fully describe real data.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. $$$$$ An STSG is a collection of (ordered) pairs of aligned elementary trees.
As tree-to-string translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. $$$$$ The latter are small and simple (Alshawi et al., 2000): tree nodes are words, and there need be no other structure to recover or align.
