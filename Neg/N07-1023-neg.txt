Modifications of this model are reported in Turner and Charniak (2005) and Galley and McKeown (2007) with improved results. $$$$$ A sample alignment is provided in Figure 2.

(Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules. $$$$$ Markovization for sentence compression provides several benefits, including the ability to condition deletions on a flexible amount of syntactic context, to treat head-modifier dependencies independently, and to lexicalize SCFG productions.
(Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules. $$$$$ We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000).
(Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules. $$$$$ In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in syntactic parsing (Collins, 1999; Klein and Manning, 2003) to alleviate issues intrinsic to relative frequency estimation of treebank productions.
(Galley and McKeown, 2007) extended the noisy-channel approach and proposed a head-driven Markovization formulation of synchronous context free grammar (SCFG) deletion rules. $$$$$ We now describe methods to train SCFG models from sentence pairs.

The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001). $$$$$ We also use a robust approach for tree-to-tree alignment between arbitrary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora.
The last sentence compression method we use is the lexicalized Markov grammar-based approach (Galley and McKeown, 2007) with edit word detection (Charniak and Johnson, 2001). $$$$$ Indeed, current SCFG models such as K&M have no direct way of preventing highly improbable single word removals, such as deletions of adverbs “never” or “nowhere”, which may turn a negative statement into a positive one.4 A second type of annotation that can be added to syntactic categories is the so-called parent annotation (Johnson, 1998), which was effectively used in syntactic parsing to break unreasonable context-free assumptions.

 $$$$$ Many debugging features have been added.
 $$$$$ While their approach proved successful, their reliance on standard maximum likelihood estimators for SCFG productions results in considerable sparseness issues, especially given the relative flat structure of PTB trees; in practice, many SCFG productions are seen only once.
 $$$$$ In subsequent experiments, we experimented with different Markovizations and lexical dependency combination, and finally settled with a model (s = 1 and v = 1) incorporating all conditioning variables listed in the last line of Table 2.

Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown,2007). $$$$$ After inspection, we found that our parser assigned particularly errorful trees to those inputs, which may partially explain these ungrammatical outputs.
Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown,2007). $$$$$ In our second evaluation reported in Table 2, we 7We relied on the SRI language modeling (SRILM) toolkit library for all smoothing experiments.

Galley and McKeown (2007) show improvements to the noisy-channel approach based on rule lexicalization and rule Markovization. $$$$$ In our first evaluation, we experimented with different horizontal and vertical Markovizations (Table 1).
Galley and McKeown (2007) show improvements to the noisy-channel approach based on rule lexicalization and rule Markovization. $$$$$ Secondly, syntactic categories in the PTB are particularly coarse grained, and lead to many incorrect context-free assumptions.
Galley and McKeown (2007) show improvements to the noisy-channel approach based on rule lexicalization and rule Markovization. $$$$$ For our empirical evaluations, we split the data as follows: among the 1,055 sentences that were taken to train systems described in K&M, we selected the first 32 sentence pairs to be an auxiliary test corpus (for future work), the next 200 sentences to be our development corpus, and the remaining 823 to be our base training corpus (ZD-0), which will be augmented with additional data as explained in the next section.

We use the sentence pairs available in the Ziff-Davis Tree Alignment corpus (Galley and McKeown, 2007). $$$$$ Consider for example the tree pair in Figure 2: the two sentences are syntactically very close, but the substitution of “computer” with “unit” makes this sentence pair unusable in the framework presented in K&M.
We use the sentence pairs available in the Ziff-Davis Tree Alignment corpus (Galley and McKeown, 2007). $$$$$ Finally, we used the same test data as K&M for human evaluation purposes (32 sentence pairs).
We use the sentence pairs available in the Ziff-Davis Tree Alignment corpus (Galley and McKeown, 2007). $$$$$ While POS annotation is clearly advantageous compared to using only syntactic categories, adding lexical variables to the model also helps.
We use the sentence pairs available in the Ziff-Davis Tree Alignment corpus (Galley and McKeown, 2007). $$$$$ In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in syntactic parsing (Collins, 1999; Klein and Manning, 2003) to alleviate issues intrinsic to relative frequency estimation of treebank productions.

Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. $$$$$ For our empirical evaluations, we split the data as follows: among the 1,055 sentences that were taken to train systems described in K&M, we selected the first 32 sentence pairs to be an auxiliary test corpus (for future work), the next 200 sentences to be our development corpus, and the remaining 823 to be our base training corpus (ZD-0), which will be augmented with additional data as explained in the next section.
Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. $$$$$ We empirically evaluated different Markov structures, and obtained a best system that generates particularly grammatical sentences according to a human evaluation.
Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. $$$$$ To achieve the above desiderata for better SCFG probability estimates—i.e., reduce the amount of sister annotation within each SCFG production, by conditioning deletions on a context smaller than an entire right-hand side, and at the same time increase the amount of ancestor and descendent annotation through parent (or ancestor) annotation and lexicalization—we follow the approach of (Collins, 1999; Klein and Manning, 2003), i.e., factorize n-ary grammar productions into products of n right-hand side probabilities, a technique sometimes called Markovization.
Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. $$$$$ For instance, the STSG rule rooted at S can be decomposed into two SCFG productions if we allow unary rules such as VP —* VP to be freely added to the compressed tree.
