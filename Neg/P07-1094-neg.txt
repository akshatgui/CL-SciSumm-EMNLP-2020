The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known $$$$$ In unsupervised learning, it is not always reasonable to assume that a large tag dictionary is available.
The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known $$$$$ We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.
The application of MCMC techniques, including Gibbs sampling, to HMM inference problems is relatively well-known $$$$$ Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and Eisner, 2005).

This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. $$$$$ The first version (BHMM1) uses a single value of Q for all word classes (as above); the second version (BHMM2) uses a separate Qj for each tag class j.
This model differs from other non-recursive computational models of grammar induction (e.g. Goldwater and Griffiths, 2007) since it is not based on Hidden Markov Models. $$$$$ To perform inference in our model, we use Gibbs 2.2 Model Definition sampling (Geman and Geman, 1984), a stochastic Our model has the structure of a standard trigram procedure that produces samples from the posterior HMM, with the addition of symmetric Dirichlet pri- distribution P(t|w, α, β) a P(w|t, β)P(t|α).

Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). $$$$$ We ors over the transition and output distributions: initialize the tags at random, then iteratively resamti|ti−1 = t,ti−2 = t′, τ(t,t′) — Mult(τ(t,t′)) ple each tag according to its conditional distribution wi|ti = t, ω(t) — Mult(ω(t)) given the current values of all other tags.
Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). $$$$$ We also evaluate using tag accuracy when possible.
Other work aims to do truly unsupervised learning of taggers, such as Goldwater and Griffiths (2007) and Johnson (2007). $$$$$ (This also fixes Wt, the number of possible it was generated.

Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. $$$$$ Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.
Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. $$$$$ We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.
Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. $$$$$ A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.
Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. $$$$$ Section 5 presents results for a range of corpus sizes and dictionary information, and Section 6 concludes.

In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). $$$$$ For a multinomial with parameters B = ... , BK), a natural choice of prior is the K-dimensional Dirichlet distribution, which is conjugate to the For simplicity, we initially assume that all K parameters (also known as hyperparameters) of the Dirichlet distribution are equal to Q, i.e. the Diri chlet is symmetric.
In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). $$$$$ Standard deviations (a) for the BHMM results fell below those shown for each corpus size. over 5 random tag assignments).
In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). $$$$$ The Bayesian approach is particularly helpful when learning is less constrained, either because less data is available or because dictionary information is limited or absent.
In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on "diluted dictionaries" in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ). $$$$$ Unsupervised learning of linguistic structure is a difficult problem.

Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). $$$$$ Not surprisingly, the advantages of BHMM are most pronounced on the smallest corpus: the effects of parameter integration and sensible priors are stronger when less evidence is available from the input.
Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). $$$$$ We find improvements both when training from data alone, and using a tagging dictionary.
Variation of Information is an information-theoretic measure summing mutual information between tags and states, proposed by (Meila?, 2002), and first used for Unsupervised Part of Speech in (Goldwater and Griffiths, 2007). $$$$$ We define a model with parameters 0, some observed variables w (the linguistic input), and some latent variables t (the hidden structure).

Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). $$$$$ Errors are often sensible: adjectives and nouns are frequently confused, as are verbs and adverbs.
Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). $$$$$ In the experiments reported in the following section, we used two different versions of our model.
Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). $$$$$ The kinds of results produced by BHMM1 and BHMM2 are more similar to each other than to the results of MLHMM, but the differences are still informative.
Each integral can be computed in closed form using multinomial-Dirichlet conjugacy (and by making the above-mentioned simplifying assumption that all other tags were generated separately by their transition and super lingual 87 parameters), just as in the monolingual Bayesian HMM of (Goldwater and Griffiths, 2007). $$$$$ Informally, to update the value of hyperparameter α, we sample a proposed new value α′ from a normal distribution with p = α and a = .1α.

Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. $$$$$ To determine the effects of reduced or absent dictionary information, we ran a set of experiments inspired by those of Smith and Eisner (2005).
Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. $$$$$ First, we collapsed the set of 45 treebank tags onto a smaller set of 17 (the same set used by Smith and Eisner).
Results are given for a monolingual Bayesian HMM (Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here. $$$$$ The confusion matrices in Figure 3 provide a more intuitive picture of the very different sorts of clusterings produced by MLHMM and BHMM2 when no tag dictionary is available.

We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. $$$$$ Our solution is to n(ti) + Wtiβ treat the Gibbs sampler as a stochastic search prowhere n(ti−2,ti−1,ti) and n(ti,wi) are the number of cedure with the goal of identifying the MAP tag seoccurrences of the trigram (ti−2,ti−1,ti) and the quence.
We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. $$$$$ More recent work has shown that improvements can be made by modifying the basic HMM structure (Banko and Moore, 2004), using better smoothing techniques or added constraints (Wang and Schuurmans, 2005), or using a discriminative model rather than an HMM (Smith and Eisner, 2005).
We implemented the Bayesian HMM model of Goldwater and Griffiths (2007) (BHMM1) as our mono lingual baseline. $$$$$ This difference ensures that the learned structure will have high probability over a range of possible parameters, and permits the use of priors favoring the sparse distributions that are typical of natural language.

Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). $$$$$ Typically, linguistic structures are characterized by sparse distributions (e.g., POS tags are followed with high probability by only a few other tags, and have highly skewed output distributions).
Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). $$$$$ We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks.
Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). $$$$$ The confusion matrices in Figure 3 provide a more intuitive picture of the very different sorts of clusterings produced by MLHMM and BHMM2 when no tag dictionary is available.
Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al, 1992) and the incorporation of morphological features (Clark, 2003). $$$$$ Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions.

The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). $$$$$ For knowledgefree clustering, our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance.
The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). $$$$$ Informally, to update the value of hyperparameter α, we sample a proposed new value α′ from a normal distribution with p = α and a = .1α.
The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). $$$$$ Performing inference on the hyperparameters allows us to relax the assumption that every tag has the same prior on its output distribution.

In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. $$$$$ We also evaluate using tag accuracy when possible.
In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. $$$$$ The probability of accepting the new value depends on the ratio between P(t|w, α) and P(t|w, α′) and a term correcting for the asymmetric proposal distribution.
In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. $$$$$ Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.
In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. $$$$$ Section 5 presents results for a range of corpus sizes and dictionary information, and Section 6 concludes.

 $$$$$ Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.
 $$$$$ Standard deviations (a) for the BHMM results fell below those shown for each corpus size. over 5 random tag assignments).
 $$$$$ Perhaps the most well-known is that of Merialdo (1994), who used MLE to train a trigram hidden Markov model (HMM).
 $$$$$ A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.

We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). $$$$$ Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions.
We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). $$$$$ In many linguistic models, including HMMs, the distributions over variables are multinomial.
We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). $$$$$ However, choosing hyperparameters in this way is timeconsuming at best and impossible at worst, if there is no gold standard available.
We start with a well-known application of Bayesian inference, unsupervised POS tagging (Goldwater and Griffiths, 2007). $$$$$ We augment the model with priors over the hyperparameters (here, we assume an improper uniform prior), and use a single Metropolis-Hastings update (Gilks et al., 1996) to resample the value of each hyperparameter after each iteration of the Gibbs sampler.

First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing $$$$$ Performing inference on the hyperparameters allows us to relax the assumption that every tag has the same prior on its output distribution.
First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing $$$$$ Importantly, trigrams (and outputs) remain by Merialdo (1994), using a tag dictionary to conexchangeable: the probability of a set of trigrams strain the possible parts of speech allowed for each (outputs) is the same regardless of the order in which word.
First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing $$$$$ We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks.
First, if we want to find the MAP derivations of the training strings, then following Goldwater and Griffiths (2007), we can use annealing $$$$$ Our model has the structure of a standard trigram HMM, yet its accuracy is closer to that of a state-of-the-art discriminative model (Smith and Eisner, 2005), up to 14 percentage points better than MLE.

We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. $$$$$ Hyperparameter inference leads to slightly lower scores than are obtained by oracle hyperparameter selection, but both versions of BHMM are still far superior to MLHMM for all corpus sizes.
We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. $$$$$ In this paper, we hope to unify the problems of POS disambiguation and syntactic clustering by presenting results for conditions ranging from a full tag dictionary to no dictionary at all.
We use the same modeling approach as as Goldwater and Griffiths (2007), using a probabilistic tag bigram model in conjunction with a tag-to-word model. $$$$$ If, instead of estimating B, we integrate over all possible values, we no longer encounter such difficulties.

In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. $$$$$ Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions.
In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. $$$$$ The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08.
In the remainder of this paper, we first present the Bayesian Hidden Markov Model (BHMM; Goldwater and Griffiths (2007)) that is used as the baseline model of category acquisition, as well as our extensions to the model, which incorporate sentence type information. $$$$$ Luckily, the Bayesian approach allows us to automatically select values for the hyperparameters by treating them as additional variables in the model.

We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. $$$$$ Hyperparameter inference leads to slightly lower scores than are obtained by oracle hyperparameter selection, but both versions of BHMM are still far superior to MLHMM for all corpus sizes.
We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. $$$$$ In all cases, we list the best score reported for any contrast neighborhood using trigram (but no spelling) features.
We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. $$$$$ For the experiments in this section, we used a 24,000-word subset of the treebank as our unlabeled training corpus.
We use a Bayesian HMM (Goldwater and Griffiths, 2007) as our baseline model. $$$$$ First, we collapsed the set of 45 treebank tags onto a smaller set of 17 (the same set used by Smith and Eisner).

In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. $$$$$ A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.
In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. $$$$$ Informally, to update the value of hyperparameter α, we sample a proposed new value α′ from a normal distribution with p = α and a = .1α.
In the experiments reported below, we use the Gibbs sampler described by (Goldwater and Griffiths, 2007) for the BHMM, and modify it as necessary for our own extended models. $$$$$ We introduce the use of a new information-theoretic criterion, variation of information (Meilˇa, 2002), which can be used to compare a gold standard clustering to the clustering induced from a tagger’s output, regardless of the cluster labels.

Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers. $$$$$ In the limit as corpus size goes to infinity, the BHMM and MLHMM will make identical predictions.
Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers. $$$$$ This explains the difference in VI between the two systems, as well as the higher accuracy of BHMM1 for d > 3: the single Q discourages placing lowfrequency items in their own cluster, so they are more likely to be clustered with items that have similar transition probabilities.
Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers. $$$$$ However, choosing hyperparameters in this way is timeconsuming at best and impossible at worst, if there is no gold standard available.
Slight corrections need to be made to Equation 5 to account for sampling tags from the middle of the sequence rather than from the end; these are given in (Goldwater and Griffiths, 2007) and are followed in our own samplers. $$$$$ A common approach is to define a generative model and maximize the probability of the hidden structure given the observed data.
