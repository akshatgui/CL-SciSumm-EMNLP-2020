ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ We presented new dependency parsers based on concise ILP formulations.
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ For some special parameter settings—e.g., when b is an integer vector and A is totally unimodular5—all vertices of the constraining polyhedron are integer points; in these cases, the integer constraint may be suppressed and (3) is guaranteed to have integer solutions (Schrijver, 2003).
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines.
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ We add valency indicators zval ik , ff(vi = k) for i ∈ V and k = 0,... , n − 1.

 $$$$$ In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms.
 $$$$$ This paper presents new, concise ILP formulations for projective and non-projective dependency parsing.
 $$$$$ Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.
 $$$$$ Our formulations rely on a concise polyhedral representation of the set of candidate dependency parse trees, as sketched in §2.1.

Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ Our formulations offer the following comparative advantages: from data.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ We presented new dependency parsers based on concise ILP formulations.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ Smith was supported by NSF IIS-0836431 and an IBM Faculty Award.

 $$$$$ While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems.
 $$$$$ 9 by za ∈ B, a ∈ A.
 $$$$$ Martins was supported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Inform´atica.

 $$$$$ Smith was supported by NSF IIS-0836431 and an IBM Faculty Award.
 $$$$$ We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.
 $$$$$ Martins was supported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Inform´atica.

Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according to Dan Bikel’s randomized method (http://www.cis.upenn.edu/-dbikel/software.html). tures and constraints can lead to further improvements on accuracy.
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ This is an ILP with O(|A|) variables and constraints (hence, quadratic in n); if we drop the integer constraint the problem becomes the LP relaxation.
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. (2008).
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ The authors thank the reviewers for their comments.

Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ This induces a decomposition of the feature vector f(x, y) as: Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as shown by McDonald et al. (2005), is an instance of the maximal arborescence problem.
Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ Under this model, the root node must send one unit of flow to every other node.

The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ Tractability s usually ensured ing raiing gloally normalized log-linear modht they can be sed in many important earning bystrong factorization assumptions, like the one els, syntactic language modeling, and nsupervied nd inference problem including minrisk decod underlying the arc-factored mode (Esne, 1996; ing training globally normalized log-linear modMcDonald et a., 2005), which forbids any feature els syntactic language modeling and unsupervised that depends on two or more arcs.
The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data.
The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ We add valency indicators zval ik , ff(vi = k) for i ∈ V and k = 0,... , n − 1.

We present a unified view of two state-of-the art non-projective dependency parsers, both approximate $$$$$ In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses.
We present a unified view of two state-of-the art non-projective dependency parsers, both approximate $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.
We present a unified view of two state-of-the art non-projective dependency parsers, both approximate $$$$$ By storing the arclocal feature vectors into the columns of a matrix F(x) , [fa(x)]aEA, and defining the score vector s , F(x)Tw (each entry is an arc score) the inference problem can be written as where A is a sparse constraint matrix (with O(|A|) non-zero elements), and b is the constraint vector; A and b encode the constraints (4–9).
We present a unified view of two state-of-the art non-projective dependency parsers, both approximate $$$$$ Smith was supported by NSF IIS-0836431 and an IBM Faculty Award.

In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ The authors thank the reviewers for their comments.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ The authors thank the reviewers for their comments.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing $$$$$ Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.

The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.

Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.
Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems.

Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms.
Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages.

Figure 3 $$$$$ The last column of Table 1 compares the accuracy of this approximate method with the exact one.
Figure 3 $$$$$ Smith was supported by NSF IIS-0836431 and an IBM Faculty Award.
Figure 3 $$$$$ The authors thank the reviewers for their comments.
Figure 3 $$$$$ The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8We used the provided train/test splits except for English, for which we tested on the development partition.

We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.
We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints.
We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.

(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms.
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al.
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ Define: zfirst child , ij 0 otherwise. but this would yield a constraint matrix with O(n4) non-zero elements.
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others.

In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.
In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NPhard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods.
In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ A dependency tree is called projective if it only contains projective arcs.
In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.

 $$$$$ Consider a sentence x = hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol.
 $$$$$ We believe that our formulations can pave the way for efficient exploitation of global features and constraints in parsing applications, leading to more powerful models.
 $$$$$ While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems.

Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ We presented new dependency parsers based on concise ILP formulations.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ To cope with higher-order features of the form fa1,...,aK(x) (i.e., features whose values depend on the simultaneous inclusion of arcs a1, ... , aK on a candidate dependency tree), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables zal...aK , zal ∧...∧zaK.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.

Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ Our formulations offer the following comparative advantages: from data.
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ Bold indicates the best result for a language.
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ Martins was supported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Inform´atica.
