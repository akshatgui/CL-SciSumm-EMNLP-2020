ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ But there are more efficient ways of accomplish this.
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ Differences are with respect to exact inference for the same set of features.
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ For testing, all sentences were kept (the longest one has length 118).
ILPs have since been used successfully in many NLP applications involving complex structures Punyakanok et al (2008) for semantic role labeling, Riedel and Clarke (2006) and Martins et al (2009) for dependency parsing and several others. $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.

 $$$$$ Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996).
 $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.
 $$$$$ We evaluate the performance of the new parsers on standard parsing tasks in seven languages.
 $$$$$ Considering simultaneously all incidence vectors of legal dependency trees and taking the convex hull, we obtain a polyhedron that we call the arborescence polytope, denoted by Z(x).

Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ We evaluate the performance of the new parsers on standard parsing tasks in seven languages.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ The authors thank the reviewers for their comments.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ This paper presents new, concise ILP formulations for projective and non-projective dependency parsing.
Finally, we follow Martins et al (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). $$$$$ The authors thank the reviewers for their comments.

 $$$$$ We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.
 $$$$$ In §3, we will provide a compact representation of an outer polytope ¯Z(x) ⊇ Z(x) whose integer vertices correspond to dependency trees.
 $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.
 $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.

 $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.
 $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.
 $$$$$ A dependency tree is called projective if it only contains projective arcs.
 $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.

Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ A similar idea was aplied to word alignment by Lacoste-Julien that dependency graphs must be trees.
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ One wants hw. to g bh pttion io a dge pect haveasmallcexpected loss; the typictlnloss functionnis thereHamming loss,cle(y'; y)n°_  |{hi, jid∈ we sho y0: hi, ji ∈/ y}|.
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ Although not pursued here, the same kind of constraints employed by Riedel and Clarke (2006) can straightforwardly fit into our model, after extending it to perform labeled dependency parsing.
Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. $$$$$ Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others.

Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ Learning through a max-margin framework is made effective by the means of a LPrelaxation.
Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ For scalability (and noting that some of the models require O(|V  |� |A|) constraints and variables, which, when A = V 2, grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser.
Although Martins et al (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. $$$$$ Learning through a max-margin framework is made effective by the means of a LPrelaxation.

The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ The two first conditions can be easily expressed by linear constraints on the incidence vector z: Condition 3 is somewhat harder to express.
The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.
The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ Unlike the ensembles, it directly handles non-local output features by optimizing a single global objective.
The multicommodity flow formulation was introduced by Martins et al (2009a), along with some of the parts considered here. $$$$$ Their formulation includes an exponential number of constraints—one for each possible cycle.

We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ Connectedness of graphs can be imposed via flow constraints (by requiring that, for any v ∈ V \ {0}, there is a directed path in B connecting 0 to v).
We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ In §3, we will provide a compact representation of an outer polytope ¯Z(x) ⊇ Z(x) whose integer vertices correspond to dependency trees.
We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.
We present a unified view of two state-of-the art non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al (2009). $$$$$ In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses.

In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009). $$$$$ Martins was supported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Inform´atica.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009). $$$$$ The oracle constrained to pick parents from these lists achieves > 98% in every case.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009). $$$$$ Bold indicates the best result for a language.
In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al, 2009). $$$$$ We believe that our formulations can pave the way for efficient exploitation of global features and constraints in parsing applications, leading to more powerful models.

The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ The authors thank the reviewers for their comments.
The connection is made clear by writing the explicit declarative optimization problem underlying Smith and Eisner (2008) and by showing the factor graph underlying Martins et al (2009). $$$$$ Our formulations offer the following comparative advantages: from data.

Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ While, as pointed out by McDonald and Satta (2007), the inclusion of these features makes inference NPhard, by relaxing the integer constraints we obtain approximate algorithms that are very efficient and competitive with state-of-the-art methods.
Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ ILP formulations focus more on the modeling of problems, rather than algorithm design.
Recall that (i) Smith and Eisner (2008) proposed a afactor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al (2009) approximate parsing as the solution of a linear program. $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.

Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ Consider a sentence x = hw0,... , wni, where wi denotes the word at the ith position, and w0 = $ is a wall symbol.
Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.
Let zA , hzaia∈A; the local agreement constraints at the TREE factor (see Table 1) are written as zA? Ztree (x), where Ztree (x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al, 2009). $$$$$ Tractability s usually ensured ing raiing gloally normalized log-linear modht they can be sed in many important earning bystrong factorization assumptions, like the one els, syntactic language modeling, and nsupervied nd inference problem including minrisk decod underlying the arc-factored mode (Esne, 1996; ing training globally normalized log-linear modMcDonald et a., 2005), which forbids any feature els syntactic language modeling and unsupervised that depends on two or more arcs.

Figure 3: Details of the factor graph underlying the parser of Martins et al (2009). $$$$$ We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.
Figure 3: Details of the factor graph underlying the parser of Martins et al (2009). $$$$$ Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i.
Figure 3: Details of the factor graph underlying the parser of Martins et al (2009). $$$$$ We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.

We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ This induces a decomposition of the feature vector f(x, y) as: Under this decomposition, each arc receives a score; parsing amounts to choosing the configuration that maximizes the overall score, which, as shown by McDonald et al. (2005), is an instance of the maximal arborescence problem.
We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ Fig.
We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ It would be straightforward to adapt the constraints in §3.5 to allow only projective parse trees: simply force znp a = 0 for any a ∈ A.
We now turn to the concise integer LP formulation of Martins et al (2009). $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.

(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ Smith was supported by NSF IIS-0836431 and an IBM Faculty Award.
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ Martins was supported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Inform´atica.
(20) This is exactly the LP relaxation considered by Mar tins et al (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features. They also considered a configuration with non-projectivity features which fire if an arc is non-projective. That configuration can also be obtained here if variables{ n? h, m?} are To be precise, the constraints of Martins et al (2009) are recovered after eliminating the path variables, via Eqs. $$$$$ We believe that our formulations can pave the way for efficient exploitation of global features and constraints in parsing applications, leading to more powerful models.

In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ Xing was supported by NSF DBI0546594, DBI-0640543, IIS-0713379, and an Alfred Sloan Foundation Fellowship in Computer Science.
In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ This is the binary vector z °_ hzaia∈A with each component defined as za = ff(a ∈ y) (here, ff(.) denotes the indicator function).
In sum, although the approaches of Smith and Eisner (2008) and Martins et al (2009) look very different, in reality both are variational approximations emanating from Prop. $$$$$ The authors thank the reviewers for their comments.

 $$$$$ By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages.
 $$$$$ Smith was supported by NSF IIS-0836431 and an IBM Faculty Award.
 $$$$$ In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms.
 $$$$$ For testing, all sentences were kept (the longest one has length 118).

Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ For scalability (and noting that some of the models require O(|V  |� |A|) constraints and variables, which, when A = V 2, grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser.
Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al, 2010), or multi-commodity flows (Martins et al, 2009, 2011). $$$$$ We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.

Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ The model parameters are learned in a max-margin framework by employing a linear programming relaxation.
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ Martins was supported by a grant from FCT/ICTI through the CMU-Portugal Program, and also by Priberam Inform´atica.
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ Experimental results on seven languages show that our rich-featured parsers outperform arc-factored and approximate higher-order parsers, and are in line with stacked parsers, having with respect to the latter the advantage of not requiring an ensemble configuration.
Past work in dependency parsing considered either (i) a few "large" components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many "small" components, coming from a multi-commodity flow formulation (Martins et al, 2009, 2011). $$$$$ We have shown how non-local output features can be incorporated, while keeping only a polynomial number of constraints.
