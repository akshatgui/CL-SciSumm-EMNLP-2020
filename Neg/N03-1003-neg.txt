It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or the Sloan Foundation.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ Another contribution is the induction of MSA lattices from non-parallel data.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ We thank Stuart Allen, Itai Balaban, Hubie Chen, Tom Heyerman, Evelyn Kleinberg, Carl Sable, and Alex Zubatov for acting as judges.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ We point out that one such approach, recently proposed by Pang et al. (2003), also represents paraphrases by lattices, similarly to our method, although their lattices are derived using parse information.

MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ For example, suppose we have two (linearized) lattices slot1 bombed slot2 and slot3 was bombed by slot4 drawn from different corpora.
MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.
MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.

Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ Finally, given an input sentence to be paraphrased, we match it to a lattice and use a paraphrase from the matched lattice’s mate to generate an output sentence.
Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ Pairwise MSA takes two sentences and a scoring function giving the similarity between words; it determines the highest-scoring way to perform insertions, deletions, and changes to transform one of the sentences into the other.
Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ See Figure 6 for examples.

We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ We showed that word lattices can be induced from a type of corpus that can be easily obtained for many domains, broadening the applicability of this useful representation.
We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ Otherwise, we incorporate reliable synonyms12 into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes.
We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.

See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ We had numerous very useful conversations with all those mentioned above and with Eli Barzilay, Noemie Elhadad, Jon Kleinberg (who made the “pigeonhole” observation), Mirella Lapata, Smaranda Muresan and Bo Pang.
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ Lin and Pantel (2001) extract inference rules, which are related to paraphrases (for example, X wrote Y implies X is the author of Y), to improve question answering.
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.

Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ Given our interest in domain-dependent paraphrasing, we limited attention to 9MB of articles, collected using a TDT-style document clustering system, concerning individual acts of violence in Israel and army raids on the Palestinian territories.
Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ Shinyama et al. (2002) also use dependency-tree information to extract templates of a limited form (in their case, determined by the underlying information extraction application).
Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ We are very grateful to Dekang Lin for providing us with DIRT’s output.
Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ In contrast to earlier work, we not only extract paraphrasing rules, but also automatically determine which of the potentially relevant rules to apply to an input sentence and produce a revised form using them.

This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ In brief, the DIRT system works as follows.
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ A flexible pattern-matching procedure allows us to paraphrase an unseen sentence by matching it to one of the induced patterns.
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ Lattices have proven advantageous in a number of NLP contexts (Mangu et al., 2000; Bangalore et al., 2002; Barzilay and Lee, 2002; Pang et al., 2003), but were usually produced from (multi-)parallel data, which may not be readily available for many applications.
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ We had numerous very useful conversations with all those mentioned above and with Eli Barzilay, Noemie Elhadad, Jon Kleinberg (who made the “pigeonhole” observation), Mirella Lapata, Smaranda Muresan and Bo Pang.

Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ We are grateful to many people for helping us in this work.
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ During training, pattern induction is first applied independently to the two datasets making up a pair of comparable corpora.
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ The judges’ assessment of correctness was fairly constant between the full 100-instance set and just the 50-instance common set alone.

In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ Nodes with in-degree lower than the synonymy threshold are removed under the assumption that they probably represent idiosyncrasies of individual sentences.
In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ Furthermore, all backbone nodes labelled with our special generic tokens are also replaced with slot nodes, since they, too, probably represent arguments (we condense adjacent slots into one).

Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ We thank the Cornell NLP group, especially Eric Breck, Claire Cardie, Amanda Holland-Minkley, and Bo Pang, for helpful comments on previous drafts.
Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ During training, pattern induction is first applied independently to the two datasets making up a pair of comparable corpora.
Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ Lin and Pantel (2001) extract inference rules, which are related to paraphrases (for example, X wrote Y implies X is the author of Y), to improve question answering.

Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.
Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ We had numerous very useful conversations with all those mentioned above and with Eli Barzilay, Noemie Elhadad, Jon Kleinberg (who made the “pigeonhole” observation), Mirella Lapata, Smaranda Muresan and Bo Pang.

Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. $$$$$ We do this by finding the best alignment of the sentence to the existing lattices.4 If a matching lattice is found, we choose one of its comparable-corpus paraphrase lattices to rewrite the sentence, substituting in the argument values of the original sentence.
Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. $$$$$ The major goals of our algorithm are to learn: recurring patterns in the data, such as X (injured/wounded) Y people, Z seriously, where the capital letters represent variables; pairings between such patterns that represent paraphrases, for example, between the pattern X (injured/wounded) Y people, Z of them seriously and the pattern Y were (wounded/hurt) by X, among them Z were in serious condition.

The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ Instead, we made a good-faith effort to adapt the DIRT system (Lin and Pantel, 2001) to the problem, selecting the 6,534 highestscoring templates it produced when run on our datasets.
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ The only subtlety is that we do not want mismatches on sentence details (e.g., the location of a raid) causing sentences describing the same type of occurrence (e.g., a raid) from being separated, as this might yield clusters too fragmented for effective learning to take place.
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ Otherwise, we incorporate reliable synonyms12 into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes.
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ (Moreover, variability in the arguments of the sentences in a cluster is needed for our learning algorithm to succeed; see below.)

Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ Figure 3 shows an example of a lattice and the slotted lattice derived via the process just described.
Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ Figure 3 shows an example of a lattice and the slotted lattice derived via the process just described.
Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ We are very grateful to Dekang Lin for providing us with DIRT’s output.
Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ Since we do not have this information, we essentially approximate the parallelcorpus situation by correlating information from descriptions of (what we hope are) the same event occurring in the two different corpora.

While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ In our instructions to the judges, we defined two text units (such as sentences or snippets) to be paraphrases if one of them can generally be substituted for the other without great loss of information (but not necessarily vice versa).
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ Our method learns structurally similar patterns of expression from data and identifies paraphrasing pairs among them using a comparable corpus.
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ This yields as many paraphrases as there are lattice paths.
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ Otherwise, we incorporate reliable synonyms12 into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes.

There exist many different string similarity measures $$$$$ Note that we also acquire paraphrases from each of the individual corpora; but the lack of clues as to sentence equivalence in single corpora means that we must be more conservative, only selecting as paraphrases items that are structurally very similar.
There exist many different string similarity measures $$$$$ However, only two-argument templates are considered.
There exist many different string similarity measures $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.
There exist many different string similarity measures $$$$$ Otherwise, we incorporate reliable synonyms12 into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes.

Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ To compare the set of argument values of two lattices, we simply count their word overlap, giving double weight to proper names and numbers and discarding auxiliaries (we purposely ignore order because paraphrases can consist of word re-orderings).
Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ Otherwise, we incorporate reliable synonyms12 into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes.
Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ Another contribution is the induction of MSA lattices from non-parallel data.

Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ Otherwise, we incorporate reliable synonyms12 into the backbone structure by preserving all nodes that are reached by at least % of the sentences passing through the two neighboring backbone nodes.
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ We therefore first replace all appearances of dates, numbers, and proper names2 with generic tokens.
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.

Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ However, extraction methods are not easily extended to generation methods.
Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.
Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ For example, suppose we have two (linearized) lattices slot1 bombed slot2 and slot3 was bombed by slot4 drawn from different corpora.

Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ Eric Breck helped us with translating the output of the DIRT system.
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ The key intuition is that because the sentences in the cluster represent the same type of event, such as a bombing, but generally refer to different instances of said event (e.g. a bombing in Jerusalem versus in Gaza), areas of large variability in the lattice should correspond to arguments.
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ (The system of Shinyama et al. (2002) was unsuitable for evaluation purposes because their paraphrase extraction component is too tightly coupled to the underlying information extraction system.)
