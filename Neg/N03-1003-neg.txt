It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ Note that judging full sentences is inherently easier than judging templates, because template comparison requires considering a variety ofpossible slot values, while sentences are self-contained units.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ Our approach has three main steps.
It's still possible to use MSA if, for example, the input is pre-clustered to have the same constituent ordering (Barzilay and Lee (2003)). $$$$$ They assume that paths in dependency trees that take similar arguments (leaves) are close in meaning.

MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ Figure 3 shows an example of a lattice and the slotted lattice derived via the process just described.
MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ Eric Breck helped us with translating the output of the DIRT system.
MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al, 1998). $$$$$ We are grateful to many people for helping us in this work.

Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ We implemented our system on a pair of comparable corpora consisting of articles produced between September 2000 and August 2002 by the Agence France-Presse (AFP) and Reuters news agencies.
Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ Clusters with fewer than ten sentences are discarded.
Following Barzilay and Lee (2003), we approach the sentence clustering task by hierarchical complete-link clustering with a similarity metric based on word n-gram overlap (n= 1, 2, 3). $$$$$ 10 For this experiment, we randomly selected 20 AFP articles about violence in the Middle East published later than the articles in our training corpus.

We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ Lin and Pantel (2001) extract inference rules, which are related to paraphrases (for example, X wrote Y implies X is the author of Y), to improve question answering.
We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ Then, pairs of paths in which the slots tend to be filled by similar values, where the similarity measure is based on the mutual information between the value and the slot, are deemed to be paraphrases.
We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ It is important to note some important caveats in making this comparison, the most prominent being that DIRT was not designed with sentence-paraphrase generation in mind — its templates are much shorter than ours, which may have affected the evaluators’ judgments — and was originally implemented on much larger data sets.7 The point of this evaluation is simply to determine whether another corpusbased paraphrase-focused approach could easily achieve the same performance level.
We adopt the scoring function for MSA from Barzilay and Lee (2003). $$$$$ Lattices have proven advantageous in a number of NLP contexts (Mangu et al., 2000; Bangalore et al., 2002; Barzilay and Lee, 2002; Pang et al., 2003), but were usually produced from (multi-)parallel data, which may not be readily available for many applications.

See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ Shinyama et al. (2002) also use dependency-tree information to extract templates of a limited form (in their case, determined by the underlying information extraction application).
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ The key features of this approach are: Focus on paraphrase generation.
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ Finally, we evaluated the quality of the paraphrase sentences generated by our system, thus (indirectly) testing all the system components: pattern selection, paraphrase acquisition, and generation.
See Barzilay and Lee (2003) for a detailed discussion about the choice of 50% according to pigeonhole principle. $$$$$ 2002), we need neither parallel data nor explicit information about sentence semantics.

Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ For instance, nothing in the second sentence is really equivalent to “across the board”; we can only say that the entire clauses “stocks rose across the board” and “winners strongly outpaced losers” are paraphrases.
Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ Of the paraphrases generated by our system, the two evaluators deemed 81.4% and 78%, respectively, to be valid, whereas for the baseline system, the correctness results were 69.5% and 66.1%, respectively.
Barzilay and Lee (2003) proposed to apply multiple-sequence alignment (MSA) for traditional, sentence-level PR. $$$$$ The use of related corpora is key: we can capture paraphrases that on the surface bear little resemblance but that, by the nature of the data, must be descriptions of the same information.

This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ This could, for example, aid machine-translation evaluation, where it has become common to evaluate systems by comparing their output against a bank of several reference translations for the same sentences (Papineni et al., 2002).
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ Any opinions, findings, and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or the Sloan Foundation.
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ We point out that one such approach, recently proposed by Pang et al. (2003), also represents paraphrases by lattices, similarly to our method, although their lattices are derived using parse information.
This implies that our exact algorithm could be also used to find exact multi sequence alignments, an important problem in natural language processing (Barzilay and Lee, 2003) and computational biology (Durbin et al, 2006) that is almost always solved with approximate methods. $$$$$ Finally, we evaluated the quality of the paraphrase sentences generated by our system, thus (indirectly) testing all the system components: pattern selection, paraphrase acquisition, and generation.

Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ A flexible pattern-matching procedure allows us to paraphrase an unseen sentence by matching it to one of the induced patterns.
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ Previous work on automated paraphrasing has considered different levels of paraphrase granularity.
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003). $$$$$ Applications include summarization (Knight and Marcu, 2000) and rewriting (Chandrasekar and Bangalore, 1997): both could employ such a mechanism to produce candidate sentence paraphrases that other system components would filter for length, sophistication level, and so forth.'

In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ Furthermore, all backbone nodes labelled with our special generic tokens are also replaced with slot nodes, since they, too, probably represent arguments (we condense adjacent slots into one).
In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ We presented an approach for generating sentence level paraphrases, a task not addressed previously.
In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ We had numerous very useful conversations with all those mentioned above and with Eli Barzilay, Noemie Elhadad, Jon Kleinberg (who made the “pigeonhole” observation), Mirella Lapata, Smaranda Muresan and Bo Pang.
In its ability to learn paraphrases using Multiple Sequence Alignment, our system is related to Barzilay and Lee (2003). $$$$$ Next, we identify pairs of lattices from the two different corpora that are paraphrases of each other; the identification process checks whether the lattices take similar arguments.

Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ All 118 instances (59 per system) were presented in random order to two judges, who were asked to indicate whether the meaning had been preserved.
Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ In order to learn patterns, we first compute a multiplesequence alignment (MSA) of the sentences in a given cluster.
Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ See Figure 6 for examples.
Barzilay and Lee (2003) construct lattices over paraphrases using an iterative pairwise multiple sequence alignment (MSA) algorithm. $$$$$ All 118 instances (59 per system) were presented in random order to two judges, who were asked to indicate whether the meaning had been preserved.

Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ We presented an approach for generating sentence level paraphrases, a task not addressed previously.
Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ Nodes with in-degree lower than the synonymy threshold are removed under the assumption that they probably represent idiosyncrasies of individual sentences.
Similar to the work of Barzilay and Lee (2003), who have applied paraphrase generation techniques to comparable corpora consisting of different newspaper articles about the same event. $$$$$ Figure 3 shows an example of a lattice and the slotted lattice derived via the process just described.

Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. $$$$$ Furthermore, all backbone nodes labelled with our special generic tokens are also replaced with slot nodes, since they, too, probably represent arguments (we condense adjacent slots into one).
Barzilay and Lee (2003) proposed a multi-sequence alignment algorithm that takes structurally similar sentences and builds a compact lattice representation that encodes local variations. $$$$$ However, extraction methods are not easily extended to generation methods.

The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ For instance, nothing in the second sentence is really equivalent to “across the board”; we can only say that the entire clauses “stocks rose across the board” and “winners strongly outpaced losers” are paraphrases.
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ It’s shuffled off this mortal coil!
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ Furthermore, all backbone nodes labelled with our special generic tokens are also replaced with slot nodes, since they, too, probably represent arguments (we condense adjacent slots into one).
The latter study applied several MT metrics to paraphrase data from Barzilay and Lee's corpus-based system (Barzilay and Lee, 2003), and found moderate correlations with human adequacy judgments, but little correlation with fluency judgments. $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.

Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ We showed that word lattices can be induced from a type of corpus that can be easily obtained for many domains, broadening the applicability of this useful representation.
Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ One problem is that their templates often only match small fragments of a sentence.
Barzilay and Lee (2003) present an approach for generating sentence level paraphrases, learning structurally similar patterns of expression from data and identifying paraphrasing pairs among them using a comparable corpus. $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.

While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ In contrast, our method is not limited to a set of a priori-specified paraphrase types.
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ Our apapplies alignment sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patrepresented by lattice and automatically determines how to apply these patterns to rewrite new sentences.
While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and can not be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). $$$$$ We presented an approach for generating sentence level paraphrases, a task not addressed previously.

There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al, 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. $$$$$ This paper is based upon work supported in part by the National Science Foundation under ITR/IM grant IIS-0081334 and a Sloan Research Fellowship.
There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al, 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al, 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. $$$$$ We had numerous very useful conversations with all those mentioned above and with Eli Barzilay, Noemie Elhadad, Jon Kleinberg (who made the “pigeonhole” observation), Mirella Lapata, Smaranda Muresan and Bo Pang.

Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ We address the text-to-text generation problem of sentence-level paraphrasing — a phenomenon distinct from and more difficult than wordor phrase-level paraphrasing.
Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ (We chose parameters that optimized precision rather than recall on our small held-out set.)
Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ In contrast to previous work using MSA for generation (Barzilay and Lee, several versions of their component sentences.
Another direction is to build on methods to extract paraphrases from comparable corpora (Barzilay and Lee, 2003), and extend them to capture asymmetrical pairs, where entailment holds in one, but not the other, direction. $$$$$ This paper is based upon work supported in part by the National Science Foundation under ITR/IM grant IIS-0081334 and a Sloan Research Fellowship.

Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ The most important part of the transformation is to determine which words are actually instances of arguments, and so should be replaced by slots (representing variables).
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ See Figure 6 for examples.
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ We are very grateful to Dekang Lin for providing us with DIRT’s output.
Previous work aligns a group of sentences into a compact word lattice (Barzilay and Lee, 2003), a finite state automaton representation that can be used to identify commonality or variability among comparable texts and generate paraphrases. $$$$$ One might initially suppose that sentence-level paraphrasing is simply the result of word-for-word or phraseby-phrase substitution applied in a domain- and contextindependent fashion.

Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ A flexible pattern-matching procedure allows us to paraphrase an unseen sentence by matching it to one of the induced patterns.
Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ Another contribution is the induction of MSA lattices from non-parallel data.
Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003). $$$$$ The subsequent subsections provide more detailed descriptions of the individual steps.

Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ See Bangalore et al. (2002) and Barzilay and Lee (2002) for other uses of such data.
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ We presented an approach for generating sentence level paraphrases, a task not addressed previously.
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems.
Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.55 0.63 (Szpektor et al, 2004). $$$$$ Learning synonyms via distributional similarity has been well-studied (Pereira et al., 1993; Grefenstette, 1994; Lin, 1998).
