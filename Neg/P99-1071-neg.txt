As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al, 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. $$$$$ We used the training data for identification of paraphrasing rules on which our comparison algorithm is built.
As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al, 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. $$$$$ One interesting problem for future work is the question of how much context to include from a sentence from which an intersected phrase is drawn.
As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al, 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.

Barzilay et al (1999) were one of the first to use time for multi-document summarization. $$$$$ When reading an original text, it is possible to retrieve the correct temporal sequence of events which is usually available explicitly.
Barzilay et al (1999) were one of the first to use time for multi-document summarization. $$$$$ This material is based upon work supported by the National Science Foundation under grant No.
Barzilay et al (1999) were one of the first to use time for multi-document summarization. $$$$$ This initial set might allow us to automatically identify more rules and increase the performance of our comparison algorithm.
Barzilay et al (1999) were one of the first to use time for multi-document summarization. $$$$$ Our approach is unique in its usage of language generation to reformulate the wording of the summary.

Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al, 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al, 1999). $$$$$ Far more training data is needed to tune the generation portion.
Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al, 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al, 1999). $$$$$ In such cases, the resulting summary is actually false.
Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al, 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al, 1999). $$$$$ First is the need to use learning techniques to identify paraphrasing patterns in corpus data.

Then one can apply the method inspired by (Barzilay et al, 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. $$$$$ While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents.
Then one can apply the method inspired by (Barzilay et al, 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.
Then one can apply the method inspired by (Barzilay et al, 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. $$$$$ We collected a corpus of themes, that was divided into a training portion and a testing portion.
Then one can apply the method inspired by (Barzilay et al, 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. $$$$$ IRI-96-1879.

Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999). $$$$$ Assuming a set of similar sentences as input extracted from multiple documents on the same event (McKeown et al., 1999; Eskin et al., 1999), our system identifies common phrases across sentences and uses language generation to reformulate them as a coherent summary.
Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999). $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.
Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999). $$$$$ To find the time an event occurred, we use the publication date of the phrase referring to the event.
Paraphrase here includes sentences generated in an Information Fusion task (Barzilay et al, 1999). $$$$$ Be-cause this input structure and the requirements on the generator are quite different from typical language generators, we had to address the design of the input language specification and its interaction with existing features in a new way, instead of using the existing SURGE syntactic realization in a &quot;black box&quot; manner.

Barzilay et al (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. $$$$$ We would like to thank Ya.el Dahan-Netzer for her help with SURGE.
Barzilay et al (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. $$$$$ Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources.
Barzilay et al (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. $$$$$ One problem in improving output is determining how to recover from errors in tools used in early stages of the process, such as the tagger and the parser.

In this work, we use the clusters of event related sentences from the Information Fusion work by Barzilay et al [1999]. $$$$$ Extraction techniques can work only if summary sentences already appear in the article.
In this work, we use the clusters of event related sentences from the Information Fusion work by Barzilay et al [1999]. $$$$$ Our approach is unique in its usage of language generation to reformulate the wording of the summary.
In this work, we use the clusters of event related sentences from the Information Fusion work by Barzilay et al [1999]. $$$$$ IRI-96-1879.
In this work, we use the clusters of event related sentences from the Information Fusion work by Barzilay et al [1999]. $$$$$ Our approach is unique in its usage of language generation to reformulate the wording of the summary.

For our evaluation cases, we use the Information Fusion data collected by [Barzilay et al, 1999]. $$$$$ Phrases produced by theme intersection will form the content of the generated summary.
For our evaluation cases, we use the Information Fusion data collected by [Barzilay et al, 1999]. $$$$$ While we have tuned the system to perform with minor errors on the manual set of themes we have created (the missing article in the fourth sentence of the summary in Figure 1 is an example), we need more robust input data from the theme construction component, which is still under development, to train the generator before beginning large scale testing.
For our evaluation cases, we use the Information Fusion data collected by [Barzilay et al, 1999]. $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.
For our evaluation cases, we use the Information Fusion data collected by [Barzilay et al, 1999]. $$$$$ By corpus analysis, we collected a set of patterns for identification of ambiguous dates.

). $$$$$ Part of combination will involve increasing coherence of the generated text through the use of connectives, anaphora or lexical relations (Jing, 1999).
). $$$$$ While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents.
). $$$$$ This occurs just once in our test cases, but it is a serious error.
). $$$$$ Our approach is unique in its usage of language generation to reformulate the wording of the summary.

Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al, 2005), multi document summarization (Barzilay et al, 1999) and machine translation (Quantz and Schmitz, 1994. $$$$$ The evaluation task for the content selection stage is to measure how well we identify common phrases throughout multiple sentences.
Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al, 2005), multi document summarization (Barzilay et al, 1999) and machine translation (Quantz and Schmitz, 1994. $$$$$ A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown, 1998).
Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al, 2005), multi document summarization (Barzilay et al, 1999) and machine translation (Quantz and Schmitz, 1994. $$$$$ Ordering of circumstantials can easily be derived from their ordering in the input.
Semantic Role Labeling (SRL) has been implemented or suggested as a means to aid several Natural Language Processing (NLP) tasks such as information extraction (Kogan et al, 2005), multi document summarization (Barzilay et al, 1999) and machine translation (Quantz and Schmitz, 1994. $$$$$ We present system accuracy separately for each category, since identifying a verb or a subject is, in most cases, more important than identifying other sentence constituents.

As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. $$$$$ The labelling of the circumstantial as time allows SURGE to make the following decisions given a sentence such as: &quot;After they made an emergency landing, the pilots were reported missing.&quot; The semantic input also provides a solid basis to authorize sophisticated revisions to a base input.
As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. $$$$$ On Friday, a U.S. F-16 fighter jet was shot down by Bosnian Serb missile while policing the no-fly zone over the region.
As a text-to-text approach, our work is more similar to work on Information Fusion (Barzilay et al., 1999), a sub-problem in multi-document summarisation. $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.

 $$$$$ On Friday, a U.S. F-16 fighter jet was shot down by Bosnian Serb missile while policing the no-fly zone over the region.
 $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.
 $$$$$ This material is based upon work supported by the National Science Foundation under grant No.

Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi Gen (Barzilay et al, 1999), and MEAD (Radev et al, 2004). $$$$$ Our algorithm was compared against intersections extracted by human judges from each theme, producing 39 sentence-level predicateargument structures.
Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi Gen (Barzilay et al, 1999), and MEAD (Radev et al, 2004). $$$$$ Extraction cannot handle the task we address, because summarization of multiple documents requires information about similarities and differences across articles.
Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi Gen (Barzilay et al, 1999), and MEAD (Radev et al, 2004). $$$$$ One problem in improving output is determining how to recover from errors in tools used in early stages of the process, such as the tagger and the parser.
Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multi Gen (Barzilay et al, 1999), and MEAD (Radev et al, 2004). $$$$$ However, in cases where the preposition is ambiguous (e.g., &quot;in&quot; can indicate &quot;time&quot; or &quot;location&quot;) the generator must rely solely on ordering circumstantials based on ordering found in the input.

A famous effort in this direction is the information fusion approach proposed in Barzilay et al (1999). $$$$$ As we will see in the next section, we can both eliminate redundancy from the output and retain balance through the selection of common information.
A famous effort in this direction is the information fusion approach proposed in Barzilay et al (1999). $$$$$ Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources.
A famous effort in this direction is the information fusion approach proposed in Barzilay et al (1999). $$$$$ Our work will characterize the types of contextual information that should be retained and will develop algorithms for the case of negation, among others.
A famous effort in this direction is the information fusion approach proposed in Barzilay et al (1999). $$$$$ One interesting problem for future work is the question of how much context to include from a sentence from which an intersected phrase is drawn.

Barzilay et al (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. $$$$$ However, if the same information is mentioned several times in different documents, much of the summary will be redundant.
Barzilay et al (1999) does not explicitly extract facts, but instead picks out relevant repeated elements and combines them to obtain a summary which retains the semantics of the original. $$$$$ However, in cases where the preposition is ambiguous (e.g., &quot;in&quot; can indicate &quot;time&quot; or &quot;location&quot;) the generator must rely solely on ordering circumstantials based on ordering found in the input.

Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentence rewrite techniques, e.g., (Barzilay et al, 1999). $$$$$ While we have tuned the system to perform with minor errors on the manual set of themes we have created (the missing article in the fourth sentence of the summary in Figure 1 is an example), we need more robust input data from the theme construction component, which is still under development, to train the generator before beginning large scale testing.
Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentence rewrite techniques, e.g., (Barzilay et al, 1999). $$$$$ Evaluation of multi-document summarization is difficult.
Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentence rewrite techniques, e.g., (Barzilay et al, 1999). $$$$$ We performed a quantitative evaluation of our content-selection component.
Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentence rewrite techniques, e.g., (Barzilay et al, 1999). $$$$$ When reading an original text, it is possible to retrieve the correct temporal sequence of events which is usually available explicitly.

Since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (Marton et al, 2009), document summarization (Barzilay et al, 1999), and recognizing textual entailment (RTE) (Dagan et al, 2005). $$$$$ While some researchers address this problem by selecting a subset of the repetitions (Carbonell and Goldstein, 1998), this approach is not always satisfactory.
Since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (Marton et al, 2009), document summarization (Barzilay et al, 1999), and recognizing textual entailment (RTE) (Dagan et al, 2005). $$$$$ A Bosnian Serb missile shot down a U.S. F-16 over northern Bosnia on Friday.
Since paraphrases capture the variations of linguistic expressions while preserving the meaning, they are very useful in many applications, such as machine translation (Marton et al, 2009), document summarization (Barzilay et al, 1999), and recognizing textual entailment (RTE) (Dagan et al, 2005). $$$$$ This allows FUF/SURGE to map the syntactic category of the circumstantial to the semantic and syntactic features expected by SURGE.

Lexical chains have been used in text summarization (Barzilay et al, 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. $$$$$ Evaluation of multi-document summarization is difficult.
Lexical chains have been used in text summarization (Barzilay et al, 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. $$$$$ From the generation side, our main goal is to make the generated summary more concise, primarily by combining clauses together.
Lexical chains have been used in text summarization (Barzilay et al, 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. $$$$$ In such cases, the resulting summary is actually false.
Lexical chains have been used in text summarization (Barzilay et al, 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. $$$$$ A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et a., 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadakl, 1997).

It has been observed that in the context of multi-document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al, 1999). $$$$$ We would like to thank Ya.el Dahan-Netzer for her help with SURGE.
It has been observed that in the context of multi-document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al, 1999). $$$$$ However, some of the rules can only be approximated to a certain degree.
It has been observed that in the context of multi-document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al, 1999). $$$$$ In order to facilitate comparison, words are kept in canonical form.
It has been observed that in the context of multi-document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al, 1999). $$$$$ (Dras, 1997) considered sets of paraphrases required for text transformation in order to meet external constraints such as length or readability.

The merging task is a more logic-based approach than similar techniques like information fusion used in multi-document summarization (Barzilay et al, 1999). $$$$$ The implementation benefited from the bidirectional nature of FUF unification in the handling of hybrid constraints and required little change to the existing SURGE grammar.
The merging task is a more logic-based approach than similar techniques like information fusion used in multi-document summarization (Barzilay et al, 1999). $$$$$ The evaluation task for the content selection stage is to measure how well we identify common phrases throughout multiple sentences.
The merging task is a more logic-based approach than similar techniques like information fusion used in multi-document summarization (Barzilay et al, 1999). $$$$$ We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents.
The merging task is a more logic-based approach than similar techniques like information fusion used in multi-document summarization (Barzilay et al, 1999). $$$$$ Our approach is unique in its usage of language generation to reformulate the wording of the summary.
