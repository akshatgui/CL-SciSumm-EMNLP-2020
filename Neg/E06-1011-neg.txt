To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ Results are shown in Figure 3.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ Though trees are more common, some formalisms allow for words to modify multiple parents (Hudson, 1984).

In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ The work of DaumÂ´e and Marcu (2005) formalizes this intuition by presenting an online learning framework in which parameter updates are made directly with respect to errors in the inference algorithm.
In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ Kromann (2001) argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank (Kromann, 2003).
In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ Kromann (2001) argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank (Kromann, 2003).

The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.
The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ This sentence is an example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing.
The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ We implemented this method and found that the results were slightly worse.

This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ Kromann also allows for cyclic dependencies, though we deal only with acyclic dependency graphs here.
This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance.
This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ Even in freer-word order languages such as Czech, almost all non-projective dependency trees are primarily projective, modulo a few nonprojective edges.

Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ We show in the next section that this robustness extends to approximate dependency parsing.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ Figure 1 displays a dependency representation for the sentence John hit the ball with the bat.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ We use the non-projective algorithm since the Danish Dependency Treebank contains a small number of non-projective arcs.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.

A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ Given a directed graph G = (V, E), the maximum spanning tree (MST) problem is to find the highest scoring subgraph of G that satisfies the tree constraint over the vertices V .
A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ An mth-order parsing algorithm will work similarly to the second-order algorithm, except that we collect m pairs of adjacent dependents in succession before attaching them to their parent.

Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs.
Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ We described approximate dependency parsing algorithms that support higher-order features and multiple parents.
Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ Though trees are more common, some formalisms allow for words to modify multiple parents (Hudson, 1984).
Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ However, the loop could potentially take exponential time, so we will bound the number of edge transformations to a fixed value M. It is easy to argue that this will not hurt performance.

These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm.

We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause.
We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ Kromann also allows for cyclic dependencies, though we deal only with acyclic dependency graphs here.
We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ Dependency representations of sentences (Hudson, 1984; MeÂ´lËcuk, 1988) model head-dependent syntactic relations as edges in a directed graph.
We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.

This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance.
This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions.

 $$$$$ We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.
 $$$$$ An mth-order parsing algorithm will work similarly to the second-order algorithm, except that we collect m pairs of adjacent dependents in succession before attaching them to their parent.
 $$$$$ In this section, we review the work of McDonald et al. (2005b) for online large-margin dependency parsing.

McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ More formally, if the word xi0 has the children shown in this picture, This second-order factorization subsumes the first-order factorization, since the score function could just ignore the middle argument to simulate first-order scoring.
McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.

This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ This algorithm allows for the introduction of non-projective edges because we do not restrict any of the edge changes except to maintain the tree property.
This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ This framework will possibly allow us to include effectively more global features over the dependency structure than those in our current second-order model.
This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new models lead to state-of-the-art accuracy for English and the best accuracy we know of for Czech and Danish.

However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ As usual for supervised learning, we assume a training set T = {(xt, yt)}Tt=1, consisting of pairs of a sentence xt and its correct dependency representation yt.
However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ We partitioned the data into contiguous 80/20 training/testing splits.

For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ An mth-order parsing algorithm will work similarly to the second-order algorithm, except that we collect m pairs of adjacent dependents in succession before attaching them to their parent.
For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ Note that we only impose margin constraints between the single highest-scoring graph and the correct graph relative to the current weight setting.
For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ Indeed, our second-order projective parser analyzes the test set in 16m32s, and the non-projective approximate parser needs 17m03s to parse the entire evaluation set, showing that runtime for the approximation is completely dominated by the initial call to the second-order projective algorithm and that the post-process edge transformation loop typically only iterates a few times per sentence.

In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) to learning with structured outputs, in the present case dependency structures.
In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ Kromann also allows for cyclic dependencies, though we deal only with acyclic dependency graphs here.
In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ However, we would like to investigate the benefits for parsing of more principled approaches to approximate learning and inference techniques such as the learning as search optimization framework of (DaumÂ´e and Marcu, 2005).

McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000).
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ The loop of lines 2â16 exits only when no further score improvement is possible.
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ Since each word may have an arbitrary number of parents, we must use precision and recall rather than accuracy to measure performance.
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ For our experiments we used the Danish Dependency Treebank v1.0.

McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable.
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent.
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ To circumvent this, we designed an approximate algorithm based on the exact O(n3) second-order projective Eisner algorithm.
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ Restricting scores to a single edge in a dependency tree gives a very impoverished view of dependency parsing.

We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word may depend on multiple heads.
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ In particular, the complete tree metric is improved considerably.
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ We use the non-projective algorithm since the Danish Dependency Treebank contains a small number of non-projective arcs.

Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (HajiËc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al. (1999).
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ We also include conjunctions between these features and the direction and distance from sibling j to sibling k. We determined the usefulness of these features on the development set, which also helped us find out that features such as the POS tags of words between the two siblings would not improve accuracy.
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new models lead to state-of-the-art accuracy for English and the best accuracy we know of for Czech and Danish.
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid.
