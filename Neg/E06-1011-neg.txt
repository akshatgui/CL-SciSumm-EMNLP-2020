To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al., 1994).
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ However, we would like to investigate the benefits for parsing of more principled approaches to approximate learning and inference techniques such as the learning as search optimization framework of (Daum´e and Marcu, 2005).
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents.

In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ For our experiments we used the Danish Dependency Treebank v1.0.
In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. $$$$$ We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.

The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.
The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms.
The 1st- and sibling 2nd-order models are the same as McDonald and Pereira (2006)'s definitions, except the cost factors of the sibling 2nd-order model. $$$$$ Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause.

This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000).
This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ They start with a reasonably good baseline and make small transformations until the score of the structure converges.
This is different from McDonald and Pereira (2006) in that the cost factors for left children are calculated from left to right, while those in McDonald and Pereira (2006)'s definition are calculated from right to left. $$$$$ In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs.

Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ More formally, if the word xi0 has the children shown in this picture, This second-order factorization subsumes the first-order factorization, since the score function could just ignore the middle argument to simulate first-order scoring.
Zhang and Clark (2008) proposed a combination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.

A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ They start with a reasonably good baseline and make small transformations until the score of the structure converges.
A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ We showed that these approximations can be combined with online learning to achieve fast parsing with competitive parsing accuracy.
A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ To quantify this, consider again the example from Figure 1.
A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2nd-order prediction model. $$$$$ We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.

Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm.
Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. $$$$$ The work of Daum´e and Marcu (2005) formalizes this intuition by presenting an online learning framework in which parameter updates are made directly with respect to errors in the inference algorithm.

These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ However, this system relies on a much slower phrase-structure parser as its base model as well as an auxiliary reranking module.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al., 1994).
These include beam search (Lowerre, 1976), cube pruning, which we discuss in ?3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. $$$$$ The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent.

We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ We evaluate dependencies on per word accuracy, which is the percentage of words in the sentence with the correct parent in the tree, and on complete dependency analysis.
We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ For projective MST parsing, the first-order algorithm can be extended to the second-order case, as was noted by Eisner (1996).
We adopt the second-order graph-based dependency parsing model of McDonald and Pereira (2006) as our core parser, which incorporates features from the two kinds of subtrees in Fig. $$$$$ We also ignored features over triples of words since this would explode the size of the feature space.

This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ To create an approximate parsing algorithm for dependency structures with multiple parents, we start with our approximate second-order nonprojective algorithm outlined in Figure 4.
This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause.
This can be done with the Viterbi decoding algorithm described in McDonald and Pereira (2006) in O (n3) parsing time. $$$$$ However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges.

 $$$$$ Examples include verb coordination in which the subject or object is an argument of several verbs, and relative clauses in which words must satisfy dependencies both inside and outside the clause.
 $$$$$ We should note that this is one of many possible approximations we could have made.
 $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
 $$$$$ Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents.

McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ The score of adjacent edges relies on the definition of a feature representation f(i, k, j).
McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.
McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ Using this simple new approximate parsing algorithm, we train a new parser that can produce multiple parents.
McDonald and Pereira (2006) propose a second-order graph-based parser, but use a smaller feature set than our work. $$$$$ In the first-order algorithm, a word will gather its left and right dependents independently by gathering each half of the subtree rooted by its dependent in separate stages.

This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ Note that we only impose margin constraints between the single highest-scoring graph and the correct graph relative to the current weight setting.
This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ The old first-order features are built from the parent and child words, their POS tags, and the POS tags of surrounding words and those of words between the child and the parent, as well as the direction and distance from the parent to the child.
This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ The treebank contains a small number of inter-sentence and cyclic dependencies and we removed all sentences that contained such structures.
This framework is efficient for both projective and non-projective parsing and provides an online learning algorithm which combined with a rich feature set creates state-of-the-art performance across multiple languages (McDonald and Pereira, 2006). $$$$$ We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish.

However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ Unfortunately, the problem of finding the dependency structure with highest score in this setting is intractable (Chickering et al., 1994).
However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ If that is the case, we change the tree permanently and re-enter the loop.
However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ We described approximate dependency parsing algorithms that support higher-order features and multiple parents.
However, McDonald and Pereira (2006) mention the restrictive nature of this parsing algorithm. $$$$$ Kromann (2001) argued for a dependency formalism called Discontinuous Grammar and annotated a large set of Danish sentences using this formalism to create the Danish Dependency Treebank (Kromann, 2003).

For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ Figure 1 displays a dependency representation for the sentence John hit the ball with the bat.
For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. $$$$$ The resulting data set contained 5384 sentences.

In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ Results are shown in Table 1.
In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) to learning with structured outputs, in the present case dependency structures.
In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ We compared three systems, the standard second-order projective and non-projective parsing models, as well as our modified second-order non-projective model that allows for the introduction of multiple parents (Section 3).
In McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. $$$$$ They start with a reasonably good baseline and make small transformations until the score of the structure converges.

McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ The approximation works by first finding the highest scoring projective parse.
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ This sentence is an example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing.
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs.
McDonald and Pereira (2006) define this as a second-order Markov assumption. $$$$$ For the parser that can introduce multiple parents, we see an increase in recall of nearly 3% absolute with a slight drop in precision.

McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004).
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages.
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ Since each word may have an arbitrary number of parents, we must use precision and recall rather than accuracy to measure performance.
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. $$$$$ For the Czech data, we used the predefined training, development and testing split of the Prague Dependency Treebank (Hajiˇc et al., 2001), and the automatically generated POS tags supplied with the data, which we reduce to the POS tag set from Collins et al. (1999).

We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ Furthermore, it was shown that this formulation can lead to state-of-the-art results when combined with discriminative learning algorithms.
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ We described approximate dependency parsing algorithms that support higher-order features and multiple parents.
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ This framework will possibly allow us to include effectively more global features over the dependency structure than those in our current second-order model.
We also investigated an extension, McDonald and Pereira (2006)'s second-order model, where more of the parsing history is taken into account, viz. $$$$$ These results are very promising and further show the robustness of discriminative online learning with approximate parsing algorithms.

Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word.
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ We evaluate dependencies on per word accuracy, which is the percentage of words in the sentence with the correct parent in the tree, and on complete dependency analysis.
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ In the Danish Dependency Treebank, roughly 5% of words have more than one parent, which breaks the single parent (or tree) constraint we have previously required on dependency structures.
Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al, 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. $$$$$ Dependency representations of sentences (Hudson, 1984; Me´lˇcuk, 1988) model head-dependent syntactic relations as edges in a directed graph.
