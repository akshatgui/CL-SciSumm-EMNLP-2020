a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ In this way, popping actions should be made even after a complete parse tree is built, if the stack still contains more than one word.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ In contrast to the models above, it includes both graph-based and transition-based components.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other NLP tasks.

As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ We thank the anonymous reviewers for their detailed comments.
As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ We thank the anonymous reviewers for their detailed comments.
As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ We use a linear model to score each transition action, given a context: N0t, but not STwt or STwN0w), we combine features manually.

We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ Following MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006), we define the graphVariables: agenda – the beam for state items item – partial parse tree output – a set of output items index, prev – word indexes Input: x – POS-tagged input sentence.
We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ Since the beam size affects all three parsers, we study its influence first; here we show the effect on the transition-based parser.
We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ Columns “Word” and “Complete” show the precision of lexical heads and complete matches, respectively.

Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ Initialization: agenda = [“”] put the best items from output to agenda Output: the best item in agenda based parsing problem as finding the highest scoring tree y from all possible outputs given an input x: where GEN(x) denotes the set of possible parses for the input x.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ Averaging parameters is a way to reduce overfitting for perceptron training (Collins, 2002), and is applied to all our experiments.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ When tested using both English and Chinese dependency data, the combined parser was highly competitive compared to the best systems in the literature.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ We study both approaches under the framework of beamsearch.

 $$$$$ The combined parser is tested with various sets of features.
 $$$$$ This is possible because both models are global and linear.
 $$$$$ By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.
 $$$$$ This is possible because both models are global and linear.

We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy.
We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ We developed a graph-based and a transition-based projective dependency parser using beam-search, demonstrating that beam-search is a competitive choice for both parsing approaches.
We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ Our graph-based and transition-based parsers share many similarities.
We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ One drawback of deterministic parsing is error propagation, since once an incorrect action is made, the output parse will be incorrect regardless of the subsequent actions.

Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ Our conclusion is that beamsearch is a competitive choice for graph-based parsing.
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ The MaltParser works deterministically.
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ We thank the anonymous reviewers for their detailed comments.
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ To give more templates, features from templates 1 – 5 are also conjoined with the link direction and distance, while features from template 6 are also conjoined with the direction and distance between the child and its sibling.

CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ We thank the anonymous reviewers for their detailed comments.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ We study both approaches under the framework of beamsearch.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ This can be explained by the difference between the decoders.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ In particular, the transition-based model can be written as: If we take ET0∈act(y) Φ(T0, sT0) as the global feature vector ΦT(y), we have: which has the same form as the graph-based model: ScoreG(y) = ΦG(y) · ~wG We therefore combine the two models to give: Concatenating the feature vectors ΦG(y) and ΦT(y) to give a global feature vector ΦC(y), and the weight vectors ~wG and ~wT to give a weight vector ~wC, the combined model can be written as: which is a linear model with exactly the same form as both sub-models, and can be trained with the perceptron algorithm in Figure 1.

To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ By repeated application of these actions, the parser reads through the input and builds a parse tree.
To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ We develop three parsers.
To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ When tested using both English and Chinese dependency data, the combined parser was highly competitive compared to the best systems in the literature.

 $$$$$ Moreover, a beamsearch decoder does not impose restrictions on the search problem in the way that an exact inference decoder typically does, such as requiring the “optimal subproblem” property for dynamic programming, and therefore enables a comparatively wider range of features for a statistical system.
 $$$$$ We thank the anonymous reviewers for their detailed comments.
 $$$$$ We develop our transition-based parser using the transition model of the MaltParser (Nivre et al., 2006), which is characterized by the use of a stack and four transition actions: Shift, ArcRight, ArcLeft and Reduce.
 $$$$$ By comparing the two, three interesting research questions arise: (1) how to increase the flexibility in defining features for graph-based parsing; (2) how to add search to transition-based parsing; and (3) how to combine the two parsing approaches so that the strengths of each are utilized.

To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ Finally, we show that by using a beam-search decoder, we are able to combine graph-based and transition-based parsing into a single system, with the combined system significantly outperforming each individual system.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST Parser 1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transition based parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011), and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al, 2013). $$$$$ McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors.

The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ Using only graph-based features in Table 1, it gave 88.6% accuracy, which is much lower than 91.2% from the graph-based parser using the same features (“Graph [M]”).
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ Initially empty, the stack is used throughout the parsing process to store unfinished words, which are the words before the current word that may still be linked with the current or a future word.
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem, thus increasing the information available to a single statistical parser, and thereby significantly increasing the accuracy.
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking.

Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ This work is supported by the ORS and Clarendon Fund.
Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ Our observations on parsing Chinese are essentially the same as for English.
Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ We developed a graph-based and a transition-based projective dependency parser using beam-search, demonstrating that beam-search is a competitive choice for both parsing approaches.

For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ In experiments with the English and Chinese Penn Treebank data, the combined parser gave 92.1% and 86.2% accuracy, respectively, which are comparable to the best parsing results for these data sets, while the Chinese accuracy outperforms the previous best reported by 1.8%.
For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ But different from our transition-based parser, which trains all transitions for a parse globally, these models train the probability of each action separately.
For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem, thus increasing the information available to a single statistical parser, and thereby significantly increasing the accuracy.

 $$$$$ We thank the anonymous reviewers for their detailed comments.
 $$$$$ By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.
 $$$$$ Firstly, we combine the graph-based and the transition-based score models simply by summation.
 $$$$$ MaltParser is deterministic, yet its comparatively larger feature range is an advantage.

 $$$$$ Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively.
 $$$$$ The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem, thus increasing the information available to a single statistical parser, and thereby significantly increasing the accuracy.
 $$$$$ The time complexity of this algorithm is O(n2), where n is the length of the input sentence.
 $$$$$ Beam-search is applied by keeping the B best items in the agenda at each processing stage, while partial candidates are compared by scores from the graph-based model, according to partial graph up to the current word.

Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ At each processing stage, one transition action is applied to existing state items as a step to build the final parse.
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ Initialization: agenda = [“”] put the best items from output to agenda Output: the best item in agenda based parsing problem as finding the highest scoring tree y from all possible outputs given an input x: where GEN(x) denotes the set of possible parses for the input x.
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ (2008) reported parent-prediction accuracy of 92.0% using a graph-based parser with a different (larger) set of features (Carreras, 2007).
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ The transition-based algorithm, on the other hand, uses state items which contain partial parse trees, and so provides all the information needed by the graph-based parser (i.e. dependency graphs), and hence the combined system.

 $$$$$ While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding.
 $$$$$ We develop our transition-based parser using the transition model of the MaltParser (Nivre et al., 2006), which is characterized by the use of a stack and four transition actions: Shift, ArcRight, ArcLeft and Reduce.
 $$$$$ The appealing aspect of the combined parser is the incorporation of two largely different views of the parsing problem, thus increasing the information available to a single statistical parser, and thereby significantly increasing the accuracy.

The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors.
The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ In contrast, our parser combines two components in a single model, in which all parameters are trained consistently.
The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ The graph-based and transition-based approaches adopt very different views of dependency parsing.

We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ By repeated application of these actions, the parser reads through the input and builds a parse tree.
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ We thank the anonymous reviewers for their detailed comments.
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing.
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ Note that the number of transition actions needed to build different parse trees can vary.
