a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ Here “distance” refers to the difference between word indexes.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ In this way, some ambiguity is retained for future resolution.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ Nakagawa (2007) and Hall (2007) also showed the effectiveness of global features in improving the accuracy of graph-based parsing, using the approximate Gibbs sampling method and a reranking approach, respectively.
a "configuration" is sometimes called a "state" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different. $$$$$ To make the concepts clear, we classify the two types of parser by the following two criteria: By this classification, beam-search can be applied to both graph-based and transition-based parsers.

As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ Both build a parse tree incrementally, keeping an agenda of comparable state items.
As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.
As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before). $$$$$ Now because each word excluding the root must be pushed to the stack once and popped off once during the parsing process, the number of actions Inputs: training examples (xi, yi) Initialization: set w� = 0 needed to parse a sentence is always 2n − 1, where n is the length of the sentence.

We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved.
We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy.
We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ This observation suggests a combined approach: by using both graph-based information and transition-based information, parsing accuracy can be improved.
We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel. $$$$$ Our combined parser outperforms both the pure graph-based and the pure transition-based parsers.

Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ We then combined the two parsers into a single system, using discriminative perceptron training and beam-search decoding.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ The ArcLeft action adds a dependency link from the current word to the stack top, and pops the stack.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations.
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al, 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state of-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beam search mode (k=16). $$$$$ We then combined the two parsers into a single system, using discriminative perceptron training and beam-search decoding.

 $$$$$ While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding.
 $$$$$ Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi, 2006).
 $$$$$ All three methods estimate the probability of each transition action, and score a state item by the product of the probabilities of all its corresponding actions.
 $$$$$ Using only graph-based features in Table 1, it gave 88.6% accuracy, which is much lower than 91.2% from the graph-based parser using the same features (“Graph [M]”).

We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ Our graph-based and transition-based parsers share many similarities.
We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ To ensure that all final state items are built by the same number of transition actions, we require that the final state transfer the best items from output to agenda Output: the best item in agenda items must 1) have fully-built parse trees; and 2) have only one root word left on the stack.
We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. $$$$$ Initialization: agenda = [“”] put the best items from output to agenda Output: the best item in agenda based parsing problem as finding the highest scoring tree y from all possible outputs given an input x: where GEN(x) denotes the set of possible parses for the input x.

Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ In line with previous work on dependency parsing using the Penn Treebank, we focus on projective dependency parsing.
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other NLP tasks.
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007).
Hybrid systems have improved parsing by integrating outputs obtained from different parsing models (Zhang and Clark, 2008). $$$$$ We thank the anonymous reviewers for their detailed comments.

CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ This work is supported by the ORS and Clarendon Fund.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ We study both approaches under the framework of beamsearch.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ The idea of combining different approaches to the same problem using beam-search and a global model could be applied to other parsing tasks, such as constituent parsing, and possibly other NLP tasks.
CTB5 is converted to dependency structures following the standard practice of dependency parsing (Zhang and Clark, 2008b). $$$$$ In this way, popping actions should be made even after a complete parse tree is built, if the stack still contains more than one word.

To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ We develop three parsers.
To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ Using only graph-based features in Table 1, it gave 88.6% accuracy, which is much lower than 91.2% from the graph-based parser using the same features (“Graph [M]”).
To facilitate comparison with previous results, we follow Zhang and Clark (2008b) for data split and constituency-to-dependency conversion of CTB5. $$$$$ Beam-search has been successful in many NLP tasks (Koehn et al., 2003; Collins and Roark, 2004), and can achieve accuracy that is close to exact inference.

 $$$$$ More importantly, we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding, showing that it outperforms both the pure graph-based and the pure transition-based parsers.
 $$$$$ When B = 1, the transition-based parser becomes a deterministic parser.
 $$$$$ Beam-search has been successful in many NLP tasks (Koehn et al., 2003; Collins and Roark, 2004), and can achieve accuracy that is close to exact inference.
 $$$$$ Like McDonald et al. (2005), we evaluate the parsing accuracy by the precision of lexical heads (the percentage of input words, excluding punctuation, that have been assigned the correct parent) and by the percentage of complete matches, in which all words excluding punctuation have been assigned the correct parent.

To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ In this paper, we study these questions under one framework: beam-search.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ We use a linear model to score each transition action, given a context: N0t, but not STwt or STwN0w), we combine features manually.
To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology $$$$$ We study both approaches under the framework of beamsearch.

The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ The X-axis represents the number of training iterations, and the Y-axis the precision of lexical heads.
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ This work is supported by the ORS and Clarendon Fund.
The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). $$$$$ Before parsing, POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins (2002).

Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other.
Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ Here each state item contains a partial parse tree as well as a stack configuration, and state items are built incrementally by transition actions.
Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ Our graph-based parser is derived from the work of McDonald and Pereira (2006).
Graph features are defined over the factors of a graph-based dependency parser, which was shown to improve the accuracy of a transition-based parser by Zhang and Clark (2008). $$$$$ To give more templates, features from templates 1 – 5 are also conjoined with the link direction and distance, while features from template 6 are also conjoined with the direction and distance between the child and its sibling.

For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ We thank the anonymous reviewers for their detailed comments.
For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ Our transition-based algorithm keeps B different sequences of actions in the agenda, and chooses the one having the overall best score as the final parse.
For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ The ArcLeft action adds a dependency link from the current word to the stack top, and pops the stack.
For Chinese, this is the Penn Chinese Treebank 5.1 (CTB5), converted with the head-finding rules and conversion tools of Zhang and Clark (2008), and with the same split as in Zhang and Clark (2008) and Li et al (2011). $$$$$ We thank the anonymous reviewers for their detailed comments.

 $$$$$ This model is the most similar to our transition-based model, while the differences include the choice of learning and decoding algorithms, the definition of feature templates and our application of the “early update” strategy.
 $$$$$ Testing on the English and Chinese Penn Treebank data, the combined system gave state-of-the-art accuraof respectively.
 $$$$$ In particular, the transition-based model can be written as: If we take ET0∈act(y) Φ(T0, sT0) as the global feature vector ΦT(y), we have: which has the same form as the graph-based model: ScoreG(y) = ΦG(y) · ~wG We therefore combine the two models to give: Concatenating the feature vectors ΦG(y) and ΦT(y) to give a global feature vector ΦC(y), and the weight vectors ~wG and ~wT to give a weight vector ~wC, the combined model can be written as: which is a linear model with exactly the same form as both sub-models, and can be trained with the perceptron algorithm in Figure 1.

 $$$$$ Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi, 2006).
 $$$$$ Like Duan et al. (2007), we use gold-standard POS-tags for the input.

Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ Graph-based and transition-based approaches to dependency parsing adopt very different views of the problem, each view having its own strengths and limitations.
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ Our observations on parsing Chinese are essentially the same as for English.
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ Beam-search has been successful in many NLP tasks (Koehn et al., 2003; Collins and Roark, 2004), and can achieve accuracy that is close to exact inference.
Zhang and Clark (2008) was the first to combine beam search with a globally normalized discriminative model, using structured perceptron learning and the early update strategy of Collins and Roark (2004), and also explored the addition of graph based features to a transition-based parser. $$$$$ The top B newly generated candidates are then put to the agenda.

 $$$$$ Firstly, we combine the graph-based and the transition-based score models simply by summation.
 $$$$$ Second, the transition-based decoder can be used for the combined system.
 $$$$$ We developed a graph-based and a transition-based projective dependency parser using beam-search, demonstrating that beam-search is a competitive choice for both parsing approaches.
 $$$$$ MSTParser has the strength of exact inference, but its choice of features is constrained by the requirement of efficient dynamic programming.

The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ By developing a graph-based and a transition-based dependency parser, we show that a beam-search decoder is a competitive choice for both methods.
The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ The intuition is to improve learning by avoiding irrelevant information: when all the items in the current agenda are incorrect, further parsing steps will be irrelevant because the correct partial output no longer exists in the candidate ranking.
The number of modifiers to a given head is used by the graph-based submodel of Zhang and Clark (2008) and the models of Martins et al (2009) and Sagae and Tsujii (2007). $$$$$ By repeated application of these actions, the parser reads through the input and builds a parse tree.

We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ In summary, we build the combined parser by using a global linear model, the union of feature templates and the decoder from the transition-based parser.
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ Our transition-based parser is derived from the deterministic parser of Nivre et al. (2006).
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ As with the graph-based parser, we use the discriminative perceptron (Collins, 2002) to train the transition-based model (see Figure 5).
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row 'Z&C08 transition'), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row 'H&S10'), and graph based models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. (2008) (row 'K08 baeline'), and the two models of Koo and Collins (2010). $$$$$ This work is supported by the ORS and Clarendon Fund.
